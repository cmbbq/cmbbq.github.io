<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Quantization and Pruning :: Cmbbq&#39;s Encyclopedia</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="总结推理优化问题中相当重要的模型压缩技术——量化和剪枝。" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://cmbbq.github.io/posts/quantization-and-pruning/" />





<link rel="stylesheet" href="https://cmbbq.github.io/assets/style.css">

  <link rel="stylesheet" href="https://cmbbq.github.io/assets/green.css">






<link rel="apple-touch-icon" href="https://cmbbq.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://cmbbq.github.io/img/favicon/green.png">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Quantization and Pruning">
<meta property="og:description" content="总结推理优化问题中相当重要的模型压缩技术——量化和剪枝。" />
<meta property="og:url" content="https://cmbbq.github.io/posts/quantization-and-pruning/" />
<meta property="og:site_name" content="Cmbbq&#39;s Encyclopedia" />

  
    <meta property="og:image" content="https://cmbbq.github.io/img/favicon/green.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2024-09-02 00:00:00 &#43;0000 UTC" />












</head>
<body class="green">


<div class="container center headings--one-size">

  <header class="header">
  <script>
    MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: false,
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>
<script
    type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js">
</script>
    
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Cmbbq&#39;s Encyclopedia
  </div>
</a>

    </div>
    
  </div>
  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://cmbbq.github.io/posts/quantization-and-pruning/">Quantization and Pruning</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2024-09-02
        
      </span>
    
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://cmbbq.github.io/tags/sys/">sys</a>&nbsp;
    
    #<a href="https://cmbbq.github.io/tags/ai/">ai</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <p>模型压缩技术中，最实用的是量化，其次还可以尝试剪枝。量化降低精度，剪枝裁剪参数。</p>
<h1 id="quantization">Quantization<a href="#quantization" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<p>可以将k-bit量化问题视作将取值范围$(x_{min},x_{max})$的float数值$x$经过量化函数$g(x)$映射到取值范围$(0,2^{k-1})$的int型数值$Q$，并尽可能减少整体模型精度损失（well，如果没有端到端的模型accuracy/perplexity指标，也可以把目标设置为最小化output MSE/RMSE）的问题。$Q=round(g(x))$，</p>
<h2 id="uniform-vs-non-uniform">Uniform vs Non-uniform<a href="#uniform-vs-non-uniform" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>根据Q的分布是否为均匀分布，可讲量化器分为uniform quantizers和non-uniform quantizers<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</p>
<p>Non-uniform quantization往往用几个离散的等级模拟其他分布（$x$真实的分布可能是lognorm分布、norm分布），以期在更稠密的取值范围内提升精度，缺点是计算量更大一些。</p>
<h2 id="affine-vs-scale">Affine vs Scale<a href="#affine-vs-scale" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>在uniform quantization中，变换函数又有两个选择：affine和scale。前者用一个仿射函数（$g(x)=kx+b$），后者只用（$g(x)=kx$），0在映射后仍为0，$Q$和$x$在0两侧对称，因此又叫对称量化，实际上是仿射量化的一个特例，由于去掉了offset，计算更简化了，也容易向量化。</p>
<h2 id="ptq-vs-qat">PTQ vs QAT<a href="#ptq-vs-qat" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>根据是否涉及backprop，量化可以分为PTQ(post-training Quantization)和QAT(Quantization Aware Training)这两类。QAT因训练成本高昂，难以扩展到大模型。因此大模型量化更多地使用PTQ。</p>
<h2 id="dynamic-vs-static">Dynamic vs Static<a href="#dynamic-vs-static" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>静态精度量化把权重、激活函数、梯度统一转成低精度表示，比如W8A8量化。推理阶段W8A8完全是int8算术计算，不需要执行任何量化、反量化函数。</p>
<p>静态量化的参数（scale factor $k$、zero point $b$）是固定的。那么如何决定合适的$k$或$b$呢？静态量化往往需要通过在校准集上收集激活分布，寻找最小化MSE的最优解。但这种校准过程也会引入过拟合校准集的问题。在输入数据的分布非常明确，可以被校准集正确刻画的场景下，可以使用静态量化。</p>
<p>动态精度量化，又称weight-only量化，只把权重量化到低精度，激活仍保留高精度（因此模型变成了混合精度模型）。动态量化中，量化参数是推理时即时演算的，因此无需专门的校准阶段。推理时激活精度会动态调整精度（上限是模型中存储的激活精度，下限是权重精度，需施加量化函数到激活，或反量化函数到权重），因此保留了部分浮点数计算。</p>
<p>通常静态量化适合CNN，动态量化适合RNN、transformers。</p>
<p>量化器的计算量在混合精度模型（比如int4权重量化+fp16激活函数量化的gptq，或torchao QAT的8da4w，或llama.cpp的q4_k）中会影响推理性能。以llama.cpp的q4_k为例，中间张量（比如两个4-bits的sum，再用4-bits有几率产生overflow）有些需要用8-bits而非4-bits量化，可以把模型张量用计算量更高误差更少的量化器（比如non-uniform量化器，计算量虽大但不影响运行时开销），中间张量用计算量小的uniform量化器<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。</p>
<h2 id="llm-quantization">LLM Quantization<a href="#llm-quantization" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ul>
<li>
<p>GPTQ<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>：一种适用于大模型的oneshot量化方法，把所有权重都批量放入矩阵中，逐层量化，每次都最小化输出MSE。GPTQ采用int4/fp16混合精度量化，4-bit用于权重量化，激活函数仍用fp16。GPTQ利用二阶信息做误差补偿，但可能在重建过程中过拟合校准集，导致模型损失泛用性。</p>
</li>
<li>
<p>AWQ<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>：一种低比特weight-only的量化方法，只对权重进行量化，将激活函数和梯度保留为全精度。AWQ认为只有0.1~1%的权重是salient的，应跳过这些salient权重。激活后的分布比权重本身更加salient，因此AWQ根据激活分布寻找需要被跳过的权重。</p>
</li>
<li>
<p>GGUF：在llama.cpp的k-quant体系中，qN_0表示N-bits scale量化，qN_1表示N-bits affine量化，qN_k代表特殊的block-wise量化，把原模型权重分块，每个块有自己的根据最大值简单算出的scaling factor（这样显然并不最优，后来又做了改进版本，见<sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>），salient权重被量化到高精度，其他的则量化到低精度，是混合精度的。以q2_k quant为例，salient权重被量化到4-bit，而其他权重则是2bit。q4_0则分别把所有权重都量化到4-bit。</p>
</li>
<li>
<p>SmoothQuant<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>：与per-channel激活量化不同，SmoothQuant把magnitudes做了一个smooth操作，避免过于剧烈的inter-channel variation。SmoothQuant把原本非常uniform的权重分别变得稍有起伏，但实际上仍然易于计算。</p>
</li>
</ul>
<p><img src="https://cmbbq.github.io/img/smooth_quant.png" alt="smooth"></p>
<h2 id="k-bit-inference-scaling-laws">k-bit Inference Scaling Laws<a href="#k-bit-inference-scaling-laws" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>根据<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>中的三万五千次k-bit推理实验，模型总体积不变的情况下，4-bit精度几乎永远是最优解。</p>
<h1 id="pruning">Pruning<a href="#pruning" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<p>对应PTQ，也存在PTP(Post-Training Pruning)，本文主要讨论PTP，即无需高昂重训练成本（可以通过LoRA恢复）的剪枝。</p>
<h2 id="结构化稀疏">结构化稀疏<a href="#结构化稀疏" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>结构化稀疏在特定维度（chanel、conv kernel）上对卷积、矩阵乘做剪枝操作，改变其shape，生成更小的模型。</p>
<p>LLM-Pruner<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>、Torch-Pruning<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>是针对LLM的结构化稀疏方法。Isomorphic Pruning<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>则是近期针对ViT和现代CNN的SOTA方法。</p>
<h2 id="非结构化稀疏">非结构化稀疏<a href="#非结构化稀疏" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>非结构化稀疏以每一个参数为单元进行稀疏，不改变参数矩阵shape，只是令其中部分值为零。要求底层推理实现能有效地利用矩阵稀疏性进行加速。</p>
<p>SparseGPT<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>在不显著牺牲perplexity的前提下，对175B级别的模型使用（one-shot，无需retraining），达到60%非结构化稀疏。</p>
<p>SparseGPT将剪枝问题规约到超大规模稀疏回归实例上，用一种新的近似稀疏回归求解器高效求解，能在单GPU上几个小时内跑完100B级模型的稀疏。</p>
<h2 id="半结构化稀疏">半结构化稀疏<a href="#半结构化稀疏" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>N:M Pruning<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>是一种半结构化稀疏方法。SparseGPT这样的非结构化稀疏技术可以通过修改、适配成2:4 sparsity在A100上得到加速。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Raghuraman Krishnamoorthi. Quantizing Deep Convolutional Networks for Efficient Inference: A whitepaper. <a href="https://arxiv.org/pdf/1806.08342">[pdf]</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p><a href="https://github.com/ggerganov/llama.cpp/issues/397">llama.cpp issue: Investigate alternative approach for Q4 quantization</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>E. Frantar, et al. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. <a href="https://arxiv.org/pdf/2210.17323">[pdf]</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Ji Lin, et al. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. <a href="https://arxiv.org/pdf/2306.00978">[pdf]</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>G. Xiao, et al. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. <a href="https://arxiv.org/pdf/2211.10438">[pdf]</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>T. Dettmers, L. Zettlemoyer. The case for 4-bit precision: k-bit Inference Scaling Laws. <a href="https://arxiv.org/pdf/2212.09720">[pdf]</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p><a href="https://arxiv.org/pdf/2305.11627">LLM-Pruner: On the Structural Pruning of Large Language Models</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>G. Fang, et al. DepGraph: Towards Any Structural Pruning. <a href="https://arxiv.org/pdf/2301.12900">[pdf]</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>G. Fang, et al. Isomorphic Pruning for Vision Models. <a href="https://arxiv.org/pdf/2407.04616">[pdf]</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>E. Frantar, D. Alistarh. SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot. <a href="https://arxiv.org/pdf/2301.00774">[pdf]</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>A. Zhou, et al. Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch. <a href="https://arxiv.org/pdf/2102.04010">[pdf]</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </div></div>

  
  
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">阅读其他博文</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="https://cmbbq.github.io/posts/break-the-cycle/">
                <span class="button__icon">←</span>
                <span class="button__text">Break the Cycle</span>
            </a>
        </span>
        
        
        <span class="button next">
            <a href="https://cmbbq.github.io/posts/optimizing-ai-inference/">
                <span class="button__text">Optimizing AI Inference</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
  

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        
    

        <span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-envelope" viewBox="0 0 16 16">
          <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"/>
        </svg> rpb.cmbbq@gmail.com </span>
      </div>
  </div>
</footer>

<script src="https://cmbbq.github.io/assets/main.js"></script>
<script src="https://cmbbq.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
