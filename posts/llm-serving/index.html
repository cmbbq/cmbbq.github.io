<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>LLM Serving :: Cmbbq&#39;s Encyclopedia</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="本文总结LLM serving的计算形态和优化机会。" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://cmbbq.github.io/posts/llm-serving/" />





<link rel="stylesheet" href="https://cmbbq.github.io/assets/style.css">

  <link rel="stylesheet" href="https://cmbbq.github.io/assets/green.css">






<link rel="apple-touch-icon" href="https://cmbbq.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://cmbbq.github.io/img/favicon/green.png">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="LLM Serving">
<meta property="og:description" content="本文总结LLM serving的计算形态和优化机会。" />
<meta property="og:url" content="https://cmbbq.github.io/posts/llm-serving/" />
<meta property="og:site_name" content="Cmbbq&#39;s Encyclopedia" />

  
    <meta property="og:image" content="https://cmbbq.github.io/img/favicon/green.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2024-12-09 00:00:00 &#43;0000 UTC" />












</head>
<body class="green">


<div class="container center headings--one-size">

  <header class="header">
  <script>
    MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: false,
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>
<script
    type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js">
</script>
    
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Cmbbq&#39;s Encyclopedia
  </div>
</a>

    </div>
    
  </div>
  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://cmbbq.github.io/posts/llm-serving/">LLM Serving</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2024-12-09
        
      </span>
    
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://cmbbq.github.io/tags/sys/">sys</a>&nbsp;
    
    #<a href="https://cmbbq.github.io/tags/ai/">ai</a>&nbsp;
    
    #<a href="https://cmbbq.github.io/tags/llm/">llm</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <p>LLM推理有别于小模型推理，更加memory-bound，输入输出也有可以利用的独特模式，自然涌现出系统层面可优化的点。我将这些系统层面的优化机会粗略分为batching优化、sampling优化、模型压缩3类。</p>
<p>其中batching优化负责输入端将GPU等加速器的计算单元喂的更饱和、更好地利用SRAM locality<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，sampling优化<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>负责输出端更快速拿到LLM的结果，模型压缩负责让大模型不要那么大。</p>
<h2 id="llm-serving的计算形态">LLM Serving的计算形态<a href="#llm-serving的计算形态" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>LLM serving相比此前主流的深度模型serving，有一些独有的特性和约束：</p>
<ul>
<li>一次LLM调用中很多时间耗费在加载模型参数上（HBM-&gt;SRAM）。
<ul>
<li>batching可以让一次参数加载负责多个seq的推理，显著减少了总体的模型参数加载开销。</li>
</ul>
</li>
<li>Seq2Seq：变长输入、变长输出。
<ul>
<li>因此batching机制必须适应batch size，seq len皆为变量。</li>
</ul>
</li>
<li>自回归（通过若干次迭代才能处理一个请求），且为每个对话维护一个跨迭代的KV。
<ul>
<li>纯粹无状态的方法需每次重新计算所有KV，开销会大得无法接受，因此必须基于KV caching做incremental decoding。</li>
<li>RNN也是自回归，transformer相比RNN，还有一个不同之处是每次迭代KV cache都会变大。</li>
<li>非自回归模型往往以request为粒度做batching，对自回归场景并不适用，后者明显更适合以iteration为粒度。</li>
</ul>
</li>
<li>GPT不同于原版transformer，是decoder-only的。
<ul>
<li>此前encoder-only和encoder-decoder架构的变长batching padding策略不适用于LLM。</li>
</ul>
</li>
<li>GPT多了一个前置的计算密集的prefill阶段（也叫infill），用于处理prompt。
<ul>
<li>prefill阶段和增量迭代阶段的序列不好batching（计算要完全一样才好batching）。</li>
<li>这个阶段消化prompt，考虑到很多LLM应用都有非常巨大、内容也差不多的prompt，prefill阶段很多计算都是重复的。</li>
</ul>
</li>
<li>GPT在生成token概率分布后，还有一个后置的sampling阶段：基于概率密度从vocab中选择token。
<ul>
<li>token选择完成后，当前迭代选中的token还需要再作为下次迭代的forward pass输入参数。</li>
<li>若seq[A~A+5]在此前的迭代中已经进行了采样，在新一轮迭代中除了需要对seq[A+6]采样之外，仍然要对seq[A~A+5]再算一遍。</li>
<li>已处理tokens的decoding结果可以cache下来。</li>
<li>很多LLM应用会要求结构化输出，输出的格式比较固定。</li>
</ul>
</li>
<li>Transformer的attention算子的input张量形状和已处理tokens长度有关。
<ul>
<li>不同序列，序列长度不同，attn计算的输入形状不统一，就不好做batching。</li>
<li>好在attn计算做不做batching影响不大，因为attn计算并不涉及任何模型权重，也就不存在一次参数加载重用于多序列推理的加速效应。</li>
</ul>
</li>
<li>LLM在GPU上跑的一个关键瓶颈是将数据（请求+参数）加载进HBM的开销。LLM serving吞吐明显会受限于batch size——即一次性能放入多少数据而不至于爆显存。
<ul>
<li>在简单的静态batching实现中，seq_len * nr_seqs + 模型大小决定了显存占用。seq_len假设定的比较高，又用不完，那也会让nr_seqs缩小很多。</li>
<li>显存如此紧张，自然使量化、剪枝等模型压缩技术在LLM serving中变得尤为重要，比如awq, gptq, gguf, smooth quant。</li>
</ul>
</li>
</ul>
<h2 id="continuous-batching">Continuous Batching<a href="#continuous-batching" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>上文提及LLM做batching好处虽多，困难更多，并不存在特别简单、显而易见的GPT特供batching机制。</p>
<p>Orca<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>首先为GPT模型的batching提出了一个完整的解决方案，在迭代层面进行调度，并通过一个必要的selective batching机制剔除不能做batching的操作（若不做剔除，两个请求整体能做batching的几率可以忽略不计），这些操作虽然不能做batching，但对整体性能提升的影响很小。</p>
<p><img src="https://cmbbq.github.io/img/orca.png" alt="orca"></p>
<h2 id="paged-attention">Paged Attention<a href="#paged-attention" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Orca方案并未考虑KV cache的HBM占用，默认预分配max_seq_len。</p>
<p>但monolithic KV cache导致HBM碎片化，为每个seq预分配巨大内存进而导致并发不足也是真实瓶颈所在，vLLM中就对此进行了优化，提出了Paged Attention<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>，一种近似页表的分块kv cache技术：</p>
<ul>
<li>在prefill阶段允许kv cache在非连续内存中以页的方式组织起来，因此不必为max_seq_len提前分配内存，运行时再分配就好。</li>
<li>大多数seq显然不会触及max_seq_len，paged attention因此节省了大量内存，也就允许batch size大大提高。</li>
<li>vLLM的实现中并未采纳Orca的selective batching，主要是因为它的paged attention算子是自己写的cuda，可以与非attn算子一起兼容batching。vLLM将prefill和decoding分开做batching，整体上就不需要实现selective batching这么麻烦的机制了。
<ul>
<li>但这种做法也阻止了prefill和decoding step的融合。如果某个prompt过长，prefill开销太大，确实会出现block后续所有decoding batch的情形。</li>
</ul>
</li>
</ul>
<p><img src="https://cmbbq.github.io/img/block_table.png" alt="block_table"></p>
<p>详见<a href="https://cmbbq.github.io/posts/paged-attention">Paged Attention</a>。</p>
<h2 id="dynamic-splitfuse">Dynamic SplitFuse<a href="#dynamic-splitfuse" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>DeepSpeed-FastGen<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>中提出了SplitFuse，也是Continuous Batching的一个演化版本。思路是切分长prompt请求成若干个小的step，这些小的step开销较低，可填充调度缝隙，同时还保证prefill(prompt generation)和decode(token generation)的steps开销一致，就可以确保不存在大小不同的workload，取得一定的吞吐提升，但最主要的优势是能稳住tail-latency，在线服务场景下下限更高。</p>
<p><img src="https://cmbbq.github.io/img/split_fuse.png" alt="split_fuse"></p>
<h2 id="quantization-and-pruning">Quantization and Pruning<a href="#quantization-and-pruning" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>由于Nvidia架构上天然偏向图形负载，显存天然就给的不足，各种量化、剪枝技术除了降低计算量外，在LLM serving方向上还起到了关键性的降低显存占用的效果。</p>
<p>详见<a href="https://cmbbq.github.io/posts/quantization-and-pruning">Quantization and Pruning</a></p>
<h2 id="radix-attention">Radix Attention<a href="#radix-attention" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>SGLang采用了Radix attention<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>技术，将common prefix的KV以radix tree的形式保留下来，使kv cache的生命周期不局限于一次请求，而是真正构成跨多次请求的LRU cache，适应prompt巨大且大多有相同前缀的实际应用场景。</p>
<h2 id="flash-attention">Flash Attention<a href="#flash-attention" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Continuous batching提升了非attn操作的SRAM locality，针对kv计算，Flash Attention<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>则令attn计算内层循环fit in SRAM。</p>
<p><img src="https://cmbbq.github.io/img/fast_attention.png" alt="fast_att"></p>
<p>详见<a href="https://cmbbq.github.io/posts/flash-attention">Flash Attention</a>。</p>
<h2 id="speculative-decoding">Speculative Decoding<a href="#speculative-decoding" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Speculative decoding<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>的思路是选用tokenizer相同，大小不同的两个模型。假设大模型的latency大体上是小模型的N倍。小模型输出N个token的时间内，大模型把这N个token拿过来append到seq上形成input，再输出1个token，总计生成N+1个token。基于greedy decoding对这N+1个token进行采样，采样结果如果和小模型的结果match，就直接用了，不match就停下，在停下的地方把原本的小模型token改成大模型采样结果对应的token。整个过程中，大模型实际上只需要一个forward pass，运气好就能一下子输出N+1个token，运气差就输出1个token。</p>
<p><img src="https://cmbbq.github.io/img/speculative_decoding.png" alt="speculative_decoding"></p>
<h2 id="structured-decoding">Structured Decoding<a href="#structured-decoding" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>SGLang基于一个压缩有限状态机实现了structured decoding<sup id="fnref1:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>，用于对特定结构化输出（比如支持regex的JSON模板）进行加速，一次性decode多个token。假设这个结构化输出的JSON模板中总是有一个key是&quot;top5 candidate&quot;，那就可以把&quot;top5 candidate&quot;这个multi-token词组当成一个token一轮迭代处理掉。</p>
<p><img src="https://cmbbq.github.io/img/structured_decoding.png" alt="structured_decoding"></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>GPU/NPU/TPU等加速器需将模型参数从off-chip memory加载到on-chip SRAM才能进行底层硬件算子的计算，对较大的模型，这种加载开销往往才是瓶颈所在。因此batching不仅仅能提升加速器计算单元的利用率，还能通过一份模型参数在多个请求中重用，更好地利用SRAM locality。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>sampling指的基于density做token-selection的过程，decoding指的是整个decoder-only transformer的inference过程。&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Gyeong-In Yu and Joo Seong Jeong. Orca: A Distributed Serving System for Transformer-Based Generative Models. OSDI 22. <a href="https://www.usenix.org/system/files/osdi22-yu.pdf">[pdf]</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>W. Kwon, et al. Efficient Memory Management for Large Language Model Serving with PagedAttention. <a href="https://arxiv.org/pdf/2309.06180">[pdf]</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>C. Holmes, et al. DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference. <a href="https://arxiv.org/pdf/2401.08671">[pdf]</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>L. Zheng, et al. SGLang: Efficient Execution of Structured Language Model Programs. <a href="https://arxiv.org/pdf/2312.07104">[pdf]</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>T. Dao, et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. <a href="https://arxiv.org/pdf/2205.14135">[pdf]</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Y. Leviathan, et al. Fast Inference from Transformers via Speculative Decoding. <a href="https://arxiv.org/pdf/2211.17192">[pdf]</a>&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </div></div>

  
  
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">阅读其他博文</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        
        <span class="button next">
            <a href="https://cmbbq.github.io/posts/engineering-practices/">
                <span class="button__text">Idiomatic Practices in C&#43;&#43; Systems Engineering</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
  

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        
    

        <span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-envelope" viewBox="0 0 16 16">
          <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"/>
        </svg> rpb.cmbbq@gmail.com </span>
      </div>
  </div>
</footer>

<script src="https://cmbbq.github.io/assets/main.js"></script>
<script src="https://cmbbq.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
