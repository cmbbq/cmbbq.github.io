<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Artificial Intuition: Reasoning Abilities of LLMs :: Cmbbq&#39;s Encyclopedia</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="对近期对LLM的调研文献进行梳理总结，讨论目前大语言模型架构的推理能力边界。" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://cmbbq.github.io/posts/on-reasoning-abilities-of-llms/" />




<link rel="stylesheet" href="https://cmbbq.github.io/assets/style.css">

  <link rel="stylesheet" href="https://cmbbq.github.io/assets/green.css">






<link rel="apple-touch-icon" href="https://cmbbq.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://cmbbq.github.io/img/favicon/green.png">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Artificial Intuition: Reasoning Abilities of LLMs">
<meta property="og:description" content="对近期对LLM的调研文献进行梳理总结，讨论目前大语言模型架构的推理能力边界。" />
<meta property="og:url" content="https://cmbbq.github.io/posts/on-reasoning-abilities-of-llms/" />
<meta property="og:site_name" content="Cmbbq&#39;s Encyclopedia" />

  
    <meta property="og:image" content="https://cmbbq.github.io/img/favicon/green.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2023-11-29 00:00:00 &#43;0000 UTC" />












</head>
<body class="green">


<div class="container center headings--one-size">

  <header class="header">
  <script>
    MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: false,
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>
<script
    type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js">
</script>
    
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Cmbbq&#39;s Encyclopedia
  </div>
</a>

    </div>
    
  </div>
  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://cmbbq.github.io/posts/on-reasoning-abilities-of-llms/">Artificial Intuition: Reasoning Abilities of LLMs</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2023-11-29
        
      </span>
    
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://cmbbq.github.io/tags/ai/">ai</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <h2 id="llm具有system2吗">LLM具有System2吗？<a href="#llm具有system2吗" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>System1和System2的划分来源于心理学家Daniel Kahneman的理论<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>，描述了思考的两种模式，System1指的是快速、自动、直觉且不费力的模式，System2则是慢速、刻意、分析性且有意识的模式。</p>
<p>现有的LLM除了已被证明的“将数据分布映射到不相干低维子空间<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>的反射式推理能力”(System1)，是否具有一定的“慢速思考”、多步推理和规划能力(System2)?</p>
<p>LLM横空出世之初，有很多狂野声明，比如&quot;LLM as zero-shot planner&quot;，&ldquo;LLMs are zero-shot reasoner&rdquo;，但这些声音在现在看来更像是一时跟风和hype。</p>
<p>现在，无论是普通从业者，还是知名学者，如Yann Lecun，Yoshua Bengio，马毅，Subbarao Kambhampati逐渐形成共识，认为LLM不具备System2，很多研究也通过一些reasoning benchmark对这一观点进行佐证，比如Huang et al., 2023<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>，Jin et al., 2023<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>，Valmeekam, Marquez, 2023<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>，Stechly et al., 2023<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>，Dziri et al., 2023<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>。</p>
<p>也有少数学者，比如Ilya Sutskever在访谈中说scaling足以产生System2能力，无需架构创新，但Ilya似乎并不坦诚，动机不明<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>。有研究者认为LLM加上Chain-of-Thought prompting是具有推理能力，如Saparov &amp; He, 2023<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>，Feng et al., 2023<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>，但终究依赖了外界的prompting，且实验方法上并不能排除近似信息提取模拟了逻辑推理的可能。</p>
<p>LLM本身，考虑到transformer每次生成回答计算量是有上确界的，注定不可能为某个问题倾注不成比例的计算量，仅此一点，就可以从理性层面否定LLM具备Human-like System2。毕竟System2从定义上，就是一个长期保持专注的慢思考模式，理应具有潜在无限时间的思考能力。</p>
<h2 id="假设system1足够强能替代system2吗">假设System1足够强，能替代System2吗？<a href="#假设system1足够强能替代system2吗" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>理论上来说，无限的精度的transformer都不需要无限权重，已被证明是图灵完备的<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>，也就是说只要以恰当的方式学习，就足以学习到任何算法。</p>
<p>假设未来的LLM能具有近乎无限的精度、权重，并用超强算力做训推，则确实可能以System1模拟出System2的推理能力，用简单的直觉映射模拟演绎推理。但考虑到我们现在用作推理的LLM精度已经低到8bit，甚至4bit，即使如此成本都已经难以维持，有理由认为这种思路是不切实际的，这种架构所需的能耗和物质基础，轻而易举就能超出人类物质文明极限好几个数量级。</p>
<p>强无敌的System1能在刹那间把“意识”制造出来吗？在穿过多层transformer时意识萌发，再让意识消亡于logits的softmax。无限精度假设下似乎也不是不可能，毕竟能模拟一切算法，模拟出一种System2也不足为奇，那就相当于在一次执行过程中，制造了意识/生命的虚拟机，哪怕真正执行它的物理机只是朴素的transformer——一种纯粹以提升预测下个token似然为目标的自回归模型。这也是Ilya认为scaling足以产生AGI的理论依据。</p>
<h2 id="artificial-intelligence更像是artificial-intuition">Artificial Intelligence？更像是Artificial Intuition<a href="#artificial-intelligence更像是artificial-intuition" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>总而言之，将LLM称为人工智能仍然是夸大其词的，它从原理上讲就是一种人造直觉。这个直觉或许可以在无限算力无限精度无限权重假设下模拟出近人智能，但诚实的Datacenter AI从业者会承认—现有大模型在尺度上已接近了先进计算和先进互连的硬件极限，而相比算法突破，硬件的发展曲线是平缓且存在上确界的。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Thinking, Fast and Slow&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>White-Box Transformers via Sparse Rate Reduction <a href="https://arxiv.org/pdf/2306.01129.pdf">[pdf]</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Large Language Models Cannot Self-Correct Reasoning Yet <a href="https://arxiv.org/pdf/2310.01798.pdf">[pdf]</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Can Large Language Models Infer Causation from Correlation?<a href="https://arxiv.org/pdf/2306.05836.pdf">[pdf]</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Can Large Language Models Really Improve by Self-critiquing Their Own Plans? <a href="https://arxiv.org/pdf/2310.08118.pdf">[pdf]</a>&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>GPT-4 Doesn&rsquo;t Know It&rsquo;s Wrong: An Analysis of Iterative Prompting for Reasoning Problems <a href="https://arxiv.org/pdf/2310.12397.pdf">[pdf]</a>&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Faith and Fate: Limits of Transformers on Compositionality <a href="https://arxiv.org/abs/2305.18654">[pdf]</a>&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>不负责任的猜想：考虑到OpenAI在尝试Q*这样的架构突破，Ilya在访谈时很可能存在故意误导的动机，不愿透露OpenAI的研究思路。此外强调scaling有利于凸现其个人的历史贡献，夸大AGI危机也有利于其负责的super alignment项目。&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought <a href="https://openreview.net/pdf?id=qFVVBzXxR2V">[pdf]</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Language Models can be Logical Solvers <a href="https://arxiv.org/pdf/2311.06158.pdf">[pdf]</a>&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>On the Turing Completeness of Modern Neural Network Architectures <a href="https://arxiv.org/abs/1901.03429">[pdf]</a>&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </div></div>

  
  
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">阅读其他博文</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="https://cmbbq.github.io/posts/ebpf/">
                <span class="button__icon">←</span>
                <span class="button__text">eBPF Tracing for Memory-Stalled Applications</span>
            </a>
        </span>
        
        
        <span class="button next">
            <a href="https://cmbbq.github.io/posts/on-order/">
                <span class="button__text">Order Emerges from Self-Assembly of Dissipative Structures</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
  

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        
    

        <span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-envelope" viewBox="0 0 16 16">
          <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"/>
        </svg> rpb.cmbbq@gmail.com </span>
      </div>
  </div>
</footer>

<script src="https://d3js.org/d3.v7.min.js"></script>
<script src="https://cmbbq.github.io/assets/main.js"></script>
<script src="https://cmbbq.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
