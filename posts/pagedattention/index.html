<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Paged Attention :: Cmbbq&#39;s Encyclopedia</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Paged Attention：提升同时处理多请求的显存利用率和吞吐。" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://cmbbq.github.io/posts/pagedattention/" />





<link rel="stylesheet" href="https://cmbbq.github.io/assets/style.css">

  <link rel="stylesheet" href="https://cmbbq.github.io/assets/green.css">






<link rel="apple-touch-icon" href="https://cmbbq.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://cmbbq.github.io/img/favicon/green.png">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Paged Attention">
<meta property="og:description" content="Paged Attention：提升同时处理多请求的显存利用率和吞吐。" />
<meta property="og:url" content="https://cmbbq.github.io/posts/pagedattention/" />
<meta property="og:site_name" content="Cmbbq&#39;s Encyclopedia" />

  
    <meta property="og:image" content="https://cmbbq.github.io/img/favicon/green.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2024-07-30 00:00:00 &#43;0000 UTC" />












</head>
<body class="green">


<div class="container center headings--one-size">

  <header class="header">
  <script>
    MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: false,
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>
<script
    type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js">
</script>
    
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Cmbbq&#39;s Encyclopedia
  </div>
</a>

    </div>
    
  </div>
  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://cmbbq.github.io/posts/pagedattention/">Paged Attention</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2024-07-30
        
      </span>
    
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://cmbbq.github.io/tags/sys/">sys</a>&nbsp;
    
    #<a href="https://cmbbq.github.io/tags/ai/">ai</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <p>在LLM serving场景，通过恰当的batching积攒足够多的请求，可提升LLM吞吐。但每个请求对应的KV Cache非常巨大——在原始实现中，KV Cache需要为<code>max_tokens</code>预留内存，但每个请求实际上携带的tokens数目普遍远小于<code>max_tokens</code>，造成大量内存浪费和反复的动态内存分配。</p>
<p><code>PagedAttention</code><sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>的目标就是消除这种内存浪费、在请求内部和请求之间灵活共享一些KV cache。vLLM通过<code>PagedAttention</code>达到此前sota的<code>FasterTransformer</code>和<code>Orca</code>的2~4倍吞吐。</p>
<p>此前，朴素的KV Cache实现如下图所示，简短的prompt和并不长的当前iteration只占据了7+4个slots，剩余2038个slots不得不预留在内存里以支撑最大序列长达2048的承诺（采样结束才能知道实际tokens数目），在这之后才能是下一个请求的slots，中间预留的部分就完全浪费掉了。</p>
<p><img src="https://cmbbq.github.io/img/naive_kv_cache.png" alt="naive_kv_cache"></p>
<p><code>PagedAttention</code>把连续的kv存在不连续的内存空间上，借用类似页表的机制（引入一个block_table）规避内存碎片化问题。具体来讲，<code>PagedAttention</code>把KV cache分区成若干个K blocks和V blocks，每个K/V block容纳固定数量tokens所对应的K/V向量，因此attention计算也被转化为blockwise计算。这和<code>FlashAttention</code>有点像，不过应用的尺度不同，前者是为了大规模serving时克服碎片化、按需取用，后者是为了单次self-attention计算能全部fit in SRAM。</p>
<p><img src="https://cmbbq.github.io/img/block_table.png" alt="block_table"></p>
<p>这个物理kv blocks显然是支持多个请求复用的。如下图所示，为每个请求维护一个小的block table即可。</p>
<p><img src="https://cmbbq.github.io/img/vllm_two_requests.png" alt="vllm_two_requests"></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>W. Kwon, et al. Efficient Memory Management for Large Language Model Serving with PagedAttention. <a href="https://arxiv.org/pdf/2309.06180">[pdf]</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </div></div>

  
  
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">阅读其他博文</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="https://cmbbq.github.io/posts/optimizing-ai-inference/">
                <span class="button__icon">←</span>
                <span class="button__text">Optimizing AI Inference</span>
            </a>
        </span>
        
        
        <span class="button next">
            <a href="https://cmbbq.github.io/posts/flashattention/">
                <span class="button__text">Flash Attention</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
  

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        
    

        <span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-envelope" viewBox="0 0 16 16">
          <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"/>
        </svg> rpb.cmbbq@gmail.com </span>
      </div>
  </div>
</footer>

<script src="https://cmbbq.github.io/assets/main.js"></script>
<script src="https://cmbbq.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
