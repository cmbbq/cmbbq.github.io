<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Flash Attention :: Cmbbq&#39;s Encyclopedia</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Flash Attention，一言以蔽之：tiling &#43; selective gradient checkpointing。" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://cmbbq.github.io/posts/flash-attention/" />





<link rel="stylesheet" href="https://cmbbq.github.io/assets/style.css">

  <link rel="stylesheet" href="https://cmbbq.github.io/assets/green.css">






<link rel="apple-touch-icon" href="https://cmbbq.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://cmbbq.github.io/img/favicon/green.png">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Flash Attention">
<meta property="og:description" content="Flash Attention，一言以蔽之：tiling &#43; selective gradient checkpointing。" />
<meta property="og:url" content="https://cmbbq.github.io/posts/flash-attention/" />
<meta property="og:site_name" content="Cmbbq&#39;s Encyclopedia" />

  
    <meta property="og:image" content="https://cmbbq.github.io/img/favicon/green.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2024-07-22 00:00:00 &#43;0000 UTC" />












</head>
<body class="green">


<div class="container center headings--one-size">

  <header class="header">
  <script>
    MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: false,
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>
<script
    type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js">
</script>
    
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Cmbbq&#39;s Encyclopedia
  </div>
</a>

    </div>
    
  </div>
  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://cmbbq.github.io/posts/flash-attention/">Flash Attention</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2024-07-22
        
      </span>
    
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://cmbbq.github.io/tags/sys/">sys</a>&nbsp;
    
    #<a href="https://cmbbq.github.io/tags/ai/">ai</a>&nbsp;
    
    #<a href="https://cmbbq.github.io/tags/llm/">llm</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <p>由于<code>self-attention</code>的时、空复杂度都是序列长的平方，长序列LLM和高分辨率ViT都是非常吃内存的。</p>
<p>此前针对<code>self-attention</code>的优化大多是近似计算，其核心是优化FLOP，把理论上的时间复杂度降低到O(N)，但这并不能有效加速<code>self-attention</code>，因为该操作（以及transformer中多数操作）的实际瓶颈在访存——更准确地说是HBM和SRAM之间的IO。</p>
<p><code>FlashAttention</code>的原理就是基于<code>tiling</code>，确保内层循环计算fit in SRAM，减少了HBM和SRAM之间的IO频次，因而真切有效地提升了transformer性能，解锁了更长的context，迅速在各种高性能框架中得到应用。</p>
<h2 id="传统的self-attention实现">传统的Self-Attention实现<a href="#传统的self-attention实现" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Attention Layer具有将局部信息和张量中较远位置的信息结合起来的能力——通过为所得张量的每个组件与输入张量的每个组件计算注意力得分，不受局部性约束地，在整个张量范围内对特征进行平均。</p>
<p>给定$N^Q\times D^{QK}$维queries张量$Q$，$N^{KV}\times D^{QK}$维keys张量$K$，$N^{KV}\times D^V$维values张量$V$，通过<code>Attention操作</code>$att(K,Q,V)$计算得到$N^Q\times D^V$维的张量Y：</p>
<p>$$Y = att(K,Q,V) = \underbrace{softargmax(\frac{QK^T}{\frac{1}{\sqrt{D^{QK}}}})}_A V$$</p>
<p>整个过程分两步，第一步先计算每个query index $q$和每个key index $k$的注意力得分，即queries和keys点乘后的<code>softargmax</code>结果：$A_{q,k} = \frac{exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_k ) }{\sum_l exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_l)}$，其中$\frac{1}{\sqrt{D^{QK}}}$是一个缩放参数，用于保证取值范围在$D^{QK}$变大时大体不变。</p>
<p><img src="https://cmbbq.github.io/img/attention.png" alt="att"></p>
<p>得到注意力得分$A_{q,k}$后，再进行第二步计算：$Y_q = \sum_k A_{q,k}V_k$。注意力得分即queries和keys之间的匹配程度，匹配程度越高，该权重就越高。如果某个query和某个key匹配度达到极限，注意力得分接近1，则直接拿到这个key对应的value。如果和好几个key都有中等水准的匹配，则按注意力得分做加权平均。</p>
<h2 id="flashattention的io优化">FlashAttention的IO优化<a href="#flashattention的io优化" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p><code>FlashAttention</code>注意到事实上并不需要完整的输入K、V、Q，可以分批读入、分批计算、分批把结果写回O，即所谓<code>tiling</code>：</p>
<ul>
<li>在外层循环，遍历K、V矩阵。每次只需加载一个block的 $K^T$ 和 $V$ 到片上SRAM中。</li>
<li>在内层循环，遍历Q的各个block，加载到SRAM中，进行 $att(K,Q,V)$ 计算，把局部结果写回HBM上 $N\times d$ 的结果矩阵<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>。</li>
<li>对softmax归一化因子做相应调整，即可保证最后加起来得到的最终结果和标准实现等价，具体代数见<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>附录。</li>
<li>设置K、V的block size为 $\lceil \frac{M}{4d} \rceil$，Q、O的block size为$min(\lceil \frac{M}{4d} \rceil, d)$<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>。</li>
</ul>
<p><img src="https://cmbbq.github.io/img/fast_attention.png" alt="fast_att"></p>
<p>此外，对于训练负载来说，<code>FlashAttention</code>还在反向传播中复用前馈过程暂存的softmax归一化因子$\frac{1}{\sqrt{D^{QK}}}$，这也比从HBM读$N\times N$的巨大attention中间矩阵要快得多。这可被视作selective gradient checkpointing。</p>
<h2 id="flashattention2-改进并行和工作切分">FlashAttention2: 改进并行和工作切分<a href="#flashattention2-改进并行和工作切分" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>相比GEMM，<code>FlashAttention</code>只达到了25~40%理论FLOPs/s，可优化空间巨大。<code>FlashAttention2</code><sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>在原版基础上做了并行化和工作切分优化。</p>
<ul>
<li>减少非矩阵乘算子，因为GPU矩阵乘是高度优化的，其他算子与之差距非常大。
<ul>
<li>避免循环中算O时每次都rescale，而是在算最终结果时施加softmax归一化因子。</li>
<li>对反向传播时保持的状态进行了精简。</li>
</ul>
</li>
<li>在不同线程块[^8]上进行并行计算，从而充分利用GPU资源。
<ul>
<li>原版一个线程块处理一个head/一个batch，每个线程块跑在一个SM上。不过在长序列场景，head数目和batch size可能都偏小，导致二者相乘后都未必能打满A100的128个SM。</li>
<li>显而易见可以并行的部分是外层循环，可以随便调度到不同线程块上，相互之间完全没有通信需求。</li>
<li>反向传播时并行化外层循环也只有dQ更新时需要简单的通信/同步，这是一个顺序不重要的加法，atomic add足以解决。</li>
</ul>
</li>
</ul>
<p>既然有了线程块，就要考虑在每个线程块内，不同warps之间如何进行工作的切分。
<img src="https://cmbbq.github.io/img/work_part.png" alt="work_part"></p>
<p>如上图所示，<code>FlashAttention</code>的切分方式导致内层循环中各个warp都需要把结果写到共享内存且做一个同步加，存在一定的通信开销，而<code>FlashAttention2</code>的切分方式可以保证warp之间完全没有通信需求。</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>其中，d为head dimension。N为序列长度。$N \gg d$。GPT2中 $N=1024，d=64$。&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>T. Dao, et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. <a href="https://arxiv.org/pdf/2205.14135">[pdf]</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>其中，M为SRAM大小。&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. <a href="https://arxiv.org/pdf/2307.08691">[pdf]</a>&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

      </div></div>

  
  
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">阅读其他博文</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="https://cmbbq.github.io/posts/paged-attention/">
                <span class="button__icon">←</span>
                <span class="button__text">Paged Attention</span>
            </a>
        </span>
        
        
        <span class="button next">
            <a href="https://cmbbq.github.io/posts/category-theory-2-resources/">
                <span class="button__text">CAT02: Resources</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
  

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        
    

        <span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-envelope" viewBox="0 0 16 16">
          <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"/>
        </svg> rpb.cmbbq@gmail.com </span>
      </div>
  </div>
</footer>

<script src="https://cmbbq.github.io/assets/main.js"></script>
<script src="https://cmbbq.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
