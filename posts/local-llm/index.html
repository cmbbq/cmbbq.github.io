<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Local LLM :: Cmbbq&#39;s Encyclopedia</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="用llama.cpp在本地cpu上跑ggul格式的mistral7b的简明教程。" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://cmbbq.github.io/posts/local-llm/" />





<link rel="stylesheet" href="https://cmbbq.github.io/assets/style.css">

  <link rel="stylesheet" href="https://cmbbq.github.io/assets/green.css">






<link rel="apple-touch-icon" href="https://cmbbq.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://cmbbq.github.io/img/favicon/green.png">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Local LLM">
<meta property="og:description" content="用llama.cpp在本地cpu上跑ggul格式的mistral7b的简明教程。" />
<meta property="og:url" content="https://cmbbq.github.io/posts/local-llm/" />
<meta property="og:site_name" content="Cmbbq&#39;s Encyclopedia" />

  
    <meta property="og:image" content="https://cmbbq.github.io/img/favicon/green.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2023-11-02 00:00:00 &#43;0000 UTC" />












</head>
<body class="green">


<div class="container center headings--one-size">

  <header class="header">
  <script>
    MathJax = {
      tex: {
        inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
        displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
        processEscapes: false,
      },
      svg: {
        fontCache: 'global'
      }
    };
</script>
<script
    type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@4.0.0-beta.4/tex-mml-chtml.js">
</script>
    
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Cmbbq&#39;s Encyclopedia
  </div>
</a>

    </div>
    
  </div>
  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://cmbbq.github.io/posts/local-llm/">Local LLM</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2023-11-02
        
      </span>
    
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://cmbbq.github.io/tags/ai/">ai</a>&nbsp;
    
    #<a href="https://cmbbq.github.io/tags/sys/">sys</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <h2 id="minimalist-local-llm">Minimalist Local LLM<a href="#minimalist-local-llm" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>新一轮AI热潮中，stable diffusion和LLM均呈现出了前所未有的社区繁荣。繁荣的同时带来了信息的爆炸和臃肿，极简主义显得尤为珍贵。</p>
<p>在预训练模型中，极简主义的代表是小而美的“大模型”Mistral7B。在推理框架中，极简主义的代表是对抗各种大公司重量级项目的llama.cpp。</p>
<p>Mistral7B是开源模型领域的新近进展，模型虽小，却展示了惊人的能力。根据Mistral AI自己的benchmark和一些社区的测试反馈，几乎在各方面击败llama2 13B。</p>
<p><img src="https://cmbbq.github.io/img/mistral.png" alt="mistral"></p>
<p>Mistral AI提供了在本地GPU、云端跑Mistral7B的官方支持。</p>
<p>不过这么小的模型，其实也可以在本地CPU上跑的——借助llama.cpp。</p>
<p>llama.cpp和whisper.cpp大体上是Georgi Gerganov的个人项目，基于自己写的ml primitive库ggml实现，用最少的代码量实现了尽可能多的推理优化，针对Apple芯片做得尤其好，也适用于Linux服务器，适用于有或无GPU，单或多GPU的硬件setup。ggml规定的模型文件格式ggul甚至也在社区中被广泛使用，成为事实上的标准之一。</p>
<h2 id="intel-xeon-platinum-8336c--debian10">Intel Xeon Platinum 8336C + Debian10<a href="#intel-xeon-platinum-8336c--debian10" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ol>
<li>从TheBloke上下载特定量化版本的模型：https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF</li>
<li><code>git clone https://github.com/ggerganov/llama.cpp.git</code></li>
<li><code>apt-get install libopenblas-dev</code> 安装blas库，这里选用openblas。</li>
<li>修改pkgconfig/openblas.pc或Makefile解决llama.cpp对openblas的依赖问题。最终要保证给编译期提供正确的LD Flags。</li>
<li><code>make LLAMA_OPENBLAS=1</code> 完成编译。</li>
<li><code>./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &quot;xx&quot;</code></li>
</ol>
<p>64物理核打满，采样速率4000t/s，prompt处理速率80t/s，生成速率20t/s。</p>
<h2 id="apple-m1-pro--macos-monterey">Apple M1 Pro + macOS Monterey<a href="#apple-m1-pro--macos-monterey" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<ol>
<li>下载模型和llama.cpp repo。</li>
<li>直接<code>make</code>就默认启用metal gpu加速和Accelerated Framework。</li>
<li><code>./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &quot;xx&quot;</code></li>
</ol>
<p>M1 GPU打到90%（剩下还有10%WindowServer在用），采样速率9000t/s，prompt处理速率60t/s，生成速率20t/s。</p>
<p>一边看视频（chrome GPU占用约20%），一边跑mistral也能有19.69t/s。</p>
<h2 id="token-sampling">Token Sampling<a href="#token-sampling" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>不同的sampling机制对文本生成有显著影响，本地LLM的可玩性来源很大程度上就是客制sampling。</p>
<p>Transformer模型根据前n个token，预测下一个token。每个token都有其next tokens的score，而next tokens的取值范围就是vocabulary（transformer架构最后有一个linear layer，将输出映射到vocabulary上，所有tokens都有对应的scores/logits），这些score经过softmax将score数组转化成probability数组，根据probability挑选下一个token的过程就称为token sampling。</p>
<ul>
<li>Greedy采样： 如果每次都确定性地选择probability最高的那个token，单步决策，就是greedy sampler，优势是速度快，劣势是牺牲了模型的多样性，容易陷入重复循环和对训练数据的过拟合。llama.cpp里设置&ndash;top_p 0就相当于开greedy采样。</li>
<li>Top-k采样： 从概率分布的前k个token里面进行随机采样。</li>
<li>Top-p采样： 引入超参p，把token probability降序排序后，选取前面一部分，使这部分概率和为p，而不是固定选前k个token。</li>
</ul>
<p>在使用LLM时，采样机制不同，效果也会大不相同。想要更高的确定性，更朴实无华的预测，则可以尝试greedy、低k的topk、低p的top-p采样。想要多样性和新颖性，则可尝试高k的top-k和高p的top-p。</p>
<p>除了top-k，top-p外，llama.cpp还实现了一些额外的采样机制：</p>
<ul>
<li>Min p filter：允许采样环节剔除低于阈值的低分候选。</li>
<li>Tail free sampling：根据概率的二阶导数之和采样，即根据概率降速剔除尾部低概率tokens。</li>
<li>Locally typical samplling：参数控制是否倾向局部语境内的典型的tokens。</li>
<li>Mirostat sampling：<a href="https://arxiv.org/abs/2007.14966">Mirostat算法</a>会调整top-k的k，避免陷入boredom trap（模式崩塌）和perplexity trap（不一致）。</li>
<li>logit bias: 人为指定某个token的优先级，比如&ndash;logit-bias 29905-inf就把&rsquo;\&rsquo; token设为负无穷。</li>
<li>temperature: 在对score向量（logits）做softmax前，把logits/temperature，则temp越高，softmax后概率分布的高低悬殊就会越接近，也就更有利于低概率tokens崭露头角。</li>
</ul>
<h2 id="prompting">Prompting<a href="#prompting" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>让llm假装自己是一个javascript console，然后进行交互式的指令应答，甚至还能进行简单的算术计算，理解函数调用的返回值类型。当然，不能对正确性抱有期待。
<img src="https://cmbbq.github.io/img/js_console.png" alt="console"></p>
<p>用一大段prompt，让llm假装自己是一个爱说emoji，语言风格浮夸的音乐推荐bot，效果相当不错。
<img src="https://cmbbq.github.io/img/music_bot.png" alt="bot"></p>

      </div></div>

  
  
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">阅读其他博文</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        <span class="button previous">
            <a href="https://cmbbq.github.io/posts/computational-conciousness/">
                <span class="button__icon">←</span>
                <span class="button__text">Computational Consciousness</span>
            </a>
        </span>
        
        
        <span class="button next">
            <a href="https://cmbbq.github.io/posts/efficient-anns-at-scale/">
                <span class="button__text">Efficient ANNS at Scale</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
  

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        
    

        <span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-envelope" viewBox="0 0 16 16">
          <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"/>
        </svg> rpb.cmbbq@gmail.com </span>
      </div>
  </div>
</footer>

<script src="https://cmbbq.github.io/assets/main.js"></script>
<script src="https://cmbbq.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
