<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Federated Learning vs In-center Distributed Training :: Cmbbq Wiki</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="本文介绍基于个人设备的联邦学习和数据中心语境下的分布式训练。" />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://cmbbq.github.io/posts/federated-learning-vs-in-center-distributed-training/" />




<link rel="stylesheet" href="https://cmbbq.github.io/assets/style.css">

  <link rel="stylesheet" href="https://cmbbq.github.io/assets/green.css">






<link rel="apple-touch-icon" href="https://cmbbq.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://cmbbq.github.io/img/favicon/green.png">



<meta name="twitter:card" content="summary" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Federated Learning vs In-center Distributed Training">
<meta property="og:description" content="本文介绍基于个人设备的联邦学习和数据中心语境下的分布式训练。" />
<meta property="og:url" content="https://cmbbq.github.io/posts/federated-learning-vs-in-center-distributed-training/" />
<meta property="og:site_name" content="Cmbbq Wiki" />

  
    <meta property="og:image" content="https://cmbbq.github.io/img/favicon/green.png">
  

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2023-06-01 00:00:00 &#43;0000 UTC" />












</head>
<body class="green">


<div class="container center headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Cmbbq Wiki
  </div>
</a>

    </div>
    
  </div>
  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://cmbbq.github.io/posts/federated-learning-vs-in-center-distributed-training/">Federated Learning vs In-center Distributed Training</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2023-06-01
        
      </span>
    
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://cmbbq.github.io/tags/sys/">sys</a>&nbsp;
    
  </span>
  
  


  

  <div class="post-content"><div>
        <p>[WIP]</p>
<h1 id="联邦学习">联邦学习<a href="#联邦学习" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<p><a href="https://arxiv.org/pdf/1602.05629.pdf">联邦学习</a>由McMahan于2016年提出，指的是许多移动设备在一个中央服务器的编排下协作训练模型，保持训练数据离散，避免对用户数据进行收集，仅将客户端模型更新上传中央服务器汇总成新的全局模型的机器学习模式。</p>
<p>联邦学习的naive实现如下：</p>
<ol>
<li>中央服务器选定一些clients，让这些client下载模型。</li>
<li>每个client根据自己的数据计算更新。</li>
<li>每个client将更新（即新的完整模型）上传到中央服务器。</li>
<li>中央服务器用某种方式（比如取平均）聚合这些模型得到一个全局模型。</li>
</ol>
<h2 id="上传模型更新到中央服务器开销大的问题">上传模型更新到中央服务器开销大的问题<a href="#上传模型更新到中央服务器开销大的问题" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>大量文献对联邦学习进行了讨论。<a href="https://arxiv.org/pdf/1610.05492.pdf">Federated Learning: Strategies For Improving Communication Efficiency</a>指出联邦学习的naive实现的第3步很容易出现通信瓶颈，这是首当其冲的问题，提出了降低上行链路通信成本的两种方法：结构化更新，缩略更新。</p>
<p>联邦学习的问题可以被形式化地表述为：学习模型中的参数。全连接层的参数可用一个实数矩阵（这只是为了简化问题，所以讨论单个矩阵）表示 W ∈ R^(d1×d2)，shape为(#input × #output)，d1和d2表示输出维度和输入维度。卷积层kernel是4d tensor(#input × width × height × #output) ，需reshape到 (#input × width × height) × #output 。</p>
<p>用W(t)表示当前回合(t)的模型，W(i,t)表示本地更新后的模型，所谓更新就是H(i,t) = W(i,t) - W(t)。中央服务器可聚合得到新模型：W(t+1) = W(t) + η(t)H(t)，其中H(t) = Sum(H(i,t))/N，η(t)是learning rate。</p>
<h3 id="结构化更新">结构化更新<a href="#结构化更新" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>结构化更新是指直接从一个用少量变量参数化的受限空间里学习更新，而不是学习整个模型的更新。</p>
<p>所谓结构化更新，也就是impose structure on updates，论文里提出两种结构，一种是低秩矩阵，另一种是随机掩码。</p>
<p>在结构化更新之低秩矩阵方法中，本地模型的更新H(i,t)必须是一个rank &lt; k的低秩矩阵，这里k是一个固定的数字。将H(i,t)表示为H(i,t)=A(i,t)B(i,t)，其中A(i,t)∈ R^(d1×k)，B(i,t)∈ R^(k×d2)，在后续计算中，随机生成一个A(i,t)视为常数，然后优化B(i,t)。这样A的数据就不必上传，可以坍塌成一个随机种子，只需上传B(i,t)。这种优化，本质上是利用降维的压缩技术，使用矩阵将原始数据进行降维处理，然后使用重构矩阵将降维后的数据重新构建为原始数据。
A(i,t)每回合都为每个client随机生成一次。立刻就得到d1/k的上传开销减少。固定B，训练A，或者同时训练A和B都试过，效果不如固定A，训练B。对此的解释是A可视作重构矩阵（从变换后的向量中重新构建出原始向量），B是投影矩阵（将一个向量投影到另一个向量的子空间上）。固定A训练B实际上相当于解决这样一个问题：给定一个随机的重构矩阵，什么样的投影矩阵可以恢复最多的信息？</p>
<p>在结构化更新之随机掩码方法中，本地模型的更新H(i,t)必须是用某个预定义的随机掩码生成的稀疏矩阵。同样是每回合对每个client重新生成一次。稀疏掩码可以坍塌成一个随机种子，因此只需上传H(i,t)的非零值和种子。</p>
<h3 id="缩略更新">缩略更新<a href="#缩略更新" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h3>
<p>缩略(sketched)更新，学习一个完整模型更新，学成之后，再通过有损的量化、随机旋转、子采样将其压缩后再发给服务器。</p>
<p>量化：将权重做概率量化，用更小的标量类型充当原始权重的unbiased estimator。</p>
<p>随机旋转：其实就是用一个随机正交矩阵乘一下，防止大部分数据都是0的情形，导致量化效果差。</p>
<p>子采样：原本上传的是H(i,t)，subsampling之后只上传一个随机子集。</p>
<h2 id="算法隐私安全工程等个层面的挑战">算法、隐私、安全、工程等个层面的挑战<a href="#算法隐私安全工程等个层面的挑战" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p><a href="https://arxiv.org/pdf/1912.04977.pdf">Advances and Open Problems in Federated Learning</a>, 。</p>
<p>todo</p>
<h2 id="通信延迟高的问题">通信延迟高的问题<a href="#通信延迟高的问题" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>还有一个关键问题——高通信延迟，在之前的论文中没被很好地address。[https://dga.hanlab.ai/assets/neurips21_dga.pdf](Delayed Gradient Averaging: Tolerate the
Communication Latency in Federated Learning)一文提出了一种延迟进行梯度平均的算法，用16节点是树莓派集群模拟现实世界中的移动节点和无线网络环境做了个实验，经验性地证明应用延迟梯度平均可以使联邦学习过程容忍高网络延迟，同时还不牺牲准确度。</p>
<p>todo</p>
<h1 id="in-center-distribtued-training">In-center Distribtued Training<a href="#in-center-distribtued-training" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>

      </div></div>

  
  
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">阅读其他博文</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        
        <span class="button next">
            <a href="https://cmbbq.github.io/posts/on-transparency/">
                <span class="button__text">On Transparency</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  
  

  
</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        
    

        <span> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-envelope" viewBox="0 0 16 16">
          <path d="M0 4a2 2 0 0 1 2-2h12a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V4Zm2-1a1 1 0 0 0-1 1v.217l7 4.2 7-4.2V4a1 1 0 0 0-1-1H2Zm13 2.383-4.708 2.825L15 11.105V5.383Zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 0 0 2 13h12a1 1 0 0 0 .966-.741ZM1 11.105l4.708-2.897L1 5.383v5.722Z"/>
        </svg> rpb.cmbbq@gmail.com </span>
      </div>
  </div>
</footer>

<script src="https://cmbbq.github.io/assets/main.js"></script>
<script src="https://cmbbq.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
