<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Cmbbq Wiki</title>
    <link>https://cmbbq.github.io/posts/</link>
    <description>Recent content in Posts on Cmbbq Wiki</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://cmbbq.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Distributed Computing Systems At Scale</title>
      <link>https://cmbbq.github.io/posts/distributed-computing-systems/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/distributed-computing-systems/</guid>
      <description>Distributed Computing Systems At Scale 分布式计算系统完成规模化的跳跃后，核心挑战是正确性和系统效率。前者对应共识抽象，后者对应性能工程。
Consensus Abstraction 分布式系统的正确性实际上蕴含了稳定性、一致性、可用性等理想属性。 分布式天然意味着并发执行、进程失败、不可靠的消息传输，这些因素都使正确性在各种常见的分布式应用语境下变得极为困难：
并发意味着执行轨迹空间维度爆炸，和(进程数*步数)的阶乘成正比。很难保证每个执行路径都是对的。 进程失败可分为：崩溃（进程没了，网卡或CPU故障）、丢请求（没崩，网络阻塞或服务降级导致临时不可用）、恢复后崩溃（反复重启，最终恢复到正确状态，往往基于日志）、拜占庭失败（不可预料，来自宇宙射线或恶意攻击）。 消息传输面临篡改、丢包、重传、乱序等风险，即使是可靠网络协议也不完全可靠。 共识是指多个进程就某个问题（2-general问题、复制状态机保证全局序的问题、分布式事务）最终形成一致决定。理想的共识算法应具备安全性(safety)和活性(liveliness)。安全性包括validity（只有一个提议值被选择）、agreement（正确的节点保持一致）、integrity（每个节点最多选一次）。活性则是指可终止，每个正确节点最终总会选择一个值，而不是hang住。
如果不考虑安全性的话，TCP其实就是一个最简单、粗糙、有效的共识协议，TCP协议本质是让通信的两个进程对连接的状态达成了共识，才能可靠地通信。比如进程关闭连接，就会把本地状态改为TIME_WAIT，等几分钟后才真正释放资源，以确保对方收到ACK，用简单的等待，达成高概率的共识，避免双方开始下一轮连接后收到旧报文而出错。
如果追求完全安全，简单的场景是只对一个值进行商议和决策，即单值共识。
单值共识前提下的最简场景则是拥有完美失败检测能力，泛洪共识（只有获得所有进程提议才做出决定）、等级统一共识就是这种场景下的简单解决方案。
若只具备最终失败检测能力，则需引入代次变更概念，用代次共识（往往是Paxos变种）去解决。Paxos是唯一已知的completely-safe &amp;amp; largely-live的容错共识算法。2PC其实也是一种代次共识，不过2PC里协调者宕机就会全局阻塞，是safe but not live的典型。Paxos甚至可以通过将代次(round)的偏序关系从&amp;lt;改成整除，来实现多维的先后代次，从而兼具2PC的功能。
如果再把单值共识推广到序列共识（在分布式存储中更常用，因为log-structured store的写操作是一个append-only序列，而非单值），则从Paxos进化为某种Multi-Paxos，或Raft。Raft本质上解决了复制状态机问题，而序列共识问题恰好可以归约为复制状态机问题。
如果再考虑拜占庭失败，则需BFT、PBFT、PoW。
Performance Engineering 性能的重要性毋庸置疑，端上应用（语音识别、智能客服）的延迟够不够低直接决定项目是否成立，而大规模的IDC应用(搜广推、存储、在线推理、训练)的吞吐性能则直接和金钱与环保挂钩。
计算系统的性能与分布式密不可分。因为几乎所有计算系统都是分布式的，包括单机，单卡，单设备，因为单机（in-node）仍然有网络结构，比如NVLink和NVSwitch，甚至Xeon CPU都是众核通过总线连起来的，网卡、SSD等设备，拆开来看都是网联设备（networked device）。通过分区、并行、软硬件协同、减少不必要的分层获取性能的方法是普适的。
如果存在基线，性能工程的主要工作就是优化、迭代，和渐进式创新。 当基线不够好时，性能工程(performance engineering)是发散的：跨越层次的，不同抽象层次、不同的模块都有可能成为瓶颈，不应存在优化盲点，必要时还需反分层，比如将整个IO路径放到用户态，比如暴露hypervisor状态给vm以便高效调度。 当基线已经非常好时，性能路径(performance path)则是收敛的：明确指向软硬件协同，毕竟性能最终来源于对硬件资源的量体裁衣和极限压榨，如果协同也足够好，那就只能指向硬件更新：更大更强的N卡、TPUv4这样的DSA、OCS这样的光学路由以及更进一步的AI DC建设。
如果是全新的问题，或对已有问题产生了非凡洞察，对其施加更有效结构，就产生了新的架构，新的计算和IO形态：
大模型时代来临后，模型参数本身就已经放不进顶级的GPU，更不用说神经网络计算所需的额外内存了，这就使原本在系统效率和正确性之间平衡的非常好的Parameter Server架构面临挑战，不得不用全新的架构和策略解决大模型训练的问题：Node内（8卡组一个Node）的权重切分和模型并行，算子内的数据和指令并行，子图切分、算子摆布和流水线并行（8个Node做8级流水）、16组8*8做Batch16的数据并行，于是产生了Alpa over Ray训练大模型的解决方案。这又间接导致带宽需求激增，以至于需要Google的光学交换机OCS这样的网络解决方案，避免光电模块和解析报文计算的开销——光交换劣势在于切换慢，但训练周期里数据交换的主要流量路由是固定的，用来建设AI DC正好规避了这个缺点。 KVM的作者根据hypervisor在NUMA系统上的性能瓶颈的经验，很容易就发现应用层共享内存的并行模型是有缺陷的，在众核时代尤其严重，因此创造了seastar。利用hardware locality，避免线程切换、数据拷贝、NUMA远端内存访问，对访存密集型CPU-bound应用来说，是非常重要的性能路径。 </description>
      <content>&lt;h1 id=&#34;distributed-computing-systems-at-scale&#34;&gt;Distributed Computing Systems At Scale&lt;/h1&gt;
&lt;p&gt;分布式计算系统完成规模化的跳跃后，核心挑战是正确性和系统效率。前者对应共识抽象，后者对应性能工程。&lt;/p&gt;
&lt;h2 id=&#34;consensus-abstraction&#34;&gt;Consensus Abstraction&lt;/h2&gt;
&lt;p&gt;分布式系统的正确性实际上蕴含了稳定性、一致性、可用性等理想属性。
分布式天然意味着并发执行、进程失败、不可靠的消息传输，这些因素都使正确性在各种常见的分布式应用语境下变得极为困难：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;并发意味着执行轨迹空间维度爆炸，和(进程数*步数)的阶乘成正比。很难保证每个执行路径都是对的。&lt;/li&gt;
&lt;li&gt;进程失败可分为：崩溃（进程没了，网卡或CPU故障）、丢请求（没崩，网络阻塞或服务降级导致临时不可用）、恢复后崩溃（反复重启，最终恢复到正确状态，往往基于日志）、拜占庭失败（不可预料，来自宇宙射线或恶意攻击）。&lt;/li&gt;
&lt;li&gt;消息传输面临篡改、丢包、重传、乱序等风险，即使是可靠网络协议也不完全可靠。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;共识是指多个进程就某个问题（2-general问题、复制状态机保证全局序的问题、分布式事务）最终形成一致决定。理想的共识算法应具备安全性(safety)和活性(liveliness)。安全性包括validity（只有一个提议值被选择）、agreement（正确的节点保持一致）、integrity（每个节点最多选一次）。活性则是指可终止，每个正确节点最终总会选择一个值，而不是hang住。&lt;/p&gt;
&lt;p&gt;如果不考虑安全性的话，TCP其实就是一个最简单、粗糙、有效的共识协议，TCP协议本质是让通信的两个进程对连接的状态达成了共识，才能可靠地通信。比如进程关闭连接，就会把本地状态改为TIME_WAIT，等几分钟后才真正释放资源，以确保对方收到ACK，用简单的等待，达成高概率的共识，避免双方开始下一轮连接后收到旧报文而出错。&lt;/p&gt;
&lt;p&gt;如果追求完全安全，简单的场景是只对一个值进行商议和决策，即单值共识。&lt;/p&gt;
&lt;p&gt;单值共识前提下的最简场景则是拥有完美失败检测能力，泛洪共识（只有获得所有进程提议才做出决定）、等级统一共识就是这种场景下的简单解决方案。&lt;/p&gt;
&lt;p&gt;若只具备最终失败检测能力，则需引入代次变更概念，用代次共识（往往是Paxos变种）去解决。Paxos是唯一已知的completely-safe &amp;amp; largely-live的容错共识算法。2PC其实也是一种代次共识，不过2PC里协调者宕机就会全局阻塞，是safe but not live的典型。Paxos甚至可以通过将代次(round)的偏序关系从&amp;lt;改成整除，来实现多维的先后代次，从而兼具2PC的功能。&lt;/p&gt;
&lt;p&gt;如果再把单值共识推广到序列共识（在分布式存储中更常用，因为log-structured store的写操作是一个append-only序列，而非单值），则从Paxos进化为某种Multi-Paxos，或Raft。Raft本质上解决了复制状态机问题，而序列共识问题恰好可以归约为复制状态机问题。&lt;/p&gt;
&lt;p&gt;如果再考虑拜占庭失败，则需BFT、PBFT、PoW。&lt;/p&gt;
&lt;h2 id=&#34;performance-engineering&#34;&gt;Performance Engineering&lt;/h2&gt;
&lt;p&gt;性能的重要性毋庸置疑，端上应用（语音识别、智能客服）的延迟够不够低直接决定项目是否成立，而大规模的IDC应用(搜广推、存储、在线推理、训练)的吞吐性能则直接和金钱与环保挂钩。&lt;/p&gt;
&lt;p&gt;计算系统的性能与分布式密不可分。因为几乎所有计算系统都是分布式的，包括单机，单卡，单设备，因为单机（in-node）仍然有网络结构，比如NVLink和NVSwitch，甚至Xeon CPU都是众核通过总线连起来的，网卡、SSD等设备，拆开来看都是网联设备（networked device）。通过分区、并行、软硬件协同、减少不必要的分层获取性能的方法是普适的。&lt;/p&gt;
&lt;p&gt;如果存在基线，性能工程的主要工作就是优化、迭代，和渐进式创新。
当基线不够好时，性能工程(performance engineering)是发散的：跨越层次的，不同抽象层次、不同的模块都有可能成为瓶颈，不应存在优化盲点，必要时还需反分层，比如将整个IO路径放到用户态，比如暴露hypervisor状态给vm以便高效调度。
当基线已经非常好时，性能路径(performance path)则是收敛的：明确指向软硬件协同，毕竟性能最终来源于对硬件资源的量体裁衣和极限压榨，如果协同也足够好，那就只能指向硬件更新：更大更强的N卡、TPUv4这样的DSA、OCS这样的光学路由以及更进一步的AI DC建设。&lt;/p&gt;
&lt;p&gt;如果是全新的问题，或对已有问题产生了非凡洞察，对其施加更有效结构，就产生了新的架构，新的计算和IO形态：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大模型时代来临后，模型参数本身就已经放不进顶级的GPU，更不用说神经网络计算所需的额外内存了，这就使原本在系统效率和正确性之间平衡的非常好的Parameter Server架构面临挑战，不得不用全新的架构和策略解决大模型训练的问题：Node内（8卡组一个Node）的权重切分和模型并行，算子内的数据和指令并行，子图切分、算子摆布和流水线并行（8个Node做8级流水）、16组8*8做Batch16的数据并行，于是产生了Alpa over Ray训练大模型的解决方案。这又间接导致带宽需求激增，以至于需要Google的光学交换机OCS这样的网络解决方案，避免光电模块和解析报文计算的开销——光交换劣势在于切换慢，但训练周期里数据交换的主要流量路由是固定的，用来建设AI DC正好规避了这个缺点。&lt;/li&gt;
&lt;li&gt;KVM的作者根据hypervisor在NUMA系统上的性能瓶颈的经验，很容易就发现应用层共享内存的并行模型是有缺陷的，在众核时代尤其严重，因此创造了seastar。利用hardware locality，避免线程切换、数据拷贝、NUMA远端内存访问，对访存密集型CPU-bound应用来说，是非常重要的性能路径。&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Knowledge is Embeddings of Reality</title>
      <link>https://cmbbq.github.io/posts/reality-knowledge/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/reality-knowledge/</guid>
      <description>现实有无限维度，现实的背后还有潜在现实，结构的边缘会涌现出新的深层结构。
人类对现实的感知、认知、理解，本质上是将高维现实投射到一个低维表示上，这种低维表示在统计学和机器学习中被称为embedding（嵌入），没错，知识就是将现实在人类有限的认知空间的一个嵌入，紧凑、浅薄且片面。
举个具体的例子，人眼观察到的彩虹呈现七色。
但这只是对无限广博的现实的最初观察，是最直观的认知。
人类通过对人体生理学的研究，发现七色感知是一个更深层的潜在现实的涌现结构：多种光感受器的叠加反应。
人类颜色感知系统对红绿蓝敏感的三种光感受器，叠加反应产生的三个峰和四个谷决定了人类能感知到7种颜色。 鸟类由于拥有对紫外线敏感的光感受器，就能感知到9种颜色，因此可以看到九色彩虹。
无论人类的七色彩虹还是鸟类的九色彩虹，都只是知识（或者说embedding），而不是现实。
人类通过数学工具和电磁学研究，进一步认识到光谱是连续的，包括无线电波、微波、热、红外线、可见光、紫外线等。
这接近现实了吗？不，这依然只是浅薄的embedding。。
人类通过量子物理学的研究，发现现有的电磁学依旧是一个更深层的潜在现实的涌现模型——电磁波是量子粒子流的一种表现形式：光子。
光子的认知，折射出更复杂的现实，因为一个光子同时拥有无限的位置和频率，直到被观察到才能确定其位置或能量，需用海森堡不确定性原理、薛定谔方程来描述，这依旧不是真理，不是终极，仍然是在现实之上所施加的结构(structure imposed upon reality)。
类似地，现实中的一首歌曲，看似简单平凡，实际上却拥有无穷维度，我们可以从任意一个角度去理解它，将它嵌入到任意一个向量空间。
假设用户手机的传感器捕捉了一段音频，将其上传到平台上，消重（时频图的mask特征）、指纹(时频图的landmark特征)、翻唱（浅层cnn）、哼唱（whisper大模型）、多模态大模型应用分别捕捉了这段音频的某种特征，产生了不同层次的知识（embedding），理解愈发深邃，愈发不精确，应用也就愈发灵活，愈发贴近人类思维。
后端的消重服务或指纹服务将表示音频的信号采样数据通过fft计算得到时频图，再基于时频图提取某种mask特征或landmark特征，用于直接刻画其能量分布特征，然后与音频特征库里存着的现有特征做对比就可以找到匹配的音频，这就是音频消重和指纹识别的过程：对信号做简单数学处理，生成的就是高度特化、具体、紧凑、确定、可解释的知识，表示这种知识的embedding或不具有抗噪性（mask特征），或具有少许抗噪性（landmark特征），但无论如何都只适用于原曲匹配，不知何为翻唱，何为remix，何为二创，何为串烧。
翻唱识别服务能通过一个学习了歌曲旋律的神经网络对音频信号进行处理，产出一定程度上表达旋律特征的embedding，将曲库里的翻唱embedding存入向量数据库，通过KNN/ANN召回的结果往往不限于原曲，还能容忍变调、音色上的改变。
哼唱识别服务可以基于Whisper这样的音频大模型实现，这种深层网络学习了人类的自然语言，因此对音频信号处理时，提取了表达歌词信息的embedding。基于这样的embedding，可以在跑调非常严重，且没有伴奏的情况下，依然能识别出用户在哼什么歌。
考虑未来的多模态大模型，更是可以直接将embedding存在transformer的ffn层，不必借助外部的向量数据库或倒排索引，直接在模型内部刻画embedding之间的复杂关系，让更多概念之间产生关联：识别出歌曲的意境、歌唱者的情绪、歌词内容的图片，甚至联系用户交互的上下文，衍生出更客制化、场景相关的输出。
人工智能领域用transformer简单搭建的大模型拥有出人意料的逻辑能力，昭示着人类意识的本质或许原本就极端肤浅：将知识内化、知识关联后，用天生的脑回路做贝叶斯推理，就产生了逻辑，诚然现有的结构远不如人脑高效，但有和无的界限已经被突破。
足够好的计算神经网络可以被视作神经科学的涌现模型。 一如往昔。 一如神经科学是细胞生物学的涌现。 一如细胞生物学是分子生物学的涌现。 一如分子生物学是物理化学的涌现。 一如物理化学是量子物理的涌现。</description>
      <content>&lt;p&gt;现实有无限维度，现实的背后还有潜在现实，结构的边缘会涌现出新的深层结构。&lt;/p&gt;
&lt;p&gt;人类对现实的感知、认知、理解，本质上是将高维现实投射到一个低维表示上，这种低维表示在统计学和机器学习中被称为embedding（嵌入），没错，知识就是将现实在人类有限的认知空间的一个嵌入，紧凑、浅薄且片面。&lt;/p&gt;
&lt;p&gt;举个具体的例子，人眼观察到的彩虹呈现七色。&lt;/p&gt;
&lt;p&gt;但这只是对无限广博的现实的最初观察，是最直观的认知。&lt;/p&gt;
&lt;p&gt;人类通过对人体生理学的研究，发现七色感知是一个更深层的潜在现实的涌现结构：多种光感受器的叠加反应。&lt;/p&gt;
&lt;p&gt;人类颜色感知系统对红绿蓝敏感的三种光感受器，叠加反应产生的三个峰和四个谷决定了人类能感知到7种颜色。
&lt;img src=&#34;https://cmbbq.github.io/img/color.png&#34; alt=&#34;color&#34;&gt;&lt;/p&gt;
&lt;p&gt;鸟类由于拥有对紫外线敏感的光感受器，就能感知到9种颜色，因此可以看到九色彩虹。&lt;/p&gt;
&lt;p&gt;无论人类的七色彩虹还是鸟类的九色彩虹，都只是知识（或者说embedding），而不是现实。&lt;/p&gt;
&lt;p&gt;人类通过数学工具和电磁学研究，进一步认识到光谱是连续的，包括无线电波、微波、热、红外线、可见光、紫外线等。&lt;/p&gt;
&lt;p&gt;这接近现实了吗？不，这依然只是浅薄的embedding。。&lt;/p&gt;
&lt;p&gt;人类通过量子物理学的研究，发现现有的电磁学依旧是一个更深层的潜在现实的涌现模型——电磁波是量子粒子流的一种表现形式：光子。&lt;/p&gt;
&lt;p&gt;光子的认知，折射出更复杂的现实，因为一个光子同时拥有无限的位置和频率，直到被观察到才能确定其位置或能量，需用海森堡不确定性原理、薛定谔方程来描述，这依旧不是真理，不是终极，仍然是在现实之上所施加的结构(structure imposed upon reality)。&lt;/p&gt;
&lt;p&gt;类似地，现实中的一首歌曲，看似简单平凡，实际上却拥有无穷维度，我们可以从任意一个角度去理解它，将它嵌入到任意一个向量空间。&lt;/p&gt;
&lt;p&gt;假设用户手机的传感器捕捉了一段音频，将其上传到平台上，消重（时频图的mask特征）、指纹(时频图的landmark特征)、翻唱（浅层cnn）、哼唱（whisper大模型）、多模态大模型应用分别捕捉了这段音频的某种特征，产生了不同层次的知识（embedding），理解愈发深邃，愈发不精确，应用也就愈发灵活，愈发贴近人类思维。&lt;/p&gt;
&lt;p&gt;后端的消重服务或指纹服务将表示音频的信号采样数据通过fft计算得到时频图，再基于时频图提取某种mask特征或landmark特征，用于直接刻画其能量分布特征，然后与音频特征库里存着的现有特征做对比就可以找到匹配的音频，这就是音频消重和指纹识别的过程：对信号做简单数学处理，生成的就是高度特化、具体、紧凑、确定、可解释的知识，表示这种知识的embedding或不具有抗噪性（mask特征），或具有少许抗噪性（landmark特征），但无论如何都只适用于原曲匹配，不知何为翻唱，何为remix，何为二创，何为串烧。&lt;/p&gt;
&lt;p&gt;翻唱识别服务能通过一个学习了歌曲旋律的神经网络对音频信号进行处理，产出一定程度上表达旋律特征的embedding，将曲库里的翻唱embedding存入向量数据库，通过KNN/ANN召回的结果往往不限于原曲，还能容忍变调、音色上的改变。&lt;/p&gt;
&lt;p&gt;哼唱识别服务可以基于Whisper这样的音频大模型实现，这种深层网络学习了人类的自然语言，因此对音频信号处理时，提取了表达歌词信息的embedding。基于这样的embedding，可以在跑调非常严重，且没有伴奏的情况下，依然能识别出用户在哼什么歌。&lt;/p&gt;
&lt;p&gt;考虑未来的多模态大模型，更是可以直接将embedding存在transformer的ffn层，不必借助外部的向量数据库或倒排索引，直接在模型内部刻画embedding之间的复杂关系，让更多概念之间产生关联：识别出歌曲的意境、歌唱者的情绪、歌词内容的图片，甚至联系用户交互的上下文，衍生出更客制化、场景相关的输出。&lt;/p&gt;
&lt;p&gt;人工智能领域用transformer简单搭建的大模型拥有出人意料的逻辑能力，昭示着人类意识的本质或许原本就极端肤浅：将知识内化、知识关联后，用天生的脑回路做贝叶斯推理，就产生了逻辑，诚然现有的结构远不如人脑高效，但有和无的界限已经被突破。&lt;/p&gt;
&lt;p&gt;足够好的计算神经网络可以被视作神经科学的涌现模型。
一如往昔。
一如神经科学是细胞生物学的涌现。
一如细胞生物学是分子生物学的涌现。
一如分子生物学是物理化学的涌现。
一如物理化学是量子物理的涌现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/em.png&#34; alt=&#34;em&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Federated Learning</title>
      <link>https://cmbbq.github.io/posts/federated-learning/</link>
      <pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/federated-learning/</guid>
      <description>联邦学习 联邦学习由McMahan于2016年提出，指的是许多移动设备在一个中央服务器的编排下协作训练模型，保持训练数据离散，避免对用户数据进行收集，仅将客户端模型更新上传中央服务器汇总成新的全局模型的机器学习模式。
联邦学习的naive实现如下：
中央服务器选定一些clients，让这些client下载模型。 每个client根据自己的数据计算更新。 每个client将更新（即新的完整模型）上传到中央服务器。 中央服务器用某种方式（比如取平均）聚合这些模型得到一个全局模型。 应对上传模型开销大的问题 大量文献对联邦学习进行了讨论。Federated Learning: Strategies For Improving Communication Efficiency指出联邦学习的naive实现的第3步很容易出现通信瓶颈，这是首当其冲的问题，提出了降低上行链路通信成本的两种方法：结构化更新，缩略更新。
联邦学习的问题可以被形式化地表述为：学习模型中的参数。全连接层的参数可用一个实数矩阵（这只是为了简化问题，所以讨论单个矩阵）表示 W ∈ R^(d1×d2)，shape为(#input × #output)，d1和d2表示输出维度和输入维度。卷积层kernel是4d tensor(#input × width × height × #output) ，需reshape到 (#input × width × height) × #output 。
用W(t)表示当前回合(t)的模型，W(i,t)表示本地更新后的模型，所谓更新就是H(i,t) = W(i,t) - W(t)。中央服务器可聚合得到新模型：W(t+1) = W(t) + η(t)H(t)，其中H(t) = Sum(H(i,t))/N，η(t)是learning rate。
结构化更新 结构化更新是指直接从一个用少量变量参数化的受限空间里学习更新，而不是学习整个模型的更新。
所谓结构化更新，也就是impose structure on updates，论文里提出两种结构，一种是低秩矩阵，另一种是随机掩码。
在结构化更新之低秩矩阵方法中，本地模型的更新H(i,t)必须是一个rank &amp;lt; k的低秩矩阵，这里k是一个固定的数字。将H(i,t)表示为H(i,t)=A(i,t)B(i,t)，其中A(i,t)∈ R^(d1×k)，B(i,t)∈ R^(k×d2)，在后续计算中，随机生成一个A(i,t)视为常数，然后优化B(i,t)。这样A的数据就不必上传，可以坍塌成一个随机种子，只需上传B(i,t)。这种优化，本质上是利用降维的压缩技术，使用矩阵将原始数据进行降维处理，然后使用重构矩阵将降维后的数据重新构建为原始数据。 A(i,t)每回合都为每个client随机生成一次。立刻就得到d1/k的上传开销减少。固定B，训练A，或者同时训练A和B都试过，效果不如固定A，训练B。对此的解释是A可视作重构矩阵（从变换后的向量中重新构建出原始向量），B是投影矩阵（将一个向量投影到另一个向量的子空间上）。固定A训练B实际上相当于解决这样一个问题：给定一个随机的重构矩阵，什么样的投影矩阵可以恢复最多的信息？
在结构化更新之随机掩码方法中，本地模型的更新H(i,t)必须是用某个预定义的随机掩码生成的稀疏矩阵。同样是每回合对每个client重新生成一次。稀疏掩码可以坍塌成一个随机种子，因此只需上传H(i,t)的非零值和种子。
缩略更新 缩略(sketched)更新，学习一个完整模型更新，学成之后，再通过有损的量化、随机旋转、子采样将其压缩后再发给服务器。
量化：将权重做概率量化，用更小的标量类型充当原始权重的unbiased estimator。
随机旋转：其实就是用一个随机正交矩阵乘一下，防止大部分数据都是0的情形，导致量化效果差。
子采样：原本上传的是H(i,t)，subsampling之后只上传一个随机子集。
完全去中心化联邦学习在各个领域面临的挑战 Advances and Open Problems in Federated Learning中讨论了完全去中心化联邦学习在算法、隐私、安全、工程等个层面的挑战:</description>
      <content>&lt;h1 id=&#34;联邦学习&#34;&gt;联邦学习&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.05629.pdf&#34;&gt;联邦学习&lt;/a&gt;由McMahan于2016年提出，指的是许多移动设备在一个中央服务器的编排下协作训练模型，保持训练数据离散，避免对用户数据进行收集，仅将客户端模型更新上传中央服务器汇总成新的全局模型的机器学习模式。&lt;/p&gt;
&lt;p&gt;联邦学习的naive实现如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;中央服务器选定一些clients，让这些client下载模型。&lt;/li&gt;
&lt;li&gt;每个client根据自己的数据计算更新。&lt;/li&gt;
&lt;li&gt;每个client将更新（即新的完整模型）上传到中央服务器。&lt;/li&gt;
&lt;li&gt;中央服务器用某种方式（比如取平均）聚合这些模型得到一个全局模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;应对上传模型开销大的问题&#34;&gt;应对上传模型开销大的问题&lt;/h2&gt;
&lt;p&gt;大量文献对联邦学习进行了讨论。&lt;a href=&#34;https://arxiv.org/pdf/1610.05492.pdf&#34;&gt;Federated Learning: Strategies For Improving Communication Efficiency&lt;/a&gt;指出联邦学习的naive实现的第3步很容易出现通信瓶颈，这是首当其冲的问题，提出了降低上行链路通信成本的两种方法：结构化更新，缩略更新。&lt;/p&gt;
&lt;p&gt;联邦学习的问题可以被形式化地表述为：学习模型中的参数。全连接层的参数可用一个实数矩阵（这只是为了简化问题，所以讨论单个矩阵）表示 W ∈ R^(d1×d2)，shape为(#input × #output)，d1和d2表示输出维度和输入维度。卷积层kernel是4d tensor(#input × width × height × #output) ，需reshape到 (#input × width × height) × #output 。&lt;/p&gt;
&lt;p&gt;用W(t)表示当前回合(t)的模型，W(i,t)表示本地更新后的模型，所谓更新就是H(i,t) = W(i,t) - W(t)。中央服务器可聚合得到新模型：W(t+1) = W(t) + η(t)H(t)，其中H(t) = Sum(H(i,t))/N，η(t)是learning rate。&lt;/p&gt;
&lt;h3 id=&#34;结构化更新&#34;&gt;结构化更新&lt;/h3&gt;
&lt;p&gt;结构化更新是指直接从一个用少量变量参数化的受限空间里学习更新，而不是学习整个模型的更新。&lt;/p&gt;
&lt;p&gt;所谓结构化更新，也就是impose structure on updates，论文里提出两种结构，一种是低秩矩阵，另一种是随机掩码。&lt;/p&gt;
&lt;p&gt;在结构化更新之低秩矩阵方法中，本地模型的更新H(i,t)必须是一个rank &amp;lt; k的低秩矩阵，这里k是一个固定的数字。将H(i,t)表示为H(i,t)=A(i,t)B(i,t)，其中A(i,t)∈ R^(d1×k)，B(i,t)∈ R^(k×d2)，在后续计算中，随机生成一个A(i,t)视为常数，然后优化B(i,t)。这样A的数据就不必上传，可以坍塌成一个随机种子，只需上传B(i,t)。这种优化，本质上是利用降维的压缩技术，使用矩阵将原始数据进行降维处理，然后使用重构矩阵将降维后的数据重新构建为原始数据。
A(i,t)每回合都为每个client随机生成一次。立刻就得到d1/k的上传开销减少。固定B，训练A，或者同时训练A和B都试过，效果不如固定A，训练B。对此的解释是A可视作重构矩阵（从变换后的向量中重新构建出原始向量），B是投影矩阵（将一个向量投影到另一个向量的子空间上）。固定A训练B实际上相当于解决这样一个问题：给定一个随机的重构矩阵，什么样的投影矩阵可以恢复最多的信息？&lt;/p&gt;
&lt;p&gt;在结构化更新之随机掩码方法中，本地模型的更新H(i,t)必须是用某个预定义的随机掩码生成的稀疏矩阵。同样是每回合对每个client重新生成一次。稀疏掩码可以坍塌成一个随机种子，因此只需上传H(i,t)的非零值和种子。&lt;/p&gt;
&lt;h3 id=&#34;缩略更新&#34;&gt;缩略更新&lt;/h3&gt;
&lt;p&gt;缩略(sketched)更新，学习一个完整模型更新，学成之后，再通过有损的量化、随机旋转、子采样将其压缩后再发给服务器。&lt;/p&gt;
&lt;p&gt;量化：将权重做概率量化，用更小的标量类型充当原始权重的unbiased estimator。&lt;/p&gt;
&lt;p&gt;随机旋转：其实就是用一个随机正交矩阵乘一下，防止大部分数据都是0的情形，导致量化效果差。&lt;/p&gt;
&lt;p&gt;子采样：原本上传的是H(i,t)，subsampling之后只上传一个随机子集。&lt;/p&gt;
&lt;h2 id=&#34;完全去中心化联邦学习在各个领域面临的挑战&#34;&gt;完全去中心化联邦学习在各个领域面临的挑战&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1912.04977.pdf&#34;&gt;Advances and Open Problems in Federated Learning&lt;/a&gt;中讨论了完全去中心化联邦学习在算法、隐私、安全、工程等个层面的挑战:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;中央服务器可能成为瓶颈和单点故障风险点。由此产生p2p/完全去中心化的设计思路。&lt;/li&gt;
&lt;li&gt;完全去中心化的算法需应对client可用性和网络稳定性的局限。&lt;/li&gt;
&lt;li&gt;设计一个试图达到最快收敛速度的模型平均策略是困难的。&lt;/li&gt;
&lt;li&gt;去中心化的场景会导致算法易受恶意攻击、不可靠数据或标注的威胁。&lt;/li&gt;
&lt;li&gt;client的通信带宽和电量不足，，将已有的压缩算法移植到移动端比较困难。&lt;/li&gt;
&lt;li&gt;隐私问题：如何防止一个client重建另一个client的隐私数据。&lt;/li&gt;
&lt;li&gt;如何实现的问题：区块链作为分布式账簿本质上是一个最终一致的复制状态机。不过以太坊这样的区块链上的数据是公开的，如果要适用于联邦学习，还需要改造。&lt;/li&gt;
&lt;li&gt;cross-silo场景（多个组织或公司一起训一个模型，但数据不能直接共享，比如多个银行一起训一个fraud detection模型）对数据进行分区，并增加incentive机制。&lt;/li&gt;
&lt;li&gt;通信和压缩瓶颈。&lt;/li&gt;
&lt;li&gt;公平性：联邦学习引入了新的bias来源——设备型号、地理位置、活动模式、本地数据集大小等。&lt;/li&gt;
&lt;li&gt;安全性计算问题：如何应对恶意服务器？如何应对外部攻击？&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;non-iid数据分布问题&#34;&gt;Non-IID数据分布问题&lt;/h3&gt;
&lt;p&gt;Non-IID(independent &amp;amp; identically distributed) data问题：样本的统计属性没有均匀分布，对于任何client-partitioned数据集来说都是常见的。&lt;/p&gt;
&lt;p&gt;论文中给出了多种non-identical client分布（考虑对特征x，标签y进行有监督学习，(x,y)~Pi(x,y)即为client i的本地分布，P(x,y) = P(y|x)P(x) = P(x|y)P(y)）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature distribution skew(covariate shift)，即不同client的P(y|x)相同，但特征的边际分布P(x)不同。比如笔迹识别中不同用户写相同的字，但笔法、书写习惯还是不同。&lt;/li&gt;
&lt;li&gt;Label distribution skew(prior probability shift)，即不同client的P(x|y)相同，但标签的边际分布P(y)不同。比如澳洲的日常动物识别应用，标签里就会频繁出现袋鼠，其他地区则不会。&lt;/li&gt;
&lt;li&gt;Same label, different features(concept drift)，即不同client的P(y)相同，但条件分布P(x|y)不同，相同标签在不同client上对应到了不同的特征。比如豪宅，在香港和加州尺度是不同的。&lt;/li&gt;
&lt;li&gt;Same features, different label(concept shift)，即不同client的P(x)相同，但条件分布P(y|x)不同，相同特征被标注成立不同标签。比如有人把熊猫标注为宠物，也有人把熊猫标注为猛兽。&lt;/li&gt;
&lt;li&gt;Quantity skew or unbalancedness，不同的client的数据量差异大。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;违反independence同样常见，因为client的分布很容易收到触发训练的约束条件的影响：比如很多训练是利用夜间睡眠时间跑的，那就导致往往一个经度的地区的clients更容易遇到一起。&lt;/p&gt;
&lt;p&gt;应对Non-IID数据，一个可行的方法是用一个不涉及隐私数据的全局共享小数据集做数据增强。此外，还可以限制一个用户每天能做的贡献上限，避免量上面的不平衡。此外，某些场景还可以把Non-IID从bug转化为feature，就训一个本地客制化的专用模型出来，提供个性化服务，而不是最终产生一个全局模型。文章后面还介绍了Non-IID数据集上的优化算法和收敛速率。&lt;/p&gt;
&lt;h3 id=&#34;应对隐私问题split-learning&#34;&gt;应对隐私问题：Split Learning&lt;/h3&gt;
&lt;p&gt;Split Learning是模型执行路径层面的横切，同时适用于训练和推理：最简单场景是让每个client一直前向算到某个特定的cut layer停下，将cut layer的输出（smashed data）传给中央服务器或peer接着算，于是就完成了无需数据共享就实现的前向传播。类似地，梯度反向传播是从最后一层到cut layer停下，仅将cut layer的梯度回传给client。这样整个过程中其他节点都不会直接访问本地数据。&lt;/p&gt;
&lt;p&gt;考虑到cut layer的权重本身也能一定程度上反映底层的数据现实，Split Learning是否能提供形式化的隐私承诺仍然是个开放问题。&lt;/p&gt;
&lt;h2 id=&#34;应对通信延迟高的问题&#34;&gt;应对通信延迟高的问题&lt;/h2&gt;
&lt;p&gt;还有一个关键问题——高通信延迟，由于无线和长距离传输的特性而无法回避，但在之前的论文中没被很好地address。&lt;a href=&#34;https://dga.hanlab.ai/assets/neurips21_dga.pdf&#34;&gt;Delayed Gradient Averaging: Tolerate the
Communication Latency in Federated Learning&lt;/a&gt;一文提出了一种延迟进行梯度平均的算法，用16节点是树莓派集群模拟现实世界中的移动节点和无线网络环境做了个实验，经验性地证明应用延迟梯度平均可以使联邦学习过程容忍高网络延迟，同时还不牺牲准确度。&lt;/p&gt;
&lt;p&gt;In-center环境下，同一个机柜的延迟&amp;lt;1us，同机房则是ms级别。无线环境大概是20ms，跨洋连接则至少100ms。在解决带宽问题后，延迟就成为最大瓶颈。这篇论文提出的DGA(Delayed Gradient Aggregation)算法的核心思路是延迟梯度平均到未来的某个迭代，即模型更新时接收过时的平均梯度，从而允许通信和计算流水线化。论文将问题形式化为：最小化随机函数的和。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/DGA.png&#34; alt=&#34;DGA&#34;&gt;&lt;/p&gt;
&lt;p&gt;N表示client数量，fi表示client i的stochastic损失函数。随机变量ζi关联一个mini-batch样本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/DGA2.png&#34; alt=&#34;DGA2&#34;&gt;&lt;/p&gt;
&lt;p&gt;算法的主要思路是允许averaging通信过程中做本地更新（averaging通信和本地更新并行，）。FedAvg中clients在每轮结束发送参数到彼此，等averaging结束再开启下一轮。DGA里把averaging barrier延迟到了后续迭代(iteration，指的是本地更新的迭代)。因此clients可以立刻开启下一轮(round，指的是最外层循环，即一轮更新)。第一轮下收到外部信息时迭代已经发生了D次，延迟了D个迭代后进行梯度修正。理想情况下不存在通信延迟，D=0时，DGA恢复成最初的FedAvg。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;clients在第t轮彼此发更新。&lt;/li&gt;
&lt;li&gt;clients在本地更新后继续用最新的本地参数继续本地更新。(averaging通信延迟 &amp;gt; 单次甚至若干次本地更新)&lt;/li&gt;
&lt;li&gt;当其他client的第t轮信息到达，则client已进行了D次额外本地更新。&lt;/li&gt;
&lt;li&gt;将本地t轮梯度替换为接受到的平均梯度。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在最宽泛的场景下（延迟极高），延迟梯度可能要几个轮次之后才能抵达。这就需要将延迟参数D表示为D = sK + r，其中s&amp;gt;=0, r &amp;lt;=K。DGA仍能保证不同client只在最近D个梯度上是不同的，t-D轮之前的梯度都是一样的。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>On Transparency</title>
      <link>https://cmbbq.github.io/posts/on-transparency/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-transparency/</guid>
      <description>透明度是软件工程中长期被忽略的理想属性，是目前互联网行业技术管理体系中的薄弱环节。本文的透明度主要是指组件内部实现对研发团队的可见度、可解释性和可掌控程度，或者说白盒指数。
一个项目自上而下依赖的第三方黑盒越少，各个抽象层次上诸多组件对研发团队就越是透明的。高透明度意味着高度可维护，高度hackable，随时可拆卸，定位到任何抽象层次上存在性能瓶颈都能毫无桎梏地zoom in，然后修改、重构。习惯性地使用第三方依赖，哪怕这个依赖是得到广泛应用，甚至接近行业标准的高声望项目，依然会注入实现需求不完全匹配的风险。
高性能计算的一个重要启发式设计原则(heuristic)即特化(specialization)，特化显然包括软件特化——基于新的真实需求创造最直接贴合需求的新解决方案(direct solutions to the needs)，即造轮。
国内互联网同行们对造轮持过分谨慎态度，将其视为anti-pattern，遇到问题时立刻开始技术选型，对使用第三方依赖形成了不假思索的路径依赖。缺乏洞察问题的新颖性和独特性的敏锐度和饥渴，自然故步自封于技术选型，而无法做到真正的技术创新——计算系统的创新是根植于真实需求，对非结构化的现实需求施加结构，创造新的计算和存储形态。就以全文检索为例，美团外卖搜索用ES，微信聊天搜索用SQLite，都是没有选择自研高性能检索引擎，导致业务场景和经典解决方案出现错配，后期遇到了性能瓶颈后的所谓优化方案也只不过是对第三方库进行魔改，让它更贴合原本的需求罢了。比如lucene压根就不是in-memory索引，以至于外卖店家商品搜索这么小规模的场景都不能保证实时性。再说SQLite，默认当然不支持拆表+并行搜索，针对大库做简单的拆表和scatter-gather并行搜索提升性能其实是理所当然的解决方案。
如此视造轮为畏途实让人唏嘘，在性能上碰壁后，再钻进别人的故纸堆中修修补补，浪费的精力，产生额外的痛苦，远远超过当初用C++/Rust实现一个小而美的in-memory search index(例如pisa)。自研检索引擎用极少的代码量就可以击败lucene、sqlite、postgres，也可以很轻松地支持多线程实时索引更新，而不必牺牲索引性能，还可以针对现代物理机进行深度cache优化、减少核间通信和远端内存访问，将现代硬件的性能在检索这种访存密集应用上发挥到极致。</description>
      <content>&lt;p&gt;透明度是软件工程中长期被忽略的理想属性，是目前互联网行业技术管理体系中的薄弱环节。本文的透明度主要是指组件内部实现对研发团队的可见度、可解释性和可掌控程度，或者说&lt;a href=&#34;https://en.wikipedia.org/wiki/White_box_(software_engineering)&#34;&gt;白盒&lt;/a&gt;指数。&lt;/p&gt;
&lt;p&gt;一个项目自上而下依赖的第三方黑盒越少，各个抽象层次上诸多组件对研发团队就越是透明的。高透明度意味着高度可维护，高度hackable，随时可拆卸，定位到任何抽象层次上存在性能瓶颈都能毫无桎梏地zoom in，然后修改、重构。习惯性地使用第三方依赖，哪怕这个依赖是得到广泛应用，甚至接近行业标准的高声望项目，依然会注入实现需求不完全匹配的风险。&lt;/p&gt;
&lt;p&gt;高性能计算的一个重要启发式设计原则(heuristic)即特化(specialization)，特化显然包括软件特化——基于新的真实需求创造最直接贴合需求的新解决方案(direct solutions to the needs)，即造轮。&lt;/p&gt;
&lt;p&gt;国内互联网同行们对造轮持过分谨慎态度，将其视为anti-pattern，遇到问题时立刻开始技术选型，对使用第三方依赖形成了不假思索的路径依赖。缺乏洞察问题的新颖性和独特性的敏锐度和饥渴，自然故步自封于技术选型，而无法做到真正的技术创新——计算系统的创新是根植于真实需求，对非结构化的现实需求施加结构，创造新的计算和存储形态。就以全文检索为例，&lt;a href=&#34;https://tech.meituan.com/2022/11/17/elasicsearch-optimization-practice-based-on-run-length-encoding.html&#34;&gt;美团外卖搜索用ES&lt;/a&gt;，&lt;a href=&#34;https://zhuanlan.zhihu.com/p/608082104&#34;&gt;微信聊天搜索用SQLite&lt;/a&gt;，都是没有选择自研高性能检索引擎，导致业务场景和经典解决方案出现错配，后期遇到了性能瓶颈后的所谓优化方案也只不过是对第三方库进行魔改，让它更贴合原本的需求罢了。比如lucene压根就不是in-memory索引，以至于外卖店家商品搜索这么小规模的场景都不能保证实时性。再说SQLite，默认当然不支持拆表+并行搜索，针对大库做简单的拆表和scatter-gather并行搜索提升性能其实是理所当然的解决方案。&lt;/p&gt;
&lt;p&gt;如此视造轮为畏途实让人唏嘘，在性能上碰壁后，再钻进别人的故纸堆中修修补补，浪费的精力，产生额外的痛苦，远远超过当初用C++/Rust实现一个小而美的in-memory search index(例如&lt;a href=&#34;https://github.com/pisa-engine/pisa&#34;&gt;pisa&lt;/a&gt;)。自研检索引擎用极少的代码量就可以击败lucene、sqlite、postgres，也可以很轻松地支持多线程实时索引更新，而不必牺牲索引性能，还可以针对现代物理机进行深度cache优化、减少核间通信和远端内存访问，将现代硬件的性能在检索这种访存密集应用上发挥到极致。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>String Lookups Reduce to Parsing</title>
      <link>https://cmbbq.github.io/posts/string-lookups-could-reduce-to-parsing/</link>
      <pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/string-lookups-could-reduce-to-parsing/</guid>
      <description>标题即结论：字符串查找问题可归约为解析问题。
这个结论源于近期一个有趣的观察：用ragel写高性能的ascii protocol parser，本质上是利用nfa转dfa提升性能，这和toplingdb中将同一层各个sst对应的trie（本质是dfa）合并成一个dfa(大量dfa-&amp;gt;nfa-&amp;gt;1dfa)的思路是同构的。
上述同构隐隐蕴含着一个reduction：字符串查找和字符串解析，本质都用尽可能紧凑的结构和高效的算法从字符流中抽取状态，lookups可以视作一类特殊（且模式相当规则）的parsing。LSM key lookup更是比较特殊的海量索引的key range无overlap的场景，存在大量可以轻松合并的DFA。因此龙书中的大量DFA转NFA转DFA算法可以派上用场。
KV Store的in-memory key index，可以是红黑树，可以是skiplist，可以是hashmap，可以说patricia trie(一种radix tree变种)，也可以是toplingdb中的NestLoudsTrie。这种结构我们同样可以在路由表实现中看到。字符串索引，归根到底是字符串lookup结构。正如路由表实现可以通过把所有routes合并到一个DFA里（很多routes都包含regex），kv数据库也把Trie这种特殊的dfa（Trie的状态转移图是树，树是一种无向图，多了个任意两结点只由一个边连接的约束）做多索引合并，每个索引对应的key range还不重叠（LSM特性），因此合并速度非常快，合并后的DFA表示起来也简单、紧凑，详见自动机算法在数据库索引中的应用，我在作者的文章下面追问了一下DFA合并的触发条件和DFA合并开销，作者的答复是compaction/flush时触发，在整个lsm更新过程中占比很小，也不涉及多线程，无需考虑线程安全。</description>
      <content>&lt;p&gt;标题即结论：字符串查找问题可归约为解析问题。&lt;/p&gt;
&lt;p&gt;这个结论源于近期一个有趣的观察：用ragel写高性能的ascii protocol parser，本质上是利用&lt;a href=&#34;https://en.wikipedia.org/wiki/Nondeterministic_finite_automata&#34;&gt;nfa&lt;/a&gt;转&lt;a href=&#34;https://en.wikipedia.org/wiki/Deterministic_finite_automaton&#34;&gt;dfa&lt;/a&gt;提升性能，这和&lt;a href=&#34;https://github.com/topling/toplingdb&#34;&gt;toplingdb&lt;/a&gt;中将同一层各个sst对应的trie（本质是dfa）合并成一个dfa(大量dfa-&amp;gt;nfa-&amp;gt;1dfa)的思路是同构的。&lt;/p&gt;
&lt;p&gt;上述同构隐隐蕴含着一个reduction：字符串查找和字符串解析，本质都用尽可能紧凑的结构和高效的算法从字符流中抽取状态，lookups可以视作一类特殊（且模式相当规则）的parsing。LSM key lookup更是比较特殊的海量索引的key range无overlap的场景，存在大量可以轻松合并的DFA。因此龙书中的大量DFA转NFA转DFA算法可以派上用场。&lt;/p&gt;
&lt;p&gt;KV Store的in-memory key index，可以是红黑树，可以是skiplist，可以是hashmap，可以说patricia trie(一种&lt;a href=&#34;https://en.wikipedia.org/wiki/Radix_tree&#34;&gt;radix tree&lt;/a&gt;变种)，也可以是toplingdb中的NestLoudsTrie。这种结构我们同样可以在路由表实现中看到。字符串索引，归根到底是字符串lookup结构。正如路由表实现可以通过把所有routes合并到一个DFA里（很多routes都包含regex），kv数据库也把Trie这种特殊的dfa（Trie的状态转移图是树，树是一种无向图，多了个任意两结点只由一个边连接的约束）做多索引合并，每个索引对应的key range还不重叠（LSM特性），因此合并速度非常快，合并后的DFA表示起来也简单、紧凑，详见&lt;a href=&#34;https://zhuanlan.zhihu.com/p/628057993&#34;&gt;自动机算法在数据库索引中的应用&lt;/a&gt;，我在作者的文章下面追问了一下DFA合并的触发条件和DFA合并开销，作者的答复是compaction/flush时触发，在整个lsm更新过程中占比很小，也不涉及多线程，无需考虑线程安全。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Paradigms of Generic Programming: Archetype, Ducktype, Subtype</title>
      <link>https://cmbbq.github.io/posts/paradigms-of-generic-programming/</link>
      <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/paradigms-of-generic-programming/</guid>
      <description>泛型不等价于模板 什么是泛型编程？提起generic programming，很多人都会立刻想到template，但模板编程只是泛型编程的一种范式。最初发明&amp;quot;generic programming&amp;quot;这种说法的Alexander Stepanov反复强调过： generic programming is not about how to use template。第一个版本的STL尽管名字里带template，实际上是基于scheme实现的。
要泛型，就必须有规约 规约的制定和对规约的遵循是泛型编程的基础。 任何泛型编程都需要一定规约，有规约才能让一种抽象适配多个具体实体。没有规约，那写具体实现的人都不知道遵循什么规范，实现什么接口，满足什么条件，泛型编程自然就无从谈起。
规约：在JavaScript里是prototype；在Swift里是protocol；在Rust里是trait；在C++模板编程里是concept或者SFINAE。
遵循规约：在JavaScript里是运行时将具体对象关联到某个原型对象；在Swift里是用类似继承的冒号让具体类型conforms to某些protocol；在Rust里是用impl SomeTrait for SomeStruct语法显式地为某个类型提供某个trait的实现；在C++里模板实参无需特殊语法声明遵循模板形参，但要心照不宣地遵循模板代码的要求。
三种规约，三种范式 根据规约不同，可命名泛型编程的三种范式：Archetype, Ducktype, Subtype。
Archetype：以类型的类型(type-of-type)或者说协议(protocol)、接口(interface)为规约。
Rust trait: &amp;ldquo;defines shared behavior&amp;rdquo; Carbon interface: &amp;ldquo;defines an API that a given type can implement&amp;rdquo; Swift protocol: &amp;ldquo;defines a blueprint of methods, properties, and other requirements&amp;rdquo; C++ type erasure idiom: &amp;ldquo;captures the concept shared among all the concrete types&amp;rdquo; Ducktype：基于模板进行文本替换的结构化规约。
模板本质上就是编译期ducktyping，不能提前独立进行类型和语法检查，只有实例化之后才能做类型和语法检查，能鸭子叫，编译通过，叫不出鸭子叫，编译报错。 C++20中模板结构化规约是concept, 此前则是SFINAE技巧，或者干脆不成文：要么因循旧例(iterable的T一定要有begin和end)，要么心照不宣（自己写的代码，只有自己懂，无需对外公开规约）。 由于规约本身并非类型，无法用普通容器存储遵循规约的一系列具体类型，只能用类型推导的tuple-like容器。 Subtype: 以基类为规约。</description>
      <content>&lt;h2 id=&#34;泛型不等价于模板&#34;&gt;泛型不等价于模板&lt;/h2&gt;
&lt;p&gt;什么是泛型编程？提起generic programming，很多人都会立刻想到template，但模板编程只是泛型编程的一种范式。最初发明&amp;quot;generic programming&amp;quot;这种说法的Alexander Stepanov反复强调过：
generic programming is not about how to use template。第一个版本的STL尽管名字里带template，实际上是基于scheme实现的。&lt;/p&gt;
&lt;h2 id=&#34;要泛型就必须有规约&#34;&gt;要泛型，就必须有规约&lt;/h2&gt;
&lt;p&gt;规约的制定和对规约的遵循是泛型编程的基础。
任何泛型编程都需要一定规约，有规约才能让一种抽象适配多个具体实体。没有规约，那写具体实现的人都不知道遵循什么规范，实现什么接口，满足什么条件，泛型编程自然就无从谈起。&lt;/p&gt;
&lt;p&gt;规约：在JavaScript里是prototype；在Swift里是protocol；在Rust里是trait；在C++模板编程里是concept或者SFINAE。&lt;/p&gt;
&lt;p&gt;遵循规约：在JavaScript里是运行时将具体对象关联到某个原型对象；在Swift里是用类似继承的冒号让具体类型conforms to某些protocol；在Rust里是用impl SomeTrait for SomeStruct语法显式地为某个类型提供某个trait的实现；在C++里模板实参无需特殊语法声明遵循模板形参，但要心照不宣地遵循模板代码的要求。&lt;/p&gt;
&lt;h2 id=&#34;三种规约三种范式&#34;&gt;三种规约，三种范式&lt;/h2&gt;
&lt;p&gt;根据规约不同，可命名泛型编程的三种范式：Archetype, Ducktype, Subtype。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Archetype&lt;/strong&gt;：以类型的类型(type-of-type)或者说协议(protocol)、接口(interface)为规约。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://doc.rust-lang.org/book/ch10-02-traits.html&#34;&gt;Rust trait&lt;/a&gt;: &amp;ldquo;defines shared behavior&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/generics/terminology.md#interface&#34;&gt;Carbon interface&lt;/a&gt;: &amp;ldquo;defines an API that a given type can implement&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.swift.org/swift-book/documentation/the-swift-programming-language/protocols/&#34;&gt;Swift protocol&lt;/a&gt;: &amp;ldquo;defines a blueprint of methods, properties, and other requirements&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://davekilian.com/cpp-type-erasure.html&#34;&gt;C++ type erasure idiom&lt;/a&gt;: &amp;ldquo;captures the concept shared among all the concrete types&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ducktype&lt;/strong&gt;：基于模板进行文本替换的结构化规约。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模板本质上就是编译期ducktyping，不能提前独立进行类型和语法检查，只有实例化之后才能做类型和语法检查，能鸭子叫，编译通过，叫不出鸭子叫，编译报错。&lt;/li&gt;
&lt;li&gt;C++20中模板结构化规约是concept, 此前则是SFINAE技巧，或者干脆不成文：要么因循旧例(iterable的T一定要有begin和end)，要么心照不宣（自己写的代码，只有自己懂，无需对外公开规约）。&lt;/li&gt;
&lt;li&gt;由于规约本身并非类型，无法用普通容器存储遵循规约的一系列具体类型，只能用类型推导的tuple-like容器。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Subtype&lt;/strong&gt;: 以基类为规约。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;子类系统是面向对象语言最普及的泛型编程范式，往往基于class hierarchy +『虚表存放函数指针』+『对象内置虚指针』或『callsite胖指针』实现。&lt;/li&gt;
&lt;li&gt;子类系统固然是简单自然的多态，但毕竟subtyping有一丢丢的性能开销，不是零成本抽象，而且难以非侵入式地让一个新接口适配已有代码。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本节命名的三种范式名称恰好都以type结尾，一方面是因为这样比较帅，另一方面是因为（对C++/Rust这种抽象层次的语言来说）编程本身就是在打造一个个类型，泛型编程就是在打造一个个类型规约+遵循规约的类型。Archetype和Subtype范式中，类型规约恰好就是一种抽象类型，所以这两种范式写起来更简单自然。Ducktype范式（C++模板）中，类型规约是结构化规约，可以是语言实体(concept)，也可以心照不宣，无论如何都不是类型。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Paradigms&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Archetype&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Ducktype&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Subtype&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;具体语言实例&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Swift protocol, Carbon interface, Rust trait&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;C++ template(constrained or not), Rust generic&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;C++/Java/Python class hierarchy&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;在哪里写泛型代码？&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;泛型函数&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;函数模板&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;子类方法&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;承载泛型代码的语言实体&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;普通函数，只不过是以规约类型为参数&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;模板文本&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;普通函数，不过函数指针被语言特性暗中与callsite指针绑定了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;规约&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;定义一些类的方法或属性共性的协议类型&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;模板参数应符合的一组需求闭集，可以是成文约束Concept或SFINAE，也可以是不成文约束&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;面向对象语言继承图中的某个基类的虚函数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;承载规约的语言实体&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;类，类型擦除之后的共性类，类型的类型，本质上仍然是类型&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;对文本替换的placeholder的结构化约束。即使约束了，我们也无法对placeholder做类型/语法检查&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;被基类列入必要功能列表的函数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;遵循规约的语言实体&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;具体的类&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;用于模板实例化的模板参数&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;子类&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;何时做name lookup决议？&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;早绑定，允许独立编译&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;晚绑定，实例化时，不用的话就一直不绑定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;早绑定，允许独立编译&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;何时做类型检查？&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;独立编译时&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;实例化后&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;独立编译时&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;是否允许支持动态绑定？&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;是&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;否&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;是&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;如何将已有泛型接口在新的类型上进行扩展？&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;允许基于已有类型创造一个数据表示兼容的新类型，只不过规约变了，允许它实现某个新的规约，或提供与原类型的已实现的某个规约提供不同的实现。&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;为新偏特化场景写新的函数模板即可扩展，可用Concept或SFINAE修订重载决议规则。也可以在函数模板里用某个约定好的函数名、成员变量名、关联类型作为客制点，新的类型只需实现这些客制点。&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;继承，子类数据表示可能会变&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;archetype范式&#34;&gt;Archetype范式&lt;/h2&gt;
&lt;p&gt;Archetype在Rust中是语言的核心机制——trait，要写好Rust的泛型代码，就要习惯于基于archetype编程。相比与DuckType、SubType，Archetype范式天然有一些优势。&lt;/p&gt;
&lt;h3 id=&#34;generic-code-is-just-normal-code&#34;&gt;Generic code is just normal code&lt;/h3&gt;
&lt;p&gt;和模板相比，&amp;ldquo;Generic code is just normal code&amp;quot;是Archetype范式最大优势，也是Rust语言相比C++的巨大优势。让非专家也能写出高度抽象，同时还零成本，具有高度可复用性的泛型代码。&lt;/p&gt;
&lt;p&gt;C++基于模板文本替换的泛型代码和普通类型的具体实现代码，在写法上有一定区别，编译链接也有区别。
这是因为C++中基于concept模板，其实是没办法提前编译、提前语法检查，只能在实例化之后再做语法检查。所以是一种难度相当高的编程范式，能用模板写C++库的一般是业界专家，普通人很难掌握，也没有足够动机去掌握。&lt;/p&gt;
&lt;p&gt;Rust基于trait的泛型代码则和普通类型的具体实现代码，在写法上基本没区别，编译链接方式也一样。
这是因为Rust中的trait是一种archetype，type-of-type，是规定一组类型应该具备什么接口的元类型，无论它多么抽象多么特殊，究其本质仍是一种类型。只要是类型，就可以单独提前编译，就可以被提前语法检查。&lt;/p&gt;
&lt;h3 id=&#34;adapting-erases-interoperability&#34;&gt;Adapting Erases Interoperability&lt;/h3&gt;
&lt;p&gt;和继承相比，&amp;ldquo;Adapting rather than extending a type&amp;quot;是Archetype范式的优势之一。
Subtype范式在实践中不能保证所有类型继承自同一个基类，比如说对第三方的代码没有控制权，或者说这个类型不是个class，而是int, float这种内置类型。
Archetype范式中不仅可以为自己的类型提供多种Archetype adaption，或使自己的代码遵循第三方Archetype，还可以为第三方类型提供自己的Archetype实现——前两点还好，这最后一点是Subtype范式做不到的，只能加个丑陋的wrapper，不仅工作量特别大，而且容易出错。&lt;/p&gt;
&lt;p&gt;有人会问，允许修改已有的类型是不是比较危险？这是一种误解。继承是修改，因此是危险的。Adapt（或者说override，newtype）其实不是修改，而是新增。
继承改变了类型的数据表示，Adapt机制则不改变类型的数据表示，只为其新增接口——换一种说法，override Archetype for T机制实际上是为已有类型T新建了一个遵循规约Archetype的入口。&lt;/p&gt;
&lt;h3 id=&#34;archetypes-in-c&#34;&gt;Archetypes in C++&lt;/h3&gt;
&lt;p&gt;有些人会argue，C++无所不能，的确C++也可以实现archetype范式，比如std::function，以及其他类型擦除。但是基于现有语法写出来的泛型代码和普通代码之间还是没那么像。One has to drastically change the programming style in order to &amp;ldquo;go generic&amp;rdquo;. 实现archetype范式在C++中相对困难。但即使如此，archetype范式的固有优势还是让某些标准或准标准选择了它，比如std::function, std::any, boost::any_range。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>On NCO</title>
      <link>https://cmbbq.github.io/posts/on-nco/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-nco/</guid>
      <description>凸优化收敛时间一般是polynomial的，线性规划和最小二乘就是凸优化的特例。
非凸优化non-convex optimization是一种至少np-hard的问题，不存在通用解法。想要确定问题是否有解，局部最优是否全局最优，或目标函数是否有界都会随着变量和约束数目指数爆炸blow up，局部优化手段对算法参数敏感，又高度依赖initial guess，这使局部非凸优化more-art-than-technology，相比而言线性规划是毫无art可言的。
深度神经网络作为通用函数拟合器，最重要的作用是拟合非凸函数，因为复杂问题一般不可以用凸函数拟合。ChatGPT这类生成式模型就是对target和input间互信息的非凸优化。怎么训好模型，目前依然是一个art。
随机梯度下降stochastic gradient descent(SGD)被证明可以收敛于凸函数、可微和利普希茨连续函数，但还不能确定在非凸函数上的效果，SGD收敛缓慢，还不一定达到局部最优，更不一定达到全局最优。如果选择一个足够靠近全局最优的点，或许可以用SGD收敛到全局最优，但这一方面耗时间，另一方面只适用于特殊场景。对于深度神经网络来说，一旦陷入错误的局部最优，就要用不同的初始化配置或加入额外的梯度更新噪音。如果遇到鞍点，则需找到海森矩阵或计算下降方向。如果陷入低梯度区域，则需batchnorm，或使用relu做激活函数。如果因高曲率而使得steps过大，则应使用adaptive step size或限制梯度step尺度。此外，如果超参有问题，还需要用各种超参优化的方法。总之，目前深度学习的NCO还是处于art的阶段。</description>
      <content>&lt;p&gt;凸优化收敛时间一般是polynomial的，线性规划和最小二乘就是凸优化的特例。&lt;/p&gt;
&lt;p&gt;非凸优化non-convex optimization是一种至少np-hard的问题，不存在通用解法。想要确定问题是否有解，局部最优是否全局最优，或目标函数是否有界都会随着变量和约束数目指数爆炸blow up，局部优化手段对算法参数敏感，又高度依赖initial guess，这使局部非凸优化more-art-than-technology，相比而言线性规划是毫无art可言的。&lt;/p&gt;
&lt;p&gt;深度神经网络作为通用函数拟合器，最重要的作用是拟合非凸函数，因为复杂问题一般不可以用凸函数拟合。ChatGPT这类生成式模型就是对target和input间互信息的非凸优化。怎么训好模型，目前依然是一个art。&lt;/p&gt;
&lt;p&gt;随机梯度下降stochastic gradient descent(SGD)被证明可以收敛于凸函数、可微和利普希茨连续函数，但还不能确定在非凸函数上的效果，SGD收敛缓慢，还不一定达到局部最优，更不一定达到全局最优。如果选择一个足够靠近全局最优的点，或许可以用SGD收敛到全局最优，但这一方面耗时间，另一方面只适用于特殊场景。对于深度神经网络来说，一旦陷入错误的局部最优，就要用不同的初始化配置或加入额外的梯度更新噪音。如果遇到鞍点，则需找到海森矩阵或计算下降方向。如果陷入低梯度区域，则需batchnorm，或使用relu做激活函数。如果因高曲率而使得steps过大，则应使用adaptive step size或限制梯度step尺度。此外，如果超参有问题，还需要用各种超参优化的方法。总之，目前深度学习的NCO还是处于art的阶段。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>A Decade of Tussle between CPU and GPU</title>
      <link>https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/</guid>
      <description>GPU和CPU方法的边界何在？ 做AI应用的架构工作，遇到新的计算密集任务的第一问往往是：这应该上GPU，还是NUMA物理机？
这个问题可以归约为On the Limits of GPU Acceleration(2010)中提出的问题：&amp;ldquo;大致相同能耗的前提下，能和不能用GPU有效加速计算的边界在哪？&amp;rdquo; Vuduc的这个研究分析了3种具有代表性复杂行为和访存不规则性的计算：稀疏线性系统的迭代求解器、稀疏矩阵的乔列斯基分解、快速多极子算法，得出的结论是——大致地说，良好优化的GPU实现在相同能耗下和良好优化的CPU实现在性能上是相仿的。可见当年用GPGPU加速还是一个普遍得不偿失的工作，毕竟要付出剧烈变更编程模型的代价。
GPU和CPU的价格-性能趋势 上述结论在2010年成立，如今13年过去了，GPU固然飞速发展（无论是硬件、软件，还是生态、应用、资金投入），但GPU发展速度是否已经甩开CPU了呢？
摩尔定律是两年翻倍，而黄氏定律则是宣称通过软硬件协同能达到1.08年翻倍。
甚至如果我们考虑成本因素，根据经验数据，GPU FLOP/s每刀的增长速度是和CPU相仿的。这和2023年工业界的经验是吻合的——CPU在大多数应用语境下依然是成本更低的选择。</description>
      <content>&lt;h2 id=&#34;gpu和cpu方法的边界何在&#34;&gt;GPU和CPU方法的边界何在？&lt;/h2&gt;
&lt;p&gt;做AI应用的架构工作，遇到新的计算密集任务的第一问往往是：这应该上GPU，还是NUMA物理机？&lt;/p&gt;
&lt;p&gt;这个问题可以归约为&lt;a href=&#34;https://www.usenix.org/legacy/event/hotpar10/tech/full_papers/main.pdf&#34;&gt;On the Limits of GPU Acceleration(2010)&lt;/a&gt;中提出的问题：&amp;ldquo;大致相同能耗的前提下，能和不能用GPU有效加速计算的边界在哪？&amp;rdquo; Vuduc的这个研究分析了3种具有代表性复杂行为和访存不规则性的计算：稀疏线性系统的迭代求解器、稀疏矩阵的乔列斯基分解、快速多极子算法，得出的结论是——大致地说，良好优化的GPU实现在相同能耗下和良好优化的CPU实现在性能上是相仿的。可见当年用GPGPU加速还是一个普遍得不偿失的工作，毕竟要付出剧烈变更编程模型的代价。&lt;/p&gt;
&lt;h2 id=&#34;gpu和cpu的价格-性能趋势&#34;&gt;GPU和CPU的价格-性能趋势&lt;/h2&gt;
&lt;p&gt;上述结论在2010年成立，如今13年过去了，GPU固然飞速发展（无论是硬件、软件，还是生态、应用、资金投入），但GPU发展速度是否已经甩开CPU了呢？&lt;/p&gt;
&lt;p&gt;摩尔定律是两年翻倍，而&lt;a href=&#34;https://en.wikipedia.org/wiki/Huang%27s_law&#34;&gt;黄氏定律&lt;/a&gt;则是宣称通过软硬件协同能达到1.08年翻倍。&lt;/p&gt;
&lt;p&gt;甚至如果我们考虑成本因素，根据&lt;a href=&#34;https://epochai.org/blog/trends-in-gpu-price-performance&#34;&gt;经验数据&lt;/a&gt;，GPU FLOP/s每刀的增长速度是和CPU相仿的。这和2023年工业界的经验是吻合的——CPU在大多数应用语境下依然是成本更低的选择。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/4.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar1&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/5.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar2&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/6.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar3&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>On ABI</title>
      <link>https://cmbbq.github.io/posts/on-abi/</link>
      <pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-abi/</guid>
      <description>前注：ABI在本文中特指系统语言的ABI，这里系统语言，即system programming language，指的是C，C++，Rust这样应用于系统编程的的编译语言。有时候我们也会讨论常用库或基础库的ABI，比如gcc5就把std::basic_string和std::list的实现改了，自然也就影响了跨版本的ABI兼容性，这种层面的ABI兼容性虽然也是一个工程上的疑难杂症，但不在本文的讨论范围内，毕竟源码都不同了。本文讨论ABI兼容性的是同一份源码在不同ISA、操作系统、编译器上对应二进制产物的兼容性。
系统语言的ABI、应用层软件的API、微处理器架构的ISA都是描述各自抽象层次互操作能力的接口。
ABI的独特之处在于它本身就是繁重的机制实现，而非轻量的协议声明——实现和声明之间有概念上的分野，这已经体现在法律上：复制ABI是赤裸裸的抄袭，而复制API或ISA则属于fair use。你显然可以合法地为他人写的API/ISA出书做注，Google实现自己的Java时合法复制了Oracle的Java API，无需license。
ABI之所以复杂，就是因为它处于两个抽象层次之间。ABI中的很大一部分内容既要向上对语言标准负责，向下要对ISA负责，必须遵循两个方向的约束，将复杂度留给自身，从而成就语言标准和ISA这两个抽象契约的简洁、干净，和人类友好。
ABI中的重要组成部分之一calling convention受到ISA的约束，i386中用cdecl, stdcall, fastcall, vectorcall, thiscall, amd64中用systemv, msnative, vectorcall，arm32用aapcs，arm64用aapcs64。ABI中也有很大一部分是指令集无关的（ISA-agnostic），比如name mangling、class layout。这些机制往往足够底层，又与其他指令集相关的部分有强耦合，加上历史因素，往往也只有一小部分能被写入语言标准。
如果ISA差异足够大，维护不同的ABI无疑是必要的，强行用一套ABI兼容不仅不自然，也不高效。以早期的Windows为例，微软就为早先的i386（IA-32）和后来的Intel IA-64提供了两套ABI。再后来，AMD赢得64位战争，Intel也遵循了前向兼容IA-32的amd64，又名x86_64, x64，IA-64就式微了。
ABI以ISA提供的指令、寄存器、内存管理能力为构件（building block），详尽且精确地描述如何实现一个系统语言在特定硬件、操作系统、编译器下的执行模型，并允许分开编译的产物之间能互操作。例如C ABI需定义调用约定（calling convention）、基础数据类型的表示、聚合数据类型的内存布局；C++03 ABI需额外定义异常机制、RTTI信息存储、虚表布局和动态绑定机制、重载函数/运算符/模板实例化所需的名字重置（name mangling）；C++11 ABI需进一步定义lambda的实现、自动类型推导机制等新增语言机制。
无论是C，还是C++都没有在语言层面制定官方的ABI标准，毕竟ABI标准本身也限制了实现的自由。目前最主流的Itanium C++ ABI号称被许多操作系统采用，能适配多数微处理器架构，被大多数编译器实现，包括gcc和clang这两个主要玩家。但值得注意的是，被多方支持，不意味着多方支持的是完全相同的东西。同样打着Itanium ABI名号，clang在arm32 linux上编译的C++库能给amd64 windows上的程序使用吗？显然不行，因为最基础的calling convention，甚至基础数据类型表示都不同。Itanium C++ ABI即使实现大一统，积极意义也仅限于允许一些依赖CPU无关规则的危险技巧（比如修改vtable，这种操作我们现在只能称之为hack）在相当多的环境下通用罢了——只要C++语言标准不将ABI纳入讨论范围，对ABI做出假设的技巧永远是危险的，想要创造新的语言机制，就必须在C++标准层面推进某种ABI共识。
那么系统语言是否应该对某种基于特定ABI实现的语言特性进行标准化呢？从C++的发展历史来看，这种做法已经有了先例，而且是相当危险冒失的。将非零抽象开销的dynamic exception、rtti引入C++客观上导致了社区分裂，有相当一部分C++使用者至今依然选择-fno-exeptions或-fno-rtti。近期的提案Zero-Overhead Deterministic Exceptions: Catching Values给出了一个对exception ABI进行改动的零开销异常机制，是对历史错误的亡羊补牢。系统语言的语言标准应审慎地只对ISA无关且零抽象开销的ABI规则进行标准化，以便在此基础上创造新的语言机制或为应用层开发提供便利。</description>
      <content>&lt;p&gt;前注：ABI在本文中特指系统语言的ABI，这里系统语言，即system programming language，指的是C，C++，Rust这样应用于系统编程的的编译语言。有时候我们也会讨论常用库或基础库的ABI，比如gcc5就把std::basic_string和std::list的实现改了，自然也就影响了跨版本的ABI兼容性，这种层面的ABI兼容性虽然也是一个工程上的疑难杂症，但不在本文的讨论范围内，毕竟源码都不同了。本文讨论ABI兼容性的是同一份源码在不同ISA、操作系统、编译器上对应二进制产物的兼容性。&lt;/p&gt;
&lt;p&gt;系统语言的ABI、应用层软件的API、微处理器架构的ISA都是描述各自抽象层次互操作能力的接口。&lt;/p&gt;
&lt;p&gt;ABI的独特之处在于它本身就是繁重的机制实现，而非轻量的协议声明——实现和声明之间有概念上的分野，这已经体现在法律上：复制ABI是赤裸裸的抄袭，而复制API或ISA则属于fair use。你显然可以合法地为他人写的API/ISA出书做注，Google实现自己的Java时合法复制了Oracle的Java API，无需license。&lt;/p&gt;
&lt;p&gt;ABI之所以复杂，就是因为它处于两个抽象层次之间。ABI中的很大一部分内容既要向上对语言标准负责，向下要对ISA负责，必须遵循两个方向的约束，将复杂度留给自身，从而成就语言标准和ISA这两个抽象契约的简洁、干净，和人类友好。&lt;/p&gt;
&lt;p&gt;ABI中的重要组成部分之一calling convention受到ISA的约束，i386中用cdecl, stdcall, fastcall, vectorcall, thiscall, amd64中用systemv, msnative, vectorcall，arm32用aapcs，arm64用aapcs64。ABI中也有很大一部分是指令集无关的（ISA-agnostic），比如name mangling、class layout。这些机制往往足够底层，又与其他指令集相关的部分有强耦合，加上历史因素，往往也只有一小部分能被写入语言标准。&lt;/p&gt;
&lt;p&gt;如果ISA差异足够大，维护不同的ABI无疑是必要的，强行用一套ABI兼容不仅不自然，也不高效。以早期的Windows为例，微软就为早先的i386（IA-32）和后来的Intel IA-64提供了两套ABI。再后来，AMD赢得64位战争，Intel也遵循了前向兼容IA-32的amd64，又名x86_64, x64，IA-64就式微了。&lt;/p&gt;
&lt;p&gt;ABI以ISA提供的指令、寄存器、内存管理能力为构件（building block），详尽且精确地描述如何实现一个系统语言在特定硬件、操作系统、编译器下的执行模型，并允许分开编译的产物之间能互操作。例如C ABI需定义调用约定（calling convention）、基础数据类型的表示、聚合数据类型的内存布局；C++03 ABI需额外定义异常机制、RTTI信息存储、虚表布局和动态绑定机制、重载函数/运算符/模板实例化所需的名字重置（name mangling）；C++11 ABI需进一步定义lambda的实现、自动类型推导机制等新增语言机制。&lt;/p&gt;
&lt;p&gt;无论是C，还是C++都没有在语言层面制定官方的ABI标准，毕竟ABI标准本身也限制了实现的自由。目前最主流的Itanium C++ ABI号称被许多操作系统采用，能适配多数微处理器架构，被大多数编译器实现，包括gcc和clang这两个主要玩家。但值得注意的是，被多方支持，不意味着多方支持的是完全相同的东西。同样打着Itanium ABI名号，clang在arm32 linux上编译的C++库能给amd64 windows上的程序使用吗？显然不行，因为最基础的calling convention，甚至基础数据类型表示都不同。Itanium C++ ABI即使实现大一统，积极意义也仅限于允许一些依赖CPU无关规则的危险技巧（比如修改vtable，这种操作我们现在只能称之为hack）在相当多的环境下通用罢了——只要C++语言标准不将ABI纳入讨论范围，对ABI做出假设的技巧永远是危险的，想要创造新的语言机制，就必须在C++标准层面推进某种ABI共识。&lt;/p&gt;
&lt;p&gt;那么系统语言是否应该对某种基于特定ABI实现的语言特性进行标准化呢？从C++的发展历史来看，这种做法已经有了先例，而且是相当危险冒失的。将非零抽象开销的dynamic exception、rtti引入C++客观上导致了社区分裂，有相当一部分C++使用者至今依然选择-fno-exeptions或-fno-rtti。近期的提案&lt;a href=&#34;https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2021/p2232r0.html&#34;&gt;Zero-Overhead Deterministic Exceptions: Catching Values&lt;/a&gt;给出了一个对exception ABI进行改动的零开销异常机制，是对历史错误的亡羊补牢。系统语言的语言标准应审慎地只对ISA无关且零抽象开销的ABI规则进行标准化，以便在此基础上创造新的语言机制或为应用层开发提供便利。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>A Taxonomy of Stateful Distributed Systems</title>
      <link>https://cmbbq.github.io/posts/a-taxonomy-of-stateful-distributed-systems/</link>
      <pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/a-taxonomy-of-stateful-distributed-systems/</guid>
      <description>CAP theorem的讨论范围是狭窄的 在分布式系统领域，CAP theorem广泛被引用，而且经常被用以指导超出它讨论边界的问题。
被形式化证明（见Brewer&amp;rsquo;s Conjecture and the Feasibility of Consistent, Available, Partition-​Tolerant Web Services）的CAP theorem其实只局限在read-write storage场景：一个只包含get、set(x)两种操作的存储系统，这种系统被称为register。
在异步网络（指的是消息传递时间无界）中实现register是无法同时满足下列属性的：
Availability：所有发往register的请求最终都能完成。这和大多数真实系统的定义是不同的，因为真实系统不要求100%完成请求，只要保证SLA够高，同时又往往有一定时间约束，超时就会返回timeout错误。 Consistency：所有读、写操作均是linearizable的：B操作在A操作之后成功执行，则B看到的系统状态不能比A完成时的系统状态更旧。 Partition tolerance：允许网络丢包。 Partition tolerance默认是满足的，因此CAP-availability、CAP-consistency两种属性二选一。 离开了形式化证明的场景后，CAP theorem还有指导意义吗？答案是否定的——除非重新定义availability和consistency，将其推广到更通用的场景：从单对象单操作到多对象多操作的事务系统。 重新定义availability和consistency后的分布式系统分类学虽然和CAP theorem非常类似，但不能称之为CAP theorem。
一致性 更通用的“一致性”应定义为“并发系统中共享状态更新的可见性”。 现代微处理器、分布式系统、数据库的共性是——它们都是存在共享数据的并发系统。 当我们讨论一致性（consistency）时，可能指的是微处理器架构和系统编程领域的一致性模型，也可能指的是分布式系统领域的副本一致性，还可能指的是数据库领域的事务隔离性。这些领域的抽象层次不同，共同点是讨论的系统都是并发系统。
一致性模型（consistency model）是用于描述微处理器架构领域的多核并发场景下，各个处理器被允许的乱序程度——乱序的约束越少，效率越高，并发程序正确性越难保证。
最强的strict consistency指的是任何写在任何时钟周期都立刻对任何处理器可见，显然不能推广到分布式系统领域。 次之的sequential consistency指的是写操作顺序对于每个副本而言都是一致的，即各进程内部的program order一致，而不同进程执行的顺序可以不一致。这个概念最初也是Lamport在讨论multi-processor computer如何正确执行并发程序时提出的。和分布式系统的副本一致性无关。C++中std::memory_order_seq_cst即可保证线程内部的program order。 更宽松的causal consistency指的是写操作中有依赖关系的那一部分的顺序是一致的，即各进程中的dependency order一致。现代CPU基本上都是out-of-order流水线，在保证dependency order这个底线后，能多乱序就多乱序。在C++中用std::memory_order_consume的load(A)和std::memory_order_acquire的store(B)配合，即可保证这个store之前所有写操作中load(A)依赖的那一部分对load(A)是可见的。如果每个依赖都保证Release-Consume ordering，则依赖链就有序，整体上可满足causal consistency。 除了上述几个著名模型外，还有几十个不同方法、领域中应用的一致性模型，下图就包含了非事务分布式存储系统中种种一致性模型（详见Consistency in Non-Transactional Distributed Storage Systems）。 并发程序显然可以很容易推广到分布式复制状态机，只是增加了网络延迟。因此，一致性模型可以推广应用到分布式系统中的副本一致性。以sequential consistency为例，增加了实时约束后就是分布式系统领域中更被广泛引用的linearizability，指的是单个被复制对象上的单个操作满足：A是写操作，B是对副本的读，A happened-before（因果律上的先于，见https://en.wikipedia.org/wiki/Happened-before） B，则A的写对B的读总是可见。比较一下C++的sequentially consistent ordering定义：everything that happened-before a store in one thread becomes a visible side effect in the thread that did a load。二者是一致的。</description>
      <content>&lt;h2 id=&#34;cap-theorem的讨论范围是狭窄的&#34;&gt;CAP theorem的讨论范围是狭窄的&lt;/h2&gt;
&lt;p&gt;在分布式系统领域，CAP theorem广泛被引用，而且经常被用以指导超出它讨论边界的问题。&lt;/p&gt;
&lt;p&gt;被形式化证明（见&lt;a href=&#34;https://users.ece.cmu.edu/~adrian/731-sp04/readings/GL-cap.pdf&#34;&gt;Brewer&amp;rsquo;s Conjecture and the Feasibility of Consistent, Available, Partition-​Tolerant Web Services&lt;/a&gt;）的CAP theorem其实只局限在read-write storage场景：一个只包含get、set(x)两种操作的存储系统，这种系统被称为register。&lt;/p&gt;
&lt;p&gt;在异步网络（指的是消息传递时间无界）中实现register是无法同时满足下列属性的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Availability：所有发往register的请求最终都能完成。这和大多数真实系统的定义是不同的，因为真实系统不要求100%完成请求，只要保证SLA够高，同时又往往有一定时间约束，超时就会返回timeout错误。&lt;/li&gt;
&lt;li&gt;Consistency：所有读、写操作均是linearizable的：B操作在A操作之后成功执行，则B看到的系统状态不能比A完成时的系统状态更旧。&lt;/li&gt;
&lt;li&gt;Partition tolerance：允许网络丢包。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Partition tolerance默认是满足的，因此CAP-availability、CAP-consistency两种属性二选一。
离开了形式化证明的场景后，CAP theorem还有指导意义吗？答案是否定的——除非重新定义availability和consistency，将其推广到更通用的场景：从单对象单操作到多对象多操作的事务系统。
重新定义availability和consistency后的分布式系统分类学虽然和CAP theorem非常类似，但不能称之为CAP theorem。&lt;/p&gt;
&lt;h2 id=&#34;一致性&#34;&gt;一致性&lt;/h2&gt;
&lt;p&gt;更通用的“一致性”应定义为“并发系统中共享状态更新的可见性”。
现代微处理器、分布式系统、数据库的共性是——它们都是存在共享数据的并发系统。
当我们讨论一致性（consistency）时，可能指的是微处理器架构和系统编程领域的一致性模型，也可能指的是分布式系统领域的副本一致性，还可能指的是数据库领域的事务隔离性。这些领域的抽象层次不同，共同点是讨论的系统都是并发系统。&lt;/p&gt;
&lt;p&gt;一致性模型（consistency model）是用于描述微处理器架构领域的多核并发场景下，各个处理器被允许的乱序程度——乱序的约束越少，效率越高，并发程序正确性越难保证。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最强的strict consistency指的是任何写在任何时钟周期都立刻对任何处理器可见，显然不能推广到分布式系统领域。&lt;/li&gt;
&lt;li&gt;次之的sequential consistency指的是写操作顺序对于每个副本而言都是一致的，即各进程内部的program order一致，而不同进程执行的顺序可以不一致。这个概念最初也是Lamport在讨论multi-processor computer如何正确执行并发程序时提出的。和分布式系统的副本一致性无关。C++中std::memory_order_seq_cst即可保证线程内部的program order。&lt;/li&gt;
&lt;li&gt;更宽松的causal consistency指的是写操作中有依赖关系的那一部分的顺序是一致的，即各进程中的dependency order一致。现代CPU基本上都是out-of-order流水线，在保证dependency order这个底线后，能多乱序就多乱序。在C++中用std::memory_order_consume的load(A)和std::memory_order_acquire的store(B)配合，即可保证这个store之前所有写操作中load(A)依赖的那一部分对load(A)是可见的。如果每个依赖都保证Release-Consume ordering，则依赖链就有序，整体上可满足causal consistency。&lt;/li&gt;
&lt;li&gt;除了上述几个著名模型外，还有几十个不同方法、领域中应用的一致性模型，下图就包含了非事务分布式存储系统中种种一致性模型（详见&lt;a href=&#34;https://arxiv.org/pdf/1512.00168.pdf&#34;&gt;Consistency in Non-Transactional Distributed Storage Systems&lt;/a&gt;）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/1.png&#34; alt=&#34;Consistency1&#34;&gt;&lt;/p&gt;
&lt;p&gt;并发程序显然可以很容易推广到分布式复制状态机，只是增加了网络延迟。因此，一致性模型可以推广应用到分布式系统中的副本一致性。以sequential consistency为例，增加了实时约束后就是分布式系统领域中更被广泛引用的linearizability，指的是单个被复制对象上的单个操作满足：A是写操作，B是对副本的读，A happened-before（因果律上的先于，见https://en.wikipedia.org/wiki/Happened-before） B，则A的写对B的读总是可见。比较一下C++的sequentially consistent ordering定义：everything that happened-before a store in one thread becomes a visible side effect in the thread that did a load。二者是一致的。&lt;/p&gt;
&lt;p&gt;必须注意，迄今为止讨论的对象仅限单个被复制到不同副本中的对象上的单个操作，分布式存储不可能只存一个对象，有很多分布式存储支持事务或BatchWrite，涉及到多个对象上的多个操作。将单对象上单操作的可见性推广到多对象上多操作也不困难——事务的ACID隔离级别本质上就是将单个共享对象上单个操作的可见性推广到多个对象上一组操作上。下图的左侧就是数据库领域熟知的隔离级别，和分布式系统、微处理器架构、多核编程一样，乱序的约束越少，效率越高，并发程序正确性越难保证。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/2.png&#34; alt=&#34;Consistency2&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;可用性&#34;&gt;可用性&lt;/h2&gt;
&lt;p&gt;更通用的“可用性”应定义为“在施加某种约束后，系统仍最终能响应所有请求，无论网络分区持续多久”。
比CAP theorem更完善的分布式系统分类学可参考&lt;a href=&#34;https://arxiv.org/pdf/1302.0309.pdf&#34;&gt;Highly Available Transactions: Virtues and Limitations&lt;/a&gt;。
这篇论文里给出了新的可用性定义：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;High availability：每个用户向运行中的系统发的请求，最终都会得到回复，无论网络分区持续多久。这就是CAP-availability，或者说traditional availability的标准定义。&lt;/li&gt;
&lt;li&gt;Sticky availability：每当用户事务在一个数据库状态（该状态反应了之前该用户所有操作）拷贝上执行，最终都会得到回复，无论网络分区持续多久。 这比CAP-availability要求更高。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;只追求high availability，用户可以访问系统中的任何一个replica，不同操作在不同replica上响应也没关系；但若追求sticky availability，用户则需要保证连续的若干操作总是在同一个replica上。比如Dynamo这种multi-writer的分布式存储，不能写一会儿A节点，再写一会儿B节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Transactional availability：分布式系统文献的一致性模型大多考虑的都是单对象上单个操作的场景，而数据库文献中关注的是事务：多个对象上多个操作合起来称为一个事务。显然CAP-availability定义也不适用于事务。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;事务的replica availability：事务能为它需要访问的各个对象联系到至少一个replica。这个要求是比CAP-availability低的。&lt;/li&gt;
&lt;li&gt;事务的liveliness：假设我们让每个事务都abort，就可以保证100%的及时响应，完美实现CAP-Availability，但又有什么意义呢？因此还需要保证尽可能让事务commit，而不是abort。&lt;/li&gt;
&lt;li&gt;因此，最终给出的transactional availability定义是：对事务中每个数据都保证replica availability，并且最终能够在N次retry内commit成功，或internal abort（由事务自己主动选择的abort，而非系统实现将其abort）。&lt;/li&gt;
&lt;li&gt;更进一步还可以给出sticky transactional availability的定义：如果系统能保证sticky availability，则能保证transactional availability。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据这种定义，可以将现有的事务系统的consistency（隔离级别）与其availability进行比较，得出下图中的结果：在新的分类学中，availability要求越高，consistency要求越宽松是成立的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/3.png&#34; alt=&#34;Availability&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
