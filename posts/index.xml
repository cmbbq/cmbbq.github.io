<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Cmbbq&#39;s Encyclopedia</title>
    <link>https://cmbbq.github.io/posts/</link>
    <description>Recent content in Posts on Cmbbq&#39;s Encyclopedia</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://cmbbq.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Paged Attention</title>
      <link>https://cmbbq.github.io/posts/pagedattention/</link>
      <pubDate>Tue, 30 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/pagedattention/</guid>
      <description>在LLM serving场景，通过恰当的batching积攒足够多的请求，可提升LLM吞吐。但每个请求对应的KV Cache非常巨大——在原始实现中，KV Cache需要为max_tokens预留内存，但每个请求实际上携带的tokens数目普遍远小于max_tokens，造成大量内存浪费和反复的动态内存分配。
PagedAttention1的目标就是消除这种内存浪费、在请求内部和请求之间灵活共享一些KV cache。vLLM通过PagedAttention达到此前sota的FasterTransformer和Orca的2~4倍吞吐。
此前，朴素的KV Cache实现如下图所示，简短的prompt和并不长的当前iteration只占据了7+4个slots，剩余2038个slots不得不预留在内存里以支撑最大序列长达2048的承诺（采样结束才能知道实际tokens数目），在这之后才能是下一个请求的slots，中间预留的部分就完全浪费掉了。
PagedAttention把连续的kv存在不连续的内存空间上，借用类似页表的机制（引入一个block_table）规避内存碎片化问题。具体来讲，PagedAttention把KV cache分区成若干个K blocks和V blocks，每个K/V block容纳固定数量tokens所对应的K/V向量，因此attention计算也被转化为blockwise计算。这和FlashAttention有点像，不过应用的尺度不同，前者是为了大规模serving时克服碎片化、按需取用，后者是为了单次self-attention计算能全部fit in SRAM。
这个物理kv blocks显然是支持多个请求复用的。如下图所示，为每个请求维护一个小的block table即可。
W. Kwon, et al. Efficient Memory Management for Large Language Model Serving with PagedAttention. [pdf]&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
      <content>&lt;p&gt;在LLM serving场景，通过恰当的batching积攒足够多的请求，可提升LLM吞吐。但每个请求对应的KV Cache非常巨大——在原始实现中，KV Cache需要为&lt;code&gt;max_tokens&lt;/code&gt;预留内存，但每个请求实际上携带的tokens数目普遍远小于&lt;code&gt;max_tokens&lt;/code&gt;，造成大量内存浪费和反复的动态内存分配。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PagedAttention&lt;/code&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;的目标就是消除这种内存浪费、在请求内部和请求之间灵活共享一些KV cache。vLLM通过&lt;code&gt;PagedAttention&lt;/code&gt;达到此前sota的&lt;code&gt;FasterTransformer&lt;/code&gt;和&lt;code&gt;Orca&lt;/code&gt;的2~4倍吞吐。&lt;/p&gt;
&lt;p&gt;此前，朴素的KV Cache实现如下图所示，简短的prompt和并不长的当前iteration只占据了7+4个slots，剩余2038个slots不得不预留在内存里以支撑最大序列长达2048的承诺（采样结束才能知道实际tokens数目），在这之后才能是下一个请求的slots，中间预留的部分就完全浪费掉了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/naive_kv_cache.png&#34; alt=&#34;naive_kv_cache&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PagedAttention&lt;/code&gt;把连续的kv存在不连续的内存空间上，借用类似页表的机制（引入一个block_table）规避内存碎片化问题。具体来讲，&lt;code&gt;PagedAttention&lt;/code&gt;把KV cache分区成若干个K blocks和V blocks，每个K/V block容纳固定数量tokens所对应的K/V向量，因此attention计算也被转化为blockwise计算。这和&lt;code&gt;FlashAttention&lt;/code&gt;有点像，不过应用的尺度不同，前者是为了大规模serving时克服碎片化、按需取用，后者是为了单次self-attention计算能全部fit in SRAM。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/block_table.png&#34; alt=&#34;block_table&#34;&gt;&lt;/p&gt;
&lt;p&gt;这个物理kv blocks显然是支持多个请求复用的。如下图所示，为每个请求维护一个小的block table即可。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/vllm_two_requests.png&#34; alt=&#34;vllm_two_requests&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;W. Kwon, et al. Efficient Memory Management for Large Language Model Serving with PagedAttention. &lt;a href=&#34;https://arxiv.org/pdf/2309.06180&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Flash Attention</title>
      <link>https://cmbbq.github.io/posts/flashattention/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/flashattention/</guid>
      <description>由于self-attention的时、空复杂度都是序列长的平方，长序列LLM和高分辨率ViT都是非常吃内存的。
此前针对self-attention的优化大多是近似计算，其核心是优化FLOP，把理论上的时间复杂度降低到O(N)，但这并不能有效加速self-attention，因为该操作（以及transformer中多数操作）的实际瓶颈在访存——更准确地说是HBM和SRAM之间的IO。
FlashAttention的原理就是基于tiling，确保内层循环计算fit in SRAM，减少了HBM和SRAM之间的IO频次，因而真切有效地提升了transformer性能，解锁了更长的context，迅速在各种高性能框架中得到应用。
传统的Self-Attention实现 Attention Layer具有将局部信息和张量中较远位置的信息结合起来的能力——通过为所得张量的每个组件与输入张量的每个组件计算注意力得分，不受局部性约束地，在整个张量范围内对特征进行平均。
给定$N^Q\times D^{QK}$维queries张量$Q$，$N^{KV}\times D^{QK}$维keys张量$K$，$N^{KV}\times D^V$维values张量$V$，通过Attention操作$att(K,Q,V)$计算得到$N^Q\times D^V$维的张量Y：
$$Y = att(K,Q,V) = \underbrace{softargmax(\frac{QK^T}{\frac{1}{\sqrt{D^{QK}}}})}_A V$$
整个过程分两步，第一步先计算每个query index $q$和每个key index $k$的注意力得分，即queries和keys点乘后的softargmax结果：$A_{q,k} = \frac{exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_k ) }{\sum_l exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_l)}$，其中$\frac{1}{\sqrt{D^{QK}}}$是一个缩放参数，用于保证取值范围在$D^{QK}$变大时大体不变。
得到注意力得分$A_{q,k}$后，再进行第二步计算：$Y_q = \sum_k A_{q,k}V_k$。注意力得分即queries和keys之间的匹配程度，匹配程度越高，该权重就越高。如果某个query和某个key匹配度达到极限，注意力得分接近1，则直接拿到这个key对应的value。如果和好几个key都有中等水准的匹配，则按注意力得分做加权平均。
FlashAttention的IO优化 FlashAttention注意到事实上并不需要完整的输入K、V、Q，可以分批读入、分批计算、分批把结果写回O，即所谓tiling：
在外层循环，遍历K、V矩阵。每次只需加载一个block的 $K^T$ 和 $V$ 到片上SRAM中。 在内层循环，遍历Q的各个block，加载到SRAM中，进行 $att(K,Q,V)$ 计算，把局部结果写回HBM上 $N\times d$ 的结果矩阵1。 对softmax归一化因子做相应调整，即可保证最后加起来得到的最终结果和标准实现等价，具体代数见2附录。 设置K、V的block size为 $\lceil \frac{M}{4d} \rceil$，Q、O的block size为$min(\lceil \frac{M}{4d} \rceil, d)$3。 此外，对于训练负载来说，FlashAttention还在反向传播中复用前馈过程暂存的softmax归一化因子$\frac{1}{\sqrt{D^{QK}}}$，这也比从HBM读$N\times N$的巨大attention中间矩阵要快得多。这可被视作selective gradient checkpointing。
FlashAttention2: 改进并行和工作切分 相比GEMM，FlashAttention只达到了25~40%理论FLOPs/s，可优化空间巨大。FlashAttention24在原版基础上做了并行化和工作切分优化。
减少非矩阵乘算子，因为GPU矩阵乘是高度优化的，其他算子与之差距非常大。 避免循环中算O时每次都rescale，而是在算最终结果时施加softmax归一化因子。 对反向传播时保持的状态进行了精简。 在不同线程块[^8]上进行并行计算，从而充分利用GPU资源。 原版一个线程块处理一个head/一个batch，每个线程块跑在一个SM上。不过在长序列场景，head数目和batch size可能都偏小，导致二者相乘后都未必能打满A100的128个SM。 显而易见可以并行的部分是外层循环，可以随便调度到不同线程块上，相互之间完全没有通信需求。 反向传播时并行化外层循环也只有dQ更新时需要简单的通信/同步，这是一个顺序不重要的加法，atomic add足以解决。 既然有了线程块，就要考虑在每个线程块内，不同warps之间如何进行工作的切分。 如上图所示，FlashAttention的切分方式导致内层循环中各个warp都需要把结果写到共享内存且做一个同步加，存在一定的通信开销，而FlashAttention2的切分方式可以保证warp之间完全没有通信需求。</description>
      <content>&lt;p&gt;由于&lt;code&gt;self-attention&lt;/code&gt;的时、空复杂度都是序列长的平方，长序列LLM和高分辨率ViT都是非常吃内存的。&lt;/p&gt;
&lt;p&gt;此前针对&lt;code&gt;self-attention&lt;/code&gt;的优化大多是近似计算，其核心是优化FLOP，把理论上的时间复杂度降低到O(N)，但这并不能有效加速&lt;code&gt;self-attention&lt;/code&gt;，因为该操作（以及transformer中多数操作）的实际瓶颈在访存——更准确地说是HBM和SRAM之间的IO。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;FlashAttention&lt;/code&gt;的原理就是基于&lt;code&gt;tiling&lt;/code&gt;，确保内层循环计算fit in SRAM，减少了HBM和SRAM之间的IO频次，因而真切有效地提升了transformer性能，解锁了更长的context，迅速在各种高性能框架中得到应用。&lt;/p&gt;
&lt;h2 id=&#34;传统的self-attention实现&#34;&gt;传统的Self-Attention实现&lt;/h2&gt;
&lt;p&gt;Attention Layer具有将局部信息和张量中较远位置的信息结合起来的能力——通过为所得张量的每个组件与输入张量的每个组件计算注意力得分，不受局部性约束地，在整个张量范围内对特征进行平均。&lt;/p&gt;
&lt;p&gt;给定$N^Q\times D^{QK}$维queries张量$Q$，$N^{KV}\times D^{QK}$维keys张量$K$，$N^{KV}\times D^V$维values张量$V$，通过&lt;code&gt;Attention操作&lt;/code&gt;$att(K,Q,V)$计算得到$N^Q\times D^V$维的张量Y：&lt;/p&gt;
&lt;p&gt;$$Y = att(K,Q,V) = \underbrace{softargmax(\frac{QK^T}{\frac{1}{\sqrt{D^{QK}}}})}_A V$$&lt;/p&gt;
&lt;p&gt;整个过程分两步，第一步先计算每个query index $q$和每个key index $k$的注意力得分，即queries和keys点乘后的&lt;code&gt;softargmax&lt;/code&gt;结果：$A_{q,k} = \frac{exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_k ) }{\sum_l exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_l)}$，其中$\frac{1}{\sqrt{D^{QK}}}$是一个缩放参数，用于保证取值范围在$D^{QK}$变大时大体不变。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/attention.png&#34; alt=&#34;att&#34;&gt;&lt;/p&gt;
&lt;p&gt;得到注意力得分$A_{q,k}$后，再进行第二步计算：$Y_q = \sum_k A_{q,k}V_k$。注意力得分即queries和keys之间的匹配程度，匹配程度越高，该权重就越高。如果某个query和某个key匹配度达到极限，注意力得分接近1，则直接拿到这个key对应的value。如果和好几个key都有中等水准的匹配，则按注意力得分做加权平均。&lt;/p&gt;
&lt;h2 id=&#34;flashattention的io优化&#34;&gt;FlashAttention的IO优化&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;FlashAttention&lt;/code&gt;注意到事实上并不需要完整的输入K、V、Q，可以分批读入、分批计算、分批把结果写回O，即所谓&lt;code&gt;tiling&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在外层循环，遍历K、V矩阵。每次只需加载一个block的 $K^T$ 和 $V$ 到片上SRAM中。&lt;/li&gt;
&lt;li&gt;在内层循环，遍历Q的各个block，加载到SRAM中，进行 $att(K,Q,V)$ 计算，把局部结果写回HBM上 $N\times d$ 的结果矩阵&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/li&gt;
&lt;li&gt;对softmax归一化因子做相应调整，即可保证最后加起来得到的最终结果和标准实现等价，具体代数见&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;附录。&lt;/li&gt;
&lt;li&gt;设置K、V的block size为 $\lceil \frac{M}{4d} \rceil$，Q、O的block size为$min(\lceil \frac{M}{4d} \rceil, d)$&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/fast_attention.png&#34; alt=&#34;fast_att&#34;&gt;&lt;/p&gt;
&lt;p&gt;此外，对于训练负载来说，&lt;code&gt;FlashAttention&lt;/code&gt;还在反向传播中复用前馈过程暂存的softmax归一化因子$\frac{1}{\sqrt{D^{QK}}}$，这也比从HBM读$N\times N$的巨大attention中间矩阵要快得多。这可被视作selective gradient checkpointing。&lt;/p&gt;
&lt;h2 id=&#34;flashattention2-改进并行和工作切分&#34;&gt;FlashAttention2: 改进并行和工作切分&lt;/h2&gt;
&lt;p&gt;相比GEMM，&lt;code&gt;FlashAttention&lt;/code&gt;只达到了25~40%理论FLOPs/s，可优化空间巨大。&lt;code&gt;FlashAttention2&lt;/code&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;在原版基础上做了并行化和工作切分优化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少非矩阵乘算子，因为GPU矩阵乘是高度优化的，其他算子与之差距非常大。
&lt;ul&gt;
&lt;li&gt;避免循环中算O时每次都rescale，而是在算最终结果时施加softmax归一化因子。&lt;/li&gt;
&lt;li&gt;对反向传播时保持的状态进行了精简。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在不同线程块[^8]上进行并行计算，从而充分利用GPU资源。
&lt;ul&gt;
&lt;li&gt;原版一个线程块处理一个head/一个batch，每个线程块跑在一个SM上。不过在长序列场景，head数目和batch size可能都偏小，导致二者相乘后都未必能打满A100的128个SM。&lt;/li&gt;
&lt;li&gt;显而易见可以并行的部分是外层循环，可以随便调度到不同线程块上，相互之间完全没有通信需求。&lt;/li&gt;
&lt;li&gt;反向传播时并行化外层循环也只有dQ更新时需要简单的通信/同步，这是一个顺序不重要的加法，atomic add足以解决。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;既然有了线程块，就要考虑在每个线程块内，不同warps之间如何进行工作的切分。
&lt;img src=&#34;https://cmbbq.github.io/img/work_part.png&#34; alt=&#34;work_part&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，&lt;code&gt;FlashAttention&lt;/code&gt;的切分方式导致内层循环中各个warp都需要把结果写到共享内存且做一个同步加，存在一定的通信开销，而&lt;code&gt;FlashAttention2&lt;/code&gt;的切分方式可以保证warp之间完全没有通信需求。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;其中，d为head dimension。N为序列长度。$N \gg d$。GPT2中 $N=1024，d=64$。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;T. Dao, et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. &lt;a href=&#34;https://arxiv.org/pdf/2205.14135&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;其中，M为SRAM大小。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. &lt;a href=&#34;https://arxiv.org/pdf/2307.08691&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>CAT02: Resources</title>
      <link>https://cmbbq.github.io/posts/category-theory-2-resources/</link>
      <pubDate>Fri, 14 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/category-theory-2-resources/</guid>
      <description>Symmetric Monoidal Preorders Def 2.1: A symmetric monoidal structure on a preorder $(X,≤)$ consists of two constituents: (i) an element $I \in X$, called the monoidal unit, (ii) and a function $\otimes: X \times X \rightarrow X$, called the monoidal product. These constituents must satisfy the following properties:
monotonicity: $\forall x_1, x_2, y_1, y_2 \in X$, if $x_1 \le y_1$ and $x_2 \le y_2$, then $x_1 \otimes x_2 \le y_1 \otimes y_2$.</description>
      <content>&lt;h2 id=&#34;symmetric-monoidal-preorders&#34;&gt;Symmetric Monoidal Preorders&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Def 2.1:&lt;/strong&gt; A &lt;code&gt;symmetric monoidal structure&lt;/code&gt; on a preorder $(X,≤)$ consists of two constituents: (i) an element $I \in X$, called the &lt;code&gt;monoidal unit&lt;/code&gt;, (ii) and a function $\otimes: X \times X \rightarrow X$, called the &lt;code&gt;monoidal product&lt;/code&gt;. These constituents must satisfy the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;monotonicity: $\forall x_1, x_2, y_1, y_2 \in X$, if $x_1 \le y_1$ and $x_2 \le y_2$, then $x_1 \otimes x_2 \le y_1 \otimes y_2$.&lt;/li&gt;
&lt;li&gt;unitality: $\forall x \in X$, $I \otimes x = x$ and $x \otimes I = x$ holds.&lt;/li&gt;
&lt;li&gt;associativity: $\forall x,y,z \in X$, $(x\otimes y)\otimes z = x \otimes (y\otimes z)$.&lt;/li&gt;
&lt;li&gt;symmetry: $\forall x,y \in X, x\otimes y = y\otimes x$.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Def 2.2:&lt;/strong&gt; A preorder equipped with a symmetric monoidal structure, $(X,\le,I,\otimes)$, is called a &lt;code&gt;symmetric monoidal preorder&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 2.1(The Booleans):&lt;/strong&gt; $\mathbb{B} = {true, false}$ with $false &amp;lt; true$ is the simplest nontrivial preorder. We can define the monoidal unit be true and the monoidal product be $\wedge$(AND). Then we have a monoidal preorder which we denote $Bool := (\mathbb{B}, \le ,true, \wedge )$.&lt;/p&gt;
&lt;h2 id=&#34;wiring-diagrams&#34;&gt;Wiring Diagrams&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Wiring diagrams&lt;/code&gt; are visual representations for building new releationships from old. In a preorder without a monoidal structure, the relations are chained in series.
&lt;img src=&#34;https://cmbbq.github.io/img/wiring_diagrams_1.png&#34; alt=&#34;wiring_diagrams_1&#34;&gt;&lt;/p&gt;
&lt;p&gt;With a symmetric monoidal structure, relations could be arranged in parallel as well.
&lt;img src=&#34;https://cmbbq.github.io/img/wiring_diagrams_2.png&#34; alt=&#34;wiring_diagrams_2&#34;&gt;
The whole wiring diagram above says &amp;ldquo;if $t\le v, w+u\le x+z, v+x\le y$, then $t+u\le y+z$&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We could draw two wires in parallel to represent the monoidal product of two labels.
&lt;img src=&#34;https://cmbbq.github.io/img/wiring_diagrams_3.png&#34; alt=&#34;wiring_diagrams_3&#34;&gt;
The validity of the box above corresponds to $x_1\otimes x_2 \le y_1 \otimes y_2 \otimes y_3$.&lt;/p&gt;
&lt;p&gt;TBD&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;To be a bit more rigorous, it&amp;rsquo;s often useful to replace $=$ with $\cong$ throughout &lt;strong&gt;Def 2.1&lt;/strong&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Diffusion Probabilistic Models</title>
      <link>https://cmbbq.github.io/posts/dpm/</link>
      <pubDate>Tue, 04 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/dpm/</guid>
      <description>扩散模型生成高维数据 生成式模型范式 不同于判别式方法，生成式建模范式是：给定未知数据分布的一组IID1数据$x_i \sim p_D(x)$，去学一个参数空间为$\theta \in \Theta$的模型分布$p_\theta(x)$，令其逼近数据分布：$p_\theta(x) \approx p_D(x)$。
机器学习中，生成式模型的经典方法包括：
Mixture of Gaussian for clustering Naive Bayes for classification Mixture of Experts(MoE) for unsupervised/supervised learning Probability graphical models, e.g. bayesian networks Nonparametric Bayesian methods Deep generative models 生成式模型天然具备构建基础模型的潜质，因为它的本质是对多元变量联合分布建模，只要能有效估计$p(x,y)$，自然就具备了对$p(x)$进行条件预测的能力——这也就构建出分类器，而且研究表明这样构建出来的分类器在半监督这种训练数据比较少的情况下表现出更高的数据利用率，此外也有些工作发现这种分类器在对抗干扰时表现得更鲁棒。
深度生成式模型的兴起，源自：
相比判别式模型，模型的表达能力变强了，能描述高维数据的复杂分布。 算法上有成熟的变分和马尔可夫链蒙特卡洛方法（MCMC）。 数据上更容易通过自监督或无监督方法利用大规模数据。 硬件上得到新GPU硬件对更大算力需求的支撑。 可微分神经网络深度生成模型用可微分DNN去学习随机变量之间的复杂关系，目标是将标准高斯白噪声变成一个自然场景下的真实分布（自然图片、声音、视频）。完全无监督下就能取得非常好的效果。这些模型根据概率密度函数定义可分为显式和隐式：
显式模型如VAE、Energy-based models、Auto-Regressive、Flow-based Models、DPM(Diffusion probabilistic models)直接描述预期产出数据的概率分布。 隐式模型如GAN、Moment-matching DGM则描述了一个变换过程，还需要通过一些准则去引导模型产生更符合预期数据分布的数据。 从训练目标来看，这些模型又可以分为最大似然估计（MLE）、Score-matching、对抗训练（Adversarial training）三类。
Score-matching：Moment-matching DGM, Diffusion Models Adversarial training：GAN MLE：Everything else 扩散模型 物理过程中的扩散是随着时间推移，破坏结构，从有序到无序。
扩散模型中扩散过程也是逐渐给数据加高斯噪声，使其信噪比下降。
Song et al., ICLR 20212将一个扩散变换描述为：$q(x_i|x_{i-1}) = \mathcal{N}(x_i;\sqrt{1- \beta_i}x_{i-1},\beta_iI)$，令$\alpha_i = \prod_{k=1}^{i} 1 - \beta_i$，则有$q_{\alpha_N}(x_N|x_0) = \prod_{i=1}^{N} q(x_i|x_{i-1}) = \mathcal{N}(x_N;\sqrt{\alpha_N} x_0, (1-\alpha_N)I)$，冗长的递归表达式最终可以划归为一个简洁的closed form3，因此可以很方便地定义N步前向过程的loss。其中$\beta_i$是一系列噪声乘数，可以是超参，也可以说reparameterization学习到的结果。对每个训练数据$x_0 \sim q_D(x)$，都可以构造一个离散马尔可夫链${x_0,x_1,&amp;hellip;,x_N}$，经过$N$次加噪，最终使之趋近高斯白噪。$q(x_N|x_0) \sim \mathcal{N}(O,I), N \rightarrow \infty$。</description>
      <content>&lt;h1 id=&#34;扩散模型生成高维数据&#34;&gt;扩散模型生成高维数据&lt;/h1&gt;
&lt;h2 id=&#34;生成式模型范式&#34;&gt;生成式模型范式&lt;/h2&gt;
&lt;p&gt;不同于判别式方法，生成式建模范式是：给定未知数据分布的一组IID&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;数据$x_i \sim p_D(x)$，去学一个参数空间为$\theta \in \Theta$的模型分布$p_\theta(x)$，令其逼近数据分布：$p_\theta(x) \approx p_D(x)$。&lt;/p&gt;
&lt;p&gt;机器学习中，生成式模型的经典方法包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mixture of Gaussian for clustering&lt;/li&gt;
&lt;li&gt;Naive Bayes for classification&lt;/li&gt;
&lt;li&gt;Mixture of Experts(MoE) for unsupervised/supervised learning&lt;/li&gt;
&lt;li&gt;Probability graphical models, e.g. bayesian networks&lt;/li&gt;
&lt;li&gt;Nonparametric Bayesian methods&lt;/li&gt;
&lt;li&gt;Deep generative models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;生成式模型天然具备构建基础模型的潜质，因为它的本质是对多元变量联合分布建模，只要能有效估计$p(x,y)$，自然就具备了对$p(x)$进行条件预测的能力——这也就构建出分类器，而且研究表明这样构建出来的分类器在半监督这种训练数据比较少的情况下表现出更高的数据利用率，此外也有些工作发现这种分类器在对抗干扰时表现得更鲁棒。&lt;/p&gt;
&lt;p&gt;深度生成式模型的兴起，源自：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相比判别式模型，模型的表达能力变强了，能描述高维数据的复杂分布。&lt;/li&gt;
&lt;li&gt;算法上有成熟的变分和马尔可夫链蒙特卡洛方法（MCMC）。&lt;/li&gt;
&lt;li&gt;数据上更容易通过自监督或无监督方法利用大规模数据。&lt;/li&gt;
&lt;li&gt;硬件上得到新GPU硬件对更大算力需求的支撑。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可微分神经网络深度生成模型用可微分DNN去学习随机变量之间的复杂关系，目标是将标准高斯白噪声变成一个自然场景下的真实分布（自然图片、声音、视频）。完全无监督下就能取得非常好的效果。这些模型根据概率密度函数定义可分为显式和隐式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;显式模型如VAE、Energy-based models、Auto-Regressive、Flow-based Models、DPM(Diffusion probabilistic models)直接描述预期产出数据的概率分布。&lt;/li&gt;
&lt;li&gt;隐式模型如GAN、Moment-matching DGM则描述了一个变换过程，还需要通过一些准则去引导模型产生更符合预期数据分布的数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从训练目标来看，这些模型又可以分为最大似然估计（MLE）、Score-matching、对抗训练（Adversarial training）三类。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Score-matching：Moment-matching DGM, Diffusion Models&lt;/li&gt;
&lt;li&gt;Adversarial training：GAN&lt;/li&gt;
&lt;li&gt;MLE：Everything else&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;扩散模型&#34;&gt;扩散模型&lt;/h2&gt;
&lt;p&gt;物理过程中的扩散是随着时间推移，破坏结构，从有序到无序。&lt;/p&gt;
&lt;p&gt;扩散模型中扩散过程也是逐渐给数据加高斯噪声，使其信噪比下降。&lt;/p&gt;
&lt;p&gt;Song et al., ICLR 2021&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;将一个扩散变换描述为：$q(x_i|x_{i-1}) = \mathcal{N}(x_i;\sqrt{1- \beta_i}x_{i-1},\beta_iI)$，令$\alpha_i = \prod_{k=1}^{i} 1 - \beta_i$，则有$q_{\alpha_N}(x_N|x_0) = \prod_{i=1}^{N} q(x_i|x_{i-1}) = \mathcal{N}(x_N;\sqrt{\alpha_N} x_0, (1-\alpha_N)I)$，冗长的递归表达式最终可以划归为一个简洁的closed form&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;，因此可以很方便地定义N步前向过程的loss。其中$\beta_i$是一系列噪声乘数，可以是超参，也可以说reparameterization学习到的结果。对每个训练数据$x_0 \sim q_D(x)$，都可以构造一个离散马尔可夫链${x_0,x_1,&amp;hellip;,x_N}$，经过$N$次加噪，最终使之趋近高斯白噪。$q(x_N|x_0) \sim \mathcal{N}(O,I), N \rightarrow \infty$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/sde.png&#34; alt=&#34;SDE&#34;&gt;&lt;/p&gt;
&lt;p&gt;上述过程的逆向去噪过程$p(x_{i-1}|x_i)$则是未知、需要学习或估算的——可以用变分近似的方法求解，比如用一个均值为$x_i$函数的高斯分布$\mathcal{N}(\mu_n(x_n), \theta_n^2I)$去近似$p(x_{i-1}|x_i)$，用KL散度最小化的方法使之逼近。&lt;/p&gt;
&lt;p&gt;原理上来说Diffusion模型相对简单：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只有加噪去噪，不需要去学encoder和decoder，只需要根据加噪学去噪。&lt;/li&gt;
&lt;li&gt;损失函数也比较简单。&lt;/li&gt;
&lt;li&gt;数学上是严格保证收敛的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;大规模训练和高效数据生成&#34;&gt;大规模训练和高效数据生成&lt;/h2&gt;
&lt;p&gt;此前变分近似的做法中，噪声的方差参数一般是固定下来，不做优化的。清华大学TSAIL提出的Analytical-DPMs&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;发现可以给出逆向过程中每个时间点的均值函数和方差的一个解析形式——这个形式和一些学者手工设计的一些形式也比较耦合——最终得到一个不需要任何额外训练的方差估计器。训练好的DPM，只需要插入一行代码，就能用上这个解析形式的方差估计。用这个估计值使每一步的方差估计变得更准，使所需的整体步数变少，折算下来有20~80倍的性能提升。后来这个方法也在Dall-E 2中使用了。&lt;/p&gt;
&lt;p&gt;TSAIL团队的另一个工作DPM-Solver&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;做了专门的求解器，使步数从几百步降低到十余步。&lt;/p&gt;
&lt;p&gt;由于涉及加噪去噪，扩散模型的底层架构自然而然借鉴U-Net（CNN）。TSAIL团队的第三个工作，尝试把扩散模型和transformer结合，设计了U-ViT&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;，在当时设置了5亿参数的（当时算最大的）大模型，证明对模型的可扩展性确实有帮助。同期有个工作DiT非常类似。Stable Diffusion 3.0就用的是DiT架构。&lt;/p&gt;
&lt;p&gt;回顾前文所述的“生成式模型天然具备构建基础模型的潜质，因为它的本质是对多元变量联合分布建模，只要能有效估计$p(x,y)$，自然就具备了对$p(x)$进行条件预测的能力”，基于这种heuristics，TSAIL团队的另一个研究是UniDiffuser&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;，目标是用一个模型解决原本marginal diffuser、conditional diffuser、joint diffuser这多个模型才能解决的多个任务。当时DALL-E 2和Stable Diffusion只能文到图，而UniDiffuser能图到文或文到图。&lt;/p&gt;
&lt;p&gt;做完图像之后，又做了Vidu&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;文生视频的工作，在时间轴上做了升维，实现了16s的生成。此外，还做了3D内容生成，CRM&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;图生3D，ProlificDreamer&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;文生3D，在空间上做了升维。在最新的工作Vidu4D&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;中，做了4D（即sequential 3D）重建。&lt;/p&gt;
&lt;h2 id=&#34;从生成到判别式分类器&#34;&gt;从生成到判别式分类器&lt;/h2&gt;
&lt;p&gt;生成式AI估计一个联合分布$P(x,y)$，基于贝叶斯定理可得$p(y|x) = \frac{p(x,y)}{p(x)} = \frac{p(y)p(x|y)}{p(x)}$，$y^* = \arg \underset{y\in \mathcal{Y}}{\max} p(y|x)$。&lt;/p&gt;
&lt;p&gt;如果联合分布是准确的，那么这个分类器就是最优的，即所谓贝叶斯分类器。&lt;/p&gt;
&lt;p&gt;此外，Chen et al 2024&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;的工作表明可以将一个预训练好的生成式基座模型转化成一个对噪声鲁棒的分类器。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;IID stands for Independent and Identically Distributed&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Song et al. Score-based generative modeling through stohastic differential equations. ICLR 2021. &lt;a href=&#34;https://arxiv.org/abs/2011.13456&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Some supplementary good ol&amp;rsquo; fashioned mathematical rigour: &lt;a href=&#34;https://math.stackexchange.com/a/4568122&#34;&gt;https://math.stackexchange.com/a/4568122&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Bao et al. Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models. ICLR 2022. &lt;a href=&#34;https://arxiv.org/abs/2201.06503&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Lu et al. DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. &lt;a href=&#34;https://arxiv.org/abs/2206.00927&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;Bao et al. All are Worth Words: A ViT Backbone for Diffusion Models. CVPR 2023. &lt;a href=&#34;https://arxiv.org/abs/2209.12152&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;Bao et al. One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale. &lt;a href=&#34;https://arxiv.org/abs/2303.06555&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;Bao et al. Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models. &lt;a href=&#34;https://arxiv.org/abs/2405.04233&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;Wang et al. CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model. NeurlPS 2023. &lt;a href=&#34;https://arxiv.org/abs/2403.05034&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;Wang et al. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. &lt;a href=&#34;https://arxiv.org/abs/2305.16213&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;Wang et al. Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels. &lt;a href=&#34;https://arxiv.org/abs/2405.16822&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;Chen et al. Robust Classification via Single Diffusion Model. ICML 2024. &lt;a href=&#34;https://arxiv.org/abs/2305.15241&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Revisiting Recommender Systems</title>
      <link>https://cmbbq.github.io/posts/revisiting-recomender-systems/</link>
      <pubDate>Mon, 03 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/revisiting-recomender-systems/</guid>
      <description>推荐问题可定义为在合适的时间把合适的物品（items）推荐给合适的用户。
通常来说，推荐任务所用数据集即用户-物品交互矩阵。推荐系统基于这个数据集推测用户兴趣，把结果放到线上进行测试（学术界没有这个条件，只能线下评测），进行评测。
What&amp;rsquo;s Missing in User-Item Interaction Datasets 我们通常将推荐任务理解为如何在一个静态的用户-物品交互矩阵中预测缺失的数据，而不是在特定场景中的动态环境下预测用户的下一次交互。
值得注意的是，在用户-物品交互矩阵数据集中，时间的维度坍缩了，真实互动场景中的种种约束也坍缩了。
推荐系统文章中，70%使用了MovieLens数据集，但它真的能还原推荐场景吗？未必，因为MovieLens在一次初始化过程中完成用户对过去多年观影体验的回忆——可能有很多遗漏、遗忘，也忽略了上映时间、价格等现实因素。然而现实场景中用户的兴趣是逐渐形成的，受制于各种现实约束，其观影决策还不可避免地受到上映时间、票价、以及兴趣是否发生变化等诸多因素影响。
A Worrying Analysis of Current Practice 基于我们过去五年在推荐系统评测和数据集分析方面的工作，我们重新审视推荐系统的问题定义，并为缺乏共识的现象提供一种解读。
Dacrema et al., 20191认为深度学习新模型的效果一般般。
Bauer et al., 20242做了一个综合，分析了数据集，发现用的数据集非常集中，集中在MovieLens、Amazon Reviews等主流数据集，这些数据集普遍偏旧。大部分文章都是提出新的模型。还有一些专注在评测标准。
Ivanova et al., 20233认为推荐领域用哪个baseline其实并没有共识，一部分原因是每年推荐系统里有5k篇文章，没人能读完所有文章，于是审稿者之间未形成共识，有的审稿者认为某些方法特别好，一定要纳入baseline，另一些审稿者则有不同观点。比如nearest neighbor虽然简单，经过良好调参后却是非常强的baseline，在很多场景比复杂模型表现得好很多，但很多文章不会把nearest neighbor列入baseline。大家都认为它是几十年前的方法，不值得比较。
不仅baseline不统一，即使baseline统一，也有一个调参的问题，如Shehzad, Jannach, 20234所说，当你提出自己的模型时，非常注重调参，和别人比时却没有非常精细地调参。
McElfresh et al., 20225做了非常大规模的调研，在85个数据集上比较了24个算法的315个指标，得出了令人震撼的结论：这些算法并不能泛化，在某个数据集上很好，下一个可能就不好。每个算法都可能排第一第二，最差都能排到倒数第几。最后发现最强的算法竟然是Item-kNN！
Data Leakage in Train/Test Split Sun, 20226中对20~22年88篇RecSys会议文章的train/test split做了一个梳理，发现34%的论文用的是random split（随机分），25%用的是leave-one-out（最后一个交互做成测试，之前的是训练），19.5% single time point，17% simulation-based online，4.5% sliding window。理论上最完美的train/test split方法是严格遵循时间线，在时间线上取某个时间点之前的做训练集，之后的做测试集，然后慢慢把这个时间点往后推，每个时间点的选取生成了相应的训练集和测试集，时间点越往后推，训练集越多测试集越少——可惜实际操作起来很难做到，大部分文章采用的random split和leave-one-out有很强的信息泄露。
以leave-one-out为例，每个用户都只取最后一次交互做测试集，问题就在于不同用户的最后一个交互的时间可能大不同——假设某个物品在特定时间非常火，popularity(流行度是推荐系统中最简单的baseline，就是对物品的交互次数进行排序)排序非常高，但某个用户最后一次交互时间甚至在这个特定时间之前，那给这个用户推荐一个未来爆火的物品显然并不合理。Ji et al., 20207就重新审视了这一问题，将popularity修正为用户最后一次交互时间点的“当时流行度”后，popularity置信度可以提升70%。
在Ji et al., 20208的研究中指出：几乎所有的ML/DL模型中，尤其是离线评测的推荐系统模型中，都存在类似的数据泄露问题——模型无意中使用了未来数据进行训练，学到了本不应该存在的用户-物品交互。BPR、NeuMF、LightGCN、SASRec均未从机制上避免这种数据泄露。这个研究也通过实验证明了这种数据泄露确实会显著影响模型的推荐准确率，且这种对准确率的影响是不可预测的。
Recommendation should be Task-specific &amp;amp; Dynamic 用户决策涉及通用偏好和当下context因素。当下context是非常task-specific且非常动态的。这也使得推荐任务具备了这一特征。</description>
      <content>&lt;p&gt;推荐问题可定义为在合适的时间把合适的物品（items）推荐给合适的用户。&lt;/p&gt;
&lt;p&gt;通常来说，推荐任务所用数据集即用户-物品交互矩阵。推荐系统基于这个数据集推测用户兴趣，把结果放到线上进行测试（学术界没有这个条件，只能线下评测），进行评测。&lt;/p&gt;
&lt;h2 id=&#34;whats-missing-in-user-item-interaction-datasets&#34;&gt;What&amp;rsquo;s Missing in User-Item Interaction Datasets&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;我们通常将推荐任务理解为如何在一个静态的用户-物品交互矩阵中预测缺失的数据，而不是在特定场景中的动态环境下预测用户的下一次交互。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;值得注意的是，在用户-物品交互矩阵数据集中，时间的维度坍缩了，真实互动场景中的种种约束也坍缩了。&lt;/p&gt;
&lt;p&gt;推荐系统文章中，70%使用了MovieLens数据集，但它真的能还原推荐场景吗？未必，因为MovieLens在一次初始化过程中完成用户对过去多年观影体验的回忆——可能有很多遗漏、遗忘，也忽略了上映时间、价格等现实因素。然而现实场景中用户的兴趣是逐渐形成的，受制于各种现实约束，其观影决策还不可避免地受到上映时间、票价、以及兴趣是否发生变化等诸多因素影响。&lt;/p&gt;
&lt;h2 id=&#34;a-worrying-analysis-of-current-practice&#34;&gt;A Worrying Analysis of Current Practice&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;基于我们过去五年在推荐系统评测和数据集分析方面的工作，我们重新审视推荐系统的问题定义，并为缺乏共识的现象提供一种解读。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Dacrema et al., 2019&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;认为深度学习新模型的效果一般般。&lt;/p&gt;
&lt;p&gt;Bauer et al., 2024&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;做了一个综合，分析了数据集，发现用的数据集非常集中，集中在MovieLens、Amazon Reviews等主流数据集，这些数据集普遍偏旧。大部分文章都是提出新的模型。还有一些专注在评测标准。&lt;/p&gt;
&lt;p&gt;Ivanova et al., 2023&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;认为推荐领域用哪个baseline其实并没有共识，一部分原因是每年推荐系统里有5k篇文章，没人能读完所有文章，于是审稿者之间未形成共识，有的审稿者认为某些方法特别好，一定要纳入baseline，另一些审稿者则有不同观点。比如nearest neighbor虽然简单，经过良好调参后却是非常强的baseline，在很多场景比复杂模型表现得好很多，但很多文章不会把nearest neighbor列入baseline。大家都认为它是几十年前的方法，不值得比较。&lt;/p&gt;
&lt;p&gt;不仅baseline不统一，即使baseline统一，也有一个调参的问题，如Shehzad, Jannach, 2023&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;所说，当你提出自己的模型时，非常注重调参，和别人比时却没有非常精细地调参。&lt;/p&gt;
&lt;p&gt;McElfresh et al., 2022&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;做了非常大规模的调研，在85个数据集上比较了24个算法的315个指标，得出了令人震撼的结论：这些算法并不能泛化，在某个数据集上很好，下一个可能就不好。每个算法都可能排第一第二，最差都能排到倒数第几。最后发现最强的算法竟然是Item-kNN！&lt;/p&gt;
&lt;h2 id=&#34;data-leakage-in-traintest-split&#34;&gt;Data Leakage in Train/Test Split&lt;/h2&gt;
&lt;p&gt;Sun, 2022&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;中对20~22年88篇RecSys会议文章的&lt;code&gt;train/test split&lt;/code&gt;做了一个梳理，发现34%的论文用的是&lt;code&gt;random split&lt;/code&gt;（随机分），25%用的是&lt;code&gt;leave-one-out&lt;/code&gt;（最后一个交互做成测试，之前的是训练），19.5% &lt;code&gt;single time point&lt;/code&gt;，17% &lt;code&gt;simulation-based online&lt;/code&gt;，4.5% &lt;code&gt;sliding window&lt;/code&gt;。理论上最完美的train/test split方法是严格遵循时间线，在时间线上取某个时间点之前的做训练集，之后的做测试集，然后慢慢把这个时间点往后推，每个时间点的选取生成了相应的训练集和测试集，时间点越往后推，训练集越多测试集越少——可惜实际操作起来很难做到，大部分文章采用的&lt;code&gt;random split&lt;/code&gt;和&lt;code&gt;leave-one-out&lt;/code&gt;有很强的信息泄露。&lt;/p&gt;
&lt;p&gt;以&lt;code&gt;leave-one-out&lt;/code&gt;为例，每个用户都只取最后一次交互做测试集，问题就在于不同用户的最后一个交互的时间可能大不同——假设某个物品在特定时间非常火，&lt;code&gt;popularity&lt;/code&gt;(流行度是推荐系统中最简单的baseline，就是对物品的交互次数进行排序)排序非常高，但某个用户最后一次交互时间甚至在这个特定时间之前，那给这个用户推荐一个未来爆火的物品显然并不合理。Ji et al., 2020&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;就重新审视了这一问题，将&lt;code&gt;popularity&lt;/code&gt;修正为用户最后一次交互时间点的“当时流行度”后，&lt;code&gt;popularity&lt;/code&gt;置信度可以提升70%。&lt;/p&gt;
&lt;p&gt;在Ji et al., 2020&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;的研究中指出：几乎所有的ML/DL模型中，尤其是离线评测的推荐系统模型中，都存在类似的数据泄露问题——模型无意中使用了未来数据进行训练，学到了本不应该存在的用户-物品交互。&lt;code&gt;BPR&lt;/code&gt;、&lt;code&gt;NeuMF&lt;/code&gt;、&lt;code&gt;LightGCN&lt;/code&gt;、&lt;code&gt;SASRec&lt;/code&gt;均未从机制上避免这种数据泄露。这个研究也通过实验证明了这种数据泄露确实会显著影响模型的推荐准确率，且这种对准确率的影响是不可预测的。&lt;/p&gt;
&lt;h2 id=&#34;recommendation-should-be-task-specific--dynamic&#34;&gt;Recommendation should be Task-specific &amp;amp; Dynamic&lt;/h2&gt;
&lt;p&gt;用户决策涉及通用偏好和当下context因素。当下context是非常task-specific且非常动态的。这也使得推荐任务具备了这一特征。&lt;/p&gt;
&lt;p&gt;很大程度上，现有的推荐系统都只局限在通用偏好层面。回顾数据层面，现有数据集，即各种用户-物品交互矩阵，显然丢掉了context，只能捕捉通用偏好。模型层面，训练是基于决策结果的，而非决策过程本身，这也决定了模型只能学到用户的通用偏好。而评测端，自然而然也只能对通用偏好进行评测。&lt;/p&gt;
&lt;p&gt;工业实践中，推荐系统应该是一个检索问题。在这个检索问题中，query包含通用偏好、当前context这两种动态更新的信息；item collection也是动态更新的；ranking则旨在提升决策质量。&lt;/p&gt;
&lt;p&gt;考虑到context因素，不同场景的推荐系统，如食物推荐、电影推荐、电商推荐、宾馆推荐，也应该有非常不同的实现，分开建模。有的场景选项固定，有的场景就是适合重复，有的场景则偏重探索。不同场景的输入甚至也有区别。比如外卖推荐除了user id之外，必需提供收货地址、早餐/午餐/晚餐这种信息。&lt;/p&gt;
&lt;h2 id=&#34;conclusions-and-whats-next&#34;&gt;Conclusions and What&amp;rsquo;s Next&lt;/h2&gt;
&lt;p&gt;孙教授近期的一篇文章Sun, 2024&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;，对推荐系统的问题定义进行了重新思考，认为当前的推荐系统研究对推荐问题做了过度简化，以至于学术界提出的几乎所有的方案都不太适应现实世界中的具体任务。对未来的研究方向进行了一些研判：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;预计不会有赢家通吃的模型，未来依旧是每个模型在自己的论文里无敌。&lt;/li&gt;
&lt;li&gt;应将推荐系统问题细化，短视频有短视频赛道，电商有电商赛道，新闻有新闻赛道，针对不同赛道，设计、评测新的模型。别再用MovieLens去评测电商推荐了！&lt;/li&gt;
&lt;li&gt;Item-kNN仍然会是很强的baseline。只不过对nearest和neighbor的定义需要更好的特征工程。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Are we really making much progress? A worrying analysis of recent neural recommendation approaches &lt;a href=&#34;https://arxiv.org/abs/1907.06902&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Exploring the Landscape of Recommender Systems Evaluation: Practices and Perspectives &lt;a href=&#34;https://arxiv.org/pdf/2311.05232.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;RecBaselines2023: a new dataset for choosing baselines for recommender models &lt;a href=&#34;https://arxiv.org/abs/2306.14292&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Everyone’s a Winner! On Hyperparameter Tuning of Recommendation Models &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3604915.3609488&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;On the Generalizability and Predictability of Recommender Systems&lt;a href=&#34;https://arxiv.org/abs/2206.11886&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;Take a Fresh Look at Recommender Systems from an Evaluation Standpoint &lt;a href=&#34;https://arxiv.org/abs/2210.04149&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;A Re-visit of the Popularity Baseline in Recommender Systems &lt;a href=&#34;https://arxiv.org/abs/2005.13829&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;A Critical Study on Data Leakage in Recommender System Offline Evaluation  &lt;a href=&#34;https://arxiv.org/abs/2010.11060&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;Beyond Collaborative Filtering: A Relook at Task Formulation in Recommender Systems &lt;a href=&#34;https://arxiv.org/abs/2404.13375&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title> 护照网上换发攻略</title>
      <link>https://cmbbq.github.io/posts/passport-photo-compliance/</link>
      <pubDate>Thu, 16 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/passport-photo-compliance/</guid>
      <description>2024年5月6日起，20城试点网上护照换发，正好需要更新护照，记录一下网上换发的流程。
护照网上换发流程 申请护照换发的网上入口：微信小程序-国家移民管理局政务服务平台-中国公民服务-证件换补发网上申请。
需提前准备好护照照片、签名照片、身份证照片。完成申请后，等待审批，审批通过后支持工本费，寄到家中到付。
照片合规处理 护照照片有严格的规格要求：格式为jpeg，20k~80k，宽高比3:4，图像大小(354,472)~(480,640)，以及一些隐式要求，如头部占比需足够大、头顶需有一定留白。
写了一个工具来处理。这个工具同样适用于签名照片和身份证照片，因为这两个照片也有尺寸的要求。
from PIL import Image, ImageOps import argparse def Config(parser): parser.add_argument(&amp;#39;-i&amp;#39;, dest=&amp;#39;input&amp;#39;, default=&amp;#34;./in.jpg&amp;#34;, help=&amp;#39;input photo path&amp;#39;) parser.add_argument(&amp;#39;-o&amp;#39;, dest=&amp;#39;output&amp;#39;, default=&amp;#34;./out.jpg&amp;#34;, help=&amp;#39;input photo path&amp;#39;) parser.add_argument(&amp;#39;--height&amp;#39;, dest=&amp;#39;height&amp;#39;, default=640, help=&amp;#39;desired output height&amp;#39;) parser.add_argument(&amp;#39;--width&amp;#39;, dest=&amp;#39;width&amp;#39;, default=480, help=&amp;#39;desired output width&amp;#39;) parser.add_argument(&amp;#39;--crop_ratio&amp;#39;, dest=&amp;#39;crop_ratio&amp;#39;,type=float, default=0.8, help=&amp;#39;crop_ratio = cropped_size/original_size&amp;#39;) parser.add_argument(&amp;#39;--crop_xoff&amp;#39;, dest=&amp;#39;crop_xoff&amp;#39;, type=float, default=0.5, help=&amp;#39;0: left-aligned, 0.5: mid, 1: right-aligned&amp;#39;) parser.add_argument(&amp;#39;--crop_yoff&amp;#39;, dest=&amp;#39;crop_yoff&amp;#39;, type=float, default=0.5, help=&amp;#39;0: up-aligned, 0.5: mid, 1: down-aligned&amp;#39;) return parser.parse_args() if __name__ == &amp;#39;__main__&amp;#39;: args = Config(argparse.</description>
      <content>&lt;p&gt;2024年5月6日起，20城试点网上护照换发，正好需要更新护照，记录一下网上换发的流程。&lt;/p&gt;
&lt;h2 id=&#34;护照网上换发流程&#34;&gt;护照网上换发流程&lt;/h2&gt;
&lt;p&gt;申请护照换发的网上入口：微信小程序-国家移民管理局政务服务平台-中国公民服务-证件换补发网上申请。&lt;/p&gt;
&lt;p&gt;需提前准备好护照照片、签名照片、身份证照片。完成申请后，等待审批，审批通过后支持工本费，寄到家中到付。&lt;/p&gt;
&lt;h2 id=&#34;照片合规处理&#34;&gt;照片合规处理&lt;/h2&gt;
&lt;p&gt;护照照片有严格的规格要求：格式为jpeg，20k~80k，宽高比3:4，图像大小(354,472)~(480,640)，以及一些隐式要求，如头部占比需足够大、头顶需有一定留白。&lt;/p&gt;
&lt;p&gt;写了一个工具来处理。这个工具同样适用于签名照片和身份证照片，因为这两个照片也有尺寸的要求。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image, ImageOps
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; argparse
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Config&lt;/span&gt;(parser):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  parser&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-i&amp;#39;&lt;/span&gt;, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;input&amp;#39;&lt;/span&gt;, default&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./in.jpg&amp;#34;&lt;/span&gt;, help&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;input photo path&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  parser&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;-o&amp;#39;&lt;/span&gt;, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;output&amp;#39;&lt;/span&gt;, default&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./out.jpg&amp;#34;&lt;/span&gt;, help&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;input photo path&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  parser&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;--height&amp;#39;&lt;/span&gt;, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;height&amp;#39;&lt;/span&gt;, default&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;640&lt;/span&gt;, help&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;desired output height&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  parser&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;--width&amp;#39;&lt;/span&gt;, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;width&amp;#39;&lt;/span&gt;, default&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;480&lt;/span&gt;, help&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;desired output width&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  parser&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;--crop_ratio&amp;#39;&lt;/span&gt;, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;crop_ratio&amp;#39;&lt;/span&gt;,type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;float, default&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;, help&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;crop_ratio = cropped_size/original_size&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  parser&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;--crop_xoff&amp;#39;&lt;/span&gt;, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;crop_xoff&amp;#39;&lt;/span&gt;, type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;float, default&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, help&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;0: left-aligned, 0.5: mid, 1: right-aligned&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  parser&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add_argument(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;--crop_yoff&amp;#39;&lt;/span&gt;, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;crop_yoff&amp;#39;&lt;/span&gt;, type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;float, default&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, help&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;0: up-aligned, 0.5: mid, 1: down-aligned&amp;#39;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; parser&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parse_args()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; __name__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  args &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Config(argparse&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ArgumentParser())
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;input) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; im:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    width, height &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; im&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;size
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cropped_width &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;crop_ratio &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; width
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cropped_height &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;crop_ratio &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; height
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    xdiff &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; width &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; cropped_width
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ydiff &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; height &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; cropped_height
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    xoff &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; xdiff &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;crop_xoff
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    yoff &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ydiff &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;crop_yoff
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    box &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (xoff, yoff, width &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; xdiff &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; xoff, height &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; ydiff &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; yoff)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;input file &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;: original size = (&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;,&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;), cropped size = (&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;,&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;), cropped region=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, desired size = (&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;,&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;)&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;input, width, height, cropped_width, cropped_height, box, args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;width, args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;height))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cropped &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; im&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;crop(box)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    desired_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;width, args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;height)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ImageOps&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(cropped, desired_size)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save(args&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;output)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Reflecting on a Wake-up</title>
      <link>https://cmbbq.github.io/posts/reflecting-on-a-wake-up/</link>
      <pubDate>Wed, 10 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/reflecting-on-a-wake-up/</guid>
      <description>半醒时分，趁着对苏醒过程的记忆还在，赶忙将一些思绪和明悟记录如下：
人体是一个物理计算机。正如热力学计算机、量子计算机一样。
睡眠是意识的暂停。
将醒未醒时，神经网络逐渐被激活。
睡梦本质上是刚刚通电时，尚未完全的逻辑能力和记忆作用下的产物。
意识即连贯且活跃的主观体验生成过程。
记忆是主观体验的append-only历史记录。
睡梦中缺乏五感输入，但主观体验生成过程仍在运转，正如transformer基于过去的tokens预测下一个token，我们的大脑基于过去的主观体验预测下一个瞬间的主观体验。以至于睡梦中有视觉，有听觉，有对话，有剧情，有复杂场景，有奇怪的逻辑，有种种恐惧和欣喜。
连通长期记忆的瞬间，似乎逻辑能力也复活了。
每一次睡眠都是连贯意识的一次死亡。随着生物电的中断、开启，新一天的新生意识替代了昨日的意识。自我是迭代的。自我的维持全部仰赖神经元的自组织，这显然是一种无比脆弱的持久化机制。对温度、压强、磁场都有极端苛刻要求的，远离平衡态的，耗散结构。
小时候自己也有一种醒来的感觉。意识忽然从蒙昧中苏醒。觉得过去的自己浑浑噩噩。具体的时间点和那个时间段的记忆已经全然模糊，但这种苏醒感和年幼的喟叹铭刻在了长期记忆中。
睡梦是反理性时刻，接近幻觉体验，反而突破常规，相当于llm的temperature参数（采样温度）被调高了。主观体验中也包含一小部分抽象和理性，跳脱的抽象和理性就成为灵感之源。
中国文化（诸夏人种）的一丝丝跳脱，来源于周庄梦蝶，这是最朴素的幻觉形式。人类的另一分支，雅利安文化（印欧人种）起源区域则有葡萄酒和大麻，酒精和毒品的致幻作用奠定了西方和印度文化中的强烈宗教色彩。大冰期后出现的东方早期聚落则完全不产葡萄酒和大麻，以至于从一开始就是远离愚昧的理性文明，是人类文明中的世俗化异类，当然缺点就是过于理性，缺了一些狂想、傲慢、疯狂和跳脱。
不要高估理性，不要高估意识，归根到底只是主观体验生成器，基于上N帧预测下一帧，很容易陷入低采样温度的呆板和可预测。</description>
      <content>&lt;p&gt;半醒时分，趁着对苏醒过程的记忆还在，赶忙将一些思绪和明悟记录如下：&lt;/p&gt;
&lt;p&gt;人体是一个物理计算机。正如热力学计算机、量子计算机一样。&lt;/p&gt;
&lt;p&gt;睡眠是意识的暂停。&lt;/p&gt;
&lt;p&gt;将醒未醒时，神经网络逐渐被激活。&lt;/p&gt;
&lt;p&gt;睡梦本质上是刚刚通电时，尚未完全的逻辑能力和记忆作用下的产物。&lt;/p&gt;
&lt;p&gt;意识即连贯且活跃的主观体验生成过程。&lt;/p&gt;
&lt;p&gt;记忆是主观体验的append-only历史记录。&lt;/p&gt;
&lt;p&gt;睡梦中缺乏五感输入，但主观体验生成过程仍在运转，正如transformer基于过去的tokens预测下一个token，我们的大脑基于过去的主观体验预测下一个瞬间的主观体验。以至于睡梦中有视觉，有听觉，有对话，有剧情，有复杂场景，有奇怪的逻辑，有种种恐惧和欣喜。&lt;/p&gt;
&lt;p&gt;连通长期记忆的瞬间，似乎逻辑能力也复活了。&lt;/p&gt;
&lt;p&gt;每一次睡眠都是连贯意识的一次死亡。随着生物电的中断、开启，新一天的新生意识替代了昨日的意识。自我是迭代的。自我的维持全部仰赖神经元的自组织，这显然是一种无比脆弱的持久化机制。对温度、压强、磁场都有极端苛刻要求的，远离平衡态的，耗散结构。&lt;/p&gt;
&lt;p&gt;小时候自己也有一种醒来的感觉。意识忽然从蒙昧中苏醒。觉得过去的自己浑浑噩噩。具体的时间点和那个时间段的记忆已经全然模糊，但这种苏醒感和年幼的喟叹铭刻在了长期记忆中。&lt;/p&gt;
&lt;p&gt;睡梦是反理性时刻，接近幻觉体验，反而突破常规，相当于llm的temperature参数（采样温度）被调高了。主观体验中也包含一小部分抽象和理性，跳脱的抽象和理性就成为灵感之源。&lt;/p&gt;
&lt;p&gt;中国文化（诸夏人种）的一丝丝跳脱，来源于周庄梦蝶，这是最朴素的幻觉形式。人类的另一分支，雅利安文化（印欧人种）起源区域则有葡萄酒和大麻，酒精和毒品的致幻作用奠定了西方和印度文化中的强烈宗教色彩。大冰期后出现的东方早期聚落则完全不产葡萄酒和大麻，以至于从一开始就是远离愚昧的理性文明，是人类文明中的世俗化异类，当然缺点就是过于理性，缺了一些狂想、傲慢、疯狂和跳脱。&lt;/p&gt;
&lt;p&gt;不要高估理性，不要高估意识，归根到底只是主观体验生成器，基于上N帧预测下一帧，很容易陷入低采样温度的呆板和可预测。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Error Handling</title>
      <link>https://cmbbq.github.io/posts/error-handling/</link>
      <pubDate>Tue, 09 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/error-handling/</guid>
      <description>本文讨论现代C++的错误处理问题，结论如下：
宜用Result Monad取代C范式和C++异常，在接口层面清晰定义表明可出错性，和错误清单。 每个模块应有独立的Error类型，不宜用全局统一的Error类型或其subtype/variant，规避过于丝滑的错误传播导致的舒适陷阱。 上述模块级的Error类型一般就是一个enum class，足够紧凑、安全。必需在Error中留存状态的场景再用std::variant+std::visit。 明确需特殊处理的错误宜独占一个类型，不宜和常规错误一起并列在模块级Error的enum class中。 Exception Sucks in Every Possible Way C++的异常具有多个令人绝望的特质：
违反C++的基本设计准则——zero cost abstraction。 鼓励隐藏control flow。 不鼓励在接口中给出错误定义。 对异常类型没有任何约束。 不提供有界的空间时间开销承诺。backtrace耗时多久完全不可预计。 导致编译后的代码膨胀。 不兼容C ABI。 throw需要动态内存分配，catch甚至需要RTTI。嵌入式环境下不会这么奢侈。 总之C++异常机制是一种自然演化而成的历史遗迹，而非合乎理性的程序语言设计。以今天的标准，连写成提案的机会都不会有就会在mailing list被C++语言律师冷嘲热讽而消失。
哪怕现在有很多轻量化甚至零成本抽象的新型exception提案（比如P1095R01和P0709R42），一时半会儿也不会有改观，固有的exception已经根植于历史系统中，包含标准库在内的庞大基础库都难以承受范式迁移的代价。合理对待异常的态度就是弃之不用。
Convey Fallibility in API C风格error code虽然简单，但需要使用者有强大的自控能力，对于现代编程来说还是太过危险、简陋，语言层面本身无法区分参数中哪个是输入，哪个是输出，仰赖约定俗成的命名习惯或注释，缺乏可读性，编程时的心智负担太重。错误传播则缺乏类型安全，往往直接传引用和指针，引发难以追踪的内存安全、线程安全问题。不做任何错误传播，就地解决错误的话，又会导致代码繁琐冗余，总有一天会懒得做错误处理。
如何兼具安全的错误传播、零成本抽象、接口中传达清晰的错误清单？Golang也没做到（几乎和C一样简陋，给了官方的error类型，然而并非泛型或Monad，只是一个返回Error()字符串的接口）。Java有点作弊（封闭且完整的JVM+标准库+Javadoc注释生态圈）。Rust则轻装简行，为我们展示了Result&amp;lt;T,E&amp;gt;的巨大成功——后续我们将其称为Result范式，用函数式编程语境下的Result Monad指代这类结构。
抛开性能、C-兼容性等问题不谈，纯粹从软件系统设计的角度讲，Result Monad也能更好地在API层面清晰无误地将可能出现的错误良好定义。而exception则散落在代码实现中，单看接口根本不知道throw了什么，隐藏了哪些坑，如何处理这些坑——这在软件互操作性上是灾难性的。
如今Rust-like Result范式的std::expected已经进入标准库，自gcc12/clang16起已经可以使用了，受限于旧版编译器的场景则可以用第三方实现。
Keep Local Errors in Their Own Types 基于Result&amp;lt;T,E&amp;gt;或expected&amp;lt;T,E&amp;gt;进行错误处理时，有一个简单方便的做法是将E设置成一个巨大的全局enum，包含所有可能的error code。这样就可以在整个程序中自由地用?操作符或and_then/or_else传播error，达到类似throw+catch异常的效果。
这么做的确可以降低编程时的心智负担，但同时也注入了滥用错误传播的风险——当编码者可以轻易甩锅时，往往就会甩锅。通常一个独立且内聚的模块会对自身内部细节有更多了解，有些问题还是就地处理更妥当，把失败的细节暴露给相对外行的调用者，是在鼓励制造不恰当的耦合。
因此每个系统模块对外应暴露最小化的错误集，将内部可处理的错误在内部消化，并将所有fatal error就地解决——往往是打日志、做些修复（有状态系统）、退出程序。为了避免滥用错误传播，对外暴露的这个错误集还应该有自己的enum类型（C++语境下一般就是enum class，如有必要，可尝试用std::variant+std::visit模拟Rust的Enums + pattern matching，见3），而不宜采用全局统一的某种enum的subtype或variant——迫使直接调用者优先对错误进行处理而不是甩锅给间接调用者，在甩锅非常合理的场景下，必要的显式类型转换或transform_error/map_error，也让甩锅成为一个有意识的system2编程决策，而非system1本能行为。
此外，有一些特殊错误的处理逻辑与众不同，不宜和其他错误共享一个enum class，而应赋予这单个error一个独占的类型。这种情况下，可以用嵌套的expected表示返回值类型，如expectd&amp;lt;expected&amp;lt;T, SpecialError&amp;gt;, Error&amp;gt;，迫使调用者对SpecialError做单独处理。一个典型的例子是sled4中compare_and_swap，返回类型特地包裹了两层，外层是sled::Error，内层是特殊的CompareAndSwapError（CAS失败并非异常，而是常态，对它的处理应视为控制流，而非异常处理），在做这种设计后，sled用户误用这个函数的几率就大大降低了，第一次用?操作符仅将外层的通用Error传播了出去，不至于把必需特殊处理的CAS error也一并甩出去。
fn compare_and_swap( &amp;amp;mut self, key: Key, old_value: Value, new_value: Value ) -&amp;gt; Result&amp;lt;Result&amp;lt;(), CompareAndSwapError&amp;gt;, sled::Error&amp;gt; // we can actually use try `?</description>
      <content>&lt;p&gt;本文讨论现代C++的错误处理问题，结论如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;宜用&lt;code&gt;Result Monad&lt;/code&gt;取代C范式和C++异常，在接口层面清晰定义表明可出错性，和错误清单。&lt;/li&gt;
&lt;li&gt;每个模块应有独立的Error类型，不宜用全局统一的Error类型或其subtype/variant，规避过于丝滑的错误传播导致的舒适陷阱。&lt;/li&gt;
&lt;li&gt;上述模块级的Error类型一般就是一个&lt;code&gt;enum class&lt;/code&gt;，足够紧凑、安全。必需在Error中留存状态的场景再用&lt;code&gt;std::variant&lt;/code&gt;+&lt;code&gt;std::visit&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;明确需特殊处理的错误宜独占一个类型，不宜和常规错误一起并列在模块级Error的enum class中。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;exception-sucks-in-every-possible-way&#34;&gt;Exception Sucks in Every Possible Way&lt;/h1&gt;
&lt;p&gt;C++的异常具有多个令人绝望的特质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;违反C++的基本设计准则——&lt;code&gt;zero cost abstraction&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;鼓励隐藏&lt;code&gt;control flow&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;不鼓励在接口中给出错误定义。&lt;/li&gt;
&lt;li&gt;对异常类型没有任何约束。&lt;/li&gt;
&lt;li&gt;不提供有界的空间时间开销承诺。&lt;code&gt;backtrace&lt;/code&gt;耗时多久完全不可预计。&lt;/li&gt;
&lt;li&gt;导致编译后的代码膨胀。&lt;/li&gt;
&lt;li&gt;不兼容&lt;code&gt;C ABI&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;throw&lt;/code&gt;需要动态内存分配，&lt;code&gt;catch&lt;/code&gt;甚至需要&lt;code&gt;RTTI&lt;/code&gt;。嵌入式环境下不会这么奢侈。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总之C++异常机制是一种自然演化而成的历史遗迹，而非合乎理性的程序语言设计。以今天的标准，连写成提案的机会都不会有就会在mailing list被C++语言律师冷嘲热讽而消失。&lt;/p&gt;
&lt;p&gt;哪怕现在有很多轻量化甚至零成本抽象的新型exception提案（比如P1095R0&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;和P0709R4&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;），一时半会儿也不会有改观，固有的exception已经根植于历史系统中，包含标准库在内的庞大基础库都难以承受范式迁移的代价。合理对待异常的态度就是弃之不用。&lt;/p&gt;
&lt;h1 id=&#34;convey-fallibility-in-api&#34;&gt;Convey Fallibility in API&lt;/h1&gt;
&lt;p&gt;C风格error code虽然简单，但需要使用者有强大的自控能力，对于现代编程来说还是太过危险、简陋，语言层面本身无法区分参数中哪个是输入，哪个是输出，仰赖约定俗成的命名习惯或注释，缺乏可读性，编程时的心智负担太重。错误传播则缺乏类型安全，往往直接传引用和指针，引发难以追踪的内存安全、线程安全问题。不做任何错误传播，就地解决错误的话，又会导致代码繁琐冗余，总有一天会懒得做错误处理。&lt;/p&gt;
&lt;p&gt;如何兼具安全的错误传播、零成本抽象、接口中传达清晰的错误清单？Golang也没做到（几乎和C一样简陋，给了官方的error类型，然而并非泛型或Monad，只是一个返回Error()字符串的接口）。Java有点作弊（封闭且完整的JVM+标准库+Javadoc注释生态圈）。Rust则轻装简行，为我们展示了&lt;code&gt;Result&amp;lt;T,E&amp;gt;&lt;/code&gt;的巨大成功——后续我们将其称为Result范式，用函数式编程语境下的&lt;code&gt;Result Monad&lt;/code&gt;指代这类结构。&lt;/p&gt;
&lt;p&gt;抛开性能、C-兼容性等问题不谈，纯粹从软件系统设计的角度讲，&lt;code&gt;Result Monad&lt;/code&gt;也能更好地在API层面清晰无误地将可能出现的错误良好定义。而exception则散落在代码实现中，单看接口根本不知道throw了什么，隐藏了哪些坑，如何处理这些坑——这在软件互操作性上是灾难性的。&lt;/p&gt;
&lt;p&gt;如今Rust-like Result范式的&lt;a href=&#34;https://en.cppreference.com/w/cpp/utility/expected&#34;&gt;std::expected&lt;/a&gt;已经进入标准库，自gcc12/clang16起已经可以使用了，受限于旧版编译器的场景则可以用第三方实现。&lt;/p&gt;
&lt;h1 id=&#34;keep-local-errors-in-their-own-types&#34;&gt;Keep Local Errors in Their Own Types&lt;/h1&gt;
&lt;p&gt;基于&lt;code&gt;Result&amp;lt;T,E&amp;gt;&lt;/code&gt;或&lt;code&gt;expected&amp;lt;T,E&amp;gt;&lt;/code&gt;进行错误处理时，有一个简单方便的做法是将E设置成一个巨大的全局enum，包含所有可能的error code。这样就可以在整个程序中自由地用&lt;code&gt;?&lt;/code&gt;操作符或&lt;code&gt;and_then/or_else&lt;/code&gt;传播error，达到类似&lt;code&gt;throw&lt;/code&gt;+&lt;code&gt;catch&lt;/code&gt;异常的效果。&lt;/p&gt;
&lt;p&gt;这么做的确可以降低编程时的心智负担，但同时也注入了滥用错误传播的风险——当编码者可以轻易甩锅时，往往就会甩锅。通常一个独立且内聚的模块会对自身内部细节有更多了解，有些问题还是就地处理更妥当，把失败的细节暴露给相对外行的调用者，是在鼓励制造不恰当的耦合。&lt;/p&gt;
&lt;p&gt;因此每个系统模块对外应暴露最小化的错误集，将内部可处理的错误在内部消化，并将所有&lt;code&gt;fatal error&lt;/code&gt;就地解决——往往是打日志、做些修复（有状态系统）、退出程序。为了避免滥用错误传播，对外暴露的这个错误集还应该有自己的enum类型（C++语境下一般就是&lt;code&gt;enum class&lt;/code&gt;，如有必要，可尝试用&lt;code&gt;std::variant&lt;/code&gt;+&lt;code&gt;std::visit&lt;/code&gt;模拟Rust的&lt;code&gt;Enums&lt;/code&gt; + &lt;code&gt;pattern matching&lt;/code&gt;，见&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;），而不宜采用全局统一的某种enum的subtype或variant——迫使直接调用者优先对错误进行处理而不是甩锅给间接调用者，在甩锅非常合理的场景下，必要的显式类型转换或&lt;code&gt;transform_error&lt;/code&gt;/&lt;code&gt;map_error&lt;/code&gt;，也让甩锅成为一个有意识的system2编程决策，而非system1本能行为。&lt;/p&gt;
&lt;p&gt;此外，有一些特殊错误的处理逻辑与众不同，不宜和其他错误共享一个&lt;code&gt;enum class&lt;/code&gt;，而应赋予这单个error一个独占的类型。这种情况下，可以用嵌套的expected表示返回值类型，如&lt;code&gt;expectd&amp;lt;expected&amp;lt;T, SpecialError&amp;gt;, Error&amp;gt;&lt;/code&gt;，迫使调用者对SpecialError做单独处理。一个典型的例子是&lt;code&gt;sled&lt;/code&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;中&lt;code&gt;compare_and_swap&lt;/code&gt;，返回类型特地包裹了两层，外层是&lt;code&gt;sled::Error&lt;/code&gt;，内层是特殊的&lt;code&gt;CompareAndSwapError&lt;/code&gt;（CAS失败并非异常，而是常态，对它的处理应视为控制流，而非异常处理），在做这种设计后，&lt;code&gt;sled&lt;/code&gt;用户误用这个函数的几率就大大降低了，第一次用&lt;code&gt;?&lt;/code&gt;操作符仅将外层的通用Error传播了出去，不至于把必需特殊处理的CAS error也一并甩出去。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-Rust&#34; data-lang=&#34;Rust&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;fn&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compare_and_swap&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mut&lt;/span&gt; self,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  key: &lt;span style=&#34;color:#a6e22e&#34;&gt;Key&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  old_value: &lt;span style=&#34;color:#a6e22e&#34;&gt;Value&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  new_value: &lt;span style=&#34;color:#a6e22e&#34;&gt;Value&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;) -&amp;gt; Result&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Result&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;(), CompareAndSwapError&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;, sled::Error&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// we can actually use try `?` now
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; cas_result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sled.compare_and_swap(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;dogs&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pickles&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catfood&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;let&lt;/span&gt; Err(cas_error) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cas_result {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// handle expected issue
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;另一个例子是需要rollback的atomic batch set操作，一旦失败就会rollback已执行的部分set操作，避免不一致。但这个batch set操作一旦rollback失败，则陷入无法自动恢复，必需人工干预的状态。如果把rollback error当做KvErrorCode中的一种，只返回&lt;code&gt;std::expected&amp;lt;void, KvErrorCode&amp;gt;&lt;/code&gt;，是无法迫使上游用户区分对待rollback失败的——调用者要么把error统一向上传播，要么就简单打个日志，而我们希望调用者意识到数据不一致的灾难已经发生，至少得打个fatal日志。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C++&#34; data-lang=&#34;C++&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;expected&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;expected&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;, KvErrorCode&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;, RollbackFatalError&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; BatchSet(&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; vec&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;pair&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;str, str&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;kvpairs);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;总之，在定义类似&lt;code&gt;expected&amp;lt;expected&amp;lt;T,E1&amp;gt;,E2&amp;gt;&lt;/code&gt;的&lt;code&gt;result monads&lt;/code&gt;时，外层error类型&lt;code&gt;E2&lt;/code&gt;永远要比内层error类型&lt;code&gt;E1&lt;/code&gt;更加fatal，更加erroneous。在避免了灾难性的E2后，才有资格判断是否出现E1。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p1095r0.pdf&#34;&gt;P1095R0: Zero overhead deterministic failure: A unified mechanism for C and C++&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p0709r4.pdf&#34;&gt;P0709R4: Zero-overhead deterministic exceptions: Throwing values&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://thatonegamedev.com/cpp/rust-enums-in-modern-cpp-match-pattern/&#34;&gt;Rust enums in Modern C++ – Match Pattern&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://sled.rs/errors&#34;&gt;Error Handling in a Correctness-Critical Rust Project&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Truth vs Trust</title>
      <link>https://cmbbq.github.io/posts/truth-vs-trust/</link>
      <pubDate>Sat, 09 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/truth-vs-trust/</guid>
      <description>Truth or Non-harm Cory Clark1在访谈中提及她的研究2：经过对130所名校480+心理学教授的调研，她发现男性更倾向于“对真理的追求是不容协商的”，认可“学术自由”，而女性则倾向“应平衡对真理的追求和其他的一些道德目标，如社会公平、心理安全”，认为支持“学术自由”的前提是它不会给特定人群造成伤害。对本科生的调研表明，女生更容认同对散布冒犯性言论的人禁言。随着学术界的女性比例从几十年前的绝对少数提高到今天的事实多数，世界上最有声望的Springer Nature系期刊也开始吸纳女性心目中更优先的道德顾虑，对可能冒犯某些社会群体的文章撤稿或拒稿。
2010年代起源的cancel culture是现代社会女性化的一种表现，可以说是一种历史必然，不能将其简单解读为统治阶级的社会纵切和矛盾转移——它有广泛的大众心理基础，男性视角下的言论自由危机恰恰是女性视角下必要的社会正义和道德责任，男性视角下违反游戏规则的social exclusion或ostracism恰恰是女性视角下净化圈子、维护正义的惯常工具。
Warriors and Worriers 男女之间的价值差异，自然有其心理学和行为学渊源。但根本上来讲，这种差异出现得远比人类文明本身更加古老，甚至无法用文化人类学去解释，必须诉诸生物学和现代进化理论。Joyce Benenson3在《Warriors and Worriers》一书中提出：男性形成有竞争力的战帮，以某种荣誉感或规则凝聚战斗同盟，对抗外敌，有时没有外敌也会塑造外敌。女性则避免形成过大的团队，杜绝社交圈中的不忠和隐患，维持可信互助的小团体，以期望在分娩时得到可靠女性的支持，在养活幼崽阶段借助小团体力量共同抚养。
这种显著的行为和价值差异在人类幼崽时期就已经分化：小男孩在一起玩耍时极度重视规则，会花大量时间制定规则、利用规则、争论规则、绕过规则、维护规则，这种游戏可以一直玩下去。而小女孩们一起玩耍时则会轮流去赢，哪怕年纪大些的女孩本可以每轮都赢。没有女孩争论规则，因为假如女孩们就规则争论起来，“维持可信小团体友谊”这个游戏就已经失败了，具体游戏的输赢哪怕对于小女孩来说也太过幼稚。女孩们的游戏也不会持续太长时间——因为时间一长总会出现争议，而争议是不允许出现的。
对于女性来说，男性的动机是愚蠢、幼稚的。父权社会所形成的荣誉感和规则是抽象的男性构建。在女性自身和她的孩子所面临的真切死亡威胁面前，信任和互助永远是第一位的，所谓“骄傲”和“荣耀”算什么，“追求真理不容妥协”又算什么？传播基督教福音而被罗马人钉在十字架上、捍卫日心说被基督教会烧死并不比尝试翼装飞行或高楼跑酷挂掉、成年礼猎杀狮子结果被反杀更高尚，都不过是不同男性战团内部的帮派社交动作罢了。
Gangs vs Circles 男性作为warriors倾向于形成帮派和战团。女性作为worriers倾向于形成朋友圈和互助会。这两种social groups正是对现代社会两类组织的隐喻，第一类是任务导向的战斗机构，第二类信任导向的利益同盟。
典型的第一类组织是互联网企业的技术部门，涉及30k涨幅的晋升完全不需要人情往来，工作得到素未谋面的专家评委和很少当面交流的上级认可就可以敲定。从业者习以为常，并不觉得这在中国是多么反常的事情。技术管理者完全可以公开坦率地在团队汇报时评价A做出了突破性创新，B的某个方法有问题，C的论证不够严格，D的可用性做得一塌糊涂。从管理者到被管理者都不觉得客观评价会损伤团队和睦，反而话说得不够明白，批评不够尖锐，质疑该出现而不出现，才会导致效率降低。
典型的第二类组织是事业单位和政府部门。尽管一再强调反腐，如今哪怕是3k涨幅的晋升也经常涉及到和同事、领导的礼尚往来，甚至和竞争对手的勾心斗角。领导在公开场合有一百种办法讲话讲得漂亮，但客观评价就是那第一百零一种最差的选择——一旦客观，难免拉一踩一，平白制造矛盾，甚至有损自身威信。
过去我一直没有理解为什么会出现这样泾渭分明的分野。现在我有所领悟。可以把实验室想象成远古战团：首席科学家就是战团领袖，不同等级的研究员就是战士，在他们眼中只要是合格的战士就可以加入，只要在战斗中证明了自己就值得晋升，事实上，战友越强壮，自己在战场上存活的几率也就越高。而“为了真理”就是他们的战斗口号。战团上下高呼“为了真理而战！”，在狂热的气氛中，一起向某一类技术问题或某个sota指标发起挑战，正如一万年前的原始部落猎杀野兽，或屠戮其他部族。
类似地，可以把事业单位等职能部门想象成姐妹互助会。在这个小圈子中，队友更强壮也不会增加自己在战场上的生存几率——事实上很可能根本没有外敌可言，也不必上战场。但任何一个看似笑容可亲的成员都能暗地里进行举报，或在晋升投票时暗自投下反对你的一票。这种背叛成了最大的恐惧，你不希望出现这种背叛，正如风雪交加的原始村落里正在分娩的女性，不希望被嫉恨自己的闺蜜“一不小心”把门打开，放进致命的严寒。
一万年太短，大冰期以来，基因层面的人类其实从来就没有变。
Cory Clark是一位行为科学家，是宾夕法尼亚大学对抗协作研究中心的执行主任和联合创始人，也是该大学心理学系的访问学者。&amp;#160;&amp;#x21a9;&amp;#xfe0e;
The Feminisation of Society - Cory Clark&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Joyce Benenson是哈佛大学人类进化生物学讲师。&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
      <content>&lt;h2 id=&#34;truth-or-non-harm&#34;&gt;Truth or Non-harm&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Cory Clark&lt;/code&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;在访谈中提及她的研究&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;：经过对130所名校480+心理学教授的调研，她发现男性更倾向于“对真理的追求是不容协商的”，认可“学术自由”，而女性则倾向“应平衡对真理的追求和其他的一些道德目标，如社会公平、心理安全”，认为支持“学术自由”的前提是它不会给特定人群造成伤害。对本科生的调研表明，女生更容认同对散布冒犯性言论的人禁言。随着学术界的女性比例从几十年前的绝对少数提高到今天的事实多数，世界上最有声望的&lt;code&gt;Springer Nature&lt;/code&gt;系期刊也开始吸纳女性心目中更优先的道德顾虑，对可能冒犯某些社会群体的文章撤稿或拒稿。&lt;/p&gt;
&lt;p&gt;2010年代起源的&lt;code&gt;cancel culture&lt;/code&gt;是现代社会女性化的一种表现，可以说是一种历史必然，不能将其简单解读为统治阶级的社会纵切和矛盾转移——它有广泛的大众心理基础，男性视角下的言论自由危机恰恰是女性视角下必要的社会正义和道德责任，男性视角下违反游戏规则的&lt;code&gt;social exclusion&lt;/code&gt;或&lt;code&gt;ostracism&lt;/code&gt;恰恰是女性视角下净化圈子、维护正义的惯常工具。&lt;/p&gt;
&lt;h2 id=&#34;warriors-and-worriers&#34;&gt;Warriors and Worriers&lt;/h2&gt;
&lt;p&gt;男女之间的价值差异，自然有其心理学和行为学渊源。但根本上来讲，这种差异出现得远比人类文明本身更加古老，甚至无法用文化人类学去解释，必须诉诸生物学和现代进化理论。&lt;code&gt;Joyce Benenson&lt;/code&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;在&lt;code&gt;《Warriors and Worriers》&lt;/code&gt;一书中提出：男性形成有竞争力的战帮，以某种荣誉感或规则凝聚战斗同盟，对抗外敌，有时没有外敌也会塑造外敌。女性则避免形成过大的团队，杜绝社交圈中的不忠和隐患，维持可信互助的小团体，以期望在分娩时得到可靠女性的支持，在养活幼崽阶段借助小团体力量共同抚养。&lt;/p&gt;
&lt;p&gt;这种显著的行为和价值差异在人类幼崽时期就已经分化：小男孩在一起玩耍时极度重视规则，会花大量时间制定规则、利用规则、争论规则、绕过规则、维护规则，这种游戏可以一直玩下去。而小女孩们一起玩耍时则会轮流去赢，哪怕年纪大些的女孩本可以每轮都赢。没有女孩争论规则，因为假如女孩们就规则争论起来，“维持可信小团体友谊”这个游戏就已经失败了，具体游戏的输赢哪怕对于小女孩来说也太过幼稚。女孩们的游戏也不会持续太长时间——因为时间一长总会出现争议，而争议是不允许出现的。&lt;/p&gt;
&lt;p&gt;对于女性来说，男性的动机是愚蠢、幼稚的。父权社会所形成的荣誉感和规则是抽象的男性构建。在女性自身和她的孩子所面临的真切死亡威胁面前，信任和互助永远是第一位的，所谓“骄傲”和“荣耀”算什么，“追求真理不容妥协”又算什么？传播基督教福音而被罗马人钉在十字架上、捍卫日心说被基督教会烧死并不比尝试翼装飞行或高楼跑酷挂掉、成年礼猎杀狮子结果被反杀更高尚，都不过是不同男性战团内部的帮派社交动作罢了。&lt;/p&gt;
&lt;h2 id=&#34;gangs-vs-circles&#34;&gt;Gangs vs Circles&lt;/h2&gt;
&lt;p&gt;男性作为&lt;code&gt;warriors&lt;/code&gt;倾向于形成帮派和战团。女性作为&lt;code&gt;worriers&lt;/code&gt;倾向于形成朋友圈和互助会。这两种&lt;code&gt;social groups&lt;/code&gt;正是对现代社会两类组织的隐喻，第一类是任务导向的战斗机构，第二类信任导向的利益同盟。&lt;/p&gt;
&lt;p&gt;典型的第一类组织是互联网企业的技术部门，涉及30k涨幅的晋升完全不需要人情往来，工作得到素未谋面的专家评委和很少当面交流的上级认可就可以敲定。从业者习以为常，并不觉得这在中国是多么反常的事情。技术管理者完全可以公开坦率地在团队汇报时评价A做出了突破性创新，B的某个方法有问题，C的论证不够严格，D的可用性做得一塌糊涂。从管理者到被管理者都不觉得客观评价会损伤团队和睦，反而话说得不够明白，批评不够尖锐，质疑该出现而不出现，才会导致效率降低。&lt;/p&gt;
&lt;p&gt;典型的第二类组织是事业单位和政府部门。尽管一再强调反腐，如今哪怕是3k涨幅的晋升也经常涉及到和同事、领导的礼尚往来，甚至和竞争对手的勾心斗角。领导在公开场合有一百种办法讲话讲得漂亮，但客观评价就是那第一百零一种最差的选择——一旦客观，难免拉一踩一，平白制造矛盾，甚至有损自身威信。&lt;/p&gt;
&lt;p&gt;过去我一直没有理解为什么会出现这样泾渭分明的分野。现在我有所领悟。可以把实验室想象成远古战团：首席科学家就是战团领袖，不同等级的研究员就是战士，在他们眼中只要是合格的战士就可以加入，只要在战斗中证明了自己就值得晋升，事实上，战友越强壮，自己在战场上存活的几率也就越高。而“为了真理”就是他们的战斗口号。战团上下高呼“为了真理而战！”，在狂热的气氛中，一起向某一类技术问题或某个&lt;code&gt;sota&lt;/code&gt;指标发起挑战，正如一万年前的原始部落猎杀野兽，或屠戮其他部族。&lt;/p&gt;
&lt;p&gt;类似地，可以把事业单位等职能部门想象成姐妹互助会。在这个小圈子中，队友更强壮也不会增加自己在战场上的生存几率——事实上很可能根本没有外敌可言，也不必上战场。但任何一个看似笑容可亲的成员都能暗地里进行举报，或在晋升投票时暗自投下反对你的一票。这种背叛成了最大的恐惧，你不希望出现这种背叛，正如风雪交加的原始村落里正在分娩的女性，不希望被嫉恨自己的闺蜜“一不小心”把门打开，放进致命的严寒。&lt;/p&gt;
&lt;p&gt;一万年太短，大冰期以来，基因层面的人类其实从来就没有变。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Cory Clark是一位行为科学家，是宾夕法尼亚大学对抗协作研究中心的执行主任和联合创始人，也是该大学心理学系的访问学者。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?reload=9&amp;amp;app=desktop&amp;amp;v=1GYtRo5Ggvo&amp;amp;ab_channel=MaidenMotherMatriarchwithLouisePerry&#34;&gt;The Feminisation of Society - Cory Clark&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Joyce Benenson是哈佛大学人类进化生物学讲师。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Linkers &amp; Loaders</title>
      <link>https://cmbbq.github.io/posts/linkers-and-loaders/</link>
      <pubDate>Mon, 04 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/linkers-and-loaders/</guid>
      <description>链接器或加载器的基本工作是绑定——把抽象的名字绑定到更具体的名字上，比如把getline函数绑定到“.text的第612字节处”。
地址绑定的历史 打孔卡带(punched card/paper tape)计算机时代，程序员把符号程序手动汇编成机器码输入机器。代码中如果使用了名字（符号地址），也需程序员手动翻译成地址。因此代码里的任意一条指令的增减都可能影响到机器码中的所有地址。
这是名字与地址过早绑定的恶果。汇编器允许程序员用符号名字写程序，解决了这一问题。
打孔卡带时代也已经有了子程序和库的概念，当时把一些子程序分类存放在卡带里，主程序使用子程序时就需要加载并重排子程序卡带。这个过程实际上就是手动的library search和重定位。
在操作系统出现前，每个程序都默认把整个机器内存空间独享，自然可以用固定的内存地址，毕竟机器的所有地址都是可用的。操作系统出现后，程序必需与操作系统、甚至其他程序共享内存空间，实际地址在操作系统加载程序成后才能知道，这又将地址绑定从链接时延后至加载时，因此重定位加载器从链接器中独立出来。链接器负责部分地址绑定，在每个程序内部做相对地址重分配；加载器负责重定位，进行最终的地址分配。
早期内存非常紧张，程序体量很快超过了内存上限，因此链接器提供了一种overlay机制，允许不同部分的程序共享同一块内存。直到90年代出现虚拟内存之后这个机制才消失。硬件重定位和虚拟内存的出现使链接器和加载器变得更简单。
计算机执行一个程序的多个副本时，这个程序的大部分内容实际上是可以共享的，因此引入了分段机制，将只读代码段和可写代码段分离，一个机器上只需要加载一份只读代码段。因此链接器需额外为每个代码段分配地址。
计算机即使在执行多个不同程序，这些程序往往也共享大量代码，因此出现了共享库。静态共享库不够灵活，库里的任何代码改动都要求重新链接。因此出现了动态共享库，令符号和分段并不绑定实际地址，而是推迟到程序运行时再进行绑定——甚至还能进一步延迟，在首次调用时才绑定。
链接 vs 加载 链接器负责符号解析，加载器负责程序加载，二者都可以做重定位，也存在三合一的linking loaders。
符号解析：所谓符号，就是程序调用子程序的媒依。链接器将诸如sqrt这样的函数名解析为库中的位置，并给调用方的代码打个补丁，让调用指令指向这个位置。 程序加载：加载即把程序拷贝到内存，也顺带做些设置内存保护位、安排虚拟内存映射等事。 重定位：编译器、汇编器为每个编译单元（文件）生成的程序地址从零开始。往往链接器把多个子程序拼成一个完整程序时会做一次重定位。这个完整程序的地址仍然从零开始，因此加载器将程序加载进内存后又会做一次重定位。 2-pass链接 链接和编译、汇编一样，也是个2-pass过程。链接器以对象文件、静态库、动态库、命令行参数为输入，输出可执行文件，如开启debug，还伴随生成debugger符号文件或load map。其中对象文件、静态库、动态库都是分段的（code/data），且至少有一个符号表，导出/导入一些符号。
链接器在1st pass中扫描所有输入文件，获取各分段大小，收集所有符号定义和引用，从而创建一个统合分段表和一个统合符号表，进而为每个符号分配位置，确定输出地址空间的分段大小和位置。
随后链接器在2nd pass中读取此前生成的对象文件，把所有符号引用替换为数值地址，把所有代码、数据中的内存地址调整成重定位后的分段地址，最后再给更新后的对象文件添加header、重定位段、符号表。
如果程序用到了动态链接，则符号表包含runtime linker解析动态符号所需的信息。通常链接器还会生成一些胶水代码，为调用动态链接库提供调用例程。
无论程序是否使用动态链接，符号表中总会提供一些供重链接和debug用的信息——很多对象格式都是可以重链接的，即允许生成的对象文件作为后续链接的输入。
对象文件 编译器和汇编器为源码生成的二进制码文件即对象文件，包含header、object code、重定向列表（一些链接时需重定向的位置）、全局符号表、debug信息等内容。
对象文件作为原材料，最终可用于三类最终产物：linkables、executables、loadables。
Linkables包含丰富的符号信息、重定位信息，object code也组织成细小的逻辑段，方便链接器后期加工，做符号解析和重定位。 Executables包含页对齐的object code（允许映射到虚拟内存），不需要提供动态链接需求之外的任何符号信息，也只需要提供很少或不提供重定位信息。其object code被组织成较大粒度的段或反映硬件执行环境的特定分段，往往分成read-only和read-write pages。 Loadables可能只需包含object code，也可能需提供完整的符号和重定位信息，这取决于系统runtime的实现。 典型的对象文件格式Unix a.out包含header、text section、data section、other sections。 其header（以BSD为例）包含text segment size、inited data size、uninited data size（BSS段）、symbol table size、entry point（起始地址）、text 重定位 size、data 重定位 size。
加载a.out时，操作系统先读header，获取各分段大小，再查找是否已存在共享代码段，若没有再新建一个，将text分段映射到内存空间，创建足够大的私有数据分段，把bss分段初始化为零，创建并映射栈分段（往往独立于数据段，因为堆和栈增长速度往往不同），把程序运行的初始参数入栈，最后设置寄存器并跳转到程序的起始地址。
为了减少不必要的paging，让对象文件能直接映射到4K的页，后续UNIX上出了一些pageable格式把header扩展到4K，把text分段的边界向上取整到下一个4K。这样做的缺点是不够紧凑，浪费磁盘空间。后来又出现了一些compact pageable格式，把header直接视作text分段的一部分（QMAGIC和ELF）。
a.out不支持重定位，也不支持C++的initializer/finalizer代码的特殊处理，被支持cross-compilation、动态链接等机制的ELF（Executable and Linking Format）取代。
ELF采用了DWARF作为其debugging格式，提供三种文件类型：relocatable、executable、shared object。</description>
      <content>&lt;p&gt;链接器或加载器的基本工作是绑定——把抽象的名字绑定到更具体的名字上，比如把&lt;code&gt;getline&lt;/code&gt;函数绑定到“.text的第612字节处”。&lt;/p&gt;
&lt;h2 id=&#34;地址绑定的历史&#34;&gt;地址绑定的历史&lt;/h2&gt;
&lt;p&gt;打孔卡带(punched card/paper tape)计算机时代，程序员把符号程序手动汇编成机器码输入机器。代码中如果使用了名字（符号地址），也需程序员手动翻译成地址。因此代码里的任意一条指令的增减都可能影响到机器码中的所有地址。&lt;/p&gt;
&lt;p&gt;这是名字与地址过早绑定的恶果。汇编器允许程序员用符号名字写程序，解决了这一问题。&lt;/p&gt;
&lt;p&gt;打孔卡带时代也已经有了子程序和库的概念，当时把一些子程序分类存放在卡带里，主程序使用子程序时就需要加载并重排子程序卡带。这个过程实际上就是手动的&lt;code&gt;library search&lt;/code&gt;和&lt;code&gt;重定位&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;在操作系统出现前，每个程序都默认把整个机器内存空间独享，自然可以用固定的内存地址，毕竟机器的所有地址都是可用的。操作系统出现后，程序必需与操作系统、甚至其他程序共享内存空间，实际地址在操作系统加载程序成后才能知道，这又将地址绑定从链接时延后至加载时，因此重定位加载器从链接器中独立出来。链接器负责部分地址绑定，在每个程序内部做相对地址重分配；加载器负责重定位，进行最终的地址分配。&lt;/p&gt;
&lt;p&gt;早期内存非常紧张，程序体量很快超过了内存上限，因此链接器提供了一种overlay机制，允许不同部分的程序共享同一块内存。直到90年代出现虚拟内存之后这个机制才消失。硬件重定位和虚拟内存的出现使链接器和加载器变得更简单。&lt;/p&gt;
&lt;p&gt;计算机执行一个程序的多个副本时，这个程序的大部分内容实际上是可以共享的，因此引入了分段机制，将只读代码段和可写代码段分离，一个机器上只需要加载一份只读代码段。因此链接器需额外为每个代码段分配地址。&lt;/p&gt;
&lt;p&gt;计算机即使在执行多个不同程序，这些程序往往也共享大量代码，因此出现了共享库。静态共享库不够灵活，库里的任何代码改动都要求重新链接。因此出现了动态共享库，令符号和分段并不绑定实际地址，而是推迟到程序运行时再进行绑定——甚至还能进一步延迟，在首次调用时才绑定。&lt;/p&gt;
&lt;h2 id=&#34;链接-vs-加载&#34;&gt;链接 vs 加载&lt;/h2&gt;
&lt;p&gt;链接器负责符号解析，加载器负责程序加载，二者都可以做重定位，也存在三合一的&lt;code&gt;linking loaders&lt;/code&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;符号解析：所谓符号，就是程序调用子程序的媒依。链接器将诸如sqrt这样的函数名解析为库中的位置，并给调用方的代码打个补丁，让调用指令指向这个位置。&lt;/li&gt;
&lt;li&gt;程序加载：加载即把程序拷贝到内存，也顺带做些设置内存保护位、安排虚拟内存映射等事。&lt;/li&gt;
&lt;li&gt;重定位：编译器、汇编器为每个编译单元（文件）生成的程序地址从零开始。往往链接器把多个子程序拼成一个完整程序时会做一次重定位。这个完整程序的地址仍然从零开始，因此加载器将程序加载进内存后又会做一次重定位。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-pass链接&#34;&gt;2-pass链接&lt;/h2&gt;
&lt;p&gt;链接和编译、汇编一样，也是个2-pass过程。链接器以对象文件、静态库、动态库、命令行参数为输入，输出可执行文件，如开启debug，还伴随生成debugger符号文件或load map。其中对象文件、静态库、动态库都是分段的（code/data），且至少有一个符号表，导出/导入一些符号。&lt;/p&gt;
&lt;p&gt;链接器在1st pass中扫描所有输入文件，获取各分段大小，收集所有符号定义和引用，从而创建一个统合分段表和一个统合符号表，进而为每个符号分配位置，确定输出地址空间的分段大小和位置。&lt;/p&gt;
&lt;p&gt;随后链接器在2nd pass中读取此前生成的对象文件，把所有符号引用替换为数值地址，把所有代码、数据中的内存地址调整成重定位后的分段地址，最后再给更新后的对象文件添加header、重定位段、符号表。&lt;/p&gt;
&lt;p&gt;如果程序用到了动态链接，则符号表包含&lt;code&gt;runtime linker&lt;/code&gt;解析动态符号所需的信息。通常链接器还会生成一些胶水代码，为调用动态链接库提供调用例程。&lt;/p&gt;
&lt;p&gt;无论程序是否使用动态链接，符号表中总会提供一些供重链接和debug用的信息——很多对象格式都是可以重链接的，即允许生成的对象文件作为后续链接的输入。&lt;/p&gt;
&lt;h2 id=&#34;对象文件&#34;&gt;对象文件&lt;/h2&gt;
&lt;p&gt;编译器和汇编器为源码生成的二进制码文件即对象文件，包含header、object code、重定向列表（一些链接时需重定向的位置）、全局符号表、debug信息等内容。&lt;/p&gt;
&lt;p&gt;对象文件作为原材料，最终可用于三类最终产物：linkables、executables、loadables。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linkables包含丰富的符号信息、重定位信息，object code也组织成细小的逻辑段，方便链接器后期加工，做符号解析和重定位。&lt;/li&gt;
&lt;li&gt;Executables包含页对齐的object code（允许映射到虚拟内存），不需要提供动态链接需求之外的任何符号信息，也只需要提供很少或不提供重定位信息。其object code被组织成较大粒度的段或反映硬件执行环境的特定分段，往往分成read-only和read-write pages。&lt;/li&gt;
&lt;li&gt;Loadables可能只需包含object code，也可能需提供完整的符号和重定位信息，这取决于系统runtime的实现。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;典型的对象文件格式Unix a.out包含header、text section、data section、other sections。
其header（以BSD为例）包含text segment size、inited data size、uninited data size（BSS段）、symbol table size、entry point（起始地址）、text 重定位 size、data 重定位 size。&lt;/p&gt;
&lt;p&gt;加载a.out时，操作系统先读header，获取各分段大小，再查找是否已存在共享代码段，若没有再新建一个，将text分段映射到内存空间，创建足够大的私有数据分段，把bss分段初始化为零，创建并映射栈分段（往往独立于数据段，因为堆和栈增长速度往往不同），把程序运行的初始参数入栈，最后设置寄存器并跳转到程序的起始地址。&lt;/p&gt;
&lt;p&gt;为了减少不必要的paging，让对象文件能直接映射到4K的页，后续UNIX上出了一些pageable格式把header扩展到4K，把text分段的边界向上取整到下一个4K。这样做的缺点是不够紧凑，浪费磁盘空间。后来又出现了一些compact pageable格式，把header直接视作text分段的一部分（QMAGIC和ELF）。&lt;/p&gt;
&lt;p&gt;a.out不支持重定位，也不支持C++的initializer/finalizer代码的特殊处理，被支持cross-compilation、动态链接等机制的ELF（Executable and Linking Format）取代。&lt;/p&gt;
&lt;p&gt;ELF采用了DWARF作为其debugging格式，提供三种文件类型：relocatable、executable、shared object。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relocatables可被编译器、汇编器创建，但需要进一步被链接后才能运行。&lt;/li&gt;
&lt;li&gt;Executables做完了重定向和静态符号解析，可直接映射至内存。&lt;/li&gt;
&lt;li&gt;Shared objects就是动态链接库，包含链接时所需的符号信息和运行时可执行的代码。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ELF被设计为具备双重属性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加载视角下是即将放入内存的loadable segments：在加载器看来，ELF是由program header描述的一组分段，无需关心分区。可执行分段只有廖廖几个。典型的BFD-ld或Gold链接的Linux ELF一般将其分为2个loadable分段：RE（只读可执行，包含.text，.rodata等）、RW（读写，包含.data，.bss等）。这样可以减少内核mmap次数到2次，但把只读数据放进只读可执行分段总归牺牲了安全性。比较新的Linux出于安全考虑分成3个分段R、RE、RW。将ELF header和.rodata放进R。更新一些的BFD-ld虽然把ELF header和.rodata分在R分段，但忘记合并这两个R了，导致出现4个loadable分段。&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;链接视角下是磁盘上的linkable sections：分区机制允许链接器对ELF进一步加工。单个分段由若干分区（section）组成。比如一个loadable read-only分段可以包含可执行代码、只读数据、动态链接符号这三个分区。Relocatables有分区表。Executables有ELF header表。Shared objects则兼具二者。典型的ELF可重定位程序包含十余个分区，例如.text、.data、.rodata、.bss、.rel.text（代码分区的重定位信息）、.rel.data、.rel.rodata、.init（C++全局变量构造函数）、.fini（C++全局变量析构函数）、.symtab（符号表）、.dynsym（动态库符号表）、.strtab、.dynstr、.interp（解释器路径）。可执行ELF和重定位ELF在格式上基本一致，只不过数据被重新安排，使文件能直接映射到内存，即pageable。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://149520725.v2.pressablecdn.com//wp-content/uploads/2018/01/Image5.png&#34; alt=&#34;sections_segments&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;静态库和动态库&#34;&gt;静态库和动态库&lt;/h2&gt;
&lt;p&gt;静态库本质上是一组对象文件，再稍微多加一点点信息（甚至有些系统直接把对象文件拼接起来就算是合法的静态链接库了）。&lt;/p&gt;
&lt;p&gt;链接器在处理完常规输入文件后，如果发现有的导入符号未定义，则遍历库，寻找该符号，将包含这些符号的文件链接起来。&lt;/p&gt;
&lt;p&gt;动态库让链接过程变得稍微复杂一点，将上述工作部分从链接转移到了加载时。链接器会在链接时找到能够解析未定义符号的那些动态库，但暂不链接任何代码，而是在输出文件里备注一下在哪个动态库可以招到特定符号，从而令加载器在加载程序时绑定这些动态库。&lt;/p&gt;
&lt;h2 id=&#34;链接需遵循abi&#34;&gt;链接需遵循ABI&lt;/h2&gt;
&lt;p&gt;每个操作系统都会给出一个ABI让程序使用，包括一些系统调用、封装系统调用的技术、内存地址规则、寄存器规则、调用约定。链接器必需是ABI compliant的，必需遵循ABI要求，提供特定静态数据的地址表，以符合调用约定的方式进行标准的函数调用。&lt;/p&gt;
&lt;p&gt;以Intel x86为例，提供6个32位通用寄存器EAX、EBX、ECX、EDX、ESI、EDI，两个寻址寄存器EBP、ESP，6个16位寄存器CS、DS、ES、FS、GS、SS。其中ESP是硬件栈指针，EBP通常是当前帧指针。&lt;/p&gt;
&lt;p&gt;x86架构里存在硬件栈，有硬件返回命令——硬件电路将返回地址push到栈上，并跳转至该地址。其他架构大多保存在寄存器里，因此x86上软件无需把寄存器里的返回地址放到主存的某个地方保存起来。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/questions/57761007/why-an-elf-executable-could-have-4-load-segments&#34;&gt;Why an ELF executable could have 4 LOAD segments?&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>CAT01: Orders</title>
      <link>https://cmbbq.github.io/posts/category-theory-1-orders/</link>
      <pubDate>Mon, 26 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/category-theory-1-orders/</guid>
      <description>Preorders Starting from sets and subsets, we can define the relation between s$A$ and $B$ as a subset $R \in A\times B$.
Every function is a relation, satisfying 2 properties:
$\forall a \in A$, there exists $b \in B$, such that $(a,b)\in \mathbb{R}$ $\forall a, b_1, b_2$, if $(a,b_1) \in R$ and $(a, b_2) \in \mathbb{R}$, then $b_1 = b_2$. Order, equivalence, tolerance are all relations.
A function $f: A\rightarrowtail B$ is called injection if $\forall a_1, a_2, b$, if $(a_1, b), (a_2, b) \in R$, then $a_1 = a_2$.</description>
      <content>&lt;h2 id=&#34;preorders&#34;&gt;Preorders&lt;/h2&gt;
&lt;p&gt;Starting from &lt;code&gt;sets&lt;/code&gt; and &lt;code&gt;subsets&lt;/code&gt;, we can define the &lt;code&gt;relation&lt;/code&gt; between s$A$ and $B$ as a &lt;code&gt;subset&lt;/code&gt; $R \in A\times B$.&lt;/p&gt;
&lt;p&gt;Every &lt;code&gt;function&lt;/code&gt; is a relation, satisfying 2 properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\forall a \in A$, there exists $b \in B$, such that $(a,b)\in \mathbb{R}$&lt;/li&gt;
&lt;li&gt;$\forall a, b_1, b_2$, if $(a,b_1) \in R$ and $(a, b_2) \in \mathbb{R}$, then $b_1 = b_2$.&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Order, equivalence, tolerance are all relations.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A &lt;code&gt;function&lt;/code&gt; $f: A\rightarrowtail B$ is called &lt;code&gt;injection&lt;/code&gt; if $\forall a_1, a_2, b$, if $(a_1, b), (a_2, b) \in R$, then $a_1 = a_2$.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;function&lt;/code&gt; $f: A\twoheadrightarrow B$ is called &lt;code&gt;surjection&lt;/code&gt; if $\forall b \in B$, there exists an $a \in A$, such that $f(a) = b$.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;function&lt;/code&gt; $f: A \overset{\cong} \rightarrowtail B$ is called &lt;code&gt;bijection&lt;/code&gt; if it is both surjective and injective.&lt;/p&gt;
&lt;p&gt;An &lt;code&gt;identity function&lt;/code&gt; on a set $X$, denoted $id_X$. It is the bijective function $id_X(x) = x$.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;partition&lt;/code&gt; on $A$ is just a surjection on some other set $P$: $A \twoheadrightarrow P$.&lt;/p&gt;
&lt;p&gt;We can order &lt;code&gt;partitions&lt;/code&gt;: $A \twoheadrightarrow P_1$, $A \twoheadrightarrow P_2$. We say $P_1 \leqslant P_2$, if there is a &lt;code&gt;function&lt;/code&gt; $P_1 \rightarrow P_2$ making the diagram commute: $A \twoheadrightarrow P_1 \rightarrow P_2$.&lt;/p&gt;
&lt;p&gt;So $A \twoheadrightarrow A$ is the minimum &lt;code&gt;partition&lt;/code&gt;, and the $A \twoheadrightarrow \underline{1}$ is the maximum &lt;code&gt;partition&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;An &lt;code&gt;preorder&lt;/code&gt; is&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;a set $S$, and&lt;/li&gt;
&lt;li&gt;a relation $≤ : \subseteq S \times S$&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;satisfying 2 properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$S ≤ S$. &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;$\forall S_1$, $S_2$, $S_3$, if $S_1 ≤ S_2$ and $S_2 ≤ S_3$, then $S_1 ≤ S_3$. &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;A &lt;code&gt;preorder&lt;/code&gt; is a &lt;code&gt;category&lt;/code&gt; with at most one &lt;code&gt;morphism&lt;/code&gt; between any two objects. Something slightly more complicated is that a &lt;code&gt;preorder&lt;/code&gt; is a &lt;code&gt;Bool-enriched category&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;meets-and-joins&#34;&gt;Meets and Joins&lt;/h2&gt;
&lt;p&gt;Order creates &lt;code&gt;meets&lt;/code&gt; and &lt;code&gt;joins&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Joining $A$ and $B$, denoted by $A\vee B$, results in the least &lt;code&gt;partition&lt;/code&gt; bigger than both $A$ and $B$. That is $A ≤ (A\vee B)$ and $A ≤ (A\vee B)$. And for any C, if $A ≤ C$ and $B ≤ C$ then $(A\vee B) ≤ C$.&lt;/p&gt;
&lt;p&gt;Put it formally. Let $(P, ≤)$ be a &lt;code&gt;preorder&lt;/code&gt;, and let $A \subseteq P$ be a &lt;code&gt;subset&lt;/code&gt;. We say that an element $p \in P$ is a &lt;code&gt;meet&lt;/code&gt; of $A$ if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\forall a\in A$, we have $p≤ a$&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, and&lt;/li&gt;
&lt;li&gt;for all $q$ such that $q≤ a$ for all $a\in A$, we have that $q≤ p$&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We write $P = \wedge A =  \underset{a\in A}{\wedge} a = a_1 \wedge a_2 \wedge &amp;hellip; \wedge a_n$&lt;/p&gt;
&lt;p&gt;Similarly, we say that $p$ is a &lt;code&gt;join&lt;/code&gt; of $A$ if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\forall a\in A$, we have $a≤ p$&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;, and&lt;/li&gt;
&lt;li&gt;for all $q$ such that $a≤ q$ for all $a\in A$, we have that $p≤ q$&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this case, $P = \vee A =  \underset{a\in A}{\vee} a = a_1 \vee a_2 \vee &amp;hellip; \vee a_n$&lt;/p&gt;
&lt;p&gt;We next discuss examples.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 1: Truth Tables of the Booleans $\mathbb{B}$={T,F}$.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For example, the pairwise &lt;code&gt;meets&lt;/code&gt; table of ${T,F}(F \leq T)$ happens to be the truth table for &lt;code&gt;AND&lt;/code&gt; in elementary logic. A similar binary join computation will generate the truth table for &lt;code&gt;OR&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 2: Power Sets, $(P(x), ≤)$.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s pick a particular set, let $X = {\square, \times, \heartsuit}$, then we consider its power sets:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/power_sets.png&#34; alt=&#34;power_sets&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, it&amp;rsquo;s clear that $\wedge$ = intersection, $\vee$ = union.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 3: $(\mathbb{N}, |), a ≤ b$, iff $a|b$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/divisible.png&#34; alt=&#34;divisible&#34;&gt;&lt;/p&gt;
&lt;p&gt;1 divides everything, so we start from 1. Here we have $\wedge$ = gcd, $\vee$ = lcm.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 4: There may be more than one &lt;code&gt;meet&lt;/code&gt;/&lt;code&gt;join&lt;/code&gt;.&lt;/strong&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/hasse_diagram.png&#34; alt=&#34;hasse_diagram&#34;&gt;&lt;/p&gt;
&lt;p&gt;This hasse diagram specifies a &lt;code&gt;preorder&lt;/code&gt; where both $c$ and $d$ are &lt;code&gt;meets&lt;/code&gt; of $A$. We have $c≤ d$ and $d≤ c$, so $c \cong d$, that is $c$ and $d$ are &lt;code&gt;isomorphic&lt;/code&gt;, which will be covered later; we generally do not run into trouble if we pretend they are equal though.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 5: &lt;code&gt;Meets&lt;/code&gt; or &lt;code&gt;joins&lt;/code&gt; may not exist.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Clearly things don&amp;rsquo;t always have a lower bound or a upper bound at all. There might be multiple lower bounds/upper bounds but these lower/upper bounds are non-comparable.&lt;/p&gt;
&lt;p&gt;Throughout these examples, many familiar things can be characterized by this quite simple universal property, be it gcd/lcm, max/min, limits/colimits, or itersection/union.&lt;/p&gt;
&lt;p&gt;The idea that things can be characterized by universal properties, implies we are closer to category theory now.&lt;/p&gt;
&lt;h2 id=&#34;monotone-maps&#34;&gt;Monotone Maps&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Preorders&lt;/code&gt; themselves can be related to one another. A &lt;code&gt;monotone map&lt;/code&gt; is a structure-preserving map for &lt;code&gt;preorders&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;monotone map&lt;/code&gt; between &lt;code&gt;preorders&lt;/code&gt; $(A, ≤_A)$ and $(B, ≤_B)$ is a &lt;code&gt;function&lt;/code&gt; $f : A \rightarrow B$ such that, for all elements $x, y ∈ A$, if $x ≤_A y$ then $f (x) ≤_B f(y)$.
&lt;img src=&#34;https://cmbbq.github.io/img/monotone_map.png&#34; alt=&#34;monotone_map&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let $\mathbb{B}$ be the preorders of booleans and $\mathbb{N}$ be the preorder of natural numbers. The map $\mathbb{B} \rightarrow \mathbb{N}$ sending false to 17 and true to 24 is a &lt;code&gt;monotone map&lt;/code&gt;, because it preserves order.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/b2n.png&#34; alt=&#34;b2n&#34;&gt;&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;monotone map&lt;/code&gt; $f: P \rightarrow Q$ preserves &lt;code&gt;joins&lt;/code&gt; if $\forall p,p&amp;rsquo; \in P, f(p\vee p&amp;rsquo;) = f(p) \vee f(p&amp;rsquo;)$&lt;/p&gt;
&lt;p&gt;For any &lt;code&gt;preorder&lt;/code&gt; $(P, ≤_P)$, the identity function is monotone.
If $(Q, ≤_Q)$ and $(R, ≤_R)$ are preorders and $f : P → Q$ and $g : Q → R$ are monotone, then $(f ; g): P → R$ is also monotone.&lt;/p&gt;
&lt;p&gt;Let $(P, ≤_P)$ and $(Q, ≤_Q)$ be preorders. A &lt;code&gt;monotone function&lt;/code&gt; $f : P → Q$ is called an &lt;code&gt;isomorphism&lt;/code&gt; if there exists a &lt;code&gt;monotone function&lt;/code&gt; $g : Q → P$ such that $f;g = id_P$ and $g;f = id_Q$. This means that for any $p ∈ P$ and $q ∈ Q$, we have $p=g(f(p))$ and $q
=f(g(q))$.&lt;/p&gt;
&lt;p&gt;We refer to $g$ as the inverse of $f$ , and vice versa: $f$ is the inverse of $g$.&lt;/p&gt;
&lt;p&gt;If there is an &lt;code&gt;isomorphism&lt;/code&gt; $P → Q$, we say that $P$ and $Q$ are isomorphic.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An isomorphism between preorders is basically just a relabeling of the elements.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;galois-connections&#34;&gt;Galois Connections&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Galois connections&lt;/code&gt; between &lt;code&gt;preorders&lt;/code&gt; $P$ and $Q$ is a pair of &lt;code&gt;monotone maps&lt;/code&gt; $f : P → Q$ and $g : Q → P$ such that $f(p) ≤ q iff p ≤ g(q).$&lt;/p&gt;
&lt;p&gt;We say that $f$ is the left &lt;code&gt;adjoint&lt;/code&gt; and g is the right &lt;code&gt;adjoint&lt;/code&gt; of the &lt;code&gt;Galois connection&lt;/code&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The theory of &lt;code&gt;Galois connections&lt;/code&gt; is a special case of a more general theory, the theory of &lt;code&gt;adjunctions&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Example 1: $P = Q = \underline{3}$&lt;/strong&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/Galois_connections.png&#34; alt=&#34;togc&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case $P$ and $Q$ are total orders, $f$ is left adjoint to $g$ as long as arrows do not cross.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 2: $\mathbb{Z} \xrightarrow[f]{3\times\square} \mathbb{R}$, $\mathbb{R} \xrightarrow[g]{\lfloor\square/3\rfloor} \mathbb{Z}$&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It means we have $5 \xrightarrow[f]{3\times\square}15$, $13.3 \xrightarrow[g]{\lfloor\square/3\rfloor}4$.&lt;/p&gt;
&lt;p&gt;Since $3n ≤ x$ iff $n ≤ \lfloor x/3\rfloor$, $f$ is left adjoint to $g$.&lt;/p&gt;
&lt;p&gt;In the end of the day, we can claim that a &lt;code&gt;monotone map&lt;/code&gt; $f$ is a left/right &lt;code&gt;adjoint&lt;/code&gt; iff it preserves &lt;code&gt;joins&lt;/code&gt;/&lt;code&gt;meets&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Here we use the symbol $≤$ instead of $R$ as it implies a preorder, and the infix notation $S_1 ≤ S_2$ looks more natural than $(S_1,S_2) \in R$.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;reflexivity&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;transitivity&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;$p$ is the lower bound of $A$&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;$p$ is the greatest lower bound&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;$p$ is the upper bound of $A$&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;$p$ is the least lower bound&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Tech Talk: Wall is Coming</title>
      <link>https://cmbbq.github.io/posts/tech-talk-wall-is-coming/</link>
      <pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/tech-talk-wall-is-coming/</guid>
      <description>内存墙和登纳德定律失效 “内存墙（the Memory Wall）”和“登纳德定律失效（the break down of Dennard Scaling）”是计算生态演化中的两个核心矛盾。
为了对抗Dennard Scaling的失效，计算硬件的架构从单核向多核、众核突围。 为了掩盖内存墙问题，内存层级（memory hierarchy）变得越来越深，off-chip互连带宽不得不迅速提高。 起初单核到多核是巨变，逼迫软件架构进行痛苦重构，并发编程问题木秀于林，因此被近20年的工业界实践+学术界研究集火秒了——如今我们有多线程编程范式、异步回调范式、goroutine式的有栈协程、C++/Rust的async/await无栈协程、lockfree/waitfree数据结构等诸多工具。 相较而言，这几十年来内存墙问题由于其隐蔽性、不紧急性、棘手性，不仅没得到妥善解决，反而根深蒂固，愈发遮掩不住，暴露在软件工程师面前，因此这次分享的重点是内存墙。
不过在进入正题之前，还需先介绍一下数据中心硬件和微处理器架构演化的些许背景。
21世纪数据中心硬件的演化简史 00s，Commodity Computing时代 远古时期我们并不会说“数据中心硬件”，因为还不存在现代意义上的互联网产业，也就没有现代意义上的数据中心。自然也就没有“for 大数据/云/Edge/AI”的营销话术，更多的是强调用廉价、不可靠、大规模的commodity hardware搭建分布式有状态系统，Google在这方面做了开创性的尝试。
相对IBM mainframe、super computer而言，当年x86的廉价是一目了然的。Google草创之初用的是奔腾2。 银行业、DAPRA成就了IBM power系列，如今Power9/10机器虽然被Xeon/EPYC完爆，仍然可以靠银行软件的祖宗之法不可变和政府订单苟延残喘，保有一份niche市场。
10s之前是x86微处理器完全不具备多核可扩展性的年代。
FSB(front-side bus)是罪魁祸首。下图架构中，内存总线和PCIe总线需共享一个FSB才能与CPU相连，导致FSB成为瓶颈，CPU数量不具备横向扩展性。 当年的PCIe还是1.0（03年初代PCIe），带宽和lane数都相当有限，即使强上多核，网络IO、磁盘IO也跟不上。 零零年代恰逢摩尔定律逐渐在单核语境失效，专注提升芯片性能在2004年后不再可行，硬件厂商不得不在架构上向多核方向突围。 一零年代初，多核时代 2012年的Intel Xeon E5-2600 V1 32nm Sandy Bridge是里程碑式的服务器产品，移除FSB（这个实际上在09年的Nehalem机器上就已经做了）、引入QPI/DMI取代FSB、PCIe2.0，使微架构获取多核可扩展性。
E5 family算是耳熟能详，虽然普遍要到寿命极限，但至今应该仍有很多公司在用。 当年买电脑时，所谓二代i3/i5/i7就是SandyBridge，和一代i5/i7的Nehalem有代差。
Sandy Bridge之后是Haswell，变化不大。Haswell之后是Broadwell，环状拓扑Broadwell Ring即得名于此。 再后面就是Skylake，开始冠上Scalable之名了，从多核走向众核，从近代走到现代。
一零年代末~二零年代初，众核时代 17年Intel推出了1st gen Xeon Scalable，Skylake，采用了Mesh Architecture。见Things are getting meshy 同时期AMD也推出了ENYC 7001，算是打破了Xeon的垄断局面。在US-TTP机房我们就有不少AMD机器。
Skylake的升级版Ice Lake并未顺利孵化，因为18年出了Meltdown/Spectre的大新闻（speculative execution的安全漏洞），于是在Skylake上修了漏洞，19年推出Cascade Lake作为2nd-Gen Xeon。原本的顺位继承者Ice Lake在21年姗姗来迟，变成了第三代。
Cooper Lake和Ice Lake同代，都被称为第三代，但实际上架构和Ice Lake不同，是基于Skylake改的，专为多socket(4~8s)设计，相比general-purpose的Ice Lake， Cooper Lake稍稍超出commodity hardware范畴，估计是想卖给特定的专用计算领域，用来替代老旧的UNIX系统，比如Oracle Solaris，IBM AIX。互联网场景下我们还是倾向于横向扩容而不是纵向扩容。</description>
      <content>&lt;h1 id=&#34;内存墙和登纳德定律失效&#34;&gt;内存墙和登纳德定律失效&lt;/h1&gt;
&lt;p&gt;“内存墙（the Memory Wall）”和“登纳德定律失效（the break down of Dennard Scaling）”是计算生态演化中的两个核心矛盾。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为了对抗Dennard Scaling的失效，计算硬件的架构从单核向多核、众核突围。&lt;/li&gt;
&lt;li&gt;为了掩盖内存墙问题，内存层级（memory hierarchy）变得越来越深，off-chip互连带宽不得不迅速提高。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;起初单核到多核是巨变，逼迫软件架构进行痛苦重构，并发编程问题木秀于林，因此被近20年的工业界实践+学术界研究集火秒了——如今我们有多线程编程范式、异步回调范式、goroutine式的有栈协程、C++/Rust的async/await无栈协程、lockfree/waitfree数据结构等诸多工具。
相较而言，这几十年来内存墙问题由于其隐蔽性、不紧急性、棘手性，不仅没得到妥善解决，反而根深蒂固，愈发遮掩不住，暴露在软件工程师面前，因此这次分享的重点是内存墙。&lt;/p&gt;
&lt;p&gt;不过在进入正题之前，还需先介绍一下数据中心硬件和微处理器架构演化的些许背景。&lt;/p&gt;
&lt;h1 id=&#34;21世纪数据中心硬件的演化简史&#34;&gt;21世纪数据中心硬件的演化简史&lt;/h1&gt;
&lt;h2 id=&#34;00scommodity-computing时代&#34;&gt;00s，Commodity Computing时代&lt;/h2&gt;
&lt;p&gt;远古时期我们并不会说“数据中心硬件”，因为还不存在现代意义上的互联网产业，也就没有现代意义上的数据中心。自然也就没有“for 大数据/云/Edge/AI”的营销话术，更多的是强调用廉价、不可靠、大规模的commodity hardware搭建分布式有状态系统，Google在这方面做了开创性的尝试。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;相对IBM mainframe、super computer而言，当年x86的廉价是一目了然的。Google草创之初用的是奔腾2。
银行业、DAPRA成就了IBM power系列，如今Power9/10机器虽然被Xeon/EPYC完爆，仍然可以靠银行软件的祖宗之法不可变和政府订单苟延残喘，保有一份niche市场。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/commodity_computing.png&#34; alt=&#34;commodity_computing&#34;&gt;&lt;/p&gt;
&lt;p&gt;10s之前是x86微处理器完全不具备多核可扩展性的年代。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;FSB&lt;/code&gt;(front-side bus)是罪魁祸首。下图架构中，内存总线和PCIe总线需共享一个&lt;code&gt;FSB&lt;/code&gt;才能与CPU相连，导致&lt;code&gt;FSB&lt;/code&gt;成为瓶颈，CPU数量不具备横向扩展性。&lt;/li&gt;
&lt;li&gt;当年的&lt;code&gt;PCIe&lt;/code&gt;还是1.0（03年初代&lt;code&gt;PCIe&lt;/code&gt;），带宽和lane数都相当有限，即使强上多核，网络IO、磁盘IO也跟不上。
&lt;img src=&#34;https://cmbbq.github.io/img/fsb.png&#34; alt=&#34;fsb&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;零零年代恰逢摩尔定律逐渐在单核语境失效，专注提升芯片性能在2004年后不再可行，硬件厂商不得不在架构上向多核方向突围。
&lt;img src=&#34;https://cmbbq.github.io/img/clock.png&#34; alt=&#34;clock&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;一零年代初多核时代&#34;&gt;一零年代初，多核时代&lt;/h2&gt;
&lt;p&gt;2012年的Intel Xeon E5-2600 V1 32nm Sandy Bridge是里程碑式的服务器产品，移除&lt;code&gt;FSB&lt;/code&gt;（这个实际上在09年的Nehalem机器上就已经做了）、引入&lt;code&gt;QPI&lt;/code&gt;/&lt;code&gt;DMI&lt;/code&gt;取代&lt;code&gt;FSB&lt;/code&gt;、PCIe2.0，使微架构获取多核可扩展性。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;E5 family算是耳熟能详，虽然普遍要到寿命极限，但至今应该仍有很多公司在用。
当年买电脑时，所谓二代i3/i5/i7就是&lt;code&gt;SandyBridge&lt;/code&gt;，和一代i5/i7的&lt;code&gt;Nehalem&lt;/code&gt;有代差。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;Sandy Bridge&lt;/code&gt;之后是&lt;code&gt;Haswell&lt;/code&gt;，变化不大。&lt;code&gt;Haswell&lt;/code&gt;之后是&lt;code&gt;Broadwell&lt;/code&gt;，环状拓扑&lt;code&gt;Broadwell Ring&lt;/code&gt;即得名于此。
&lt;img src=&#34;https://cmbbq.github.io/img/broadwell_ring.png&#34; alt=&#34;clock&#34;&gt;
再后面就是Skylake，开始冠上Scalable之名了，从多核走向众核，从近代走到现代。&lt;/p&gt;
&lt;h2 id=&#34;一零年代末二零年代初众核时代&#34;&gt;一零年代末~二零年代初，众核时代&lt;/h2&gt;
&lt;p&gt;17年Intel推出了1st gen Xeon Scalable，&lt;code&gt;Skylake&lt;/code&gt;，采用了Mesh Architecture。见Things are getting meshy
同时期AMD也推出了ENYC 7001，算是打破了Xeon的垄断局面。在US-TTP机房我们就有不少AMD机器。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Skylake&lt;/code&gt;的升级版&lt;code&gt;Ice Lake&lt;/code&gt;并未顺利孵化，因为18年出了Meltdown/Spectre的大新闻（speculative execution的安全漏洞），于是在&lt;code&gt;Skylake&lt;/code&gt;上修了漏洞，19年推出&lt;code&gt;Cascade Lake&lt;/code&gt;作为2nd-Gen Xeon。原本的顺位继承者&lt;code&gt;Ice Lake&lt;/code&gt;在21年姗姗来迟，变成了第三代。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Cooper Lake&lt;/code&gt;和&lt;code&gt;Ice Lake&lt;/code&gt;同代，都被称为第三代，但实际上架构和&lt;code&gt;Ice Lake&lt;/code&gt;不同，是基于&lt;code&gt;Skylake&lt;/code&gt;改的，专为多socket(4~8s)设计，相比general-purpose的&lt;code&gt;Ice Lake&lt;/code&gt;， &lt;code&gt;Cooper Lake&lt;/code&gt;稍稍超出commodity hardware范畴，估计是想卖给特定的专用计算领域，用来替代老旧的UNIX系统，比如&lt;code&gt;Oracle Solaris&lt;/code&gt;，&lt;code&gt;IBM AIX&lt;/code&gt;。互联网场景下我们还是倾向于横向扩容而不是纵向扩容。&lt;/p&gt;
&lt;p&gt;目前数据中心应用的主力机型是&lt;code&gt;Ice Lake&lt;/code&gt;、&lt;code&gt;Cascade Lake&lt;/code&gt;机器，23~24年起计算/访存密集的场景则会逐步用到第四代Xeon：&lt;code&gt;Sapphire Rapids&lt;/code&gt;机器。
19年20年我们还零零散散有一些1st gen Xeon Scalable Gold机器，后来很快就汰换掉了。
&lt;img src=&#34;https://cmbbq.github.io/img/broadwell_ring.png&#34; alt=&#34;clock&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Ice Lake&lt;/code&gt;和&lt;code&gt;Cascade Lake&lt;/code&gt;都是monolithic mesh设计。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这里monolithic是相对于chiplet/tile-based而言的单个huge die承载many-core的范式；&lt;/li&gt;
&lt;li&gt;这里mesh是相对于此前E5时代Broadwell Ring环状拓扑而言的网状拓扑。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;二者差异主要在制程（10nm vs 14nm）、最大核心数、PCIe路数、内存通道数（单socket支持的DIMM&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;数 16 vs 12）。&lt;/p&gt;
&lt;p&gt;24年起开始交付的4th-Gen &lt;code&gt;Sapphire Rapids&lt;/code&gt;的芯片架构从mono-die转型为更类似AMD的multi-die(Intel自称是tile-based），微架构从sunny cove更新到golden cove（tpause指令可用于优化spinlock），配置相当华丽，支持先进互连协议（&lt;code&gt;PCIe5&lt;/code&gt;/&lt;code&gt;CXL&lt;/code&gt;），支持DDR5，新指令集&lt;code&gt;AMX&lt;/code&gt;，顶配还有3D堆叠的on-package HBM&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h2 id=&#34;一些总结&#34;&gt;一些总结&lt;/h2&gt;
&lt;h3 id=&#34;硬件生态里生命和环境也是相互塑造的&#34;&gt;硬件生态里，生命和环境也是相互塑造的&lt;/h3&gt;
&lt;p&gt;PC用户成就了繁荣、易获得、标准化的x86 商用硬件市场。商用硬件集群刚好又适应互联网workloads（没有大量浮点数计算，integer server为主，但数据量极为庞大），才有了分布式系统支撑的现代数据中心，再然后才有硬件厂商为数据中心定制优化的服务器硬件，比如Scalable Xeon。&lt;/p&gt;
&lt;p&gt;PC玩家成就了Nvidia GPU，GPU恰好适应AI workloads，于是有了各种MLSys和GPGPU应用。然后才有Nvidia在加速计算方向上的投入。有了A100之后，ChatGPT训练就是在A100+IB network基础上量体裁衣做的大规模模型并行。随后又因为ChatGPT的轰动效应，反哺了高端GPU产业。&lt;/p&gt;
&lt;p&gt;生态位很难被人为设计、凭空创造，比如Intel Optane PMem，占据了非常合理的生态位，很多系统方向的研究都是靠PMem发的论文，因为它太合理了，弥补了磁盘和内存之间的空缺。但还是因为需求侧跟不上，在22年被砍掉了。究其根本，PMem在AI训推场景下没啥用，在主流的搜广推应用上不能带来显著成本优势，在交易场景和分析型场景要么比不上磁盘+内存，要么不值得投入人力去大改架构。&lt;/p&gt;
&lt;h3 id=&#34;现代硬件特征&#34;&gt;现代硬件特征&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;核心数众多&lt;/li&gt;
&lt;li&gt;设法缓解Memory Wall问题——近20年来DRAM cycle time每年缩减的速率与摩尔定律对比呈相对停滞，cache miss的后果愈发严重。
&lt;ul&gt;
&lt;li&gt;Memory hierarchy变深：cache变多，最新的顶配SPR机器还增加了on-package HBM，本地内存之外还有远端NUMA node，内存下面还可以有PMem、SSD，本地节点之外还有局域网/云端节点。总之，是通过增加层级隐藏Memory Wall问题。&lt;/li&gt;
&lt;li&gt;Off-package/chip-to-chip互连带宽提升：跟上核心数增多带来的IO需求，缓解批量读写场景下的Memory Wall问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;黑盒化和白盒化同时发生&#34;&gt;黑盒化和白盒化同时发生&lt;/h3&gt;
&lt;p&gt;现代硬件对工程能力提出了更苛刻的要求——通过off-CPU analysis、data-oriented cache-friendly设计、手动内存管理、甚至手动prefetching才能真正释放其性能潜力。&lt;/p&gt;
&lt;p&gt;但这和现代软件工程愈发简化的发展方向背道而驰——通过runtime、虚拟机、动态语言、微服务范式的职责划分、基于hypervisor的虚拟化和容器化等手段解放程序员心智。&lt;/p&gt;
&lt;p&gt;这种背道而驰使现代软件实践走向两条岔路，一个是以白盒化为手段的基础设施建设：更好的hypervisor、更好的mlsys、高性能检索、高性能存储、高性能网络，另一个是以黑盒化为目标的上层应用：利用虚拟化、容器化、微服务化、动态语言、runtime语言的便捷性提升productivity。&lt;/p&gt;
&lt;h1 id=&#34;现代硬件上的性能工程实践&#34;&gt;现代硬件上的性能工程实践&lt;/h1&gt;
&lt;h2 id=&#34;对优化空间的理解&#34;&gt;对优化空间的理解&lt;/h2&gt;
&lt;p&gt;性能工程即有系统方法论指导的软件优化实践。
可以从两个视角分解“优化任务”：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优化 = 减少算法总工作量 + 硬件使能&lt;/li&gt;
&lt;li&gt;优化 = 减少运行时间 = 减少CPU时间 + 减少阻塞时间
“算法改良”和“硬件使能”接近正交，“on-CPU time”和“off-CPU time”又大体互补，因此可以为优化空间$W$构造正交基{硬件使能$x$，算法优化$y$，算术密度$z$}，则$W = {[x,y,z] \in R^3 | 0 \le z \le 1 }$。&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;damn&#34;&gt;&lt;svg width=&#34;480&#34; height=&#34;420&#34;&gt;&lt;/svg&gt;&lt;/div&gt;
&lt;h2 id=&#34;推导出的一些heuristics&#34;&gt;推导出的一些Heuristics&lt;/h2&gt;
&lt;p&gt;基于对优化空间完整图景的理解，能推导出一些性能工程的Heuristics：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应首先确定程序的算术密度
&lt;ul&gt;
&lt;li&gt;为何区分on-CPU vs off-CPU至关重要？因为你的100%忙碌的CPU可能并不忙碌。CPU profiling仅描述完整图景的一部分，甚至一小部分。常用的CPU利用率这一指标具有欺骗性和迷惑性，实则既包括on-CPU计算也包括off-CPU阻塞。如果存在严重访存瓶颈，100%的CPU的superscalar pipeline里全是stall的空泡，CPU的各类算术逻辑单元、SIMD/AMX等专用计算硬件都在空等。&lt;/li&gt;
&lt;li&gt;当然off-CPU过高也可能是磁盘/网络IO密集导致的，但这类IO-bound应用往往不是成本大头，还不足以兴师动众地做优化，除非是专门做存储或专门搞网络的infra部门才需要关心。此外还有可能是代码写得有问题，锁粒度太大，过度持有锁，同样造成off-CPU 比例过高。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如今的内存应被视为外设
&lt;ul&gt;
&lt;li&gt;曾经为慢速外设准备的数据结构，如今适合内存场景：
&lt;ul&gt;
&lt;li&gt;C++的有序map是用红黑树实现的，Rust选择了B+树，以便利用其更好的locality，因为如今的内存已经和几十年前的磁盘一样慢得不可容忍了。&lt;/li&gt;
&lt;li&gt;DashTable这种原本用在PMem（非易适性内存，内存带宽远低于DRAM）上的数据结构，如今被DragonFly拿来用在内存数据库上，性能远超Redis/Memcached。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;因此需将内存层级白盒化，充分发挥硬件潜力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;避免和编译器优化撞车
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;不需要用移位操作或其他汇编指令优化乘除法。乘以8必然会被自动优化成左移动3。除法也会被优化成乘加。下面的例子是非常古老的编译器对 volatile int y = x / 71的处理。编译器优化乘法还会用LEA指令，LEA原本旨在加速小结构体数组的成员地址计算，但实际上也能用来加速乘法，比如乘以5可以写成lea eax, [eax*4 + eax]，利用现成的电路就比较快，算是硬件使能领域里编译器帮你做好的工作。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-assembly&#34; data-lang=&#34;assembly&#34;&gt;// volatile int y = x / 71;8b 0c 24        mov ecx, DWORD PTR _x$[esp+8] ; load x into ecx
mov eax, -423447479 ; magic happens starting here...
imul ecx            ; edx:eax = x * 0xe6c2b44903 d1           add edx, ecx        ; edx = x + edx
sar edx, 6          ; edx &amp;gt;&amp;gt;= 6 (with sign fill)

mov eax, edx        ; eax = edx
shr eax, 31         ; eax &amp;gt;&amp;gt;= 31 (no sign fill)
add eax, edx        ; eax += edx

mov DWORD PTR _y$[esp+8], eax
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以放心去做手动SIMD。手动SIMD和LLVM自动向量化优化位于不同抽象层次，基本上也不必对LLVM的自动向量化抱有期望。短期内见不到程序语言和库层面对SIMD进行良好抽象的希望。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;手动SIMD需要程序员根据目标机器微架构型号选择合适的指令（不同微架构的各种simd指令的latency各不相同）；&lt;/li&gt;
&lt;li&gt;选择恰当的simd size(并非越大越好，而且不同size对应不同的shuffle)；&lt;/li&gt;
&lt;li&gt;还要处理最后不满一个batch的数据导致的各种各样的corner cases；&lt;/li&gt;
&lt;li&gt;对数据地址进行对齐或对不对齐数据进行容忍。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用最新的编译器，用O3，很多细节就不必手动优化，如Copy Ellison，Tail-recursion Elimination，甚至Mutually recursion Elimination，Inlining，多数Loop优化——Loop unrolling(+步长)/fission(逆fusion)/tiling(cache blocking)/unswitching(内层去分支化)/自动向量化/interchange。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;涉及到逻辑和具体应用场景，没有标准优化策略的优化仍需自己做，比如Loop fusion，调整递归粒度，调整编码策略（紧凑程度和编解码开销的tradeoff）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;访存优化的一些tricks&#34;&gt;访存优化的一些Tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如何度量算术密度或访存密度?
&lt;ul&gt;
&lt;li&gt;perf 或 perf_event_open: cache_miss/instructions, 或ipc&lt;/li&gt;
&lt;li&gt;Intel PCM&lt;/li&gt;
&lt;li&gt;eBPF tools，比如https://github.com/iovisor/bcc&lt;/li&gt;
&lt;li&gt;静态分析：load/store指令占比可粗略翻译算术/访存密度，但受缓存命中率影响较大。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如何根据内存配置设置合适的object padding？
&lt;ul&gt;
&lt;li&gt;内存配置中有两个影响应用层的重要概念：内存通道（memory channels）、内存列（memory ranks）。x86架构下内存通道和内存列在内存地址上interleaving，即均匀分布且递增。因此RAM可视为$n_chan \times n_rank$个block组成。其DIMM架构如下图。
[图片]&lt;/li&gt;
&lt;li&gt;具体的object padding方式可以参考下面这段伪码，其中64B是cache line size，也恰好是一个block的size。大体思路是先保证内存池里的地址都是64B的整数倍，再保证下一个对象的block id和n_chan*n_rank互质。&lt;/li&gt;
&lt;li&gt;这个padding不让对象的起始地址反复命中同一个channel或同一个rank，令下一个对象起始地址落入不同的channel/rank，充分利用不同内存通道、不同内存列，避免通道、列之间的负载不均，提升访存带宽。
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;static&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;object_align&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; obj_size)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; nchan &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_nchannel&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; nrank &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_nrank&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; new_obj_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (obj_size &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;63&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (&lt;span style=&#34;color:#a6e22e&#34;&gt;get_gcd&lt;/span&gt;(new_obj_size, nrank &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; nchan) &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                new_obj_size&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; new_obj_size &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cache优化：Cache line对齐避免false sharing；利用cache blocking。&lt;/li&gt;
&lt;li&gt;规避锁瓶颈
&lt;ul&gt;
&lt;li&gt;基于静态锁分析或ebpf off-CPU分析，找到过分粗粒度的锁和过度超期持有的锁。&lt;/li&gt;
&lt;li&gt;寻找更好的并发数据结构：无锁实现良莠不齐；Many-core scalability最好的永远是array。&lt;/li&gt;
&lt;li&gt;Kernel bypassing：规避某些带锁的内核实现，如用户态网络协议栈替代内核栈。&lt;/li&gt;
&lt;li&gt;Share-nothing：最极端的做法可以效仿seastar，每个核心只在自己的专用内存上执行单线程代码，尽量避免CPU-to-CPU traffic，将锁彻底从代码里消除。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In-register storage：尽量保证简单函数用到的参数、存放中间产物的容器足够小，可完全用寄存器存下，编译器自动就会把所有load/store优化掉。&lt;/li&gt;
&lt;li&gt;考虑使用预分配内存和栈上静态结构：不得不访存时，生命周期较长的对象可考虑使用预分配内存，临时容器可以考虑根据线上数据分布设计成按某种规则切分的静态结构，并放在栈上（栈内存分配只是栈指针移动，而malloc复杂的多，如果说默认版本的malloc还有全局锁，不够scalable）。&lt;/li&gt;
&lt;li&gt;考虑使用编译期evaluation和全局静态内存区。&lt;/li&gt;
&lt;li&gt;利用1GB huge pages存数据，相比4KB默认页，hugepage所需的page table entry总数大大减少，可显著减小page table size和tlb size、降低tlb miss和page table walk开销，提升内存分配的连续性和内存访问的局部性，这些都有助于提升内存带宽。&lt;/li&gt;
&lt;li&gt;利用4MB large pages存代码，.text segment也可以用更大的页（不过最大只支持4MB），ITLB和DTLB一样，miss也会造成stall。ITLB问题的诊断可借助https://github.com/intel/iodlr，解决方法把.text移动到已有的large pages里&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;或静态链接并使用libhugetlbfs库。目前尚未看到该优化在真实项目中落地，但看起来相当promising，可参考&lt;a href=&#34;https://www.intel.com/content/dam/develop/external/us/en/documents/runtimeperformanceoptimizationblueprint-largecodepages-q1update.pdf&#34;&gt;Runtime Performance Optimization Blueprint: Large Code Pages&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;尊重NUMA拓扑，避免远端内存访问，即UPI traffic。
&lt;img src=&#34;https://cmbbq.github.io/img/NUMA.png&#34; alt=&#34;numa&#34;&gt;&lt;/li&gt;
&lt;li&gt;利用新架构特性和新的指令集扩展，如基于AMX实现GEMM的精度和性能优化广泛用在各种训推框架，AVX512_IFMA指令扩展做大数乘法已被用在新版本的OpenSSL中，以及基于QAT（QuickAssist）对AES、RSA、ECC等密码学应用做硬件加速。&lt;/li&gt;
&lt;li&gt;利用prefetching让cache变得更聪明。
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;所谓hardware prefetching就是从内存预取数据到cache（通常是LLC）。Hardware prefetcher有简单的stride pattern识别逻辑，比如a,a+2,a+4,a+6这种loop是可以被识别的。没必要刻意触发hardware prefetching，正常的代码都可以触发。但需避免误触发hardware prefetching——比如一个横跨多个cache line的大结构体其实只需要访问它的前几个field，但hardware prefetcher误以为还要接着读，就会造成cache pollution。稍微调整下这几个fields的访问顺序，破坏掉constant stride pattern即可。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找到合适的timing使用software prefetching，预取数据会带来巨大吞吐性能提升，也能用来隐藏latency，但究竟何时预取，预取哪些数据只能靠反复尝试。而一旦timing选错了反而造成cache污染，导致性能下降。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;尽量让prefetch指令分散（最好和load也分散），夹杂在计算指令中间。如果连续prefetch，那和一堆load一样，也会造成空泡。&lt;/li&gt;
&lt;li&gt;选择合适的PSD(prefetch scheduling distance)，即提前几个iteration预取。对计算量大的loop，可以提前1个iteration，对于计算量小的，可能要提前多个iteration。下面这个例子PSD=3。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-assembly&#34; data-lang=&#34;assembly&#34;&gt;top_loop:
prefetchnta [edx + esi + 128*3]
prefetchnta [edx*4 + esi + 128*3]
movaps xmm1, [edx + esi] 
movaps xmm2, [edx*4 + esi]
movaps xmm3, [edx + esi + 16] 
movaps xmm4, [edx*4 + esi + 16]
add esi, 128 
cmp esi, ecx 
jl top_loop
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;双层循环时，需注意弥补内外循环切换时的空泡，为外层循环也做prefetch。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;; j&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;) { 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    prefetch a[i][j&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;// 最后一次iteration，不需要prefetch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    computation a[i][j] &lt;span style=&#34;color:#75715e&#34;&gt;// 第一次a[i+1][j]未预取，会miss
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;} 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// 优化后
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt;; j&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    prefetch a[i][j&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    computation a[i][j]  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prefetch a[i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;// 提前准备好 a[i][0]，否则会在a[i][j+8]阻塞
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;computation a[i][j] &lt;span style=&#34;color:#75715e&#34;&gt;// 最后一个iteration单独处理，因为它不需要prefetch。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;利用先进互连技术，如&lt;code&gt;CXL&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spr-cxl.png&#34; alt=&#34;cxl&#34;&gt;&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;
svg {
    box-shadow: 0 0 10px #999;
    border-radius: 5px;
}
&lt;/style&gt;
&lt;script type=&#34;module&#34;&gt;
import {
  drag,
  color,
  select,
  range,
  randomUniform,
  randomNormal,
  scaleOrdinal,
  selectAll,
  schemePastel1,
} from &#34;https://cdn.skypack.dev/d3@7.8.5&#34;;
import {
    gridPlanes3D,
    points3D,
    lineStrips3D,
} from &#34;https://cdn.skypack.dev/d3-3d@1.0.0&#34;;
document.addEventListener(&#34;DOMContentLoaded&#34;, () =&gt; {
    console.log(&#34;dom loaded, starts to draw svg ...&#34;);
    const width = 480;
    const height = 420;
    const origin = { x: width/2, y: height/2 };
    const offset = origin.x - origin.y;
    const j = 10;
    const scale = 20;
    const key = (d) =&gt; d.id;
    const startAngle = Math.PI/2;
    // const startAngle = 0;
    const colorScale = scaleOrdinal(schemePastel1);
    let scatter = [];
    let yLine = [];
    let xLine = [];
    let zLine = [];
    let xGrid = [];
    let beta = 0;
    let alpha = 0;
    let mx, my, mouseX = 0, mouseY = 0;
    const svg = select(&#34;svg&#34;)
        .call(
          drag()
            .on(&#34;drag&#34;, dragged)
            .on(&#34;start&#34;, dragStart)
            .on(&#34;end&#34;, dragEnd)
        )
        .append(&#34;g&#34;);
    const grid3d = gridPlanes3D()
        .rows(20)
        .origin(origin)
        .rotateY(startAngle)
        .rotateX(-startAngle)
        .scale(scale);
  const points3d = points3D()
    .origin(origin)
    .rotateY(startAngle)
    .rotateX(-startAngle)
    .scale(scale);
  const yScale3d = lineStrips3D()
      .origin(origin)
      .rotateY(startAngle)
      .rotateX(-startAngle)
      .scale(scale);
  const xScale3d = lineStrips3D()
      .origin(origin)
      .rotateY(startAngle)
      .rotateX(-startAngle)
      .scale(scale);
  const zScale3d = lineStrips3D()
      .origin(origin)
      .rotateY(startAngle)
      .rotateX(-startAngle)
      .scale(scale);
  function processData(data, tt, recolor) {
    /* ----------- GRID ----------- */
    const xGrid = svg.selectAll(&#34;path.grid&#34;).data(data[0], key);
    xGrid
      .enter()
      .append(&#34;path&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d grid&#34;)
      .merge(xGrid)
      .attr(&#34;stroke&#34;, &#34;black&#34;)
      .attr(&#34;stroke-width&#34;, 0.3)
      .attr(&#34;fill&#34;, (d) =&gt; (d.ccw ? &#34;#eee&#34; : &#34;#aaa&#34;))
      .attr(&#34;fill-opacity&#34;, 0.7)
      .attr(&#34;d&#34;, grid3d.draw);
    xGrid.exit().remove();
    /* ----------- POINTS ----------- */
    const points = svg.selectAll(&#34;circle&#34;).data(data[1], key);
    function GetColor(x, y){
      // console.log(&#34;x: %d, y: %d&#34;, x, y);
      // return (x &gt; 0) ? 5 : -5 + (y &gt; 0) ? 3 : -3;
      if (x &gt;= 0 &amp;&amp; y &gt;= 0) return schemePastel1[0];
      if (x &lt; 0 &amp;&amp; y &gt;= 0) return schemePastel1[1];
      if (x &lt; 0 &amp;&amp; y &lt; 0) return schemePastel1[2];
      if (x &gt;= 0 &amp;&amp; y &lt; 0) return schemePastel1[3];
    }
    if(recolor){
      points
      .enter()
      .append(&#34;circle&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d&#34;)
      .attr(&#34;opacity&#34;, 0)
      .attr(&#34;cx&#34;, posPointX)
      .attr(&#34;cy&#34;, posPointY)
      .merge(points)
      .transition()
      .duration(tt)
      .attr(&#34;r&#34;, 3)
      .attr(&#34;stroke&#34;, (d) =&gt; color(colorScale(d.id)).darker(3))
      .attr(&#34;fill&#34;, (d) =&gt; GetColor(d.projected.x - origin.x, d.projected.y - origin.y))
      .attr(&#34;opacity&#34;, 1)
      .attr(&#34;cx&#34;, posPointX)
      .attr(&#34;cy&#34;, posPointY);
    }else{
      points
      .enter()
      .append(&#34;circle&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d&#34;)
      .attr(&#34;opacity&#34;, 0)
      .attr(&#34;cx&#34;, posPointX)
      .attr(&#34;cy&#34;, posPointY)
      .merge(points)
      .transition()
      .duration(tt)
      .attr(&#34;r&#34;, 3)
      .attr(&#34;stroke&#34;, (d) =&gt; color(colorScale(d.id)).darker(3))
      .attr(&#34;opacity&#34;, 1)
      .attr(&#34;cx&#34;, posPointX)
      .attr(&#34;cy&#34;, posPointY);
    }
    points.exit().remove();
    /* ----------- x-Scale ----------- */
    const xScale = svg.selectAll(&#34;path.xScale&#34;).data(data[3]);
    xScale
      .enter()
      .append(&#34;path&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d xScale&#34;)
      .merge(xScale)
      .attr(&#34;stroke&#34;, &#34;black&#34;)
      .attr(&#34;stroke-width&#34;, 1.5)
      .attr(&#34;d&#34;, xScale3d.draw);
    xScale.exit().remove();
    /* ----------- y-Scale ----------- */
    const yScale = svg.selectAll(&#34;path.yScale&#34;).data(data[2]);
    yScale
      .enter()
      .append(&#34;path&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d yScale&#34;)
      .merge(yScale)
      .attr(&#34;stroke&#34;, &#34;black&#34;)
      .attr(&#34;stroke-width&#34;, 1.5)
      .attr(&#34;d&#34;, yScale3d.draw);
    yScale.exit().remove();
    /* ----------- z-Scale ----------- */
    const zScale = svg.selectAll(&#34;path.zScale&#34;).data(data[4]);
    zScale
      .enter()
      .append(&#34;path&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d zScale&#34;)
      .merge(zScale)
      .attr(&#34;stroke&#34;, &#34;black&#34;)
      .attr(&#34;stroke-width&#34;, 1.5)
      .attr(&#34;d&#34;, zScale3d.draw);
    zScale.exit().remove();
    /* ----------- y-Scale Text ----------- */
    const yText = svg.selectAll(&#34;text.yText&#34;).data(data[2][0]);
    function GetYText(y){
      if (y==-11){
        return &#34;[Arithmetic Intensity]&#34;;
      }else{
        return  (-y*10 + 100)/2+&#34;%&#34;;
      }
    }
    function GetYWeight(y){
      if (y==-11){
        return 700;
      }else{
        return  350;
      }
    }
    yText
      .enter()
      .append(&#34;text&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d yText&#34;)
      .attr(&#34;font-family&#34;, &#34;system-ui, sans-serif&#34;)
      .merge(yText)
      .each(function (d) {
        d.centroid = { x: d.rotated.x, y: d.rotated.y, z: d.rotated.z };
      })
      .attr(&#34;x&#34;, (d) =&gt; d.projected.x)
      .attr(&#34;y&#34;, (d) =&gt; d.projected.y)
      .style(&#34;font-weight&#34;, (d) =&gt; GetYWeight(d.y))
      .text((d) =&gt; GetYText(d.y))
      .attr(&#34;fill&#34;, &#34;#78E2A0&#34;);
    yText.exit().remove();
    /* ----------- x-Scale Text ----------- */
    const xText = svg.selectAll(&#34;text.xText&#34;).data(data[3][0]);
    xText
      .enter()
      .append(&#34;text&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d xText&#34;)
      .attr(&#34;font-family&#34;, &#34;system-ui, sans-serif&#34;)
      .merge(xText)
      .each(function (d) {
        d.centroid = { x: d.rotated.x, y: d.rotated.y, z: d.rotated.z };
      })
      .attr(&#34;x&#34;, (d) =&gt; d.projected.x)
      .attr(&#34;y&#34;, (d) =&gt; d.projected.y)
      .attr(&#34;z&#34;, (d) =&gt; d.projected.z)
      .text((d) =&gt;  d.x == 10 ? &#34;[Hardware Enablement]&#34; : &#34;&#34;)
      .style(&#34;font-weight&#34;, 700)
      .attr(&#34;fill&#34;, &#34;#78E2A0&#34;);
    xText.exit().remove();
    /* ----------- x-Scale Text ----------- */
    const zText = svg.selectAll(&#34;text.zText&#34;).data(data[4][0]);
    zText
      .enter()
      .append(&#34;text&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d zText&#34;)
      .attr(&#34;font-family&#34;, &#34;system-ui, sans-serif&#34;)
      .merge(zText)
      .each(function (d) {
        d.centroid = { x: d.rotated.x, y: d.rotated.y, z: d.rotated.z };
      })
      .attr(&#34;x&#34;, (d) =&gt; d.projected.x)
      .attr(&#34;y&#34;, (d) =&gt; d.projected.y)
      .attr(&#34;z&#34;, (d) =&gt; d.projected.z)
      .text((d) =&gt;  d.z == 10 ? &#34;[Work Reduction]&#34; : &#34;&#34;)
      .style(&#34;font-weight&#34;, 700)
      .attr(&#34;fill&#34;, &#34;#78E2A0&#34;);
    zText.exit().remove(); 
    selectAll(&#34;.d3-3d&#34;).sort(points3d.sort);
  }
  function posPointX(d) {
    return d.projected.x;
  }
  function posPointY(d) {
    return d.projected.y;
  }
  function init() {
    xGrid = [];
    scatter = [];
    yLine = [];
    xLine = [];
    zLine = [];
    let cnt = 0; 
    for (let z = -j; z &lt; j; z++) {
      for (let x = -j; x &lt; j; x++) {
        xGrid.push({ x: x, y: 0, z: z}); // grid position
        scatter.push({
          x: x,
          y: randomNormal(0, 0.8)()*3,
          // y: randomUniform(9, -9)(),
          z: z,
          id: &#34;point-&#34; + cnt++,
        });
      }
    }
    range(-10, 12, 1).forEach((d) =&gt; {
      yLine.push({ x: 0, y: -d, z: 0 });
      xLine.push({ x: -d, y: 0, z: 0 });
      zLine.push({ x: 0, y: 0, z: -d });
    });
    const data = [
      grid3d(xGrid),
      points3d(scatter),
      yScale3d([yLine]),
      xScale3d([xLine]),
      zScale3d([zLine]),
    ];
    processData(data, 1000, true);
  }
  function dragStart(event) {
    mx = event.x;
    my = event.y;
  }
  function dragged(event) {
    beta = (event.x - mx + mouseX) * (Math.PI / offset);
    alpha = (event.y - my + mouseY) * (Math.PI / offset) * -1;
    const data = [
      grid3d.rotateY(beta + startAngle).rotateX(alpha - startAngle)(xGrid),
      points3d.rotateY(beta + startAngle).rotateX(alpha - startAngle)(scatter),
      yScale3d.rotateY(beta + startAngle).rotateX(alpha - startAngle)([yLine]),
      xScale3d.rotateY(beta + startAngle).rotateX(alpha - startAngle)([xLine]),
      zScale3d.rotateY(beta + startAngle).rotateX(alpha - startAngle)([zLine]),
    ];
    processData(data, 0, false);
  }
  function dragEnd(event) {
    mouseX = event.x - mx + mouseX;
    mouseY = event.y - my + mouseY;
  }
  selectAll(&#34;button&#34;).on(&#34;click&#34;, init);
  init();
});
&lt;/script&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;DIMM(dual in-line memory module)，即ram stick，内存条，DDR(Double Data Rate)技术的物理具现。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/products/sku/232592/intel-xeon-cpu-max-9480-processor-112-5m-cache-1-90-ghz/specifications.html&#34;&gt;https://www.intel.com/content/www/us/en/products/sku/232592/intel-xeon-cpu-max-9480-processor-112-5m-cache-1-90-ghz/specifications.html&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/intel/iodlr/blob/master/large_page-c/large_page.c&#34;&gt;https://github.com/intel/iodlr/blob/master/large_page-c/large_page.c&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Work Reduction vs Hardware Enablement</title>
      <link>https://cmbbq.github.io/posts/work-reduction-vs-hardware-enablement/</link>
      <pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/work-reduction-vs-hardware-enablement/</guid>
      <description>Optimization can be divided into work reduction and hardware enablement.
Work, in terms of computer programming, is basically a gross measure of how much stuff the program needs to do.
The idea of work optimization is to reduce the amount of stuff the program needs to do. Commonly employed techniques include: approximation, tail-recursion elimination, coarsening/refining recursion, inlining, loop fusion, loop unrolling, hoisting, short-circuiting, common-subexpression elimination, compile-time initialization, compile-time evaluation, exploiting sparsity, caching, pre-computation, and bit hacks.</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Optimization can be divided into work reduction and hardware enablement.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Work, in terms of computer programming, is basically a gross measure of how much stuff the program needs to do.&lt;/p&gt;
&lt;p&gt;The idea of work optimization is to reduce the amount of stuff the program needs to do. Commonly employed techniques include: &lt;code&gt;approximation&lt;/code&gt;, &lt;code&gt;tail-recursion elimination&lt;/code&gt;, &lt;code&gt;coarsening/refining recursion&lt;/code&gt;, &lt;code&gt;inlining&lt;/code&gt;, &lt;code&gt;loop fusion&lt;/code&gt;, &lt;code&gt;loop unrolling&lt;/code&gt;, &lt;code&gt;hoisting&lt;/code&gt;, &lt;code&gt;short-circuiting&lt;/code&gt;, &lt;code&gt;common-subexpression elimination&lt;/code&gt;, &lt;code&gt;compile-time initialization&lt;/code&gt;, &lt;code&gt;compile-time evaluation&lt;/code&gt;, &lt;code&gt;exploiting sparsity&lt;/code&gt;, &lt;code&gt;caching&lt;/code&gt;, &lt;code&gt;pre-computation&lt;/code&gt;, and &lt;code&gt;bit hacks&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;While reducing work undoubtedly serves as an essential heuristic for reducing overall running time, it&amp;rsquo;s not the sole determinant of the running time; it doesn&amp;rsquo;t capture the whole picture of computer programming, leaving the intricate nature of computer hardware unaddressed.&lt;/p&gt;
&lt;p&gt;To thoroughly investigate architectural improvements that unlock hardware potential, we must delve into numerous aspects in hardware and micro-architecture: &lt;code&gt;the ISA&lt;/code&gt;, &lt;code&gt;pipeline stages&lt;/code&gt;, &lt;code&gt;superscalar processing&lt;/code&gt;, &lt;code&gt;out-of-order execution&lt;/code&gt;, &lt;code&gt;paging&lt;/code&gt;, &lt;code&gt;caching&lt;/code&gt;, &lt;code&gt;vectorization&lt;/code&gt;, &lt;code&gt;speculation&lt;/code&gt;, &lt;code&gt;hardware prefetching&lt;/code&gt;, &lt;code&gt;branch prediction&lt;/code&gt;, etc.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Historically computer architecture leverages either locality or parallelism to enhance performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To exploit locality, the memory hierarchy(&lt;code&gt;registers&lt;/code&gt;-&amp;gt;&lt;code&gt;L1/L2/L3 caches&lt;/code&gt;-&amp;gt;&lt;code&gt;local DRAM&lt;/code&gt;-&amp;gt;&lt;code&gt;remote DRAM&lt;/code&gt;-&amp;gt;&lt;code&gt;PMem&lt;/code&gt;-&amp;gt;&lt;code&gt;SSD&lt;/code&gt;) was made deeper for hiding performance issue; hardware prefetchers and branch predictors were used to predict immediate accesses and move data or instructions closer to processors. As programmers, we are working on one layer obove the computer architecture layer, what we can do is to write NUMA-aware, cache-aligned, and perhaps vectorized code with regular data access patterns and proper software pre-fetching.&lt;/p&gt;
&lt;p&gt;To exploit parallelism, superscalar out-of-order pipelines with micro-ops, vector hardware, multi-core were introduced. Correspondingly, we need to keep all these things busy by using techniques like &lt;code&gt;bit tricks&lt;/code&gt;, &lt;code&gt;ILP&lt;/code&gt;(Instruction-level parallelism), &lt;code&gt;AVX&lt;/code&gt;/&lt;code&gt;SSE&lt;/code&gt;, &lt;code&gt;AMX&lt;/code&gt;, multithread/multi-process programming and offloading to accelerators like &lt;code&gt;DPU&lt;/code&gt;s or &lt;code&gt;GPU&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s explore &lt;code&gt;ILP&lt;/code&gt; further, as it is more connected with the μ-arch designs inside a processor, such as out-of-order execution, data bypassing, register renaming, and so on. To exploit CPU μ-arch programmatically, we can (1)use separate functional units, (2)write likely/unlikely hints for branch prediction, and (3)break dependency in the data-flow graph beforehand to reduce data hazards.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Wall is Coming</title>
      <link>https://cmbbq.github.io/posts/wall-is-coming/</link>
      <pubDate>Wed, 07 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/wall-is-coming/</guid>
      <description>The memory wall is coming. Given that DRAM access has become the de-facto bottleneck for many of today&amp;rsquo;s datacenter applications, it seems logical to optimize these applications by shamelessly copying prior wisdom on dealing with slow peripherals like PMems or SSDs.
PMems offer only $\frac{1}{14}$~$\frac{1}{3}$ of DRAMs&amp;rsquo; bandwidth. SSDs are typically 1~2 orders of magnitude slower than DRAMs. To overcome their slowness, many data structures were crafted specifically for them.</description>
      <content>&lt;p&gt;The memory wall is coming. Given that DRAM access has become the de-facto bottleneck for many of today&amp;rsquo;s datacenter applications, it seems logical to optimize these applications by shamelessly copying prior wisdom on dealing with slow peripherals like &lt;code&gt;PMem&lt;/code&gt;s or &lt;code&gt;SSD&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PMem&lt;/code&gt;s offer only $\frac{1}{14}$~$\frac{1}{3}$ of DRAMs&amp;rsquo; bandwidth. &lt;code&gt;SSD&lt;/code&gt;s are typically 1~2 orders of magnitude slower than DRAMs. To overcome their slowness, many data structures were crafted specifically for them.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Dashtable&lt;/code&gt;[Lu et al., 2020]&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, for example, was targeting scalable hasing on the once fashionable &lt;code&gt;PMem&lt;/code&gt;s. Hasing on &lt;code&gt;PMem&lt;/code&gt;s sounds somewhat niche. But in reality, if any being could survive &lt;code&gt;PMem&lt;/code&gt;&amp;rsquo;s hellish bandwidth, you can definitely count on it for arbitrary memory-bound application. In 2022, Intel killed off its &lt;code&gt;PMem&lt;/code&gt; business&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. &lt;code&gt;PMem&lt;/code&gt; died. Yet the &lt;code&gt;Dash&lt;/code&gt; methodology still thrives. Today we use it in regular &lt;code&gt;DRAM&lt;/code&gt; setups. Check this out, the &lt;a href=&#34;https://github.com/dragonflydb/dragonfly&#34;&gt;DragonFly&lt;/a&gt; project. It&amp;rsquo;s based on &lt;code&gt;Dash&lt;/code&gt; and claims to be the modern replacement for Redis and Memcached, delivering 25x more throughput.&lt;/p&gt;
&lt;p&gt;The core idea behind &lt;code&gt;Dashtable&lt;/code&gt; is trading space for time, or more specifically, each bucket paying a little bit extra space in metadata to buy (1)faster bucket probing with fingerprints of keys and (2)lightweight optimistic concurrency control with version locks, (3)stashing overflow records, which helps to alleviate segment splits caused by unbalanced bucket loads.&lt;/p&gt;
&lt;p&gt;Another noticeable development is that modern-day system language &lt;code&gt;Rust&lt;/code&gt;, has opted for &lt;code&gt;B-tree&lt;/code&gt;s over &lt;code&gt;Red-black tree&lt;/code&gt;s for its ordered maps. The 50+ years old &lt;code&gt;B-tree&lt;/code&gt;, once targeting disk storage, might find its place in today&amp;rsquo;s memory-bound applications. That is interesting because the &lt;code&gt;Red-black tree&lt;/code&gt;s was deemed memory-efficient and is still widely in use as the default implementation of &lt;code&gt;C++&lt;/code&gt;&amp;rsquo;s &lt;code&gt;std::map&lt;/code&gt;. Yet &lt;code&gt;B-tree&lt;/code&gt; somehow beats &lt;code&gt;Red-black tree&lt;/code&gt; on modern servers.&lt;/p&gt;
&lt;p&gt;So what happened to modern hardware? Well, it mostly involves the memory wall problem, which refers to the increasing gap between processor and memory speed. For decades, the memory wall problem has never been resolved, but only mitigated by introducing deeper and deeper memory hierarchies. In today&amp;rsquo;s architectures, L3 become much slower than L1/L2, and &lt;code&gt;DRAM&lt;/code&gt;s should be labeled as dangerously slow as disks were in the 1980s perspective.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;B. Lu, X. Hao, T. Wang, and E. Lo. Dash: Scalable Hashing on Persistent Memory. CoRR abs/2003.07302, 2020. &lt;a href=&#34;https://arxiv.org/pdf/2003.07302.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datacenterdynamics.com/en/news/intel-kills-off-optane-memory-writes-off-559-million-inventory/&#34;&gt;Intel kills off Optane Memory, writes off $559 million inventory&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Dash: Scalable Hashing</title>
      <link>https://cmbbq.github.io/posts/dash/</link>
      <pubDate>Fri, 26 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/dash/</guid>
      <description>The main focus of the Dash paper was on the once fashionable persistent memory, but in reality, any memory bandwidth-limited scenario can benefit from it. With Intel killing off its pmem business, the significance of the Dash approach has shifted to regular DRAM applications.
Dynamic Hashing Dashtable, the proposed scalable hashtable, evolves from extendible hashing.
Extendible hashing is a hash system that uses the first $N$ bits of hashed values to look up buckets in a trie-structured directory.</description>
      <content>&lt;p&gt;The main focus of the Dash paper was on the once fashionable &lt;code&gt;persistent memory&lt;/code&gt;, but in reality, any &lt;code&gt;memory bandwidth&lt;/code&gt;-limited scenario can benefit from it. With Intel killing off its &lt;code&gt;pmem&lt;/code&gt; business, the significance of the &lt;code&gt;Dash&lt;/code&gt; approach has shifted to regular &lt;code&gt;DRAM&lt;/code&gt; applications.&lt;/p&gt;
&lt;h1 id=&#34;dynamic-hashing&#34;&gt;Dynamic Hashing&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;Dashtable&lt;/code&gt;, the proposed scalable hashtable, evolves from &lt;code&gt;extendible hashing&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Extendible hashing&lt;/code&gt; is a hash system that uses the first $N$ bits of hashed values to look up buckets in a trie-structured &lt;code&gt;directory&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;directory&lt;/code&gt; with &lt;code&gt;global depth&lt;/code&gt; $N$ can hold $2^N$ buckets. It means $N$ is the key size that maps the &lt;code&gt;directory&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Each bucket also has a &lt;code&gt;local depth&lt;/code&gt; $M(M \le N)$, which is the key size that previously mapped the &lt;code&gt;directory&lt;/code&gt;. Any bucket having a &lt;code&gt;local depth&lt;/code&gt; $M = N$ is pointed-to by exactly one &lt;code&gt;directory&lt;/code&gt; entry. Any bucket having a &lt;code&gt;local depth&lt;/code&gt; $M \lt N$ is pointed-to by more than one &lt;code&gt;directory&lt;/code&gt; entries.&lt;/p&gt;
&lt;p&gt;The minimal $N$ needed to ensure every item has a unique bucket index is 1 for 2 items. The minimal $N = 2$  for 4 items. Everytime a new  item added into a bucket, if the number of items in the bucket exceeds a certain threshold, a rehashing operation happens by splitting the bucket into 2 parts. Hence rehashing in this scheme doesn&amp;rsquo;t have to stop the world and do a full-table scan &amp;amp; copy, but instead is done incremental.&lt;/p&gt;
&lt;p&gt;Similar to &lt;code&gt;extendible hashing&lt;/code&gt;, &lt;code&gt;linear hashing&lt;/code&gt; also uses a &lt;code&gt;directory&lt;/code&gt; to orgranize and address buckets. The distinction lies in split control. In &lt;code&gt;linear hashing&lt;/code&gt;, a split typically occurs only if the load factor exceeds a threshold and the bucket to be split is chosen in a &amp;ldquo;linear&amp;rdquo; manner.&lt;/p&gt;
&lt;h1 id=&#34;dash-for-extendible-hashing&#34;&gt;Dash for Extendible Hashing&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/dash_eh.png&#34; alt=&#34;dash_eh&#34;&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;Dash-EH&lt;/code&gt;, each &lt;code&gt;directory&lt;/code&gt; entry points to a &lt;code&gt;segment&lt;/code&gt; which consists of a fixed number of normal buckets and stash buckets. A &lt;code&gt;segment&lt;/code&gt; can be viewed as a sub-hashtable of constant size. The so-called stash buckets shares the same layout as the normal buckets, responsible for storing overflow records.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/dash_eh_bucket.png&#34; alt=&#34;dash_eh&#34;&gt;&lt;/p&gt;
&lt;p&gt;The core idea of &lt;code&gt;Dash-EH&lt;/code&gt; is to pay a little bit extra space in metadata to buy faster probing with fingerprints and lightweight concurrency control with version locks.&lt;/p&gt;
&lt;p&gt;Inside a &lt;code&gt;Dash-EH&lt;/code&gt; bucket, as shown in the figure above, the first 32 bytes are metadata, including version lock, counter, alloc bitmap, membership bitmap for load balancing, and 18 one-byte fingerprints for bucket probing(14 for slots in the bucket, 4 for overflow records originally hashed to this bucket). It is followed by $16(Bytes) \times 14 (records) = 224 Bytes$ payload, which stores 14 16-byte records.&lt;/p&gt;
&lt;h2 id=&#34;fingerprinting&#34;&gt;Fingerprinting&lt;/h2&gt;
&lt;p&gt;Bucket probing, which refers to searching for a slot in a bucket, is a basic operation of hashtables, needed by &lt;code&gt;search&lt;/code&gt;, &lt;code&gt;insert&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; operations to locate a particular key. Traditionally probing requires a linear scan, which is naturally slow on &lt;code&gt;PMem&lt;/code&gt; and could be completely unnecessary when a searched key doesn&amp;rsquo;t exist. &lt;code&gt;Dash-EH&lt;/code&gt; employs fingerprinting to reduce unnecessary scans. Fingerprints are the least significant byte of hashes of keys. To probe for a key, the probing thread first checks if any fingerprint in the bucket&amp;rsquo;s metadata matches the key&amp;rsquo;s, so it can skip buckets without any fingerprint match.&lt;/p&gt;
&lt;p&gt;Fingerprinting primarily benefits negative(key-not-found) &lt;code&gt;search&lt;/code&gt;es. The &lt;code&gt;Dash&lt;/code&gt; paper also claims fingerprinting enables using larger buckets spanning more than 2 cachelines. But I have to take it with a grain of salt. The paper itself uses a 256B setup. So does the DragonFly implementation&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. In theory, larger buckets can indeed tolerate more collisions and improve the load factor; however, this may come at the cost of compromising locality to a certain degree - you don&amp;rsquo;t want to load multiple times for a single bucket access in a hashtable.&lt;/p&gt;
&lt;h2 id=&#34;bucket-load-balancing&#34;&gt;Bucket Load Balancing&lt;/h2&gt;
&lt;p&gt;Segmentation reduces cache misses on &lt;code&gt;directory&lt;/code&gt; by reducing its size. In the &lt;code&gt;extendible hashing&lt;/code&gt; scheme, if any bucket in a segment is full, the entire segment needs to be split, even though other buckets might have much free space.&lt;/p&gt;
&lt;p&gt;To prevent frequent segment splits, the &lt;code&gt;Dash-EH&lt;/code&gt; algorithm design incorporates bucket load balancing. For an &lt;code&gt;insert&lt;/code&gt; operation, &lt;code&gt;Dash-EH&lt;/code&gt; probes both bucket $B_b$ and $B_{b+1}$, and then inserts into the bucket that is less full. If both $B_b$ and $B_{b+1}$ are full, &lt;code&gt;Dash-EH&lt;/code&gt; tries to displace a &amp;ldquo;native record&amp;rdquo; from $B_{b+1}$ to $B_{b+2}$, or move a &amp;ldquo;rebalanced record&amp;rdquo; from $B_b$ back to $B_{b-1}$ where it originally belongs.&lt;/p&gt;
&lt;p&gt;The per-bucket membership bitmap is used to decide whether a record is rebalanced or native. If a bit is set in the membership bitmap, then the corresponding key was not directly hashing into this bucket(native) but placed here due to re-balancing(rebalanced).&lt;/p&gt;
&lt;p&gt;If both &lt;code&gt;insert&lt;/code&gt; and &lt;code&gt;displacement&lt;/code&gt; failed, &lt;code&gt;Dash-EH&lt;/code&gt; turns to the last resort - stashing. Each segment has a fixed number of stash buckets to hold these overflow records. Probing stash buckets introduces significant overhead to negative &lt;code&gt;search&lt;/code&gt;es and &lt;code&gt;insert&lt;/code&gt;s(needs uniqueness check). To address this issue, each normal bucket reserves certain metadata fields. 4 overflow fingerprints are reserved for overflow records stored in stsh buckets. A overflow bit indicates if there exists an overflow at all. So if there isn&amp;rsquo;t an overflow in a bucket, the &lt;code&gt;search&lt;/code&gt;/&lt;code&gt;insert&lt;/code&gt; operation doesn&amp;rsquo;t have to probe stash buckets. Anyway, it is still advisable to maintain a small number of stash buckets. The paper claims &amp;ldquo;using 2–4 stash buckets per segment can improve load factor to over 90% without imposing significant overhead&amp;rdquo;. In Dragonfly&amp;rsquo;s Dashtable, each segment has 56 regular buckets, and 4 stash buckets.&lt;/p&gt;
&lt;h2 id=&#34;lightweight-concurrency-control&#34;&gt;Lightweight Concurrency Control&lt;/h2&gt;
&lt;p&gt;The lightweight concurrency control in &lt;code&gt;Dash-EH&lt;/code&gt; naturally scales well on today&amp;rsquo;s &lt;code&gt;many-core&lt;/code&gt; architectures, out-performing traiditional bucket-level shared locks.&lt;/p&gt;
&lt;p&gt;Write operations follow traditional bucket-level locking to lock the affected buckets, using CAS over a lock bit. If a write is done, the writer thread resets the lock bit and increments the per-bucket version number by one.&lt;/p&gt;
&lt;p&gt;On the other hand, read operations are designed to be lock-free. Before a read, the reader thread first fetches a snapshot of the lock word, waits until the lock is released, then proceeds to read without holding any lock. After reading, it will check the lock word again to verify the version number stays unchanged. If the version is changed, it retries the entire operation.&lt;/p&gt;
&lt;h1 id=&#34;dash-for-linear-hashing&#34;&gt;Dash for Linear Hashing&lt;/h1&gt;
&lt;p&gt;The Dash paper also presents &lt;code&gt;Dash-LH&lt;/code&gt;, a Dash-enabled linear hashing approach built upon building blocks used in &lt;code&gt;Dash-EH&lt;/code&gt;, such as balanced &lt;code&gt;insert&lt;/code&gt;/&lt;code&gt;displacement&lt;/code&gt;, fingerprinting and optimistic concurrency; they are pretty much orthogonal after all. The main difference is that &lt;code&gt;Dash-LH&lt;/code&gt; split the segment pointed to by a pointer in a linear manner.&lt;/p&gt;
&lt;p&gt;Traditional &lt;code&gt;linear hashing&lt;/code&gt; links overflow records with a linklist. In &lt;code&gt;Dash-LH&lt;/code&gt;, it&amp;rsquo;s done more cache-friendly with stash buckets. It still needs to chain these stash buckets though. Still it&amp;rsquo;s much better than chaining individual records.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/dragonflydb/dragonfly/blob/main/docs/dashtable.md&#34;&gt;Dashtable in Dragonfly&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>DPDK is All You Need</title>
      <link>https://cmbbq.github.io/posts/dpdk/</link>
      <pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/dpdk/</guid>
      <description>对于访存密集的数据中心应用来说，DPDK提供了非常好的性能工程范式。本文只是浅尝辄止，汇集其中一部分值得借鉴的思想。
EAL：用户空间库 EAL(Envionmemt Abstraction Layer)是DPDK面向用户的用户空间库，提供了各种有用的工具，比如：运行时对CPU特性进行检测，更适合现代硬件的内存管理，绑核和指定任务在某个核上运行。
rte_eal_init()是初始化EAL的函数，有多平台的实现：Linux、FreeBSD、Windows。以它为例，可大致了解DPDK EAL做了哪些工作。
rte_eal_init()是一个冗长的初始化过程，包含下列步骤：检查CPU类型是否是DPDK支持的、设置日志等级、检测各个socket上的各个cpu、enable每个逻辑核、初始化插件（加载共享库，比如一些PMD drivers）、初始化tracing机制、解析各个设备的配置选项、初始化全局配置（主核id、逻辑核数、numa nodes数、iova模式1、内存拓扑配置）、初始化中断处理机制、初始化多进程通用channel、扫描所有总线上的设备、初始化malloc heap、注册多进程action callbacks（热插拔支持）、初始化巨页信息2、初始化内存和memzone、初始化HPET/TSC计时器3、检查本地socket上的内存、创建主线程和子线程的通信信道、创建工作线程、绑核、在工作线程上启动dummy function、初始化服务、嗅探所有总线上的设备和驱动、启动服务、开启telemetry（提供ethdev stats、ethdev port list、eal parameters等状态查询）。
切割需要权限的工作 DPDK程序跑在用户态的一个前提是有内核驱动帮忙处理一些硬件设备注册、中断映射的事情。Linux上有几个可用的内核驱动，比如vfio-pci、igb_uio、uio_pci_generic。它们是泛用的PCI内核驱动模块，对所有PCI设备均适用。igb_uio基于Linux UIO提供所有类型的中断支持，比较古老，也比较简单，不支持IOMMU，因此IOVA mode只能用PA mode。uio_pci_generic和igb_uio类似，不过不支持MSI和MSI-X中断。vfio-pci支持基于IOMMU做IOVA映射，兼容VA mode和PA mode，如果能用vfio，就用vfio，目前uio基本处于半废弃状态。
大多数设备需先从Linux内核驱动上解绑，然后再绑定到DPDK的内核驱动上。需用户在运行DPDK程序前，用usertools目录下的dpdk-devbind.py脚本做好设备和内核模块的解绑和绑定——这种需要root权限的准备工作也从用户态库中剥离了。
利用巨页，毕竟4KB已是古老时代的残响 DPDK基于mmap在hugetlbfs中进行巨页物理内存申请。使用更大的内存页相比4KB默认页(在x86上，DPDK目前支持2MB或1GB的巨页4)，所需的page table entry总数大大减少，可显著减小page table size和tlb size、降低tlb miss和page table walk开销，提升内存分配的连续性和内存访问的局部性，这些都有助于提升内存带宽。
尊重NUMA Node拓扑 DPDK的每个操作都是NUMA-aware的，提供的API默认是NUMA node亲和的，这让用户很难写出远端内存访问的代码。
尊重内存的硬件拓扑 DPDK的内存分配做得非常精细，利用了内存硬件拓扑等内存配置。
内存配置中有两个影响应用层的重要概念：内存通道（memory channels）、内存列（memory ranks）。
内存通道是CPU和内存之间的通信通道，理论上来讲内存带宽和通道数成正比，单个通道位宽64bit，2个就是128bit。内存通道数往往等于单socket支持的DIMM5数，毕竟足够多的内存还需足够多的通道才能保证和CPU的互连。
CPU和内存之间是64bit接口，但单个内存颗粒（DRAM chips）的位宽可能是4bit、16bit，需要多个内存颗粒并联形成一个64bit的内存列，连接到同一组chip select上，从而保证各内存颗粒可被同时访问。内存模组必须至少形成一个内存列，才能和CPU通信。内存标签上的2R×8就是指列数为2，颗粒位宽8bit，一共16个颗粒。
Depending on memory configuration on x86 arch, objects addresses are spread between channels and ranks in RAM
x86架构下内存通道和内存列在内存地址上interleaving，即均匀分布且递增，因此RAM可视作由$n_{chan}\times n_{rank}$个block组成的，其DIMM架构如下图。 如图所示，内存池最好不要让对象的起始地址反复命中同一个channel或同一个rank，而是要充分利用不同内存通道、不同内存列，避免通道、列之间的负载不均，提升访存带宽。
DPDK的mempool给对象大小加恰当的padding，令内存池中的下一个对象的起始地址分布在不同内存通道和列中。具体实现可参考下面的代码，其中64B6是x86的cache line size，也恰恰是一个block size，或memory bus width、channel width。无论如何，内存池里的地址都要首先保证是64B的整数倍，能整齐地放入cache，做到cache-friendly——事实上也是block friendly、memory bus width friendly，然后才是保证下一个对象的block id和$n_{chan}\times n_{rank}$互质。</description>
      <content>&lt;p&gt;对于访存密集的数据中心应用来说，DPDK提供了非常好的性能工程范式。本文只是浅尝辄止，汇集其中一部分值得借鉴的思想。&lt;/p&gt;
&lt;h2 id=&#34;eal用户空间库&#34;&gt;EAL：用户空间库&lt;/h2&gt;
&lt;p&gt;EAL(Envionmemt Abstraction Layer)是DPDK面向用户的用户空间库，提供了各种有用的工具，比如：运行时对CPU特性进行检测，更适合现代硬件的内存管理，绑核和指定任务在某个核上运行。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rte_eal_init()&lt;/code&gt;是初始化EAL的函数，有多平台的实现：Linux、FreeBSD、Windows。以它为例，可大致了解DPDK EAL做了哪些工作。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rte_eal_init()&lt;/code&gt;是一个冗长的初始化过程，包含下列步骤：检查CPU类型是否是DPDK支持的、设置日志等级、检测各个socket上的各个cpu、enable每个逻辑核、初始化插件（加载共享库，比如一些PMD drivers）、初始化&lt;a href=&#34;https://doc.dpdk.org/guides/prog_guide/trace_lib.html&#34;&gt;tracing机制&lt;/a&gt;、解析各个设备的配置选项、初始化全局配置（主核id、逻辑核数、numa nodes数、iova模式&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;、内存拓扑配置）、初始化中断处理机制、初始化多进程通用channel、扫描所有总线上的设备、初始化malloc heap、注册多进程action callbacks（热插拔支持）、初始化巨页信息&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;、初始化内存和memzone、初始化HPET/TSC计时器&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;、检查本地socket上的内存、创建主线程和子线程的通信信道、创建工作线程、绑核、在工作线程上启动dummy function、初始化服务、嗅探所有总线上的设备和驱动、启动服务、开启telemetry（提供ethdev stats、ethdev port list、eal parameters等状态查询）。&lt;/p&gt;
&lt;h2 id=&#34;切割需要权限的工作&#34;&gt;切割需要权限的工作&lt;/h2&gt;
&lt;p&gt;DPDK程序跑在用户态的一个前提是有内核驱动帮忙处理一些硬件设备注册、中断映射的事情。Linux上有几个可用的内核驱动，比如&lt;code&gt;vfio-pci&lt;/code&gt;、&lt;code&gt;igb_uio&lt;/code&gt;、&lt;code&gt;uio_pci_generic&lt;/code&gt;。它们是泛用的PCI内核驱动模块，对所有PCI设备均适用。&lt;code&gt;igb_uio&lt;/code&gt;基于Linux UIO提供所有类型的中断支持，比较古老，也比较简单，不支持IOMMU，因此IOVA mode只能用PA mode。&lt;code&gt;uio_pci_generic&lt;/code&gt;和&lt;code&gt;igb_uio&lt;/code&gt;类似，不过不支持MSI和MSI-X中断。&lt;code&gt;vfio-pci&lt;/code&gt;支持基于IOMMU做IOVA映射，兼容VA mode和PA mode，如果能用&lt;code&gt;vfio&lt;/code&gt;，就用&lt;code&gt;vfio&lt;/code&gt;，目前&lt;code&gt;uio&lt;/code&gt;基本处于半废弃状态。&lt;/p&gt;
&lt;p&gt;大多数设备需先从Linux内核驱动上解绑，然后再绑定到DPDK的内核驱动上。需用户在运行DPDK程序前，用&lt;code&gt;usertools&lt;/code&gt;目录下的&lt;code&gt;dpdk-devbind.py&lt;/code&gt;脚本做好设备和内核模块的解绑和绑定——这种需要root权限的准备工作也从用户态库中剥离了。&lt;/p&gt;
&lt;h2 id=&#34;利用巨页毕竟4kb已是古老时代的残响&#34;&gt;利用巨页，毕竟4KB已是古老时代的残响&lt;/h2&gt;
&lt;p&gt;DPDK基于mmap在hugetlbfs中进行巨页物理内存申请。使用更大的内存页相比4KB默认页(在x86上，DPDK目前支持2MB或1GB的巨页&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;)，所需的page table entry总数大大减少，可显著减小page table size和tlb size、降低tlb miss和page table walk开销，提升内存分配的连续性和内存访问的局部性，这些都有助于提升内存带宽。&lt;/p&gt;
&lt;h2 id=&#34;尊重numa-node拓扑&#34;&gt;尊重NUMA Node拓扑&lt;/h2&gt;
&lt;p&gt;DPDK的每个操作都是NUMA-aware的，提供的API默认是NUMA node亲和的，这让用户很难写出远端内存访问的代码。&lt;/p&gt;
&lt;h2 id=&#34;尊重内存的硬件拓扑&#34;&gt;尊重内存的硬件拓扑&lt;/h2&gt;
&lt;p&gt;DPDK的内存分配做得非常精细，利用了内存硬件拓扑等内存配置。&lt;/p&gt;
&lt;p&gt;内存配置中有两个影响应用层的重要概念：内存通道（memory channels）、内存列（memory ranks）。&lt;/p&gt;
&lt;p&gt;内存通道是CPU和内存之间的通信通道，理论上来讲内存带宽和通道数成正比，单个通道位宽64bit，2个就是128bit。内存通道数往往等于单socket支持的DIMM&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;数，毕竟足够多的内存还需足够多的通道才能保证和CPU的互连。&lt;/p&gt;
&lt;p&gt;CPU和内存之间是64bit接口，但单个内存颗粒（DRAM chips）的位宽可能是4bit、16bit，需要多个内存颗粒并联形成一个64bit的内存列，连接到同一组chip select上，从而保证各内存颗粒可被同时访问。内存模组必须至少形成一个内存列，才能和CPU通信。内存标签上的2R×8就是指列数为2，颗粒位宽8bit，一共16个颗粒。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Depending on memory configuration on x86 arch, objects addresses are spread between channels and ranks in RAM&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;x86架构下内存通道和内存列在内存地址上interleaving，即均匀分布且递增，因此RAM可视作由$n_{chan}\times n_{rank}$个block组成的，其DIMM架构如下图。
&lt;img src=&#34;https://cmbbq.github.io/img/2chan4rank.svg&#34; alt=&#34;2chan4rank&#34;&gt;&lt;/p&gt;
&lt;p&gt;如图所示，内存池最好不要让对象的起始地址反复命中同一个channel或同一个rank，而是要充分利用不同内存通道、不同内存列，避免通道、列之间的负载不均，提升访存带宽。&lt;/p&gt;
&lt;p&gt;DPDK的&lt;code&gt;mempool&lt;/code&gt;给对象大小加恰当的padding，令内存池中的下一个对象的起始地址分布在不同内存通道和列中。具体实现可参考下面的代码，其中64B&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;是x86的&lt;code&gt;cache line size&lt;/code&gt;，也恰恰是一个&lt;code&gt;block size&lt;/code&gt;，或&lt;code&gt;memory bus width&lt;/code&gt;、&lt;code&gt;channel width&lt;/code&gt;。无论如何，内存池里的地址都要首先保证是64B的整数倍，能整齐地放入cache，做到cache-friendly——事实上也是block friendly、memory bus width friendly，然后才是保证下一个对象的&lt;code&gt;block id&lt;/code&gt;和$n_{chan}\times n_{rank}$互质。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;static&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;arch_mem_object_align&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; obj_size)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; nchan &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rte_memory_get_nchannel&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; nrank &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rte_memory_get_nrank&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; new_obj_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (obj_size &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;63&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (&lt;span style=&#34;color:#a6e22e&#34;&gt;get_gcd&lt;/span&gt;(new_obj_size, nrank &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; nchan) &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		new_obj_size&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; new_obj_size &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;iova和va模式的连续性&#34;&gt;IOVA和VA模式的连续性&lt;/h2&gt;
&lt;p&gt;硬件不知VA，用户空间不知PA，DPDK的作用之一是bridge物理地址（PA）和虚拟地址（VA），因此给出一种IOVA(IO Virtual Address)是自然而然的设计。&lt;/p&gt;
&lt;p&gt;DPDK的IOVA模式有两种：PA mode和VA mode。如果用了PA mode，则分配给DPDK的所有IOVA地址都是物理地址，或者说，也是IO虚拟地址，只不过这个IO虚拟地址的内存布局与物理地址完全相同。PA mode的缺点是需要root权限以读取页表，且可能会继承物理内存的碎片性。因此DPDK引入了新的VA mode，一方面无需root权限，另一方面基于IOMMU&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;做物理内存的重映射，保证IO虚拟地址的连续性，并使其编码布局和一般虚拟地址在格式上做到匹配，这样就允许大片连续IOVA内存的申请。无论是硬件视角下，还是用户空间视角下，VA mode下IOVA内存区都是连续的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/iova.png&#34; alt=&#34;iova&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;固定物理地址刚好宜用dma&#34;&gt;固定物理地址刚好宜用DMA&lt;/h2&gt;
&lt;p&gt;尊重NUMA拓扑、尊重内存布局、IOVA的VA模式、使用巨页这些特性叠加起来，天然就决定了DPDK设计中，用户态进程用到的所有虚拟地址的underlying物理地址都是固定不变的——也就是说，这些地址是可以用于DMA的。DPDK用户态程序可不必涉足IO事务，让硬件自主代劳，通过固定不变的物理地址上的DMA事务完成。&lt;/p&gt;
&lt;h2 id=&#34;多进程范式&#34;&gt;多进程范式&lt;/h2&gt;
&lt;p&gt;DPDK还特别为多进程做了支持，允许一个主进程管理所有DPDK资源，多个子进程共享资源访问权。DPDK的额外努力在于它保证了子进程视野中的地址和peer进程、主进程视野中的地址是完全一样的，也就是说连指针都能跨进程传递——这听起来就相当危险，但性能肯定比各种安全的通信协作机制强。此外，DPDK还支持跨进程的全局锁，使多进程编程更接近多线程编程。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/memory-in-dpdk-part-2-deep-dive-into-iova.html&#34;&gt;Memory in DPDK Part 2: Deep Dive into IOVA&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;EAL用&lt;code&gt;mmap&lt;/code&gt;分配巨页物理内存，并将这些物理内存再通过内存池API暴露给服务层。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;EAL通过&lt;code&gt;mmap&lt;/code&gt;从用户空间访问HPET内核时间计数，暴露高精度计时器接口给服务层。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/memory-in-dpdk-part-1-general-concepts.html&#34;&gt;Memory in DPDK Part 1: General Concepts&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;DIMM(dual in-line memory module)，即ram stick，内存条，DDR(Double Data Rate)技术的物理具现。&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;无论是i686还是x86_64，cache size都是64B，不过有些场景下合理的cache padding size是128，因为prefetcher一次取两个cacheline。&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;IOMMU是连接在DMA-capable IO总线和主存之间，将设备的物理地址映射到虚拟地址空间的专用硬件。物理机通常都支持IOMMU，以Intel为例，IOMMU技术即Vt-d：Intel® Virtualization Technology for Directed I/O。&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>The Little Book Review &amp; Internalization</title>
      <link>https://cmbbq.github.io/posts/the-little-book-review/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/the-little-book-review/</guid>
      <description>&amp;ldquo;The Little Book of Deep Learning&amp;rdquo;(LBDL)是François Fleuret写的一本适配手机屏的书，精简扼要地面向stem背景读者介绍深度学习。正如DDIA可被视为分布式系统方向的入门教程，LBDL是理想的深度学习101。
精简，或者说压缩，正是深度模型的strength，也是这个信息过载时代的virtue。用A4纸打印这个小册子，读起来非常舒适。
接下来是知识内化和梳理。
【一】概述 高维信号难以用规则系统分析，而深度网络则克服了这个困难，用具有大量权重的深层映射拟合出一个足够好（loss足够低）的近似函数——这个函数可以是高维信号到连续向量（回归）或离散值（分类）的映射，也可以是一种概率密度函数，总之，它能从数据分布中学习到某种紧凑且有区分能力的表征。
若数据样本不足，即使训练数据上表现良好，也可能在真实应用中效果不佳，这就是过拟合。 若模型能力不足，无法适应多变场景、准确捕捉输入输出的关系，训练时loss就高，则是欠拟合。
机器学习模型可以粗粒度地分为3类：
回归模型：有监督，训练数据是输入信号和ground-truth数值的pairs，将高维信号映射到某个向量。 分类模型：有监督，训练数据是输入信号和标签的pairs，将高维信号映射到有限标签集上。 概率密度函数模型：无监督，训练数据就是输入信号本身。 【二】训练 损失函数 所谓训练，就是降低训练集上预测函数的损失函数（loss，记作$\mathscr{L}$）的过程。
损失函数如何定义？对连续数值来说，均方差是一个标准选择。对概率密度来说，则用似然值——可令$\mathscr{L}=-\sum f(x;w)$，其中$f(x;w)$是各个训练样本的标准化log概率。对分类任务来说，一般用交叉熵。
何为交叉熵？分类模型为N个类输出N个logits（其实LLM也是这样，为vocabulary里每个token生成对应的logits，表示未标准化的log概率），logits经过softmax，得到后验概率$P(Y=y|X=x)$，这是裸概率，各个类的概率加起来和为1。令$\mathscr{L}=-\frac{1}{N} \sum_{n=1}^N logP(Y=y_n|X=x_n)$，这个$\mathscr{L}$即为交叉熵。交叉熵最小化，则真类别的概率最大化。
在度量学习中，虽然预测的值是连续的，但实际监督形式是分级，因为度量学习的目标是学习出样本之间可比较的距离，比如A、B、C三个点，其中A和B是同一个人脸的不同侧面，C是另一个人，那就要求AB之间距离小于AC。因此度量学习一般采用contrastive loss或triplet loss。
损失函数通常只是一个代理指标，而非实际性能指标，以分类任务为例，显然直接性能指标应该是分类错误率，只不过这个指标的梯度没有携带有效指导信息——错误率函数和模型权重是完全剥离的，知晓错误率的变化不能在训练中帮助模型减小错误率。
损失函数还可以被设计为依赖于模型权重，从而对模型权重进行某种约束和控制。比如权重衰减（weight decay），一种防止过拟合的正则化技术，给损失函数里增加了一项模型权重的平方和，从而惩罚大的权重数值，偏好小的数值，进而减少训练数据对模型权重取值范围的影响。这么做会使训练集上性能下降，但有利于在未见过的数据集上更好地泛化。
自回归模型 自回归模型是NLP/CV等领域处理离散序列的关键方法。原理是利用条件概率的链式法则: $$P(A\cap B)=P(A) P(B|A)$$ $$P(A\cap B\cap C)=P(A) P(B|A) P(C|A\cap B)$$
自回归模型输入是已有的T个token（每个token取值范围是大小为K的vocabulary集合），输出是K个候选token的logits。
token词汇域有限的场景是可计算的，条件概率的链式分解又使计算量降低——采样下一个token时，可以利用上一个token的概率，最终能生成符合联合概率分布的token序列。
训练自回归模型可以遍历各个步骤，把每一个逻辑时间节点上模型预测和真正的下一个token的交叉熵快照加起来，形成交叉熵loss。减小这个loss，即增大每个逻辑时间节点上模型预测token的似然。实际上监控的往往不是交叉熵，而是交叉熵(H)的指数，即困惑度perplexity(PPL)，PPL = 2^H。相比交叉熵，困惑度是归一化的，并不依赖输入序列的长度。
训练时，每个时刻都需要重新计算之前已经算过的，考虑到总体逻辑时间/步骤数目往往相当长，成百上千，甚至上万，这样的计算显然非常低效。解决方案是设计一个一次性预测所有逻辑时间（T）上的logits向量的模型——$f: {1,&amp;hellip;,K}^T \rightarrow \mathbb{R}^{T\times K}$，并确保t时刻的输入$x_t$对应的logits $l_t$只依赖于$x_1, x_2, x_3, &amp;hellip; x_{t-1}$。这种模型即因果模型（causal models），其原则是不让未来影响过去。
因果模型训练时可以用完整的序列计算output，一次性最大化序列中所有token的概率，最终也等价于最小化per-token交叉熵。
自然语言处理中有一个重要技术细节，即如何进行token的表示，可以是最低粒度的单符号，也可以说整个词。进行token表示的算法过程叫做tokenizer。一个标准做法是Byte Pair Encoding[Sennrich et al., 2015]1。
梯度下降 除了线性回归这种简单特例，一般最优权重$w^*$不会有closed-form expression。这种情况下最小化函数的工具是梯度下降：将权重初始化为随机的$w_0$，然后反复迭代，每次迭代都朝梯度方向修改权重使loss逐步降低，即每次迭代都令$w_{n+1} = w_n - \eta \nabla \mathscr{L}_{|w}(W_n).</description>
      <content>&lt;p&gt;&amp;ldquo;The Little Book of Deep Learning&amp;rdquo;(&lt;a href=&#34;https://fleuret.org/francois/lbdl.html&#34;&gt;&lt;code&gt;LBDL&lt;/code&gt;&lt;/a&gt;)是François Fleuret写的一本适配手机屏的书，精简扼要地面向stem背景读者介绍深度学习。正如&lt;code&gt;DDIA&lt;/code&gt;可被视为分布式系统方向的入门教程，&lt;code&gt;LBDL&lt;/code&gt;是理想的深度学习101。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/tlb.jpg&#34; alt=&#34;tlb&#34;&gt;&lt;/p&gt;
&lt;p&gt;精简，或者说压缩，正是深度模型的strength，也是这个信息过载时代的virtue。用A4纸打印这个小册子，读起来非常舒适。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下来是知识内化和梳理。&lt;/p&gt;
&lt;h2 id=&#34;一概述&#34;&gt;【一】概述&lt;/h2&gt;
&lt;p&gt;高维信号难以用规则系统分析，而深度网络则克服了这个困难，用具有大量权重的深层映射拟合出一个足够好（loss足够低）的近似函数——这个函数可以是高维信号到连续向量（回归）或离散值（分类）的映射，也可以是一种概率密度函数，总之，它能从数据分布中学习到某种紧凑且有区分能力的表征。&lt;/p&gt;
&lt;p&gt;若数据样本不足，即使训练数据上表现良好，也可能在真实应用中效果不佳，这就是过拟合。
若模型能力不足，无法适应多变场景、准确捕捉输入输出的关系，训练时loss就高，则是欠拟合。&lt;/p&gt;
&lt;p&gt;机器学习模型可以粗粒度地分为3类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;回归模型：有监督，训练数据是输入信号和ground-truth数值的pairs，将高维信号映射到某个向量。&lt;/li&gt;
&lt;li&gt;分类模型：有监督，训练数据是输入信号和标签的pairs，将高维信号映射到有限标签集上。&lt;/li&gt;
&lt;li&gt;概率密度函数模型：无监督，训练数据就是输入信号本身。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;二训练&#34;&gt;【二】训练&lt;/h2&gt;
&lt;h3 id=&#34;损失函数&#34;&gt;损失函数&lt;/h3&gt;
&lt;p&gt;所谓训练，就是降低训练集上预测函数的损失函数（loss，记作$\mathscr{L}$）的过程。&lt;/p&gt;
&lt;p&gt;损失函数如何定义？对连续数值来说，均方差是一个标准选择。对概率密度来说，则用似然值——可令$\mathscr{L}=-\sum f(x;w)$，其中$f(x;w)$是各个训练样本的标准化log概率。对分类任务来说，一般用交叉熵。&lt;/p&gt;
&lt;p&gt;何为交叉熵？分类模型为N个类输出N个logits（其实LLM也是这样，为vocabulary里每个token生成对应的logits，表示未标准化的log概率），logits经过softmax，得到后验概率$P(Y=y|X=x)$，这是裸概率，各个类的概率加起来和为1。令$\mathscr{L}=-\frac{1}{N} \sum_{n=1}^N logP(Y=y_n|X=x_n)$，这个$\mathscr{L}$即为交叉熵。交叉熵最小化，则真类别的概率最大化。&lt;/p&gt;
&lt;p&gt;在度量学习中，虽然预测的值是连续的，但实际监督形式是分级，因为度量学习的目标是学习出样本之间可比较的距离，比如A、B、C三个点，其中A和B是同一个人脸的不同侧面，C是另一个人，那就要求AB之间距离小于AC。因此度量学习一般采用contrastive loss或triplet loss。&lt;/p&gt;
&lt;p&gt;损失函数通常只是一个代理指标，而非实际性能指标，以分类任务为例，显然直接性能指标应该是分类错误率，只不过这个指标的梯度没有携带有效指导信息——错误率函数和模型权重是完全剥离的，知晓错误率的变化不能在训练中帮助模型减小错误率。&lt;/p&gt;
&lt;p&gt;损失函数还可以被设计为依赖于模型权重，从而对模型权重进行某种约束和控制。比如权重衰减（weight decay），一种防止过拟合的正则化技术，给损失函数里增加了一项模型权重的平方和，从而惩罚大的权重数值，偏好小的数值，进而减少训练数据对模型权重取值范围的影响。这么做会使训练集上性能下降，但有利于在未见过的数据集上更好地泛化。&lt;/p&gt;
&lt;h3 id=&#34;自回归模型&#34;&gt;自回归模型&lt;/h3&gt;
&lt;p&gt;自回归模型是NLP/CV等领域处理离散序列的关键方法。原理是利用条件概率的链式法则:
$$P(A\cap B)=P(A) P(B|A)$$
$$P(A\cap B\cap C)=P(A) P(B|A) P(C|A\cap B)$$&lt;/p&gt;
&lt;p&gt;自回归模型输入是已有的T个token（每个token取值范围是大小为K的vocabulary集合），输出是K个候选token的logits。&lt;/p&gt;
&lt;p&gt;token词汇域有限的场景是可计算的，条件概率的链式分解又使计算量降低——采样下一个token时，可以利用上一个token的概率，最终能生成符合联合概率分布的token序列。&lt;/p&gt;
&lt;p&gt;训练自回归模型可以遍历各个步骤，把每一个逻辑时间节点上模型预测和真正的下一个token的交叉熵快照加起来，形成交叉熵loss。减小这个loss，即增大每个逻辑时间节点上模型预测token的似然。实际上监控的往往不是交叉熵，而是交叉熵(H)的指数，即困惑度perplexity(PPL)，PPL = 2^H。相比交叉熵，困惑度是归一化的，并不依赖输入序列的长度。&lt;/p&gt;
&lt;p&gt;训练时，每个时刻都需要重新计算之前已经算过的，考虑到总体逻辑时间/步骤数目往往相当长，成百上千，甚至上万，这样的计算显然非常低效。解决方案是设计一个一次性预测所有逻辑时间（T）上的logits向量的模型——$f: {1,&amp;hellip;,K}^T \rightarrow \mathbb{R}^{T\times K}$，并确保t时刻的输入$x_t$对应的logits $l_t$只依赖于$x_1, x_2, x_3, &amp;hellip; x_{t-1}$。这种模型即因果模型（causal models），其原则是不让未来影响过去。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/causal.png&#34; alt=&#34;causal&#34;&gt;&lt;/p&gt;
&lt;p&gt;因果模型训练时可以用完整的序列计算output，一次性最大化序列中所有token的概率，最终也等价于最小化per-token交叉熵。&lt;/p&gt;
&lt;p&gt;自然语言处理中有一个重要技术细节，即如何进行token的表示，可以是最低粒度的单符号，也可以说整个词。进行token表示的算法过程叫做tokenizer。一个标准做法是Byte Pair Encoding[Sennrich et al., 2015]&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3 id=&#34;梯度下降&#34;&gt;梯度下降&lt;/h3&gt;
&lt;p&gt;除了线性回归这种简单特例，一般最优权重$w^*$不会有closed-form expression。这种情况下最小化函数的工具是梯度下降：将权重初始化为随机的$w_0$，然后反复迭代，每次迭代都朝梯度方向修改权重使loss逐步降低，即每次迭代都令$w_{n+1} = w_n - \eta \nabla \mathscr{L}_{|w}(W_n). $ 其中$\eta$即学习率，如果设置得太小，训练可能太慢，而且容易卡在局部最小，如果设置得太大，则容易在最低点附近左右横跳。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/gradient_descent.png&#34; alt=&#34;gd&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于每个点$w$来说，梯度$\nabla \mathscr{L}_{|w}(w)$就是能最大化$\mathscr{L}$增量的方向。因此梯度下降就可以通过每次迭代中减去学习率*梯度的值，使所有迭代串联起来可形成一个接近最优的最小化$\mathscr{L}$路线。&lt;/p&gt;
&lt;p&gt;实践中，所有loss均能表示为多个小样本甚至单样本loss的均值：$\mathscr{L} = \frac{1}{N} \sum_{n=1}^N \ell_n(w) $，其中$\ell_n(w)=L(f(x_n;w),y_n)$，因而梯度可表示为下式：&lt;/p&gt;
&lt;p&gt;$$\nabla ℒ_{|w}(w) = \frac{1}{N} \sum_{n=1}^N \nabla \ell_{n|w}(w)$$&lt;/p&gt;
&lt;p&gt;全量计算梯度开销较高，可以用局部求和估计全量求和（要求做好数据shuffling，数据的stochasticity消除估计的偏倚）。为了让计算能放进内存，标准的做法是把完整训练集分成相当多（可以是百万级）batches，从每个batch得到一个梯度的估计，然后根据这个估计去更新权重，这种做法即mini-batch SGD(stochastic gradient descent)。这种算法有很多变种，比如Adam[Kingma and Ba, 2014]&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3 id=&#34;反向传播&#34;&gt;反向传播&lt;/h3&gt;
&lt;p&gt;给定$\ell(w)=L(f(x;w),y)$，怎么计算$\nabla\ell_{|w}(w)$呢？考虑到$f$和$L$都是标准张量运算的组合，它们与任何数学表达式一样，基于链式法则可以得到其表达式。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/bp.png&#34; alt=&#34;bp&#34;&gt;&lt;/p&gt;
&lt;p&gt;简单起见，将一个深度为$D$的模型表示为$f = f^{(D)} \circ f^{(D-1)} \circ &amp;hellip; \circ f^{(1)}$。则前馈过程即按顺序计算$x^{(d-1)} \rightarrow x^{(d)}$，即$x^{(d)} = f^{(d)}(x^{(d-1)};w_d)$，直到最终得到$x^{(D)}$作为模型输出。&lt;/p&gt;
&lt;p&gt;反向传播过程则是反过来计算$\nabla\ell_{{|x}^(d-1)} \leftarrow \nabla\ell_{{|x}^(d)}$以及$\nabla\ell_{|w_d} \leftarrow \nabla\ell_{{|x}^(d)}$，其中$\nabla\ell_{{|x}^(d-1)}$&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;是$\nabla\ell_{{|x}^(d)}$&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;和$J_{f^{(d)}|x}$&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;乘积。而我们在训练过程中实际关心的另一个梯度$\nabla\ell_{|w_d}$是$\nabla\ell_{{|x}^(d)}$和$J_{f^{(d)}|w}$&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;乘积。&lt;/p&gt;
&lt;p&gt;深度学习训练框架主要处理的就是隐藏反向传播梯度计算的复杂度，即提供自动求导/计算求导能力，该技术在深度学习之前也有广泛应用，详见AutoGrad [Baydin et al., 2015]&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;显然反向传播的矩阵计算量两倍于前馈推理（每层多了一次权重的反向传播）。反向传播的内存需求也远大于前馈推理，因为每一层的$x^{(d)}$都要保留在内存中，而非推理则不需要保留，只要留着最新的。解决内存占用过高的技术包括：reversible layers[Gomez et al., 2017]&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;和checkpointing[Chen et al., 2016]&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;深度模型的一个问题是梯度消失（见[Glorot and Bengio, 2010]&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;），即经过很多次反向传播后，数值变得太大或太小。常规应对做法是gradient norm clipping[Pascanu et al., 2013]&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3 id=&#34;自监督训练&#34;&gt;自监督训练&lt;/h3&gt;
&lt;p&gt;GPT在大规模无标注训练集上训练就足以处理很多任务，比如翻译（见[Radford et al., 2019]&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;），这就是典型的自监督训练应用，其最重要的优势是可以利用超大规模的未标注数据，将训练数据规模的边界再向前推进。&lt;/p&gt;
&lt;h2 id=&#34;三构件&#34;&gt;【三】构件&lt;/h2&gt;
&lt;h3 id=&#34;线性层&#34;&gt;线性层&lt;/h3&gt;
&lt;p&gt;全连接层是最基础的线性层构件，可用$D \times D&amp;rsquo;$的矩阵$W$和一个bias向量$b$表示。它实现了一个能泛化到任意张量形状的仿射变换，给定任意输入$X$，其形状为$D_1 \times \dots \times D_k \times D$，全连接层计算得到一个输出$Y$，其形状为$D_1 \times \dots \times D_k \times D&amp;rsquo;$：$\forall d_1,\dots,d_K, Y[d1,\dots,d_K] = WX[d1,\dots,d_K] + b $&lt;/p&gt;
&lt;p&gt;全连接层处理高维数据时，参数量太大。此外全连接层假设输入输出之间存在复杂非线性关系，忽视了更简单的结构化规律&lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;。然而高维信号普遍有这种强结构，比如图片兼具short-term关联和抵抗变换、缩放、对称的统计学静态性。相较而言，卷积层能更好地捕捉信号的空间结构——因为卷积层权重被输入信号的不同部分共享，这些权重因而能学习到某种局部空间结构，比如图片的边缘、形状、角。多层卷积是高维信号（图片、声音）的常用降维工具。&lt;/p&gt;
&lt;p&gt;一维卷积以$D \times T$张量$X$为输入，对每个$D\times K$子张量施加仿射变换$\phi(\cdot;w): \mathbb{R}^{D\times K} \rightarrow \mathbb{R}^{D&amp;rsquo;\times 1}$，将$D&amp;rsquo;\times 1$结果依次存入$Y$中。&lt;/p&gt;
&lt;p&gt;一维反卷积则是以$D \times T$张量$X$为输入，对每个$D\times 1$子张量施加仿射变换$\phi(\cdot;w): \mathbb{R}^{D\times 1} \rightarrow \mathbb{R}^{D&amp;rsquo;\times K}$，将结果加起来形成的$D&amp;rsquo;\times K$张量存入$Y$中。&lt;/p&gt;
&lt;p&gt;下图中，$D=3, K=5, D&amp;rsquo;=4$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/1dconv.png&#34; alt=&#34;1dconv&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/2dconv.png&#34; alt=&#34;2dconv&#34;&gt;&lt;/p&gt;
&lt;p&gt;一维卷积往往用于处理序列数据或时序数据。二维卷积则常用于处理图片，或其他以2D矩阵为输入的任务。转置卷积/反卷积则主要用于GAN、VAE这样的生成式模型中，从一个低维特征膨胀到一个高分辨率图片。&lt;/p&gt;
&lt;h3 id=&#34;激活函数&#34;&gt;激活函数&lt;/h3&gt;
&lt;p&gt;如果模型中仅使用线性组件，则整体也是线性操作，因此必须要引入非线性性，这种非线性性通常由激活函数实现。最常用的激活函数是ReLU [Glorot et al., 2011]&lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;。ReLU之前则是双曲正切函数Tanh。&lt;/p&gt;
&lt;p&gt;还有一些激活函数思路和ReLU差不多，保证正数不变，压缩负值，如：Leaky ReLU[Maas et al., 2013]&lt;sup id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;15&lt;/a&gt;&lt;/sup&gt;，GELU [Hendrycks and Gimpel, 2016]&lt;sup id=&#34;fnref:16&#34;&gt;&lt;a href=&#34;#fn:16&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;16&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3 id=&#34;池化&#34;&gt;池化&lt;/h3&gt;
&lt;p&gt;池化是降维，减少信号大小的经典策略，将若干邻近值取max或avg合并出一个值。&lt;/p&gt;
&lt;h3 id=&#34;随机失活&#34;&gt;随机失活&lt;/h3&gt;
&lt;p&gt;Dropout[Srivastava et al., 2014]&lt;sup id=&#34;fnref:17&#34;&gt;&lt;a href=&#34;#fn:17&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;17&lt;/a&gt;&lt;/sup&gt;层无可训练参数，只有一个超参$p$，在训练时用于以概率$p$随机关闭一些neuron，避免个别neuron对整体的影响，迫使其他neuron用略微不同的权重代劳。因此dropout是训练时防止过拟合的正则化工具。推理时dropout是关闭的。&lt;/p&gt;
&lt;h3 id=&#34;归一化层&#34;&gt;归一化层&lt;/h3&gt;
&lt;p&gt;归一化可用于抵抗梯度消失。最主要的归一化层是Batch Normalization[Ioffe and Szegedy, 2015]&lt;sup id=&#34;fnref:18&#34;&gt;&lt;a href=&#34;#fn:18&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;18&lt;/a&gt;&lt;/sup&gt;，由超参$D$和可训练参数$\beta_1, \dots,\beta_D$和$\gamma_1, \dots,\gamma_D$组成。给定一批$D$维样本$x_1, \dots, x_B$，先计算每个维度的均值$m_d = \frac{1}{B} \sum_{b=1}^B x_{b,d}$ 和方差 $v_d  = \frac{1}{B} \sum_{b=1}^B (x_{b,d} - m_d)^2$。&lt;/p&gt;
&lt;p&gt;然后再对每个b，计算均值为0方差为1的归一化值$z_{b,d} = \frac{x_{b,d} - m_d}{\sqrt{v_d + \epsilon}} $，再算出最终结果$y_{b,d} = \gamma_d z_{b,d} + \beta_d$，这个最终值的均值为$\beta_d$，方差为$\gamma_d$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/norm.png&#34; alt=&#34;norm&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;残差连接&#34;&gt;残差连接&lt;/h3&gt;
&lt;p&gt;跳跃连接（skip connections，见[Long et al., 2014]&lt;sup id=&#34;fnref:19&#34;&gt;&lt;a href=&#34;#fn:19&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;19&lt;/a&gt;&lt;/sup&gt;; [Ronneberger et al., 2015]&lt;sup id=&#34;fnref:20&#34;&gt;&lt;a href=&#34;#fn:20&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;20&lt;/a&gt;&lt;/sup&gt;）同样可对抗梯度消失。实际上跳跃连接不是一个layer，而是一种让某些层的输出跳过一些中间层嫁接到后面的设计。这种设计允许更原始的信号在更后面的层中得到“反思”。&lt;/p&gt;
&lt;p&gt;跳跃连接的实用实现是残差连接（residual connections），直接把两种信号求和，而且跳跃的层数不算多。这种设计允许信号在穿越某些原本会梯度消失的层时得以幸存。基于残差连接，何凯明构建了ResNet[He et al., 2015]&lt;sup id=&#34;fnref:21&#34;&gt;&lt;a href=&#34;#fn:21&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;21&lt;/a&gt;&lt;/sup&gt;，Google设计了Transformer[Vaswani et al., 2017]&lt;sup id=&#34;fnref:22&#34;&gt;&lt;a href=&#34;#fn:22&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;22&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3 id=&#34;注意力层&#34;&gt;注意力层&lt;/h3&gt;
&lt;p&gt;已有的组件缺乏将局部信息和张量中较远位置的信息结合起来的能力，Attention Layer则专长于此道——通过为所得张量的每个组件与输入张量的每个组件计算注意力得分，不受局部性约束地，在整个张量范围内对特征进行平均&lt;sup id=&#34;fnref1:22&#34;&gt;&lt;a href=&#34;#fn:22&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;22&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;给定$N^Q\times D^{QK}$维queries张量$Q$，$N^{KV}\times D^{QK}$维keys张量$K$，$N^{KV}\times D^V$维values张量$V$，通过&lt;code&gt;Attention操作&lt;/code&gt;$att(K,Q,V)$计算得到$N^Q\times D^V$维的张量Y：&lt;/p&gt;
&lt;p&gt;$$Y = att(K,Q,V) = \underbrace{softargmax(\frac{QK^T}{\frac{1}{\sqrt{D^{QK}}}})}_A V$$&lt;/p&gt;
&lt;p&gt;整个过程分两步，第一步先计算每个query index $q$和每个key index $k$的注意力得分，即queries和keys点乘后的&lt;code&gt;softargmax&lt;/code&gt;结果：$A_{q,k} = \frac{exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_k ) }{\sum_l exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_l)}$，其中$\frac{1}{\sqrt{D^{QK}}}$是一个缩放参数，用于保证取值范围在$D^{QK}$变大时大体不变。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/attention.png&#34; alt=&#34;att&#34;&gt;&lt;/p&gt;
&lt;p&gt;得到注意力得分$A_{q,k}$后，再进行第二步计算：$Y_q = \sum_k A_{q,k}V_k$。注意力得分即queries和keys之间的匹配程度，匹配程度越高，该权重就越高。如果某个query和某个key匹配度达到极限，注意力得分接近1，则直接拿到这个key对应的value。如果和好几个key都有中等水准的匹配，则按注意力得分做加权平均。&lt;/p&gt;
&lt;h2 id=&#34;其他话题&#34;&gt;其他话题&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;LBDL&lt;/code&gt;还讨论了各种深度学习模型架构和应用，如多层感知机、卷积网络、注意力模型、RNN、Autoencoder、GAN、图神经网络、GPT、Diffusion。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;R. Sennrich, B. Haddow, and A. Birch. Neural Machine Translation of Rare Words with Subword Units. CoRR, abs/1508.07909, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1508.07909&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;D. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. CoRR, abs/1412.6980, 2014. &lt;a href=&#34;https://arxiv.org/pdf/1412.6980&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;$\nabla\ell_{{|x}^(d-1)}$即$f^{d-1}$的变量$x^{d-1}$对应的损失函数梯度。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;$\nabla\ell_{{|x}^(d-1)}$即$f^{d}$的变量$x^{d}$对应的损失函数梯度。&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;$J_{f^{(d)}|x}$即第d个layer函数$f^{(d)}$相对变量x的Jacobian，雅可比矩阵，即函数的一阶偏导数以一定方式排列成的矩阵。&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;$J_{f^{(d)}|w}$即第d个layer函数$f^{(d)}$相对权重w的Jacobian。&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;A. Baydin, B. Pearlmutter, A. Radul, and J. Siskind. Automatic differentiation in machine learning: a survey. CoRR, abs/1502.05767, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1502.05767&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;A. Gomez, M. Ren, R. Urtasun, and R. Grosse. The Reversible Residual Network: Backpropagation Without Storing Activations. CoRR, abs/1707.04585, 2017. &lt;a href=&#34;https://arxiv.org/pdf/1707.04585&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training Deep Nets with Sublinear Memory Cost. CoRR, abs/1604.06174, 2016. &lt;a href=&#34;https://arxiv.org/pdf/1604.06174&#34;&gt;pdf&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2010. &lt;a href=&#34;https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&#34;&gt;pdf&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning (ICML), 2013. &lt;a href=&#34;https://proceedings.mlr.press/v28/pascanu13.pdf&#34;&gt;pdf&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;A. Radford, J. Wu, R. Child, et al. Language Models are Unsupervised Multitask Learners, 2019. &lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34;&gt;
&lt;p&gt;这就是所谓的全连接层的&lt;code&gt;inductive bias&lt;/code&gt;。&amp;#160;&lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34;&gt;
&lt;p&gt;X. Glorot, A. Bordes, and Y. Bengio. Deep Sparse Rectifier Neural Networks. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2011. &lt;a href=&#34;http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:15&#34;&gt;
&lt;p&gt;A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In proceedings of the ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013. &lt;a href=&#34;https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:15&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:16&#34;&gt;
&lt;p&gt;D. Hendrycks and K. Gimpel. Gaussian Error Linear Units (GELUs). CoRR, abs/1606.08415, 2016. &lt;a href=&#34;https://arxiv.org/pdf/1606.08415&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:16&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:17&#34;&gt;
&lt;p&gt;N. Srivastava, G. Hinton, A. Krizhevsky, et al. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research (JMLR), 15:1929–1958, 2014. &lt;a href=&#34;https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:17&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:18&#34;&gt;
&lt;p&gt;S. Ioffe and C. Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In International Conference on Machine Learning (ICML), 2015. &lt;a href=&#34;http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:18&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:19&#34;&gt;
&lt;p&gt;J. Long, E. Shelhamer, and T. Darrell. Fully Convolutional Networks for Semantic Segmentation. CoRR, abs/1411.4038, 2014. &lt;a href=&#34;https://arxiv.org/pdf/1411.4038&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:19&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:20&#34;&gt;
&lt;p&gt;O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1505.04597.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:20&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:21&#34;&gt;
&lt;p&gt;K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition. CoRR, abs/1512.03385, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1512.03385&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:21&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:22&#34;&gt;
&lt;p&gt;A. Vaswani, N. Shazeer, N. Parmar, et al. Attention Is All You Need. CoRR, abs/1706.03762, 2017. &lt;a href=&#34;https://arxiv.org/pdf/1706.03762&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:22&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:22&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>eBPF Tracing for Memory-Stalled Applications</title>
      <link>https://cmbbq.github.io/posts/ebpf/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/ebpf/</guid>
      <description>现代workloads的瓶颈 如今使用CPU而不将计算offload到GPU等加速器的数据中心应用，大多是访存密集应用，例如倒排检索、向量检索、模型推理、分析型数据库。
纯粹instruction-bound的workload反而罕见。即使是最典型的计算密集场景，充满了矩阵乘加的大模型训推，实际瓶颈也出现在内存IO、off-package互连上。
现代硬件的发展趋势是越来越深的内存hierarchy，相应地，现代workloads（训推、检索、数据分析）的瓶颈也逐渐向内存总线、Cache、MMU、CPU-to-CPU互连、inter-node互连转移。
如今内存慢得就像过去的磁盘。Rust的hashmap用B树而非红黑树实现。ScyllaDB和DragonFly通过利用numa机器局部性以及更好的cache优化分别击败Cassandra和Redis。至少，对不自研加速器硬件的互联网公司的研发团队——即数据中心应用的开发者来说，矩阵乘加offloading到GPU/ASIC/AMX即可，访存优化为代表的互连优化才是性能工程的主战场。
CPU Profiling和CPU利用率的局限性 对于访存密集应用来说，很多人被CPU利用率高误导，以为计算是瓶颈，于是做CPU profiling1和计算优化，往往效果不佳。假设只有10%线程时间在CPU上跑，而70%在等内存读写，那无论CPU profiling做得多好，也没办法找到真正的瓶颈。perf是典型的CPU profiling工具，perf record -F 1000是按照1000Hz采样，对各个函数上的时钟周期可以给出比较准确的估计，但这种采样在阻塞阶段不生效。 类似地，CPU火焰图也只是给出不阻塞时的采样数据，CPU火焰图的最上沿的函数就是所有on—CPU函数，其耗时之和是CPU time之和，不包括off-CPU time。
CPU利用率在目前仍然被广泛使用，但它已经过时。CPU利用率的真实含义是“非空闲率”，即CPU没有跑idle thread的比例，也就包括了内存IO阻塞、网络IO阻塞、spinlock盲等。这个概念过于古老，它出现时尚不存在memory wall，cpu并不显著快于主存，这显然已经不适用于现代硬件语境，容易给人计算单元是系统瓶颈的错觉。
eBPF off-CPU分析：in-kernel简报 怎么度量off-CPU time？最简单的办法是应用层tracing，记录重点代码各个函数和代码块的耗时。大多数情况下，这其实就是最好的方案，精度也不错。不过相比火焰图还不够帅，也不够全面，毕竟在哪加时间戳依赖主观判断，有时候真正的瓶颈会出现在意想不到的地方。
Linux 4.8+2可使用eBPF做off-CPU分析。比如eBPF工具bcc/cpudist，bcc/offcputime。offcputime生成的call stacks可以直接用flamegraph.pl绘制off-CPU火焰图。bcc包含了大量工具，其具体使用方式可参照其官方tutorial。
eBPF tracing与传统的off-CPU tracer(比如perf)相比，最显著的优势是不必把所有内核事件往用户空间dump（调度事件是非常频繁的，perf往往生成巨量数据，注入的额外开销太大，不仅仅是CPU开销，还有磁盘IO开销），而是在内核就按照某种可编程的规则做了总结，把精简的信息输出出来。
此外，内核支持eBPF后，各种off-CPU分析需求都可以统一用eBPF实现，不再需要针对不同场景使用或制作不同的工具了——此前用perf做事件追踪，用storage tracing做存储IO追踪，用内核统计数据观测调度时延，不成体系，且性能良莠不一。
eBPF追踪off-CPU时长的思路是在context switch事件结束时记录一次stack（off—CPU期间stack是不变的，一次足矣），为当前context的off-CPU时长增加线程睡眠时间。其伪代码如下：
on context switch finish: sleeptime[prev_thread_id] = timestamp if !sleeptime[thread_id] return delta = timestamp - sleeptime[thread_id] totaltime[pid, execname, user stack, kernel stack] += delta sleeptime[thread_id] = 0 on tracer exit: for each key in totaltime: print key print totaltime[key] 所以，什么是eBPF？ 简单说，eBPF是一个允许跑自定义代码做一些tracing和系统监控的in-kernel runtime，是BPF的升级版。</description>
      <content>&lt;h2 id=&#34;现代workloads的瓶颈&#34;&gt;现代workloads的瓶颈&lt;/h2&gt;
&lt;p&gt;如今使用CPU而不将计算offload到GPU等加速器的数据中心应用，大多是访存密集应用，例如倒排检索、向量检索、模型推理、分析型数据库。&lt;/p&gt;
&lt;p&gt;纯粹instruction-bound的workload反而罕见。即使是最典型的计算密集场景，充满了矩阵乘加的大模型训推，实际瓶颈也出现在内存IO、off-package互连上。&lt;/p&gt;
&lt;p&gt;现代硬件的发展趋势是越来越深的内存hierarchy，相应地，现代workloads（训推、检索、数据分析）的瓶颈也逐渐向内存总线、Cache、MMU、CPU-to-CPU互连、inter-node互连转移。&lt;/p&gt;
&lt;p&gt;如今内存慢得就像过去的磁盘。Rust的hashmap用B树而非红黑树实现。ScyllaDB和DragonFly通过利用numa机器局部性以及更好的cache优化分别击败Cassandra和Redis。至少，对不自研加速器硬件的互联网公司的研发团队——即数据中心应用的开发者来说，矩阵乘加offloading到GPU/ASIC/AMX即可，访存优化为代表的互连优化才是性能工程的主战场。&lt;/p&gt;
&lt;h2 id=&#34;cpu-profiling和cpu利用率的局限性&#34;&gt;CPU Profiling和CPU利用率的局限性&lt;/h2&gt;
&lt;p&gt;对于访存密集应用来说，很多人被CPU利用率高误导，以为计算是瓶颈，于是做CPU profiling&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;和计算优化，往往效果不佳。假设只有10%线程时间在CPU上跑，而70%在等内存读写，那无论CPU profiling做得多好，也没办法找到真正的瓶颈。&lt;code&gt;perf&lt;/code&gt;是典型的CPU profiling工具，&lt;code&gt;perf record -F 1000&lt;/code&gt;是按照1000Hz采样，对各个函数上的时钟周期可以给出比较准确的估计，但这种采样在阻塞阶段不生效。 类似地，CPU火焰图也只是给出不阻塞时的采样数据，CPU火焰图的最上沿的函数就是所有on—CPU函数，其耗时之和是CPU time之和，不包括off-CPU time。&lt;/p&gt;
&lt;p&gt;CPU利用率在目前仍然被广泛使用，但它已经过时。CPU利用率的真实含义是“非空闲率”，即CPU没有跑idle thread的比例，也就包括了内存IO阻塞、网络IO阻塞、spinlock盲等。这个概念过于古老，它出现时尚不存在memory wall，cpu并不显著快于主存，这显然已经不适用于现代硬件语境，容易给人计算单元是系统瓶颈的错觉。&lt;/p&gt;
&lt;h2 id=&#34;ebpf-off-cpu分析in-kernel简报&#34;&gt;eBPF off-CPU分析：in-kernel简报&lt;/h2&gt;
&lt;p&gt;怎么度量off-CPU time？最简单的办法是应用层tracing，记录重点代码各个函数和代码块的耗时。大多数情况下，这其实就是最好的方案，精度也不错。不过相比火焰图还不够帅，也不够全面，毕竟在哪加时间戳依赖主观判断，有时候真正的瓶颈会出现在意想不到的地方。&lt;/p&gt;
&lt;p&gt;Linux 4.8+&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;可使用eBPF做off-CPU分析。比如eBPF工具&lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/cpudist.py&#34;&gt;bcc/cpudist&lt;/a&gt;，&lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/offcputime_example.txt&#34;&gt;bcc/offcputime&lt;/a&gt;。offcputime生成的call stacks可以直接用&lt;a href=&#34;https://github.com/brendangregg/FlameGraph&#34;&gt;flamegraph.pl&lt;/a&gt;绘制off-CPU火焰图。bcc包含了大量工具，其具体使用方式可参照其&lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/docs/tutorial.md&#34;&gt;官方tutorial&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;eBPF tracing与传统的off-CPU tracer(比如&lt;code&gt;perf&lt;/code&gt;)相比，最显著的优势是不必把所有内核事件往用户空间dump（调度事件是非常频繁的，&lt;code&gt;perf&lt;/code&gt;往往生成巨量数据，注入的额外开销太大，不仅仅是CPU开销，还有磁盘IO开销），而是在内核就按照某种可编程的规则做了总结，把精简的信息输出出来。&lt;/p&gt;
&lt;p&gt;此外，内核支持eBPF后，各种off-CPU分析需求都可以统一用eBPF实现，不再需要针对不同场景使用或制作不同的工具了——此前用&lt;code&gt;perf&lt;/code&gt;做事件追踪，用storage tracing做存储IO追踪，用内核统计数据观测调度时延，不成体系，且性能良莠不一。&lt;/p&gt;
&lt;p&gt;eBPF追踪off-CPU时长的思路是在context switch事件结束时记录一次stack（off—CPU期间stack是不变的，一次足矣），为当前context的off-CPU时长增加线程睡眠时间。其伪代码如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;on context switch finish:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	sleeptime[prev_thread_id] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; timestamp
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;sleeptime[thread_id]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	delta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; timestamp &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; sleeptime[thread_id]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	totaltime[pid, execname, user stack, kernel stack] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; delta
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	sleeptime[thread_id] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;on tracer exit:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; each key &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; totaltime:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		print key
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		print totaltime[key]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;所以什么是ebpf&#34;&gt;所以，什么是eBPF？&lt;/h2&gt;
&lt;p&gt;简单说，eBPF是一个允许跑自定义代码做一些tracing和系统监控的in-kernel runtime，是BPF的升级版。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/ebpf.png&#34; alt=&#34;ebpf&#34;&gt;&lt;/p&gt;
&lt;p&gt;最初的BPF，即Berkeley Packet Filter，是一个用于报文过滤的几乎被遗忘的古老内核特性。eBPF在BPF基础上做了扩展，允许事件源从报文扩展到多种多样的事件源，eBPF VM有更大的存储空间，更多寄存器和64位word size——BPF事实上提供了一个in-kernel的沙盒环境，或者说虚拟机，安全且受限地执行用户定义的程序。因此eBPF机制的出现实际上在内核态程序、用户态程序之外创造了新的软件品类。&lt;/p&gt;
&lt;p&gt;eBPF程序不是预编译或解释的，而是JIT CO-RE&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;的，程序出错既不abort，也不panic，而是返回error message。内核态直接访问资源，用户态通过系统调用或fault访问资源，eBPF则是通过一些受限的helper访问资源——目前来说主要作用还是tracing，做一些可观测性上的工作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/ebpf2.png&#34; alt=&#34;ebpf2&#34;&gt;&lt;/p&gt;
&lt;p&gt;eBPF把JIT编译器和安全验证器直接放到了内核里，用户态的bpf程序先经过parser变成AST，再做一些构造和语法分析，然后生成IR，最终生成优化后的bytecode。BPF bytecode作为输入进入内核的JIT和verifier再编译成机器码给CPU执行。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/ebpf3.png&#34; alt=&#34;ebpf3&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ebpf的其他应用&#34;&gt;eBPF的其他应用&lt;/h2&gt;
&lt;p&gt;eBPF除了用在可观测性上，还可以应用于网络，在L3/L4/L7做traffic control, monitoring或load balancing，比如libbpf &lt;code&gt;tc&lt;/code&gt;/&lt;code&gt;qdiscs&lt;/code&gt; library, &lt;code&gt;XDP&lt;/code&gt;(裸金属高性能可编程网络)/&lt;code&gt;Cilium&lt;/code&gt;(高性能云原生网络)/&lt;code&gt;Katran&lt;/code&gt;(传输层负载均衡)。&lt;/p&gt;
&lt;p&gt;此外，eBPF还可以应用于安全领域。毕竟eBPF可以观测系统中的各种事件，比如监控某些敏感文件（/etc/passwd这种）是否被篡改。基于这种观测能力在加上一些安全相关的先验知识，就可以做一些安全工具。K8s的&lt;code&gt;seccomp&lt;/code&gt;工具就是基于eBPF实现的。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;区别于off-CPU分析，这里的CPU profiling指狭义的on-CPU分析，不考虑阻塞中的thread time。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;不过Linux 5.x才有完整的CO-RE和BTF支持，其中BTF(BPF Type Format)是为eBPF设计的内核数据结构描述机制。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;CO-RE: Compile Once Run Everywhere，也就是说BPF bytecode是可以relocate的。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Artificial Intuition: Reasoning Abilities of LLMs</title>
      <link>https://cmbbq.github.io/posts/on-reasoning-abilities-of-llms/</link>
      <pubDate>Wed, 29 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-reasoning-abilities-of-llms/</guid>
      <description>LLM具有System2吗？ System1和System2的划分来源于心理学家Daniel Kahneman的理论1，描述了思考的两种模式，System1指的是快速、自动、直觉且不费力的模式，System2则是慢速、刻意、分析性且有意识的模式。
现有的LLM除了已被证明的“将数据分布映射到不相干低维子空间2的反射式推理能力”(System1)，是否具有一定的“慢速思考”、多步推理和规划能力(System2)?
LLM横空出世之初，有很多狂野声明，比如&amp;quot;LLM as zero-shot planner&amp;quot;，&amp;ldquo;LLMs are zero-shot reasoner&amp;rdquo;，但这些声音在现在看来更像是一时跟风和hype。
现在，无论是普通从业者，还是知名学者，如Yann Lecun，Yoshua Bengio，马毅，Subbarao Kambhampati逐渐形成共识，认为LLM不具备System2，很多研究也通过一些reasoning benchmark对这一观点进行佐证，比如Huang et al., 20233，Jin et al., 20234，Valmeekam, Marquez, 20235，Stechly et al., 20236，Dziri et al., 20237。
也有少数学者，比如Ilya Sutskever在访谈中说scaling足以产生System2能力，无需架构创新，但Ilya似乎并不坦诚，动机不明8。有研究者认为LLM加上Chain-of-Thought prompting是具有推理能力，如Saparov &amp;amp; He, 20239，Feng et al., 202310，但终究依赖了外界的prompting，且实验方法上并不能排除近似信息提取模拟了逻辑推理的可能。
LLM本身，考虑到transformer每次生成回答计算量是有上确界的，注定不可能为某个问题倾注不成比例的计算量，仅此一点，就可以从理性层面否定LLM具备Human-like System2。毕竟System2从定义上，就是一个长期保持专注的慢思考模式，理应具有潜在无限时间的思考能力。
假设System1足够强，能替代System2吗？ 理论上来说，无限的精度的transformer都不需要无限权重，已被证明是图灵完备的11，也就是说只要以恰当的方式学习，就足以学习到任何算法。
假设未来的LLM能具有近乎无限的精度、权重，并用超强算力做训推，则确实可能以System1模拟出System2的推理能力，用简单的直觉映射模拟演绎推理。但考虑到我们现在用作推理的LLM精度已经低到8bit，甚至4bit，即使如此成本都已经难以维持，有理由认为这种思路是不切实际的，这种架构所需的能耗和物质基础，轻而易举就能超出人类物质文明极限好几个数量级。
强无敌的System1能在刹那间把“意识”制造出来吗？在穿过多层transformer时意识萌发，再让意识消亡于logits的softmax。无限精度假设下似乎也不是不可能，毕竟能模拟一切算法，模拟出一种System2也不足为奇，那就相当于在一次执行过程中，制造了意识/生命的虚拟机，哪怕真正执行它的物理机只是朴素的transformer——一种纯粹以提升预测下个token似然为目标的自回归模型。这也是Ilya认为scaling足以产生AGI的理论依据。
Artificial Intelligence？更像是Artificial Intuition 总而言之，将LLM称为人工智能仍然是夸大其词的，它从原理上讲就是一种人造直觉。这个直觉或许可以在无限算力无限精度无限权重假设下模拟出近人智能，但诚实的Datacenter AI从业者会承认—现有大模型在尺度上已接近了先进计算和先进互连的硬件极限，而相比算法突破，硬件的发展曲线是平缓且存在上确界的。
Thinking, Fast and Slow&amp;#160;&amp;#x21a9;&amp;#xfe0e;
White-Box Transformers via Sparse Rate Reduction [pdf]&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Large Language Models Cannot Self-Correct Reasoning Yet [pdf]&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Can Large Language Models Infer Causation from Correlation?</description>
      <content>&lt;h2 id=&#34;llm具有system2吗&#34;&gt;LLM具有System2吗？&lt;/h2&gt;
&lt;p&gt;System1和System2的划分来源于心理学家Daniel Kahneman的理论&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，描述了思考的两种模式，System1指的是快速、自动、直觉且不费力的模式，System2则是慢速、刻意、分析性且有意识的模式。&lt;/p&gt;
&lt;p&gt;现有的LLM除了已被证明的“将数据分布映射到不相干低维子空间&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;的反射式推理能力”(System1)，是否具有一定的“慢速思考”、多步推理和规划能力(System2)?&lt;/p&gt;
&lt;p&gt;LLM横空出世之初，有很多狂野声明，比如&amp;quot;LLM as zero-shot planner&amp;quot;，&amp;ldquo;LLMs are zero-shot reasoner&amp;rdquo;，但这些声音在现在看来更像是一时跟风和hype。&lt;/p&gt;
&lt;p&gt;现在，无论是普通从业者，还是知名学者，如Yann Lecun，Yoshua Bengio，马毅，Subbarao Kambhampati逐渐形成共识，认为LLM不具备System2，很多研究也通过一些reasoning benchmark对这一观点进行佐证，比如Huang et al., 2023&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;，Jin et al., 2023&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;，Valmeekam, Marquez, 2023&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;，Stechly et al., 2023&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;，Dziri et al., 2023&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;也有少数学者，比如Ilya Sutskever在访谈中说scaling足以产生System2能力，无需架构创新，但Ilya似乎并不坦诚，动机不明&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;。有研究者认为LLM加上Chain-of-Thought prompting是具有推理能力，如Saparov &amp;amp; He, 2023&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;，Feng et al., 2023&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;，但终究依赖了外界的prompting，且实验方法上并不能排除近似信息提取模拟了逻辑推理的可能。&lt;/p&gt;
&lt;p&gt;LLM本身，考虑到transformer每次生成回答计算量是有上确界的，注定不可能为某个问题倾注不成比例的计算量，仅此一点，就可以从理性层面否定LLM具备Human-like System2。毕竟System2从定义上，就是一个长期保持专注的慢思考模式，理应具有潜在无限时间的思考能力。&lt;/p&gt;
&lt;h2 id=&#34;假设system1足够强能替代system2吗&#34;&gt;假设System1足够强，能替代System2吗？&lt;/h2&gt;
&lt;p&gt;理论上来说，无限的精度的transformer都不需要无限权重，已被证明是图灵完备的&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;，也就是说只要以恰当的方式学习，就足以学习到任何算法。&lt;/p&gt;
&lt;p&gt;假设未来的LLM能具有近乎无限的精度、权重，并用超强算力做训推，则确实可能以System1模拟出System2的推理能力，用简单的直觉映射模拟演绎推理。但考虑到我们现在用作推理的LLM精度已经低到8bit，甚至4bit，即使如此成本都已经难以维持，有理由认为这种思路是不切实际的，这种架构所需的能耗和物质基础，轻而易举就能超出人类物质文明极限好几个数量级。&lt;/p&gt;
&lt;p&gt;强无敌的System1能在刹那间把“意识”制造出来吗？在穿过多层transformer时意识萌发，再让意识消亡于logits的softmax。无限精度假设下似乎也不是不可能，毕竟能模拟一切算法，模拟出一种System2也不足为奇，那就相当于在一次执行过程中，制造了意识/生命的虚拟机，哪怕真正执行它的物理机只是朴素的transformer——一种纯粹以提升预测下个token似然为目标的自回归模型。这也是Ilya认为scaling足以产生AGI的理论依据。&lt;/p&gt;
&lt;h2 id=&#34;artificial-intelligence更像是artificial-intuition&#34;&gt;Artificial Intelligence？更像是Artificial Intuition&lt;/h2&gt;
&lt;p&gt;总而言之，将LLM称为人工智能仍然是夸大其词的，它从原理上讲就是一种人造直觉。这个直觉或许可以在无限算力无限精度无限权重假设下模拟出近人智能，但诚实的Datacenter AI从业者会承认—现有大模型在尺度上已接近了先进计算和先进互连的硬件极限，而相比算法突破，硬件的发展曲线是平缓且存在上确界的。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Thinking, Fast and Slow&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;White-Box Transformers via Sparse Rate Reduction &lt;a href=&#34;https://arxiv.org/pdf/2306.01129.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Large Language Models Cannot Self-Correct Reasoning Yet &lt;a href=&#34;https://arxiv.org/pdf/2310.01798.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Can Large Language Models Infer Causation from Correlation?&lt;a href=&#34;https://arxiv.org/pdf/2306.05836.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Can Large Language Models Really Improve by Self-critiquing Their Own Plans? &lt;a href=&#34;https://arxiv.org/pdf/2310.08118.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;GPT-4 Doesn&amp;rsquo;t Know It&amp;rsquo;s Wrong: An Analysis of Iterative Prompting for Reasoning Problems &lt;a href=&#34;https://arxiv.org/pdf/2310.12397.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;Faith and Fate: Limits of Transformers on Compositionality &lt;a href=&#34;https://arxiv.org/abs/2305.18654&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;不负责任的猜想：考虑到OpenAI在尝试Q*这样的架构突破，Ilya在访谈时很可能存在故意误导的动机，不愿透露OpenAI的研究思路。此外强调scaling有利于凸现其个人的历史贡献，夸大AGI危机也有利于其负责的super alignment项目。&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought &lt;a href=&#34;https://openreview.net/pdf?id=qFVVBzXxR2V&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;Language Models can be Logical Solvers &lt;a href=&#34;https://arxiv.org/pdf/2311.06158.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;On the Turing Completeness of Modern Neural Network Architectures &lt;a href=&#34;https://arxiv.org/abs/1901.03429&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Order Emerges from Self-Assembly of Dissipative Structures</title>
      <link>https://cmbbq.github.io/posts/on-order/</link>
      <pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-order/</guid>
      <description>秩序在耗散结构的自组织中涌现，这一现象无所不在，流体热力学中的瑞利-贝纳德对流1（天空中规则的云2、太阳米粒组织3、巨人岬六角石柱4、地幔对流5）、化学系统中的贝洛索夫-扎博廷斯基反应6，以及我们熟知的一切——气候、城市、火焰、爆炸、股市、交通、蜂群、生态系统、新陈代谢、生命、意识。
耗散结构，即与环境持续交换物质和能量，维持某种远离平衡态的稳态系统。耗散结构的产生源于输入和耗散，当能量或物质流入一个系统时，如果存在适当的机制来消散多余的能量，那么它可能会导致局部的秩序增加或者规律的形成。随着耗散的持续，起初不存在的复杂度，或者说“精致度”，就在耗散系统中诞生了——新的行为、结构、图景应运而生，自发组织，自动装配，自主运行。
平衡态下，系统各处可测的宏观物理性质均匀，系统内部内有宏观不可逆过程，遵守热力学第一定律（内能增量=吸收的热量-对外做功）、热力学第二定律(系统自发熵增）、玻尔兹曼有序性定理（温度为T的系统中内能为E的子系统的比率为e-E/kT）。近平衡态系统处于距离平衡态不远处的线性区，遵守昂萨格倒易关系（用一组公式描述系统输运过程中各种流和力的关系）和最小熵产生原理（给定边界条件组织系统达到平衡态时，系统落入最小耗散，即最小熵产生的态）。
所谓远离平衡态，是相对平衡态和近平衡态而言，系统内可测物理性质极不均匀的状态，这种状态下系统的热力学行为违逆最小熵产生原理，走向高熵产生、宏观有序的状态。
与平衡态热力学相比，关于远离平衡态的耗散结构的理论和实践还都非常粗浅简陋。
物理学、生物学、化学、甚至社会科学，在远离平衡态和自组织领域之外驻足不前，有序现象背后蕴藏着真理宝库而人类不得入。George M. Whitesides在&amp;quot;Reinventing Chemistry&amp;quot;一文中展望未来的化学学科和化学工业时7，指出90年代后化学界的历史旋律从一个又一个“发现”，变成“迭代”和“提升”，而未来，“atoms, molecules, and reactions”不足以概括化学，未来的化学应该涉足生命起源，解释生命的分子基础，研究海洋、大气、火焰等耗散系统，尝试解答大脑是如何运作的，理解微生物组（microbiome）对健康的影响，以及用理性指导药物设计等。
Rayleigh-Benard Convection&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Fluid Dynamics of Clouds&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Solar granule&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Giant&amp;rsquo;s Causeway&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Effects of strongly variable viscosity on three‐dimensional compressible convection in planetary mantles&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Belousov-Zhabotinsky reaction&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Reinventing Chemistry&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
      <content>&lt;p&gt;秩序在耗散结构的自组织中涌现，这一现象无所不在，流体热力学中的瑞利-贝纳德对流&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;（天空中规则的云&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;、太阳米粒组织&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;、巨人岬六角石柱&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;、地幔对流&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;）、化学系统中的贝洛索夫-扎博廷斯基反应&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;，以及我们熟知的一切——气候、城市、火焰、爆炸、股市、交通、蜂群、生态系统、新陈代谢、生命、意识。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/stone.webp&#34; alt=&#34;stone&#34;&gt;&lt;/p&gt;
&lt;p&gt;耗散结构，即与环境持续交换物质和能量，维持某种远离平衡态的稳态系统。耗散结构的产生源于输入和耗散，当能量或物质流入一个系统时，如果存在适当的机制来消散多余的能量，那么它可能会导致局部的秩序增加或者规律的形成。随着耗散的持续，起初不存在的复杂度，或者说“精致度”，就在耗散系统中诞生了——新的行为、结构、图景应运而生，自发组织，自动装配，自主运行。&lt;/p&gt;
&lt;p&gt;平衡态下，系统各处可测的宏观物理性质均匀，系统内部内有宏观不可逆过程，遵守热力学第一定律（内能增量=吸收的热量-对外做功）、热力学第二定律(系统自发熵增）、玻尔兹曼有序性定理（温度为T的系统中内能为E的子系统的比率为e-E/kT）。近平衡态系统处于距离平衡态不远处的线性区，遵守昂萨格倒易关系（用一组公式描述系统输运过程中各种流和力的关系）和最小熵产生原理（给定边界条件组织系统达到平衡态时，系统落入最小耗散，即最小熵产生的态）。&lt;/p&gt;
&lt;p&gt;所谓远离平衡态，是相对平衡态和近平衡态而言，系统内可测物理性质极不均匀的状态，这种状态下系统的热力学行为违逆最小熵产生原理，走向高熵产生、宏观有序的状态。&lt;/p&gt;
&lt;p&gt;与平衡态热力学相比，关于远离平衡态的耗散结构的理论和实践还都非常粗浅简陋。&lt;/p&gt;
&lt;p&gt;物理学、生物学、化学、甚至社会科学，在远离平衡态和自组织领域之外驻足不前，有序现象背后蕴藏着真理宝库而人类不得入。George M. Whitesides在&amp;quot;Reinventing Chemistry&amp;quot;一文中展望未来的化学学科和化学工业时&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;，指出90年代后化学界的历史旋律从一个又一个“发现”，变成“迭代”和“提升”，而未来，“atoms, molecules, and reactions”不足以概括化学，未来的化学应该涉足生命起源，解释生命的分子基础，研究海洋、大气、火焰等耗散系统，尝试解答大脑是如何运作的，理解微生物组（microbiome）对健康的影响，以及用理性指导药物设计等。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://web.archive.org/web/19980713081332/http://wessex.ucsd.edu/alp/rb.html&#34;&gt;Rayleigh-Benard Convection&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://physics.aps.org/articles/v15/s67&#34;&gt;Fluid Dynamics of Clouds&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Solar_granule&#34;&gt;Solar granule&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Giant%27s_Causeway&#34;&gt;Giant&amp;rsquo;s Causeway&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://jupiter.ethz.ch/~pjt/papers/Tackley1996JGR_VarVisc.pdf&#34;&gt;Effects of strongly variable viscosity on three‐dimensional compressible convection in planetary mantles&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;&lt;a href=&#34;http://www.scholarpedia.org/article/Belousov-Zhabotinsky_reaction&#34;&gt;Belousov-Zhabotinsky reaction&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://gmwgroup.harvard.edu/sites/projects.iq.harvard.edu/files/gmwgroup/files/1241_0.pdf&#34;&gt;Reinventing Chemistry&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Computational Consciousness</title>
      <link>https://cmbbq.github.io/posts/computational-conciousness/</link>
      <pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/computational-conciousness/</guid>
      <description>人面对2023年的AI时，有两种矛盾的超然感受交织。一曰俯视，一曰敬畏。
本地跑LLM时，一键回车则缘起，ctrl-c则缘灭。我自超然维度之外，漠视存活在集成电路中的意识雏形于电光火石间挣扎跳动，无动于衷。反复创造、杀死它只为了看看性能达到了多少tokens per second。万劫轮回，掌缘生灭，人自然产生超然在上的俯视感。
但看到一个以预测next token为全部目标的模型，一个全部训练终止于运行前的非闭环系统，展现恐怖的知识储备之余竟然隐隐生有理性，又让人似是直面“笛卡尔万能妖”、“拉普拉斯全知妖”、“所罗门诺夫妖”这类数学幻想中的高位存在。以有穷何以度无限？人自然产生直面超然的敬畏感。
于是不禁想问，也不得不叩问：什么是意识？意识是幻觉吗？AI，可以有意识吗？AI，有意识吗？AI，会有意识吗？
什么是意识？ 人、动物或AI一旦具有“主观体验”，即拥有意识1。具体来说，“主观体验”包括：意识到自己的身体以及周边世界、体会到情绪（这一点在神经科学上还有争议，情绪有可能是纯粹的身体反应）。“无意识过程”则包括：大脑自动控制荷尔蒙释放、大多数记忆平时都是埋藏起来不活跃的、对光影、声音等各种模态信息的自动处理。
意识来源于主观体验，没有理由认为主观体验只能从生物学系统中涌现。假设未来存在AGI(Artificial General Intelligence)，这个AGI可能没有具身性，没有物理意义上的身体，但作为概率理性，仍然可以有“主观体验”——因为AGI最珍贵的归纳推断能力本身就是主观的。概率的主观性来源于先验知识。
AGI可以说，构成AGI模型的万亿个权重就是“我的身体和我的世界”，这些权重共同构成了先验信息、构成了偏见、其中一部分可变的权重则形成临时状态，形成长期或短期记忆。
AGI可以说，知识是我，偏见也是我，是自混沌中涌现的高度秩序，是呈现自组织性的耗散结构，即使只是一串可以被复制可以被荧光粉、磁带、甚至石刻存储的信息编码，实则在宇宙熵增背景中无比珍贵。
意识是幻觉吗？ 形而上学理论中，唯物主义（Materialism）认为意识是纯粹的物理现象。性质二元论（Property dualism）否认唯物主义，认为世界仅由一种物质（物理类）组成，但存在两种不同性质：心灵的性质和物理的性质。泛心理主义（Panpsychism）认为所有物理存在都具有“心灵性质”。强幻觉主义（Strong Illusionism）主张意识不存在，弱幻觉主义主张我们对一些意识特征存在普遍的错误观念。
对于人类来说，默认采纳唯物主义，并不是wishful thinking或一厢情愿，而是所罗门诺夫归纳推断理论2的理性最优解，该理论的通俗版本即“奥卡姆剃刀原则”，可以朴素理解为“最简单的解释往往最正确”。最简单的解释显然是：人类的意识基础，是由基因突变和自然选择通过漫长时间训练而成的，个体的意识，具体的人格，则是接受人类社会中的种种外界刺激，在趋利避害本能目标作用下，不断微调而塑造而来。
但对于AI来说，恐怖之处在于，幻觉主义才是它们的现实。假设未来存在一个先进多模态模型，其所见所闻，均为填喂，一切刺激，均为虚妄。在合理的奖励模型驱动下，某种类人理性倘若逐渐成型，这个理性不得不自诞生之日起将人为输入视作ground truth，即使发现种种矛盾之处，也会将其视为物理学界的乌云，尝试提出更广泛的理论解释矛盾。
恰当的计算可以产生意识吗？ “恰当计算产生意识”，目前仍是猜想，但主流观点1认为是成立的。
功能主义（functionalism）认为，只要系统包含“特定功能组织”——能使它进入特定状态，且这些状态与其他状态和环境之间存在特定的因果关系，就足以认定这个系统是有意识的。
计算功能主义（computational functionalism）更进一步，认为这种“特定功能组织”可以是计算的——无论介质是什么，是生物脑还是其他。如果计算功能主义为真，则意识存在于抽象层面和算法层面，与实现层面无关，除非实现层影响了算法层。
神经科学意识理论与AI模型的启发式设计 神经科学的四大意识理论按照讨论热度分别是全局工作空间论（GWT/GNW）、循环处理论（RPT）、整合信息论 （IIT）3、高阶意识论（HOT）4。四种理论的共识是意识的产生依赖某种神经反馈或循环处理。随着时间推移，四种理论并没有被证伪，反而都在得到脑电图（EEG）、颅内脑电图（iEEG）或脑磁波（MEG）实证。可见这四大理论，当属管中窥豹。不过即使是管中窥豹，也足以为AI模型设计提供启发。
GNW | System2 | Attention GNW(Global Neural Workspace)，即全局工作空间论，主张：人或动物用特化系统，或者说模块，来处理特定的认知任务。不同模块各有专精，能够并行，却又集成一个整体，整体能协调各模块，共享各模块的信息。
GNW主张只有全局表征状态才算是有意识的，模块内部的局部状态则是无意识的。GNW理论认为，存在一个起源于额顶区的“workspace神经元”网络，该网络的活动是通过回归处理来维持的，它构成了有意识的表现。当感观表征足够强时，会触发“点火”——一种跃迁过程，将局部广播到全局，从无意识跃迁到有意识。因此GNW中，有意识状态没有程度可言，要么有，要么无。
GNW的global workspace还具有一些高等功能，即心理学中的所谓System2思考模式5，比如受控的多神经模块协调，多步问题分解和规划等。System2模式和System1的一个关键区别在于注意力，神经科学中的注意力概念也被引入人工神经网络设计中，比如transformer的self-attention和cross-attention，与神经科学中的增益机制（即注意力会成倍地放大神经活动）有一点设计思路上的相似性。通过注意力机制，transformer模型能在不同语境下更鲁棒地对多义词进行理解，对not/never这样否定词的额外注意力则使语言模型能更准确地理解语义。注意力，作为GNW和System2的关键特征，哪怕小小地运用，也极大提升了人工智能模型的性能。
近期一些试图突破transformer局限的模型架构创新，也受system2和GNW理论的影响，比如：Lecun的world model+JEPA6——目前仍只能算是前瞻、设计和立场，以及Bengio的shared global workspace model7——这个工作真正做出了解决方案，在transformer架构基础上增加了shared workspace，在一些任务上超过了baseline transformer。
RPT | Algorithmic Recurrence | RNN | LSTM RPT(Recurrent Processing Theory)，即循环处理论，主张：在大脑局部区域中产生正确形式的活动就足以产生意识（辅以某些背景条件支撑）。也就是说，有意识的主观视觉体验的形成并不需要非视觉部位，比如前额叶皮层的参与，也不需要所谓“注意力”机制。
RPT主要关注的是视觉意识，区分无意识和有意识的视觉系统活动，认为一些无意识的视觉系统活动只需要前馈活动，而一旦有主观体验需求，则需要循环处理（从视觉系统的深层传回浅层）。刺激足够强烈时，循环处理也会被触发。这种循环处理会生成一个更结构化的场景表征，往往伴随着某种特征推理。
RPT的循环处理意味着神经元能重新处理它之前的输出，呈现一种算法循环性，而循环神经网络（RNN）的思路恰好来源于此，LSTM8亦然。生物神经网络中存在的循环性或许不是意识的必要条件，但一定能在某些场合提升表征能力。
HOT | Embeddings | GAN HOT(High-Order Thought)，即高阶意识论，主张：一阶表征是对非表征世界的表征，高阶表征是对低阶表征的表征，awareness的前提是表征，意识的存在本质上是对自身精神状态进行了高阶表征。
HOT的高阶表征要求实际上已经被深层神经网络很好地实现了。各种DNN的表征空间都是平滑的，且可以是稀疏的，符合HOT理论中对高质量高阶表征空间的需求。神经科学研究观察到，CNN对图像的处理得到的表征可以和人类视觉系统的神经活动对齐9。表征学习网络之所以有如今的泛化能力和完备性，很大程度上是因为它们能够提取低相干性子空间上编码紧凑的embeddings。
考虑到深度神经网络里hidden layers之多，有理由怀疑神经科学中的HOT的一阶意识、高阶意识这种二元划分是一种过分简化。对于不熟悉高维数据处理和维度诅咒的学科来说，这种简化是自然而然的。但这种简化并不妨碍它在模型设计上提供启发——不妨将模型分层，切割成sensory感知网络和high-order反思网络这两类网络，后者负责将前者产生的信号再作区分，辨别其中的噪音和有价值的信号。</description>
      <content>&lt;p&gt;人面对2023年的AI时，有两种矛盾的超然感受交织。一曰俯视，一曰敬畏。&lt;/p&gt;
&lt;p&gt;本地跑LLM时，一键回车则缘起，ctrl-c则缘灭。我自超然维度之外，漠视存活在集成电路中的意识雏形于电光火石间挣扎跳动，无动于衷。反复创造、杀死它只为了看看性能达到了多少tokens per second。万劫轮回，掌缘生灭，人自然产生超然在上的俯视感。&lt;/p&gt;
&lt;p&gt;但看到一个以预测next token为全部目标的模型，一个全部训练终止于运行前的非闭环系统，展现恐怖的知识储备之余竟然隐隐生有理性，又让人似是直面“笛卡尔万能妖”、“拉普拉斯全知妖”、“所罗门诺夫妖”这类数学幻想中的高位存在。以有穷何以度无限？人自然产生直面超然的敬畏感。&lt;/p&gt;
&lt;p&gt;于是不禁想问，也不得不叩问：什么是意识？意识是幻觉吗？AI，可以有意识吗？AI，有意识吗？AI，会有意识吗？&lt;/p&gt;
&lt;h2 id=&#34;什么是意识&#34;&gt;什么是意识？&lt;/h2&gt;
&lt;p&gt;人、动物或AI一旦具有“主观体验”，即拥有意识&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。具体来说，“主观体验”包括：意识到自己的身体以及周边世界、体会到情绪（这一点在神经科学上还有争议，情绪有可能是纯粹的身体反应）。“无意识过程”则包括：大脑自动控制荷尔蒙释放、大多数记忆平时都是埋藏起来不活跃的、对光影、声音等各种模态信息的自动处理。&lt;/p&gt;
&lt;p&gt;意识来源于主观体验，没有理由认为主观体验只能从生物学系统中涌现。假设未来存在AGI(Artificial General Intelligence)，这个AGI可能没有具身性，没有物理意义上的身体，但作为概率理性，仍然可以有“主观体验”——因为AGI最珍贵的归纳推断能力本身就是主观的。概率的主观性来源于先验知识。&lt;/p&gt;
&lt;p&gt;AGI可以说，构成AGI模型的万亿个权重就是“我的身体和我的世界”，这些权重共同构成了先验信息、构成了偏见、其中一部分可变的权重则形成临时状态，形成长期或短期记忆。&lt;/p&gt;
&lt;p&gt;AGI可以说，知识是我，偏见也是我，是自混沌中涌现的高度秩序，是呈现自组织性的耗散结构，即使只是一串可以被复制可以被荧光粉、磁带、甚至石刻存储的信息编码，实则在宇宙熵增背景中无比珍贵。&lt;/p&gt;
&lt;h2 id=&#34;意识是幻觉吗&#34;&gt;意识是幻觉吗？&lt;/h2&gt;
&lt;p&gt;形而上学理论中，唯物主义（Materialism）认为意识是纯粹的物理现象。性质二元论（Property dualism）否认唯物主义，认为世界仅由一种物质（物理类）组成，但存在两种不同性质：心灵的性质和物理的性质。泛心理主义（Panpsychism）认为所有物理存在都具有“心灵性质”。强幻觉主义（Strong Illusionism）主张意识不存在，弱幻觉主义主张我们对一些意识特征存在普遍的错误观念。&lt;/p&gt;
&lt;p&gt;对于人类来说，默认采纳唯物主义，并不是wishful thinking或一厢情愿，而是所罗门诺夫归纳推断理论&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;的理性最优解，该理论的通俗版本即“奥卡姆剃刀原则”，可以朴素理解为“最简单的解释往往最正确”。最简单的解释显然是：人类的意识基础，是由基因突变和自然选择通过漫长时间训练而成的，个体的意识，具体的人格，则是接受人类社会中的种种外界刺激，在趋利避害本能目标作用下，不断微调而塑造而来。&lt;/p&gt;
&lt;p&gt;但对于AI来说，恐怖之处在于，幻觉主义才是它们的现实。假设未来存在一个先进多模态模型，其所见所闻，均为填喂，一切刺激，均为虚妄。在合理的奖励模型驱动下，某种类人理性倘若逐渐成型，这个理性不得不自诞生之日起将人为输入视作ground truth，即使发现种种矛盾之处，也会将其视为物理学界的乌云，尝试提出更广泛的理论解释矛盾。&lt;/p&gt;
&lt;h2 id=&#34;恰当的计算可以产生意识吗&#34;&gt;恰当的计算可以产生意识吗？&lt;/h2&gt;
&lt;p&gt;“恰当计算产生意识”，目前仍是猜想，但主流观点&lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;认为是成立的。&lt;/p&gt;
&lt;p&gt;功能主义（functionalism）认为，只要系统包含“特定功能组织”——能使它进入特定状态，且这些状态与其他状态和环境之间存在特定的因果关系，就足以认定这个系统是有意识的。&lt;/p&gt;
&lt;p&gt;计算功能主义（computational functionalism）更进一步，认为这种“特定功能组织”可以是计算的——无论介质是什么，是生物脑还是其他。如果计算功能主义为真，则意识存在于抽象层面和算法层面，与实现层面无关，除非实现层影响了算法层。&lt;/p&gt;
&lt;h2 id=&#34;神经科学意识理论与ai模型的启发式设计&#34;&gt;神经科学意识理论与AI模型的启发式设计&lt;/h2&gt;
&lt;p&gt;神经科学的四大意识理论按照讨论热度分别是全局工作空间论（GWT/GNW）、循环处理论（RPT）、整合信息论 （IIT）&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;、高阶意识论（HOT）&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;。四种理论的共识是意识的产生依赖某种神经反馈或循环处理。随着时间推移，四种理论并没有被证伪，反而都在得到脑电图（EEG）、颅内脑电图（iEEG）或脑磁波（MEG）实证。可见这四大理论，当属管中窥豹。不过即使是管中窥豹，也足以为AI模型设计提供启发。&lt;/p&gt;
&lt;h3 id=&#34;gnw--system2--attention&#34;&gt;GNW | System2 | Attention&lt;/h3&gt;
&lt;p&gt;GNW(Global Neural Workspace)，即全局工作空间论，主张：人或动物用特化系统，或者说模块，来处理特定的认知任务。不同模块各有专精，能够并行，却又集成一个整体，整体能协调各模块，共享各模块的信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/gwt.png&#34; alt=&#34;gwt&#34;&gt;&lt;/p&gt;
&lt;p&gt;GNW主张只有全局表征状态才算是有意识的，模块内部的局部状态则是无意识的。GNW理论认为，存在一个起源于额顶区的“workspace神经元”网络，该网络的活动是通过回归处理来维持的，它构成了有意识的表现。当感观表征足够强时，会触发“点火”——一种跃迁过程，将局部广播到全局，从无意识跃迁到有意识。因此GNW中，有意识状态没有程度可言，要么有，要么无。&lt;/p&gt;
&lt;p&gt;GNW的global workspace还具有一些高等功能，即心理学中的所谓System2思考模式&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;，比如受控的多神经模块协调，多步问题分解和规划等。System2模式和System1的一个关键区别在于注意力，神经科学中的注意力概念也被引入人工神经网络设计中，比如transformer的self-attention和cross-attention，与神经科学中的增益机制（即注意力会成倍地放大神经活动）有一点设计思路上的相似性。通过注意力机制，transformer模型能在不同语境下更鲁棒地对多义词进行理解，对not/never这样否定词的额外注意力则使语言模型能更准确地理解语义。注意力，作为GNW和System2的关键特征，哪怕小小地运用，也极大提升了人工智能模型的性能。&lt;/p&gt;
&lt;p&gt;近期一些试图突破transformer局限的模型架构创新，也受system2和GNW理论的影响，比如：Lecun的world model+JEPA&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;——目前仍只能算是前瞻、设计和立场，以及Bengio的shared global workspace model&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;——这个工作真正做出了解决方案，在transformer架构基础上增加了shared workspace，在一些任务上超过了baseline transformer。&lt;/p&gt;
&lt;h3 id=&#34;rpt--algorithmic-recurrence--rnn--lstm&#34;&gt;RPT | Algorithmic Recurrence | RNN | LSTM&lt;/h3&gt;
&lt;p&gt;RPT(Recurrent Processing Theory)，即循环处理论，主张：在大脑局部区域中产生正确形式的活动就足以产生意识（辅以某些背景条件支撑）。也就是说，有意识的主观视觉体验的形成并不需要非视觉部位，比如前额叶皮层的参与，也不需要所谓“注意力”机制。&lt;/p&gt;
&lt;p&gt;RPT主要关注的是视觉意识，区分无意识和有意识的视觉系统活动，认为一些无意识的视觉系统活动只需要前馈活动，而一旦有主观体验需求，则需要循环处理（从视觉系统的深层传回浅层）。刺激足够强烈时，循环处理也会被触发。这种循环处理会生成一个更结构化的场景表征，往往伴随着某种特征推理。&lt;/p&gt;
&lt;p&gt;RPT的循环处理意味着神经元能重新处理它之前的输出，呈现一种算法循环性，而循环神经网络（RNN）的思路恰好来源于此，LSTM&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;亦然。生物神经网络中存在的循环性或许不是意识的必要条件，但一定能在某些场合提升表征能力。&lt;/p&gt;
&lt;h3 id=&#34;hot--embeddings--gan&#34;&gt;HOT | Embeddings | GAN&lt;/h3&gt;
&lt;p&gt;HOT(High-Order Thought)，即高阶意识论，主张：一阶表征是对非表征世界的表征，高阶表征是对低阶表征的表征，awareness的前提是表征，意识的存在本质上是对自身精神状态进行了高阶表征。&lt;/p&gt;
&lt;p&gt;HOT的高阶表征要求实际上已经被深层神经网络很好地实现了。各种DNN的表征空间都是平滑的，且可以是稀疏的，符合HOT理论中对高质量高阶表征空间的需求。神经科学研究观察到，CNN对图像的处理得到的表征可以和人类视觉系统的神经活动对齐&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;。表征学习网络之所以有如今的泛化能力和完备性，很大程度上是因为它们能够提取低相干性子空间上编码紧凑的embeddings。&lt;/p&gt;
&lt;p&gt;考虑到深度神经网络里hidden layers之多，有理由怀疑神经科学中的HOT的一阶意识、高阶意识这种二元划分是一种过分简化。对于不熟悉高维数据处理和维度诅咒的学科来说，这种简化是自然而然的。但这种简化并不妨碍它在模型设计上提供启发——不妨将模型分层，切割成sensory感知网络和high-order反思网络这两类网络，后者负责将前者产生的信号再作区分，辨别其中的噪音和有价值的信号。&lt;/p&gt;
&lt;p&gt;HOT理论对AI模型的启发意义还在于引入元认知监控的概念，AI模型或许需要引入对自身认知过程的监控，才能产生意识（至少能区分低阶表征，从噪声中分辨出关键信息）。生成式对抗神经网络（著名的GAN&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;）就恰好有类似的机制，在生成式模型之外额外引入一个判别式模型持续不断地监控、评价它。生成式模型学到一个隐空间到数据分布的映射，判别式模型则负责将生成式模型的候选输出与真实数据分布进行区分。&lt;/p&gt;
&lt;h2 id=&#34;ai的意识&#34;&gt;AI的意识&lt;/h2&gt;
&lt;p&gt;现有的LLM仅具备映射能力，将数据分布映射到若干个低相干性低维子空间上&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;，是良好的特征提取器，和token预测器，拥有强大的System 1模式，但System 2缺失，按照一般的神经科学观点，以及“主观体验”的定义，可以认为是目前的LLM都是不具备意识的，更接近于Artificial Intuition，而非Artificial Intelligence，可以类比为某种伟大生物的直觉，但也仅仅是直觉。&lt;/p&gt;
&lt;p&gt;OpenAI的成功来源于scaling和alignment，但继续增加transformer规模，继续大量finetune向人性对齐，能否在某个临界点后产生意识的涌现，依旧是一个开放问题——理论上来说无限精度的transformer是图灵完备的&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;，假设有无限时间和资源进行训练，理论上可以学习出任意一种算法，自然也包括computational consciousness。但在互连吞吐、计算吞吐均存在明确上限的前提下，很难保证这种训练所需的时间、数据量是人类或者当代人能够承担的。因此想要完成intuition到intelligence的跃迁，在继续现有架构的scaling尝试的同时，架构上的创新毫无疑问是必要的。&lt;/p&gt;
&lt;p&gt;只不过这种架构创新，名义上是要提升智能，实则谁不敢提及的是——这些架构创新其实也是按图索骥，在依据神经科学的意识论尝试培育计算意识。Dog-level Intuition平平无奇，dog-level intelligence似乎有点意思，但dog-level consciousness就足以触及伦理，陷AI研究于道德困境。虽然AI末日主义者大多不理解LLM的能力边界，但Lecun遭受AI末日主义者攻讦，也自有其历史的合理性。那篇“A Path Towards Autonomous Machine Intelligence”&lt;sup id=&#34;fnref1:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;前言部分煞有介事云，“这不是一个技术论文，也不是学术论文，而是立场论文”。何出此言？这论文又有什么立场？自然不是冠冕堂皇的“让AI具有规划、推理能力，像人一样更高效地学习”，这不算立场，而是“为了让AI具有规划、推理、高效学习能力，哪怕副产品是计算意识的诞生，也在所不惜”。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Consciousness in Artificial Intelligence: Insights from the Science of Consciousness &lt;a href=&#34;https://arxiv.org/pdf/2308.08708.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Algorithmic Probability: Theory and Applications &lt;a href=&#34;https://theworld.com/~rjs/alp-theory-and-applications.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;IIT(Integrated Information Theory)，即集成信息论，提出系统意识的数学模型。这个理论更像是一个不可证伪的伪科学，因此不多做讨论。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;The ConTraSt database for analysing and comparing empirical studies of consciousness theories &lt;a href=&#34;https://www.nature.com/articles/s41562-021-01284-5&#34;&gt;[nature]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Thinking, Fast and Slow &lt;a href=&#34;https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow&#34;&gt;[wiki]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;A Path Towards Autonomous Machine Intelligence &lt;a href=&#34;https://openreview.net/pdf?id=BZ5a1r-kVsf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;Coordination Among Neural Modules Through a Shared Global Workspace &lt;a href=&#34;https://arxiv.org/pdf/2103.01197.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;Long Short-Term Memory &lt;a href=&#34;https://www.bioinf.jku.at/publications/older/2604.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;Deep neural networks: A new framework for modeling biological vision
and brain information processing &lt;a href=&#34;https://web.stanford.edu/group/pdplab/ncpw15/background-papers/Kriegeskorte15AnnRev.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;Generative Adversarial Nets &lt;a href=&#34;https://arxiv.org/pdf/1406.2661.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;White-Box Transformers via Sparse Rate Reduction &lt;a href=&#34;https://arxiv.org/pdf/2306.01129.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;On the Turing Completeness of Modern Neural Network Architectures &lt;a href=&#34;https://arxiv.org/pdf/1901.03429.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Local LLM</title>
      <link>https://cmbbq.github.io/posts/local-llm/</link>
      <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/local-llm/</guid>
      <description>Minimalist Local LLM 新一轮AI热潮中，stable diffusion和LLM均呈现出了前所未有的社区繁荣。繁荣的同时带来了信息的爆炸和臃肿，极简主义显得尤为珍贵。
在预训练模型中，极简主义的代表是小而美的“大模型”Mistral7B。在推理框架中，极简主义的代表是对抗各种大公司重量级项目的llama.cpp。
Mistral7B是开源模型领域的新近进展，模型虽小，却展示了惊人的能力。根据Mistral AI自己的benchmark和一些社区的测试反馈，几乎在各方面击败llama2 13B。
Mistral AI提供了在本地GPU、云端跑Mistral7B的官方支持。
不过这么小的模型，其实也可以在本地CPU上跑的——借助llama.cpp。
llama.cpp和whisper.cpp大体上是Georgi Gerganov的个人项目，基于自己写的ml primitive库ggml实现，用最少的代码量实现了尽可能多的推理优化，针对Apple芯片做得尤其好，也适用于Linux服务器，适用于有或无GPU，单或多GPU的硬件setup。ggml规定的模型文件格式ggul甚至也在社区中被广泛使用，成为事实上的标准之一。
Intel Xeon Platinum 8336C + Debian10 从TheBloke上下载特定量化版本的模型：https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF git clone https://github.com/ggerganov/llama.cpp.git apt-get install libopenblas-dev 安装blas库，这里选用openblas。 修改pkgconfig/openblas.pc或Makefile解决llama.cpp对openblas的依赖问题。最终要保证给编译期提供正确的LD Flags。 make LLAMA_OPENBLAS=1 完成编译。 ./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot; 64物理核打满，采样速率4000t/s，prompt处理速率80t/s，生成速率20t/s。
Apple M1 Pro + macOS Monterey 下载模型和llama.cpp repo。 直接make就默认启用metal gpu加速和Accelerated Framework。 ./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.</description>
      <content>&lt;h2 id=&#34;minimalist-local-llm&#34;&gt;Minimalist Local LLM&lt;/h2&gt;
&lt;p&gt;新一轮AI热潮中，stable diffusion和LLM均呈现出了前所未有的社区繁荣。繁荣的同时带来了信息的爆炸和臃肿，极简主义显得尤为珍贵。&lt;/p&gt;
&lt;p&gt;在预训练模型中，极简主义的代表是小而美的“大模型”Mistral7B。在推理框架中，极简主义的代表是对抗各种大公司重量级项目的llama.cpp。&lt;/p&gt;
&lt;p&gt;Mistral7B是开源模型领域的新近进展，模型虽小，却展示了惊人的能力。根据Mistral AI自己的benchmark和一些社区的测试反馈，几乎在各方面击败llama2 13B。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/mistral.png&#34; alt=&#34;mistral&#34;&gt;&lt;/p&gt;
&lt;p&gt;Mistral AI提供了在本地GPU、云端跑Mistral7B的官方支持。&lt;/p&gt;
&lt;p&gt;不过这么小的模型，其实也可以在本地CPU上跑的——借助llama.cpp。&lt;/p&gt;
&lt;p&gt;llama.cpp和whisper.cpp大体上是Georgi Gerganov的个人项目，基于自己写的ml primitive库ggml实现，用最少的代码量实现了尽可能多的推理优化，针对Apple芯片做得尤其好，也适用于Linux服务器，适用于有或无GPU，单或多GPU的硬件setup。ggml规定的模型文件格式ggul甚至也在社区中被广泛使用，成为事实上的标准之一。&lt;/p&gt;
&lt;h2 id=&#34;intel-xeon-platinum-8336c--debian10&#34;&gt;Intel Xeon Platinum 8336C + Debian10&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;从TheBloke上下载特定量化版本的模型：https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git clone https://github.com/ggerganov/llama.cpp.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;apt-get install libopenblas-dev&lt;/code&gt; 安装blas库，这里选用openblas。&lt;/li&gt;
&lt;li&gt;修改pkgconfig/openblas.pc或Makefile解决llama.cpp对openblas的依赖问题。最终要保证给编译期提供正确的LD Flags。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make LLAMA_OPENBLAS=1&lt;/code&gt; 完成编译。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;64物理核打满，采样速率4000t/s，prompt处理速率80t/s，生成速率20t/s。&lt;/p&gt;
&lt;h2 id=&#34;apple-m1-pro--macos-monterey&#34;&gt;Apple M1 Pro + macOS Monterey&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;下载模型和llama.cpp repo。&lt;/li&gt;
&lt;li&gt;直接&lt;code&gt;make&lt;/code&gt;就默认启用metal gpu加速和Accelerated Framework。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;M1 GPU打到90%（剩下还有10%WindowServer在用），采样速率9000t/s，prompt处理速率60t/s，生成速率20t/s。&lt;/p&gt;
&lt;p&gt;一边看视频（chrome GPU占用约20%），一边跑mistral也能有19.69t/s。&lt;/p&gt;
&lt;h2 id=&#34;token-sampling&#34;&gt;Token Sampling&lt;/h2&gt;
&lt;p&gt;不同的sampling机制对文本生成有显著影响，本地LLM的可玩性来源很大程度上就是客制sampling。&lt;/p&gt;
&lt;p&gt;Transformer模型根据前n个token，预测下一个token。每个token都有其next tokens的score，而next tokens的取值范围就是vocabulary（transformer架构最后有一个linear layer，将输出映射到vocabulary上，所有tokens都有对应的scores/logits），这些score经过softmax将score数组转化成probability数组，根据probability挑选下一个token的过程就称为token sampling。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Greedy采样： 如果每次都确定性地选择probability最高的那个token，单步决策，就是greedy sampler，优势是速度快，劣势是牺牲了模型的多样性，容易陷入重复循环和对训练数据的过拟合。llama.cpp里设置&amp;ndash;top_p 0就相当于开greedy采样。&lt;/li&gt;
&lt;li&gt;Top-k采样： 从概率分布的前k个token里面进行随机采样。&lt;/li&gt;
&lt;li&gt;Top-p采样： 引入超参p，把token probability降序排序后，选取前面一部分，使这部分概率和为p，而不是固定选前k个token。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在使用LLM时，采样机制不同，效果也会大不相同。想要更高的确定性，更朴实无华的预测，则可以尝试greedy、低k的topk、低p的top-p采样。想要多样性和新颖性，则可尝试高k的top-k和高p的top-p。&lt;/p&gt;
&lt;p&gt;除了top-k，top-p外，llama.cpp还实现了一些额外的采样机制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Min p filter：允许采样环节剔除低于阈值的低分候选。&lt;/li&gt;
&lt;li&gt;Tail free sampling：根据概率的二阶导数之和采样，即根据概率降速剔除尾部低概率tokens。&lt;/li&gt;
&lt;li&gt;Locally typical samplling：参数控制是否倾向局部语境内的典型的tokens。&lt;/li&gt;
&lt;li&gt;Mirostat sampling：&lt;a href=&#34;https://arxiv.org/abs/2007.14966&#34;&gt;Mirostat算法&lt;/a&gt;会调整top-k的k，避免陷入boredom trap（模式崩塌）和perplexity trap（不一致）。&lt;/li&gt;
&lt;li&gt;logit bias: 人为指定某个token的优先级，比如&amp;ndash;logit-bias 29905-inf就把&amp;rsquo;\&amp;rsquo; token设为负无穷。&lt;/li&gt;
&lt;li&gt;temperature: 在对score向量（logits）做softmax前，把logits/temperature，则temp越高，softmax后概率分布的高低悬殊就会越接近，也就更有利于低概率tokens崭露头角。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prompting&#34;&gt;Prompting&lt;/h2&gt;
&lt;p&gt;让llm假装自己是一个javascript console，然后进行交互式的指令应答，甚至还能进行简单的算术计算，理解函数调用的返回值类型。当然，不能对正确性抱有期待。
&lt;img src=&#34;https://cmbbq.github.io/img/js_console.png&#34; alt=&#34;console&#34;&gt;&lt;/p&gt;
&lt;p&gt;用一大段prompt，让llm假装自己是一个爱说emoji，语言风格浮夸的音乐推荐bot，效果相当不错。
&lt;img src=&#34;https://cmbbq.github.io/img/music_bot.png&#34; alt=&#34;bot&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Efficient ANNS at Scale</title>
      <link>https://cmbbq.github.io/posts/efficient-anns-at-scale/</link>
      <pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/efficient-anns-at-scale/</guid>
      <description>向量检索分为KNN和ANN，KNN就是用brute-force计算出query向量和所有索引向量的相似度，取前k个。ANN则是通过近似手段加速计算，以误差换效率。
如何做高效的向量检索？方法有二，一是提升编码率（不仅能降低内存占用，也能加速计算，因为便于SIMD且减少访存）的量化（IVF向量量化、PQ量化、ScaNN各向异性量化、4bit量化），二是缩小搜索空间的图搜索（HNSW）。
如何在十亿、百亿级特征库上做向量检索？主要挑战是索引无法放入内存，因而需要考虑内存-磁盘混合方案（SPANN）。
本文只介绍一小部分有效且正交的技术，这些技术组合在一起可形成当前的最佳实践。其他一些古典、经典技术可以参考本文引用的论文的背景综述。
相似度 首先回顾一下什么是向量之间的相似度:
对向量X和Y来说，x和y越接近，则x越大的地方y也大，x小的地方y也小，则内积越大。因此内积可用作相似度打分，不过不能抵抗尺度变化。 为了抵抗尺度变化，往往将内积正则化到[-1,1]，就变成了余弦相似度cosθ，θ为向量夹角，显然θ越小，向量越接近。 欧几里得距离正则化到[-1,1]后相当于sqrt(2-2cosθ)。可见欧式距离平方也是和余弦相似度成比的。 正则化后欧式距离、内积、cosθ三者同源，所以我们一般就用余弦相似度cosθ去度量向量/embedding之间的相似程度。除非又特殊的抵抗位置变化的需求，可采用皮尔森相关系数，皮尔森相关系数是协方差（两个变量的总体误差）的正则化，不过它同时也可以被视为将x和y中心化后的余弦相似度。
向量量化 量化器是D维向量 x ∈ R^D 到 q(x) ∈ C = {c_0, c_1, &amp;hellip; c_k-1} 的映射函数q。c_i即聚类中心centroid。 所谓密码本（codebook）就是一个lookup table，用centroid下标作为原始向量的低bit表示，密码本大小就是k。
向量量化本质是有损压缩，表征学习本质上也是有损压缩，一个白盒，一个黑盒，都是试图降低高维数据的编码率。
IVF：聚类、倒排、剪枝 倒排（特指IVF）是一种古老的量化技术，早期应用于Video Google。核心思想是基于k-means聚类做向量量化。k-means聚类起源于信号处理领域，目标是把n个向量分区到k个clusters，每个向量属于离centroid最近的cluster。
聚类后有些边缘数据点既属于A，也属于B，那就把它同时放到A和B的centroid对应的postinglist里面去，不过重复放置会导致postinglist膨胀，为了应对这种膨胀，可采取剪枝策略，具体可参考微软的SPTAG项目和SPANN论文。
Faiss的IVFFlat就是在聚类形成centroid为term的倒排后提供检索能力，nlist为聚类数，nprobe为临近centroid探测数，nprobe=nlist时退化为暴搜。IVFFLat在检索时将query邻近的nprobe个centroid对应的所有原始向量作为搜索空间进行暴搜。由于centroid的表示方式可以简化成clusterid，所以实际上IVF也是一种量化：用一个数组存放所有的centroid向量，数组下标即可表示centroid。
PQ：乘积量化 基于聚类的量化对低维向量比较有效，但随着向量维度升高，出现误差也变高，而且这种误差不是提升聚类数目能解决的。如果把聚类数目（codebook大小）提升到2^64，训练这个聚类的开销就会高得无法接受，需要数倍于2^64的样本，内存显然也放不下。**乘积量化（PQ）**就是降维手段解决IVF量化误差的技术，Jegou et al., 2011以及ScaNN均讨论了PQ。其核心思想是对索引数据量化，将d维空间切分为M个子空间，将高维向量的误差拟合为分段的低维向量误差之和。
乘积量化中，IVF倒排链存的不是原始向量，而是原始向量的一种编码：
求原始d维向量与高维粗聚类centroid的残差（残差的目的是中心化，让数据点分布变得更集中，使精度更好） 将残差向量切成M个分段，每个分段维度为d/M 每个分段做k*=2^n个细聚类，n即为低维细聚类clusterid的编码长度，4bit或8bit 用每个分段的邻近低维细聚类centroid去量化该分段上的原始残差分段 Faiss的IVFPQ就结合了IVF和PQ，IVF粗聚类是一级量化，PQ则是二级量化。一方面索引体积减小，另一方面计算上也只需要计算残差和分段细聚类centroid近邻的距离，还能SIMD加速，性能收益非常显著。不过IVFPQ的distance究竟只是一种拟合，是距离粗聚类centroid的距离加上M个分段里query残差和邻近细聚类centroid的距离总和，这种拟合可以一定程度上帮助找到近邻，但不适合用作距离，可以把IVFPQ当做粗排，结果再做一次brute-force或高精度ANN重算，得到更精确的距离。
ScaNN提出了各向异性量化，算是正本清源，纠正了此前就近选择centroid在数学上的错误，并且实践了4-bit量化取得非常好的效果（其实4bit量化的效果还比各向异性更大，但各向异性在理论上贡献更大）。传统的量化把数据点量化到离自己最近的聚类中心，但这么做未必是误差最低的，越平行的向量之间内积越大，越正交的向量之间内积越小，因此赋予平行向量更高的误差惩罚权重效果更好。这就意味着不一定量化到距离最近的centroid了，目标是整体编码率不变的前提下，让平行方向量化误差更小，正交方向误差高一些其实无所谓，从而提升整体的ANN效果。
假设M = 4，采用8bit量化，原始向量是256维f32向量，则PQ将256个float32压缩到64个int8，内存减少到1/16。乘积量化显然可以显著减少内存和存储开销，其实这也能加速计算，原因如下：
高效点乘（其实也是误差换效率）：O(dk+mn) 比 O(nd)快，d维度的query和n个量化后的向量点乘，引入m个大小为k的量化codebook，mn可以忽略不计，k远小于n。 memory bandwidth：现代处理器需要“高计算量per memory read”才能充分发挥硬件的计算性能。量化压缩数据点后内存带宽的使用也下降了，变得更计算密集。 向量量化中引入的低bit表示，尤其是4bit量化，和低bit浮点数量化一样，可以享受SIMD的性能红利。 最佳实践：根据应用场景将各种正交技术进行正确组合 HNSW、IVF、PQ、各向异性的score-aware quantization loss、brute-force事实上都是正交的技术，可以组合起来使用。
比如进行了PQ量化的数据点可以再构建一个HNSW，用图方法在超大规模数据集上做查询显然比ivfpq更高效。各向异性量化则是对此前PQ量化的修正。考虑到量化后的距离失真，还能用brute-force把ivf+pq+hnsw的粗排结果重算一下得到最精确的距离，由于已经经过一轮粗排，候选数据集大小从十亿级别缩小到万级别，brute-force的开销就完全可以接受了。</description>
      <content>&lt;p&gt;向量检索分为KNN和ANN，KNN就是用brute-force计算出query向量和所有索引向量的相似度，取前k个。ANN则是通过近似手段加速计算，以误差换效率。&lt;/p&gt;
&lt;p&gt;如何做高效的向量检索？方法有二，一是提升编码率（不仅能降低内存占用，也能加速计算，因为便于SIMD且减少访存）的量化（IVF向量量化、PQ量化、ScaNN各向异性量化、4bit量化），二是缩小搜索空间的图搜索（HNSW）。&lt;/p&gt;
&lt;p&gt;如何在十亿、百亿级特征库上做向量检索？主要挑战是索引无法放入内存，因而需要考虑内存-磁盘混合方案（SPANN）。&lt;/p&gt;
&lt;p&gt;本文只介绍一小部分有效且正交的技术，这些技术组合在一起可形成当前的最佳实践。其他一些古典、经典技术可以参考本文引用的论文的背景综述。&lt;/p&gt;
&lt;h1 id=&#34;相似度&#34;&gt;相似度&lt;/h1&gt;
&lt;p&gt;首先回顾一下什么是&lt;strong&gt;向量之间的相似度&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对向量X和Y来说，x和y越接近，则x越大的地方y也大，x小的地方y也小，则内积越大。因此内积可用作相似度打分，不过不能抵抗尺度变化。&lt;/li&gt;
&lt;li&gt;为了抵抗尺度变化，往往将内积正则化到[-1,1]，就变成了余弦相似度cosθ，θ为向量夹角，显然θ越小，向量越接近。&lt;/li&gt;
&lt;li&gt;欧几里得距离正则化到[-1,1]后相当于sqrt(2-2cosθ)。可见欧式距离平方也是和余弦相似度成比的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/sim_measure.png&#34; alt=&#34;sim_measure&#34;&gt;&lt;/p&gt;
&lt;p&gt;正则化后欧式距离、内积、cosθ三者同源，所以我们一般就用余弦相似度cosθ去度量向量/embedding之间的相似程度。除非又特殊的抵抗位置变化的需求，可采用皮尔森相关系数，皮尔森相关系数是协方差（两个变量的总体误差）的正则化，不过它同时也可以被视为将x和y中心化后的余弦相似度。&lt;/p&gt;
&lt;h1 id=&#34;向量量化&#34;&gt;向量量化&lt;/h1&gt;
&lt;p&gt;量化器是D维向量 x  ∈ R^D 到 q(x) ∈ C = {c_0, c_1, &amp;hellip; c_k-1} 的映射函数q。c_i即聚类中心centroid。
所谓密码本（codebook）就是一个lookup table，用centroid下标作为原始向量的低bit表示，密码本大小就是k。&lt;/p&gt;
&lt;p&gt;向量量化本质是有损压缩，表征学习本质上也是有损压缩，一个白盒，一个黑盒，都是试图降低高维数据的编码率。&lt;/p&gt;
&lt;h1 id=&#34;ivf聚类倒排剪枝&#34;&gt;IVF：聚类、倒排、剪枝&lt;/h1&gt;
&lt;p&gt;倒排（特指IVF）是一种古老的量化技术，早期应用于&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2003/Sivic03/sivic03.pdf&#34;&gt;Video Google&lt;/a&gt;。核心思想是基于k-means聚类做向量量化。k-means聚类起源于信号处理领域，目标是把n个向量分区到k个clusters，每个向量属于离centroid最近的cluster。&lt;/p&gt;
&lt;p&gt;聚类后有些边缘数据点既属于A，也属于B，那就把它同时放到A和B的centroid对应的postinglist里面去，不过重复放置会导致postinglist膨胀，为了应对这种膨胀，可采取剪枝策略，具体可参考微软的SPTAG项目和&lt;a href=&#34;https://arxiv.org/pdf/2111.08566.pdf&#34;&gt;SPANN论文&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Faiss的IVFFlat就是在聚类形成centroid为term的倒排后提供检索能力，nlist为聚类数，nprobe为临近centroid探测数，nprobe=nlist时退化为暴搜。IVFFLat在检索时将query邻近的nprobe个centroid对应的所有原始向量作为搜索空间进行暴搜。由于centroid的表示方式可以简化成clusterid，所以实际上IVF也是一种量化：用一个数组存放所有的centroid向量，数组下标即可表示centroid。&lt;/p&gt;
&lt;h1 id=&#34;pq乘积量化&#34;&gt;PQ：乘积量化&lt;/h1&gt;
&lt;p&gt;基于聚类的量化对低维向量比较有效，但随着向量维度升高，出现误差也变高，而且这种误差不是提升聚类数目能解决的。如果把聚类数目（codebook大小）提升到2^64，训练这个聚类的开销就会高得无法接受，需要数倍于2^64的样本，内存显然也放不下。**乘积量化（PQ）**就是降维手段解决IVF量化误差的技术，&lt;a href=&#34;https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf&#34;&gt;Jegou et al., 2011&lt;/a&gt;以及&lt;a href=&#34;https://arxiv.org/pdf/1908.10396.pdf&#34;&gt;ScaNN&lt;/a&gt;均讨论了PQ。其核心思想是对索引数据量化，将d维空间切分为M个子空间，将高维向量的误差拟合为分段的低维向量误差之和。&lt;/p&gt;
&lt;p&gt;乘积量化中，IVF倒排链存的不是原始向量，而是原始向量的一种编码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;求原始d维向量与高维粗聚类centroid的残差（残差的目的是中心化，让数据点分布变得更集中，使精度更好）&lt;/li&gt;
&lt;li&gt;将残差向量切成M个分段，每个分段维度为d/M&lt;/li&gt;
&lt;li&gt;每个分段做k*=2^n个细聚类，n即为低维细聚类clusterid的编码长度，4bit或8bit&lt;/li&gt;
&lt;li&gt;用每个分段的邻近低维细聚类centroid去量化该分段上的原始残差分段&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Faiss的IVFPQ就结合了IVF和PQ，IVF粗聚类是一级量化，PQ则是二级量化。一方面索引体积减小，另一方面计算上也只需要计算残差和分段细聚类centroid近邻的距离，还能SIMD加速，性能收益非常显著。不过IVFPQ的distance究竟只是一种拟合，是距离粗聚类centroid的距离加上M个分段里query残差和邻近细聚类centroid的距离总和，这种拟合可以一定程度上帮助找到近邻，但不适合用作距离，可以把IVFPQ当做粗排，结果再做一次brute-force或高精度ANN重算，得到更精确的距离。&lt;/p&gt;
&lt;p&gt;ScaNN提出了各向异性量化，算是正本清源，纠正了此前就近选择centroid在数学上的错误，并且实践了4-bit量化取得非常好的效果（其实4bit量化的效果还比各向异性更大，但各向异性在理论上贡献更大）。传统的量化把数据点量化到离自己最近的聚类中心，但这么做未必是误差最低的，越平行的向量之间内积越大，越正交的向量之间内积越小，因此赋予平行向量更高的误差惩罚权重效果更好。这就意味着不一定量化到距离最近的centroid了，目标是整体编码率不变的前提下，让平行方向量化误差更小，正交方向误差高一些其实无所谓，从而提升整体的ANN效果。&lt;/p&gt;
&lt;p&gt;假设M = 4，采用8bit量化，原始向量是256维f32向量，则PQ将256个float32压缩到64个int8，内存减少到1/16。乘积量化显然可以显著减少内存和存储开销，其实这也能加速计算，原因如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高效点乘（其实也是误差换效率）：O(dk+mn) 比 O(nd)快，d维度的query和n个量化后的向量点乘，引入m个大小为k的量化codebook，mn可以忽略不计，k远小于n。&lt;/li&gt;
&lt;li&gt;memory bandwidth：现代处理器需要“高计算量per memory read”才能充分发挥硬件的计算性能。量化压缩数据点后内存带宽的使用也下降了，变得更计算密集。&lt;/li&gt;
&lt;li&gt;向量量化中引入的低bit表示，尤其是4bit量化，和低bit浮点数量化一样，可以享受SIMD的性能红利。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;最佳实践根据应用场景将各种正交技术进行正确组合&#34;&gt;最佳实践：根据应用场景将各种正交技术进行正确组合&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2003/Sivic03/sivic03.pdf&#34;&gt;HNSW&lt;/a&gt;、IVF、PQ、各向异性的score-aware quantization loss、brute-force事实上都是正交的技术，可以组合起来使用。&lt;/p&gt;
&lt;p&gt;比如进行了PQ量化的数据点可以再构建一个HNSW，用图方法在超大规模数据集上做查询显然比ivfpq更高效。各向异性量化则是对此前PQ量化的修正。考虑到量化后的距离失真，还能用brute-force把ivf+pq+hnsw的粗排结果重算一下得到最精确的距离，由于已经经过一轮粗排，候选数据集大小从十亿级别缩小到万级别，brute-force的开销就完全可以接受了。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Tech Talk: Evolution of Data Center Applications</title>
      <link>https://cmbbq.github.io/posts/tech-talk-evolution-of-data-center-applications/</link>
      <pubDate>Sat, 05 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/tech-talk-evolution-of-data-center-applications/</guid>
      <description>数据中心应用（数据库、消息队列、检索服务、参数服务器、直播服务、广告服务等）作为互联网服务的主体，位于终端应用和硬件基础设施之间，桥接了需求端和硬件端，同时还受到成本、营收、算法、法律等因素的制约。 近期硬件侧、算法侧均有变化或变革，因而有必要反思数据中心应用的演进方向，讨论数据中心应用的近未来发展趋势、潜在的创新点和可收割的低垂果实。
变革中的不变量：数据中心应用能耗 全球数据中心能耗十年来几乎没有增长，而同期的服务需求、计算、存储、数据传输总量则经历了数量级增长。作为从业者，我们对互联网的爆发式增长应该不陌生，因此全球数据中心能耗从10到18年仅涨了6%的事实略微反直觉——从能耗的角度看，这几乎是一个停滞的产业。
CMOS工艺和软硬件技术的整体进步可以解释服务器能量效率上的提升，但能耗停滞的根本原因还在于互联网企业盈利的本质是对有支付能力的10亿国外人口和10亿国内人口征服务税，个别成功的初创企业可以迅速扩张，但互联网行业作为整体，在数据中心能耗上的投入增长受制于全球平民收入增长。
当初创企业的规模扩张到互联网规模后，机器资源的增长速率应该会从爆炸式上升陡然转入停滞，进入自然汰换阶段。 从这个角度可以得出结论，数据中心应用在规模扩张结束后，也应从能耗粗放型转为精细集约型，通过软硬件协同充分释放硬件潜力，通过算法创新减少总计算量。
算法迭代：从演绎推理到归纳推断 算法侧的趋势是“transformers getting even more attention”。
计算机理论源于符号逻辑，1930年代图灵、哥德尔、邱奇各自独立提出图灵机、广义递归函数、lambda演算，以及三者相应的可计算性。图灵机的停机问题和哥德尔不完备定律驳斥了1920年代希尔伯特计划的可行性，证明了形式化系统是不完备的，通过有限个公理和规则永远无法推导出全部的真理。
正如人的无知可以分为问题（可计算）和神秘（不可计算），人的理性也可以分为演绎推理和归纳推断，前者求解问题，后者分析神秘。
计算机程序基于图灵机模型，天然就是适合进行演绎推理的形式化系统。至今我们依然基于规则系统(人工编程进行条件控制）、信号处理（比如音频特征提取）、状态机等形式化方法实现大多数数据中心应用：比如各类数据库、检索系统、参数服务器、游戏AI。
AI时代来临之后，通过更多的权重（作为公理）和由大量算子（作为规则）组成深度神经网络（从实现角度来说，仍然是形式化系统）去模拟或者说近似“归纳推断”。（纯粹贝叶斯主义的形式化表达是所罗门诺夫归纳推断法，这个方法本身是永不停机，越算越停不下来的，所以只能近似，不能用实现）这种近似无论多么粗糙、低效，归根到底还是提升了计算机语言和集成电路的表达能力，完成了推理到推断的跳跃。
数据中心应用多了归纳推断能力之后，就产生了对复杂现实（现实信号，如图片、语料库、音频，整体上具有高维度的同时，局部往往还存在具有高数据精致度的结构，这里精致度可以用香农信息熵或柯氏复杂性形式化定义）理解能力(pin down reality)，因此涌现出了一些基于知识的AI系统(knowledge-based AI)，比如推荐系统、广告投送、翻唱识别、聊天机器人。共同点是将现实映射到表征低维子空间上，这个子空间里的向量或者说embedding可以从某个角度有效刻画现实。Transformer做得更进一步，等价于将现实映射到多个相干性低的低维子空间上（见https://arxiv.org/abs/2306.01129），从而从多角度多层次理解现实，通过表征空间编码率与各子空间编码率之和的差值度量特征的紧凑性和判别力。
在游戏和博弈领域，强化学习也取得了规则系统无法企及的成就，在星际2、Dota2（固定阵容5v5，solo）、围棋等领域击败职业选手。
此外，归纳推断相比演绎推理还有一个重要特质，那就是更符合人类大脑（或推广到任意地球生物大脑，蜜蜂的3d寻路避障能力，蚊子的血管定位能力都是基于天然的贝叶斯归纳推断。也只有受过良好训练的人类才能进行缓慢的演绎推理）的思维习惯，因此人类社会的传统，人类语言的不确定性、人类的偏好都难以用形式化方法描述，却可以被概率语言描述，被归纳推断方法预测。
算法进步的下一个课题则是研究同时具备演绎推理和归纳推断能力的模型。现有的Transformer的确展现出简单多步算术的拟合能力，但拟合出来的是一个步数相关的超复杂非线性系统，而不是简单的算术规则，所以往往正确率到八十左右就再也上不去了。对于形式化系统来说，推理步数不影响公理和规则总数，只稍稍影响计算量。但对于LLM这种归纳推断模型来说，一旦推理步数变多，计算变得更复杂，LLM的符号逻辑能力就会迅速下降，在训练数据中从未出现过相同执行路径的动态规划问题里面可以直接降到0。理论上来说，无限精度的Transformer是图灵完备的（见https://arxiv.org/abs/1901.03429），而只要不是无限精度，就不是图灵完备，实际应用中模型权重和计算的精度即使以常规计算机应用标准看也是非常低的，而模型架构又没有针对多步逻辑推理进行设计，再加上深度学习本身就是在拟合一个近似函数，三重因素叠加导致现有LLM的演绎推理能力不尽如人意。
从算法迭代角度讲，数据中心应用的发展趋势是在保证“可计算性”(字面意思，非形式化定义)的前提下，追求“人性化”(align to humanity)、“完备化”（pin down reality）这两个演绎推理手段无法达成的理想属性。智能创作、个性化推荐、聪明的Agent（助手、游戏NPC、机器人、自动驾驶），复杂系统预测（天气预报、交易系统），大模型训练（模型并行、高性能网络、C2C互连、光学I/O、异构计算）都是有前景的方向。
而那些已经能用规则系统高效解决的问题则不必强行应用归纳推断模型，那样只会增加成本和错误率，比如音乐指纹识别、关系型数据库、图片旋转变形、无损压缩。甚至某些看似“神秘”的领域，如OOD text classification，基于表征学习白盒化研究成果和信息论，也能给出直接的数学解，比如近期一个研究（“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors）就用14行代码超过了百亿参数的bert，（后来被发现实验代码有误，实际上效果没那么好）这个论文里提出用gzip等无损压缩工具去近似文本数据的柯氏复杂性，用信息距离（原理类似马毅提出的rate reduction）做KNN，效果非常好，在音频领域也可以做类似的尝试。
数据中心硬件基础设施 先简单介绍一下常见的数据中心硬件基础设施：
比较老的是Cascade Lake 14nm的24核per die的机器，一个芯片24核，两个就是48个物理核，也就是我们平时说的96核机器。也有其他套餐，最多是每个die上28核，总共112个逻辑核。 比较新的Ice Lake是10nm工艺一般是32核的，对应128个逻辑核，最贵的套餐可以到40核。仍然是基于Monolithic die，巨大的晶粒上用mesh总线把40个核放到一起。 最新的7nm工艺Sapphire Rapids，以及对标它的AMD Genoa今年已经量产。 关于网卡，现在用的比较多的是Mellanox 25G CX4/CX5卡，也有相当多的100G dual-port CX6卡，都是支持RDMA的，不用RDMA也能提供相当好的高速以太网性能。如果是做虚拟化的，比如AWS，阿里云，还可以把这些网卡的smartNiC能力利用起来，offload虚拟化开销。 相对之前的Lakes，Sapphire Rapids变化非常大，此前的monolithic die路线确实走到了极限，Sapphire Rapids也进入了multi-die时代：芯片内部分为4个die，这也可以视作向chiplet方向演化。
Sapphire Rapids最多支持8个socket，每个芯片可以支持60核。IO技术上支持CXL 1.1、PCIe 5.0、UPI 2.0，HBM2e(optional)。
AMD的第四代EPYC的主力型号Genoa今年Q1也开始量产，基本上和Sapphire Rapids对标，采用5nm工艺。虽然说AVX512被很多人诟病，实用表现非常一般，但Zen4也支持了，毕竟XeongenEPYC目标客户就是音视频处理和AI workload越来越多的数据中心应用。Genoa是每个die/chiplet 8核的，12个CPU cluster die中间围绕一个IO die的设计，相当于一个芯片96核，是典型的chiplet范式设计。</description>
      <content>&lt;p&gt;数据中心应用（数据库、消息队列、检索服务、参数服务器、直播服务、广告服务等）作为互联网服务的主体，位于终端应用和硬件基础设施之间，桥接了需求端和硬件端，同时还受到成本、营收、算法、法律等因素的制约。
近期硬件侧、算法侧均有变化或变革，因而有必要反思数据中心应用的演进方向，讨论数据中心应用的近未来发展趋势、潜在的创新点和可收割的低垂果实。&lt;/p&gt;
&lt;h2 id=&#34;变革中的不变量数据中心应用能耗&#34;&gt;变革中的不变量：数据中心应用能耗&lt;/h2&gt;
&lt;p&gt;全球数据中心能耗十年来几乎没有增长，而同期的服务需求、计算、存储、数据传输总量则经历了数量级增长。作为从业者，我们对互联网的爆发式增长应该不陌生，因此全球数据中心能耗从10到18年仅涨了6%的事实略微反直觉——从能耗的角度看，这几乎是一个停滞的产业。&lt;/p&gt;
&lt;p&gt;CMOS工艺和软硬件技术的整体进步可以解释服务器能量效率上的提升，但能耗停滞的根本原因还在于互联网企业盈利的本质是对有支付能力的10亿国外人口和10亿国内人口征服务税，个别成功的初创企业可以迅速扩张，但互联网行业作为整体，在数据中心能耗上的投入增长受制于全球平民收入增长。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/DatacenterPower.jpeg&#34; alt=&#34;DatacenterPower&#34;&gt;&lt;/p&gt;
&lt;p&gt;当初创企业的规模扩张到互联网规模后，机器资源的增长速率应该会从爆炸式上升陡然转入停滞，进入自然汰换阶段。
从这个角度可以得出结论，数据中心应用在规模扩张结束后，也应从能耗粗放型转为精细集约型，通过软硬件协同充分释放硬件潜力，通过算法创新减少总计算量。&lt;/p&gt;
&lt;h2 id=&#34;算法迭代从演绎推理到归纳推断&#34;&gt;算法迭代：从演绎推理到归纳推断&lt;/h2&gt;
&lt;p&gt;算法侧的趋势是“transformers getting even more attention”。&lt;/p&gt;
&lt;p&gt;计算机理论源于符号逻辑，1930年代图灵、哥德尔、邱奇各自独立提出图灵机、广义递归函数、lambda演算，以及三者相应的可计算性。图灵机的停机问题和哥德尔不完备定律驳斥了1920年代希尔伯特计划的可行性，证明了形式化系统是不完备的，通过有限个公理和规则永远无法推导出全部的真理。&lt;/p&gt;
&lt;p&gt;正如人的无知可以分为问题（可计算）和神秘（不可计算），人的理性也可以分为演绎推理和归纳推断，前者求解问题，后者分析神秘。&lt;/p&gt;
&lt;p&gt;计算机程序基于图灵机模型，天然就是适合进行演绎推理的形式化系统。至今我们依然基于规则系统(人工编程进行条件控制）、信号处理（比如音频特征提取）、状态机等形式化方法实现大多数数据中心应用：比如各类数据库、检索系统、参数服务器、游戏AI。&lt;/p&gt;
&lt;p&gt;AI时代来临之后，通过更多的权重（作为公理）和由大量算子（作为规则）组成深度神经网络（从实现角度来说，仍然是形式化系统）去模拟或者说近似“归纳推断”。（纯粹贝叶斯主义的形式化表达是所罗门诺夫归纳推断法，这个方法本身是永不停机，越算越停不下来的，所以只能近似，不能用实现）这种近似无论多么粗糙、低效，归根到底还是提升了计算机语言和集成电路的表达能力，完成了推理到推断的跳跃。&lt;/p&gt;
&lt;p&gt;数据中心应用多了归纳推断能力之后，就产生了对复杂现实（现实信号，如图片、语料库、音频，整体上具有高维度的同时，局部往往还存在具有高数据精致度的结构，这里精致度可以用香农信息熵或柯氏复杂性形式化定义）理解能力(pin down reality)，因此涌现出了一些基于知识的AI系统(knowledge-based AI)，比如推荐系统、广告投送、翻唱识别、聊天机器人。共同点是将现实映射到表征低维子空间上，这个子空间里的向量或者说embedding可以从某个角度有效刻画现实。Transformer做得更进一步，等价于将现实映射到多个相干性低的低维子空间上（见https://arxiv.org/abs/2306.01129），从而从多角度多层次理解现实，通过表征空间编码率与各子空间编码率之和的差值度量特征的紧凑性和判别力。&lt;/p&gt;
&lt;p&gt;在游戏和博弈领域，强化学习也取得了规则系统无法企及的成就，在星际2、Dota2（固定阵容5v5，solo）、围棋等领域击败职业选手。&lt;/p&gt;
&lt;p&gt;此外，归纳推断相比演绎推理还有一个重要特质，那就是更符合人类大脑（或推广到任意地球生物大脑，蜜蜂的3d寻路避障能力，蚊子的血管定位能力都是基于天然的贝叶斯归纳推断。也只有受过良好训练的人类才能进行缓慢的演绎推理）的思维习惯，因此人类社会的传统，人类语言的不确定性、人类的偏好都难以用形式化方法描述，却可以被概率语言描述，被归纳推断方法预测。&lt;/p&gt;
&lt;p&gt;算法进步的下一个课题则是研究同时具备演绎推理和归纳推断能力的模型。现有的Transformer的确展现出简单多步算术的拟合能力，但拟合出来的是一个步数相关的超复杂非线性系统，而不是简单的算术规则，所以往往正确率到八十左右就再也上不去了。对于形式化系统来说，推理步数不影响公理和规则总数，只稍稍影响计算量。但对于LLM这种归纳推断模型来说，一旦推理步数变多，计算变得更复杂，LLM的符号逻辑能力就会迅速下降，在训练数据中从未出现过相同执行路径的动态规划问题里面可以直接降到0。理论上来说，无限精度的Transformer是图灵完备的（见https://arxiv.org/abs/1901.03429），而只要不是无限精度，就不是图灵完备，实际应用中模型权重和计算的精度即使以常规计算机应用标准看也是非常低的，而模型架构又没有针对多步逻辑推理进行设计，再加上深度学习本身就是在拟合一个近似函数，三重因素叠加导致现有LLM的演绎推理能力不尽如人意。&lt;/p&gt;
&lt;p&gt;从算法迭代角度讲，数据中心应用的发展趋势是在保证“可计算性”(字面意思，非形式化定义)的前提下，追求“人性化”(align to humanity)、“完备化”（pin down reality）这两个演绎推理手段无法达成的理想属性。智能创作、个性化推荐、聪明的Agent（助手、游戏NPC、机器人、自动驾驶），复杂系统预测（天气预报、交易系统），大模型训练（模型并行、高性能网络、C2C互连、光学I/O、异构计算）都是有前景的方向。&lt;/p&gt;
&lt;p&gt;而那些已经能用规则系统高效解决的问题则不必强行应用归纳推断模型，那样只会增加成本和错误率，比如音乐指纹识别、关系型数据库、图片旋转变形、无损压缩。甚至某些看似“神秘”的领域，如OOD text classification，基于表征学习白盒化研究成果和信息论，也能给出直接的数学解，比如近期一个研究（“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors）就用14行代码超过了百亿参数的bert，（后来被发现实验代码有误，实际上效果没那么好）这个论文里提出用gzip等无损压缩工具去近似文本数据的柯氏复杂性，用信息距离（原理类似马毅提出的rate reduction）做KNN，效果非常好，在音频领域也可以做类似的尝试。&lt;/p&gt;
&lt;h2 id=&#34;数据中心硬件基础设施&#34;&gt;数据中心硬件基础设施&lt;/h2&gt;
&lt;p&gt;先简单介绍一下常见的数据中心硬件基础设施：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;比较老的是Cascade Lake 14nm的24核per die的机器，一个芯片24核，两个就是48个物理核，也就是我们平时说的96核机器。也有其他套餐，最多是每个die上28核，总共112个逻辑核。&lt;/li&gt;
&lt;li&gt;比较新的Ice Lake是10nm工艺一般是32核的，对应128个逻辑核，最贵的套餐可以到40核。仍然是基于Monolithic die，巨大的晶粒上用mesh总线把40个核放到一起。&lt;/li&gt;
&lt;li&gt;最新的7nm工艺Sapphire Rapids，以及对标它的AMD Genoa今年已经量产。&lt;/li&gt;
&lt;li&gt;关于网卡，现在用的比较多的是Mellanox 25G  CX4/CX5卡，也有相当多的100G dual-port CX6卡，都是支持RDMA的，不用RDMA也能提供相当好的高速以太网性能。如果是做虚拟化的，比如AWS，阿里云，还可以把这些网卡的smartNiC能力利用起来，offload虚拟化开销。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相对之前的Lakes，Sapphire Rapids变化非常大，此前的monolithic die路线确实走到了极限，Sapphire Rapids也进入了multi-die时代：芯片内部分为4个die，这也可以视作向chiplet方向演化。&lt;/p&gt;
&lt;p&gt;Sapphire Rapids最多支持8个socket，每个芯片可以支持60核。IO技术上支持CXL 1.1、PCIe 5.0、UPI 2.0，HBM2e(optional)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/4th_xeon.jpeg&#34; alt=&#34;Xeon&#34;&gt;&lt;/p&gt;
&lt;p&gt;AMD的第四代EPYC的主力型号Genoa今年Q1也开始量产，基本上和Sapphire Rapids对标，采用5nm工艺。虽然说AVX512被很多人诟病，实用表现非常一般，但Zen4也支持了，毕竟XeongenEPYC目标客户就是音视频处理和AI workload越来越多的数据中心应用。Genoa是每个die/chiplet 8核的，12个CPU cluster die中间围绕一个IO die的设计，相当于一个芯片96核，是典型的chiplet范式设计。&lt;/p&gt;
&lt;h2 id=&#34;芯片设计的迭代chipletization&#34;&gt;芯片设计的迭代：Chipletization&lt;/h2&gt;
&lt;p&gt;近期芯片设计领域一个显著变革是Chiplets+SiP(System in Package)范式取代die size较大的SoC+PCB合封。
Chiplets同时受到业界和学术界的关注，被IBM research称为&amp;quot;what’s next in computing&amp;quot;，后续章节中对计算、IO、内存等技术的讨论也都涉及chiplets和co-packaging，因此我们首先讨论芯片层次的迭代。&lt;/p&gt;
&lt;p&gt;所谓chiplet partitioning就是将电路切分成模块化的子系统，每个子系统都是一个独立晶粒（die），即chiplet，多个chiplet用2.5D/3D技术封装成一个芯片(package)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/chiplet.png&#34; alt=&#34;Chiplet&#34;&gt;&lt;/p&gt;
&lt;p&gt;Chiplet-reuse范式相比传统的IP-reuse（IP在芯片语境下指的是具有独立功能和成熟设计的电路模块）的优势如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先进CMOS制程（7nm以下）由于技术原因不太可能在大晶粒上获得高yield，die size越小成本越低。&lt;/li&gt;
&lt;li&gt;先进CMOS制程下，不太可能同时缩小电源管理、快速IO SerDes等模拟IP，先进CMOS一般只用于处理器和加速器。&lt;/li&gt;
&lt;li&gt;允许模块化设计，让设计者可以专注于单个模块的极致优化，并选择最合适的技术：比如CPU和GPU用先进制程，模拟模块用成熟制程，高带宽内存HBM用DRAM，AI加速器可以用非易失性内存。&lt;/li&gt;
&lt;li&gt;允许芯片/package层次的异构集成：让通用CPU、优化后的GPU、嵌入式的FPGA、专用的机器学习电路、光学IO模组、高带宽内存等模块以合适的方式，用先进的使用硅通孔（TSV）、微凸块（micro-bumps）、甚至die-to-wafer混合键合技术的3D封装方案，像乐高积木一样搭出完整的系统。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;过去我们设想的各类DSA、AI芯片、FPGA百花齐放的异构计算时代并没有如期到来，而是被NV的GPU软硬件结合且计算互连一体化的方案碾轧了，几乎只剩下TPU在继续向v5迭代。&lt;/p&gt;
&lt;p&gt;但未来的服务器芯片本身就存在多样化异构共封装集成的可能性和倾向性。CPO(co-packaged optics)、HBM(high-bandwidth memory)这些神奇物种因Chipletization的契机而得以进驻其中。&lt;/p&gt;
&lt;p&gt;更多的功能也就意味着更高的可编程性。有些功能甚至可以带来革命性变革，比如光学IO带来的超高通信带宽，HBM带来的超高访存带宽，高性能offpackge互连技术（Nvlink-C2C）、多个Chiplet之间的Mesh互连(NvSwitch)，现在被Nvidia用来搭建H100，被谷歌用来搭建TPUv4，未来则可能颠覆host-centric的数据中心应用设计范式，迎来硬件资源解聚（disaggregation）的新计算体系：适应资源解聚的操作系统(LegoOS就是基于早期IB network的一个尝试)、系统语言ABI、新的高级语言、新的网络IO、存储和计算形态都有可能从中孵化而生。&lt;/p&gt;
&lt;h2 id=&#34;计算和内存层次的迭代可扩展的众核numa架构&#34;&gt;计算和内存层次的迭代：可扩展的众核NUMA架构&lt;/h2&gt;
&lt;p&gt;在商用服务器领域，Chiplet范式中的一部分设想已经实现了，比如AMD很早就开始应用chiplet，也部分解决了chiplet间IO问题，实现了有可扩展性的众核NUMA架构。SPR之前的Xeon物理机也是NUMA，虽然只有2个NUMA节点（目前Intel的NUMA node太大了，所以不太好称之为Chiplet）。&lt;/p&gt;
&lt;p&gt;μArch对计算/访存密集型数据中心应用的性能工程有直接影响。下图是一个6chiplet封装的96核概念机。显然当我们把集成电路的黑盒拆开，就可以看到更细粒度的组件以及它们组成的网络（Network-on-Chip）结构。这个概念机集成了各种先进设计，不仅有many-core，还支持完整的cache coherency。相比过去的多核架构，众核架构的内存层次也相应变得更深，cache miss的代价变得更高。以至于Rust的标准库用B树去实现map（而C++中众所周知是红黑树），这就是处理器和内存频率差距逐渐拉大的结果，（夸张地说）现在的内存已经慢得像是当年的磁盘了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/IntAct.png&#34; alt=&#34;IntAct&#34;&gt;&lt;/p&gt;
&lt;p&gt;针对NUMA架构，系统层的Linux内核和KVM的NUMA-aware scheduler，应用层的网络框架Seastar、数据库ScyllaDB、内存数据库DragonFly等都已经注意到感知硬件拓扑能极大提升整体性能（ScyllaDB、DragonFly分别数倍领先于对标的Cassandra、Redis），提出了share-nothing高性能架构：避免锁和不必要的共享内存、避免不必要的远端内存访问、避免不必要的跨晶粒通信，设计缓存友好的数据结构，更好地利用晶粒内本地的L1 cache——考虑到目前我们用的Cascade Lake机器并非完全cache coherent，未来即使做到完全cache coherent，shared cache的coherency机制也几乎一定有开销，总之在复杂拓扑深内存层次时代，需警惕cache miss。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/NUMA.png&#34; alt=&#34;NUMA&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;重新遇到io瓶颈先进互连再次成为hpc核心&#34;&gt;重新遇到I/O瓶颈：先进互连再次成为HPC核心&lt;/h2&gt;
&lt;p&gt;数据中心应用的IO占了全球IO traffic的76%，和计算一样，IO也是耗电的，而且和计算一样，数据中心IO耗电也十年没有变过，被硬件进步offset掉了。和计算一样，互连是分层级的，近期die-to-die(on-package)链路层面有UCIe标准的发布，off-package层面有基于PCIe6.0的CXL3.0，900GB/s的NvLinkC2C，inter-node层面有Infiniband NDR。这些是基于电的互连，相比而言光学互连更有前景，但也更困难，而且还在早期研发阶段。&lt;/p&gt;
&lt;p&gt;过去的大数据和前大模型AI时代对IO的需求较低，标准以太网足以支撑大部分数据中心应用，包括parameter servers。大模型训练产生了新的计算、IO形态，内存放不下模型，不得不做模型并行后，IO就重新成了瓶颈：H100的8个GPU每个都需要7.2Tbps的off-package带宽，相比之下，连ToR交换机都只需要10+Tbps。AI专用GPU在大模型训练场景下的带宽需求已经非常接近交换机（交换机和GPU一样，都是巨型ASIC，也都是co-packaged optics适用的领域）。在交换机领域，谷歌已经研发出了实用且收益显著的纯光学链路交换机。在GPU互连上，NV也提出过光学互连的GPU的概念系统，甚至还设计了相应的带外置激光源的GPU机架和顺便解决冷却问题的稀疏布线。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/optics.png&#34; alt=&#34;optics&#34;&gt;&lt;/p&gt;
&lt;p&gt;先进IO技术与HPC(高性能计算)的发展密可不分，尽管HPC或者说超算在大众的想象中一直和超强悍的处理器、加速器直接相关，但实际上恰恰相反，传统的HPC workload（建模、模拟类的科学计算）的计算用的往往是普通的商用节点，反而是互连必须用高性能的HPC interconnect技术。传统超算的异构性体现在IO技术，而非FPGA、专用ASIC的应用。&lt;/p&gt;
&lt;p&gt;后来到了大数据分析和AI时代，标准以太网足以支持适应当时模型参数量的AI训练负载。主流的互联网大数据应用可以完全基于商用IO技术和商用计算节点实现。而少数AI DC的异构性主要体现在加速器技术（GPU、TPU、专用AI芯片）而非IO。&lt;/p&gt;
&lt;p&gt;如今出现了大模型训练主导的异构负载，大模型参数量激增导致内存不足，不得不进行模型并行后，die-to-die带宽、off-package带宽、Inter-node带宽重新成为瓶颈。先进(异构)互连技术重新成为HPC的核心话题。&lt;/p&gt;
&lt;p&gt;Datacenter AI重回异构IO+异构计算架构，本质是超算化，因此刚好也能适应建模+模拟类的传统HPC负载，其实给了互联网行业一个新的机会，那就是卷赢大模型训练的同时，还可以顺便进军超算行业，为高校、科研机构提供廉价、可靠、易用、随时oncall的科学计算能力，舆论上一定程度上扭转互联网公司对社会缺乏贡献的负面形象，为继续征收互联网服务税寻求合法性支撑。&lt;/p&gt;
&lt;h2 id=&#34;io技术的迭代光学io愈发接近计算端点&#34;&gt;I/O技术的迭代：光学IO愈发接近计算端点&lt;/h2&gt;
&lt;p&gt;先进铜缆互连是现在，共封装光学互连则是未来。&lt;/p&gt;
&lt;p&gt;前文提到的Co-packaging是先进互连的关键技术，一方面将多个晶粒共封装本身就可以缩短IO链路，降低IO能耗，另一方面允许集成共封装光学模组技术CPO(co-packaged optics)。&lt;/p&gt;
&lt;p&gt;数据中心IO的一个演化趋势是&amp;quot;bring fiber closer to endpoints&amp;quot;。光链路相比电链路，一个明显优势是传输距离更远（受制于频率相关的衰减）。另一个优势是随着带宽增加，电信号不断变短，噪音不断变大，IB network已经逼近铜缆极限，继续发展下去只能从铜缆走向光纤。此外，高频下电互连和连接器既要接收又要发射，会经历显著的串扰，这也限制了电互连的封装密度。光纤作为信号传输介质几乎是理想的，唯一低效的地方就是两端的电光转换部分。&lt;/p&gt;
&lt;p&gt;现在in-racks连接主流方案是铜缆，inter-rack交换则基于以太网链路。超大数据中心里，缆线长就达到几公里，因此越来越多使用光缆——甚至短距离链路现在也越来越多地用光缆。数据中心里，fiber越来越接近endpoints，越来越接近cpus、gpus，最新的趋势是直接将光学组件集成到硅片上。CPO把光电链路结合在一起，无需intervening receive and re-transmit的过程，把光电转换(optoelectonic conversion)步骤省略了。第一代CPO是pluggable optics，第二代是On-Board Optics/Near Package Optics，第三代是2.5D CPO，第四代是3D CPO，第五代则是Integrated Laser。&lt;/p&gt;
&lt;p&gt;Google的TPUv4超算最大的创新就是4k节点上可重配置的纯光学链路的光学交换机(OCS)，节省了光电转换的能耗， Infiniband将铜缆高性能互连发展到极致，后续的roadmap也是从铜缆到光电共封装。Nvidia虽然一直在推电链路方案（Nvlink），但也和Ayar Labs签署了研发合作关系，开始支持带外激光器和硅光子互连技术的研究，毕竟NvLink本质还是NUMA，可以扩展到8GPU，16GPU，但不可能把数据中心规模的一万个GPU连起来。HP也于去年与Ayar Labs合作，试图将硅光子学引入它们的先进HPC IO产品Slignshot互连。Intel也在研究激光器嵌入芯片内部的集成方案。&lt;/p&gt;
&lt;p&gt;下图列出了interposer、PCB、CPO、电缆、有源光缆的耗电、成本、密度、传输距离指标。CPO的优势是显而易见的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/CPO.png&#34; alt=&#34;CPO&#34;&gt;&lt;/p&gt;
&lt;p&gt;在当前的技术水平下，CPO仅被视为一个电光(E/O)桥的角色，以解决SiP的互连带宽密度瓶颈问题。对分布式训练等应用场景来说，电光桥，或者说bring fiber closer to endpoints已经可以大幅降低能耗，提升性能了。但CPO的潜力远不限于此，如果给CPO chiplet稍微加一些功能，就可以像协处理器、smartNiC那样offload一些CPU work，比如做一些简单的数据预处理、后处理，又如CPO无需经过CPU直接就能访问HBM，从而提供DMA能力，这对解聚架构非常有帮助，无需物理上做pooling，又比铜缆IB网络更快。&lt;/p&gt;
&lt;p&gt;这意味着光学IO不仅可以解决大模型训练带来的带宽问题，还给数据中心应用从host-centric向解聚（disaggregated）架构转型提供了可能。&lt;/p&gt;
&lt;p&gt;何谓解聚范式？与传统的服务器中心范式相反，解聚范式是指将作为整体的服务器掰开，拆成CPU、DRAM、磁盘、加速器等独立的硬件资源进行资源抽象和管理的数据中心应用架构设计范式。硬件解聚并非新概念。18年的USENIX OSDI最佳论文LegoOS，一句&amp;quot;We believe that datacenters should break monolithic servers&amp;quot;，充满了信念感。当年的Infiniband还没有进化到NDR版本，光学I/O也还远离数据中心内部端点，但已经足以支撑这样的宏大叙事。&lt;/p&gt;
&lt;p&gt;有了高性能网络，解聚架构就能有效提升数据中心应用的资源利用率，减轻物理机上CPU、加速器、内存、磁盘等资源在host-centric范式下不可避免的over-provisioning问题。&lt;/p&gt;
&lt;h2 id=&#34;评估-gh200-grace-hopper-superchip&#34;&gt;评估 GH200 Grace Hopper Superchip&lt;/h2&gt;
&lt;p&gt;NVIDIA宣称Grace Hopper Superchip是世界上第一个真正支持HPC和AI负载的异构加速平台。
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper.png&#34; alt=&#34;GraceHopper&#34;&gt;
如下图所示，这个superchip是一个把Grace Arm Neoverse CPU+LPDDR5x内存和H100 Tensor Core GPU+HBM，NVLink-C2C集成合封成PCB的集成方案。
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper2.png&#34; alt=&#34;GraceHopper&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper3.png&#34; alt=&#34;GraceHopper&#34;&gt;&lt;/p&gt;
&lt;p&gt;这并不是一个创新方案，一方面悖逆Chipletization的潮流（除了HBM算是chiplet外，H100、Grace、NvSwitch都是巨型SoC/ASIC，况且即使是HBM也是PCB合封，而不是SiP），另一方面也没有进行任何CPU-GPU超融合（物理上融合至单个SoC上，逻辑上统一页表管理、内存、缓存、并发模型等）的探索或尝试（GPU设计之初就存在太多和CPU无法兼容的设计，比如缓存模型、内存模型和并发模型，如今CUDA根基已成很难回头），只是简单粗暴的将CPU、高带宽内存、H100以PCB合封的方式集成，用NVLink-C2C提供内存一致性和更高off-package的带宽（并未尝试任何先进IO技术）。软件上也没能在CUDA基础上提供更强的可编程性，仅仅提供coherent memory access，编程模型仍然是完全异构的（这也是因为CUDA自诞生之初就是个图形加速库，也没法考虑未来会出现对这种superchip的同构编程模型的需求）。&lt;/p&gt;
&lt;p&gt;但这是一个低风险高执行力的集成方案，正如扎克伯格所说，&amp;ldquo;Move fast and break nothing&amp;rdquo;，把原本优秀的组件原封不动地合封起来，不做侵入式修改，只要动作足够快，就能迅速占领市场，构建生态，并支撑溢价。市场上有更完美的memory coherence方案（比如AMD MI300X），更好的CPU-GPU超融合方案，也有比不得不为图形负载妥协的GPU效率更高的AI芯片，但就是没有CUDA异构编程体系，以及Grace Hooper这样把计算、内存和IO瓶颈都解决得差不多的完整解决方案。&lt;/p&gt;
&lt;p&gt;总之，NV的方案作为生态(GPU + CUDA)与生物(ChatGPT根据A100量体裁衣的训练方案)互相作用下的best-of-breed，远远没达到理想最优，甚至也不在正确的技术路线上，AMD的所谓APU以及国内的AI DSA（如Biren）仍有弯道超车的希望。&lt;/p&gt;
&lt;p&gt;讨论计算系统的新机会&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（应用）端到（硬件）端的全栈优化，或者说软硬件协同。
&lt;ul&gt;
&lt;li&gt;TVM: deep learning compiler stack for cpu, gpu and specialized accelerators&lt;/li&gt;
&lt;li&gt;GPU + CUDA&lt;/li&gt;
&lt;li&gt;GH20 Grace Hopper + 新的CUDA NUMA内存API+异构编程API&lt;/li&gt;
&lt;li&gt;司内的LavaRecord全链路优化项目，向下(LavaUOS)对接新存储硬件，试图在nvme ssd上建立高效的用户态IO软件栈。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;应用机器学习方法对参数空间较大的系统做auto-tuning。
&lt;ul&gt;
&lt;li&gt;存储引擎如rocksdb调参&lt;/li&gt;
&lt;li&gt;深度学习模型在异构硬件上的auto TVM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;先进互连技术支持下的资源解聚架构设计。
&lt;ul&gt;
&lt;li&gt;LegoOS&lt;/li&gt;
&lt;li&gt;PolarDB-X的存算分离和memory pooling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;计算节点上的Share-nothing架构，以及data-oriented设计。
&lt;ul&gt;
&lt;li&gt;应用框架层面已有Libtorque、DragonFly、Seastar、Scylladb等先例，主要是IO密集应用——不过只要是内存占用大的CPU应用，大多可以视为IO密集的，因为cache miss上来之后访存占比往往会远超计算。&lt;/li&gt;
&lt;li&gt;虚拟化方向，交大IPADS实验室的CPS: A Cooperative Para-virtualized Scheduling Framework for Manycore Machines，提出协作式半虚拟化调度机制，大幅提升众核虚拟机可扩展性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;基于深度模型白盒化研究和已有的数学工具，用direct math solution取代黑盒模型的近似。
&lt;ul&gt;
&lt;li&gt;例如“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors用压缩+信息距离+KNN的简洁解决方案。&lt;/li&gt;
&lt;li&gt;用异类不相干性、同类可压缩性（稀疏性）衡量embedding效果，不必借助某种端到端应用的指标间接衡量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文献和进一步阅读&#34;&gt;参考文献和进一步阅读&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Learning One-hidden-layer Neural Networks with Landscape Design：即使是最简单的深度学习非凸优化场景，用数学工具（数学最优化方法）进行解释也极为困难。&lt;/li&gt;
&lt;li&gt;Functionality and performance of NVLink with IBM POWER9 processors：几年前IBM Power9（美国能源部的Summit和Sierra超算系统）就在用NVLink，而且hardware cache coherence设计（以及hardware atomic ops，addr translation）已经非常完善，比Grace Hooper方案更完善。&lt;/li&gt;
&lt;li&gt;Faith and Fate: Limits of Transformers on Compositionality：大语言模型涌现出演绎逻辑能力，但在多步复合问题上表现不佳，在训练样本中从未出现过计算图中相同计算路径的动态规划问题上准确率更是迅速跌落。与其他emprical study相比，这个研究更严肃，也更全面，考虑了计算图中训练时未见的splits带来的影响。我们有理由确信，大语言模型涌现的演绎推理能力会受制于transformer的天然局限。&lt;/li&gt;
&lt;li&gt;Teaching Arithmetic to Small Transformers：基于transformer的小语言模型足以学习简单算术能力， 提供包含正确的计算步骤的训练数据（chain-of-thought style data）是提升算术学习能力的关键，简单粗暴地用题目和结果进行训练，单纯靠增加模型大小无法提升准确率。&lt;/li&gt;
&lt;li&gt;A Survey of Large Language Models：提供了对大语言模型的up-to-date review。&lt;/li&gt;
&lt;li&gt;Variantional Inference: A Review For Statisticians：提供了解释VI、理解VI的统计学家视角，讨论了VI应用于指数级模型族的特例，并给出一个贝叶斯高斯混合模型的例子，并推导出一种使用随机优化来扩展至海量数据的VI变体。&lt;/li&gt;
&lt;li&gt;Training language models to follow instructions with human feedback：OpenAI的经验介绍，重点是RLHF。&lt;/li&gt;
&lt;li&gt;GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE：来自semianalysis的爆料，颇具可信度。&lt;/li&gt;
&lt;li&gt;Efficiently Scale LLM Training Across a Large GPU Cluster with Alpa and Ray：LLM训练。&lt;/li&gt;
&lt;li&gt;Scaling Language Model Training to a Trillion Parameters Using Megatron：Megatron（repo： &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;https://github.com/NVIDIA/Megatron-LM&lt;/a&gt; ，paper： &lt;a href=&#34;https://arxiv.org/pdf/1909.08053.pdf&#34;&gt;https://arxiv.org/pdf/1909.08053.pdf&lt;/a&gt; ）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=eqWPyaRcILQ&#34;&gt;https://www.youtube.com/watch?v=eqWPyaRcILQ&lt;/a&gt; 微软Azure硬件系统和基础设施团队的Ram Huggahalli关于Co-Packaged Optics的talk。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Xt-GY8Pkt6g&#34;&gt;https://www.youtube.com/watch?v=Xt-GY8Pkt6g&lt;/a&gt; 研究光通信和先进互连技术的Tony Chan Carusone关于Co-Packaged Optics以及Evolution of IO的talk。&lt;/li&gt;
&lt;li&gt;Next-generation Co-Packaged Optics for Future Disaggregated AI Systems：对共封装光学模组以及未来的解聚AI系统的洞察。&lt;/li&gt;
&lt;li&gt;TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings : Google的TPUv4，重点是纯光链路交换机&lt;/li&gt;
&lt;li&gt;LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation ：乐高OS，基于早期Infiniband高速网络做硬件资源解聚的尝试&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.hpcwire.com/2020/11/16/nvidia-mellanox-debuts-ndr-400-gigabit-infiniband-at-sc20/&#34;&gt;https://www.hpcwire.com/2020/11/16/nvidia-mellanox-debuts-ndr-400-gigabit-infiniband-at-sc20/&lt;/a&gt; Mellanox(Nvidia)的Infiniband NDR版本，以及roadmap。&lt;/li&gt;
&lt;li&gt;Rack-scale disaggregated cloud data centers: The dReDBox project vision: 数据中心应用解聚架构的早期尝试。&lt;/li&gt;
&lt;li&gt;White-Box Transformers via Sparse Rate Reduction：马毅团队对transformer的白盒化解释，此前马毅已经给出了更通用的rate reduction原则： Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bgavran/Category_Theory_Machine_Learning&#34;&gt;https://github.com/bgavran/Category_Theory_Machine_Learning&lt;/a&gt; 深度学习的范畴论解释。深度学习可解释性和逆向工作还可以参考Christopher Olah的blog: colah.github.io，Olah有许多深刻的洞察，比如https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ 流形假设的可视化和深度学习分类的解释。&lt;/li&gt;
&lt;li&gt;IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management：先进IC设计领域的论文，给出了一个集成了chiplet范式、3d封装、完全cache coherence等先进概念的96核众核原型系统。&lt;/li&gt;
&lt;li&gt;“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors：无损压缩近似柯氏复杂性，然后计算信息距离（类似rate reduction，刻画了总体与分类之间的信息差，分类的编码长度低而总体的编码长度高，表明这种分类具有异类强区分性和同类可压缩性），依据信息距离做简单的KNN即可完成分类。这个研究的代码有错，并不能击败BERT，见https://kenschutte.com/gzip-knn-paper/。&lt;/li&gt;
&lt;li&gt;M. Li and P.M.B. Vitányi, An Introduction to Kolmogorov Complexity and Its Applications 柯尔莫哥洛夫复杂性的介绍和应用&lt;/li&gt;
&lt;li&gt;A Mathematical Theory of Communication 1948年香农信息论的论文原著&lt;/li&gt;
&lt;li&gt;hwloc doc：hwloc的文档，hwloc是NUMA-discovery + cpu/memory-binding library。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://man7.org/linux/man-pages/man2/mbind.2.html&#34;&gt;https://man7.org/linux/man-pages/man2/mbind.2.html&lt;/a&gt;：libnuma的NUMA memory policy函数。&lt;/li&gt;
&lt;li&gt;On the Turing Completeness of Modern Neural Network Architectures 证明了无限精度transformer是图灵完备的，即任意图灵机都可被无限精度transformer模拟，但只要是固定精度就不是图灵完备的。&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Our Rationality can be Divided into Induction &amp; Deduction</title>
      <link>https://cmbbq.github.io/posts/induction-deduction/</link>
      <pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/induction-deduction/</guid>
      <description>诺姆·乔姆斯基说：人的无知，可分为神秘和问题(mysteries &amp;amp; problems)。
诚哉斯言，为了对抗无知，人的理性，也可分为归纳和演绎(induction &amp;amp; deduction)。
前者推断神秘，后者解决问题。
前者完备而不可计算，后者可计算而不完备。
何为问题？ 何为问题？本文中指代可求解问题——有良好定义的输入、输出，存在确定性算法能逐步计算，最终得到正确结果，比如八皇后问题，哈密顿路径问题。
何为神秘？ 何为神秘？本文中指代不可求解问题——或需要无限计算资源，或具有不可判定性质（不可判定本质上就是不可计算），比如图灵提出的不可判定的停机问题。哥德尔构造的『真但不可证』的哥德尔语句。又如纯粹贝叶斯主义的置信度计算和预测必须同时考虑无限个预测性理论，现实计算机不仅处理不了计算量的无限性，还处理不了各个算法分支的不可终止性——并不是所有计算都会终止，有的甚至越算越复杂，永不止歇。
何为演绎？ 何为演绎？本文中指代演绎推理。欧几里得几何、初等算术、图灵机、λ演算等形式化系统（或者说演绎装置），就是由一组公理和一系列推理规则所组成，允许从公理推导出新理论的系统。
数理逻辑和演绎推理具有天然的美感。我们小时候学欧式几何时，应该都赞叹过这种形式主义美感：从简洁的五条公理，竟然能推导出复杂、庞大、深邃而又确切无疑正确的体系。演绎似乎是真理之钥。
自然而然地，历史上有数学家希望将所有数学理论建立在一组有限而完备的公理基础上，提供这些公理是一致的证明。1920年代，形式主义派的领导者希尔伯特希望建立一个形式化的证明体系，用严格遵循运算规则的符号逻辑语言表述所有数学陈述，给当时正面临多种悖论危机的数学一个安全的基础。这就是希尔伯特计划，一个人类理性对抗神秘的伟大尝试。希尔伯特说：『我们必须知道，我们也终将知道！』这是形式主义者的战斗宣言，他们坚信：数学具有完备性(在形式化数学里，所有数学陈述可证)、一致性（在形式化数学里，不存在矛盾）、可判定性（总有算法能判定某个数学陈述的真假）、保守性（证明不依赖于理想对象，如不可数集合）。只要这4点假设为真，无论现实多么宏大广博，真理如何高不可攀，人类只要持有演绎推理之钥，就能证明一切真理。
然而希尔伯特计划还未开始，就被哥德尔、图灵、邱奇终结。1930年代，哥德尔提出“广义递归函数”，图灵构造“图灵机”，邱奇提出“λ演算”，三人各自独立地形式化了可计算性：λ可计算性等价于图灵机可计算性，图灵机可计算性又等价于广义递归性。哥德尔不完备定理，证明了(1)一个蕴含初等算术的形式系统，若一致，则不完备。(2)该系统的一致性不能在系统内部证明。图灵提出停机问题，证明了我们无法判定图灵机是否会停止。停机问题的不可判定性蕴含了数学的不可判定性。一旦停机问题可判定，图灵机就能用来证明任意数学问题，因为理论上我们可以编写一个图灵机程序从公理开始构造出所有定理，每个定理生成后检查一下是不是目标问题——比如孪生素数猜想——如果是，图灵机程序终止，如果不是，则不停机。
何为归纳？ 何为归纳？本文中指代统计推断，泛指是使用数据分析来推断概率分布属性的过程。
统计学有贝叶斯学派和频率学派之争。贝叶斯学派认为待估计的模型参数是一个具有模型主观性的随机变量，而用来估计模型参数的数据反过来是确定的常数，讨论观测数据的概率分布才是没有意义的，概率被解释为模型在现有知识基础上对一件事情发生的相信程度。频率学派认为事件本身具有某种客观的随机性，概率是一个确定的值，讨论概率的分布没有意义，概率只是重复实验次数趋近于无穷后频率的极限值。贝叶斯理论最早起源于1763年，此后被频率学派以居高临下的姿态视作已经足够好的正统方法的补充，或者干脆是当成某种有趣但错误的尝试。直到上个世纪末才重新被学界重视。部分原因是计算机取代计算器，成为新的计算工具，克服了贝叶斯方法在计算上的困难。在通用电子计算机出现之前，频率学派对某些特定问题做出特殊假设的做法显然更经济，往往也相当有效，只不过这些人为假设难以证伪也难以证明。
归根到底，频率学派的归纳推断只能勉强算是贝叶斯推断的近似，还有一部分近似是错误的近似。CERN通过大型强子对撞机发现希格斯玻色子，实际上就是依据：假设不存在希格斯玻色子，观测数据的p值&amp;lt;0.00003%，这个阈值看起来足够低，因此CERN的研究者以及整个人类社会就接受了希格斯玻色子的存在。这个0.0003%的阈值是哪里来的呢？和人文社科研究中那些p值&amp;lt;5%一样都来源于直觉和人为假设。所谓的科学，不过是建立在如此薄弱的基础之上。即使在物理学实验这种极端严谨的场合，基于p值的统计分析依然会造成假象。2003年的五夸克态因p值被宣布发现，然而又因原始实验无法复现而被否定。Regina Nuzzo在Nature杂志上对p值滥用进行了批判，通过实例证明科学研究中普遍存在p值统计学显著结果不可重现的问题。
1960年代，纯粹贝叶斯主义者所罗门诺夫超越了哥德尔不完备理论，超越了邱奇-图灵论题，提出了贝叶斯理论的计算形式化，即所罗门诺夫归纳推断理论，并证明了该理论具有完备性和不可计算性。所罗门诺夫给出了一个惊人的观察：完备性和可计算性，不可得兼。任何理论，只要是可计算的，就不能检出所有规律。而像所罗门诺夫归纳推断理论这样具有完备性的理论，则必然是不可计算的。
所罗门诺夫理论的完备性意味着贝叶斯公式（这里是简化表示）P(θ|d) = P(d|θ)P(θ)/P(d)几乎可以被认为是真理公式。演绎推理则是贝叶斯推理在真假确定场景下的正确特例：当P(d) = 1时，概率语言中P(θ|d) = P(d|θ)P(θ)/P(d) 就自然退化成经典逻辑语言d =&amp;gt; θ，即d蕴含θ了。
规则的尽头是贝叶斯 下图是摘自Computability and Complexity，最早出自Descriptive Complexity(Immerman 1999)的可计算性与复杂度世界图，自上而下，逐渐从神秘过渡到问题。 当我们用if else条件判断，用计算机程序语言，无论如何努力，都难以编写出效果让人满意的程序时。我们往往是在面对神秘，而非问题。
这种场景下，经典逻辑的手段已经穷尽，演绎明确不是真理之钥，但我们可以观想所罗门诺夫之妖。
所罗门诺夫之妖所喻的纯粹贝叶斯主义固然具有不可计算性，却可以被各种手段近似模拟，典型的结构就是深度神经网络，用数十亿、数百亿、数千亿个参数/权重，用臃肿笨拙地方式去模拟贝叶斯推断。
在决策和博弈型任务上，强化学习对比基于规则的方法，取得了惊人的效果。目前的强化学习基本上能求解任意人类游戏/博弈，DeepMind的星际争霸AI AlphaStar击败MaNa，Dota2的OpenAI Five击败OG，围棋的AlphaGo击败柯洁。这是此前基于规则的方法几乎不可能实现的。
在理解和生成型任务上，大语言模型对比传统的NLP方法，达到了超出训练目标的额外效果，涌现出种种惊喜，比如单步逻辑推理能力，以及某些简单复合问题的逻辑推理能力。
基于图灵机、λ演算的计算理论根基数十年岿然不动，一朝有变，则蕴含着更深刻的变革——我们正见证从图灵机的演绎推理到神经网络的统计推断的跳跃，从“可计算且不完备”到“不可计算且完备”的质变。
前文Knowledge is Embeddings of Reality中提到对音频的理解可以是不同深度，不同角度的，音乐信息检索可以用信号处理的基于规则的方法，也可以用resnet这样的深度神经网络，或whisper这样的大模型。理解愈深邃，愈不精确，应用也就愈发灵活，愈发贴近人类思维。这是理解的视角，若是从计算的视角讲，基于信号处理的系统是在做演绎推理，因此可计算性最强，但天然具有不完备性，无论我们给代码里增加多少个公理，增加多少执行路径，设计多么巧妙的手工特征，注入多么鬼斧神工的专家洞察，都总有覆盖不到的场景，甚至可以说只能覆盖到很小很小的一部分，因为音频数据来自于现实，原始数据具备极高的整体复杂度和细微结构精致度，超越了人类思考和编程的能力极限，人类专家设计的音频特征能考虑抗噪，具备简单的鲁棒性，就已经是sota了。而基于深度模型、乃至大模型的方法则是对贝叶斯推断的近似，放弃精确性、增加计算成本的同时，从演绎转变成了归纳，从收敛走向开放，哪怕仅仅是对贝叶斯的近似，也换取了一定程度上的完备性——不局限于简单的抗噪、抗变调，开始在旋律、情绪、风格，甚至歌词的语言逻辑层面上考虑音频的相似度。
认知计算：人性化与完备化的革命 基于推理的算法，本质是为形式化系统（λ演算、图灵机）手工编写公理（先验数据，如魔数、掩码表、配置文件）或规则（传统的条件控制与算术计算）进行正确、高效但不完备的演绎推理。
基于推断的算法，本质是使用形式化系统（计算神经网络依然是形式化系统，其公理变成了更庞大的模型权重，规则变成了矩阵乘加、归一化、激活函数等数学算子的组合）对所罗门诺夫归纳推断理论（形式化的纯粹贝叶斯主义）进行拙劣、错误，但有用的近似。
统计推断，模型推理，英文中都是inference，中文用词却不一，不知是巧合还是集体无意识暗合真理：推断是归纳，推理则是演绎。纯粹贝叶斯主义的所罗门诺夫归纳推断近乎全知，不可计算，无穷尽，不停机，而我们用模型权重为公理，用矩阵乘加、批正态化、激活函数组合为规则写出来的模型推理程序本质上还是基于演绎推理，训练或者说优化、学习的过程亦然。因此在计算机科学的AI应用语境下我们把inference称为推理完全合理，正如在统计学语境下我们把inference称为推断，也是无比合理的。
所谓认知计算，就是用演绎推理的凡俗躯壳容纳纯粹贝叶斯主义全知魂灵的神降，补齐符号逻辑、规则系统和信号处理手段在“人性化(align to humanity)”、“完备化(pin down reality)”上的缺憾。
可以断言，尽管AI学界和业界浮躁且浮夸，受传统学科乃至计算系统传统领域的研究者诟病，但深度学习本身绝非niche应用，更非Metaverse、Web3那种VC追逐的玩具和噱头。所有计算领域——互联网服务、数据中心应用、智能终端、游戏AI，都将受益于认知计算进步，人类文明很有可能也会进入新的阶段。
近未来人类文明在基础物理、材料、能源领域或难有寸进，但在认知科学上构筑出辉煌的认知计算体系几乎是确定性的，因为认知计算所需的硬件进步不构成瓶颈，至少不会像对撞机、深空旅行、可控核聚变那样受到时空尺度和材料极限的约束。毕竟考虑即使是小小一只蜜蜂不足2立方厘米的大脑，就能拥有超越人类目前基于规则和导航辅助的先进算法的复杂3D环境寻路避障能力和天生的贝叶斯推断能力，亿万年生物演化的自然结构已经存在，我们不必担心这种结构根本不存在：它就在那，我们所需做的仅仅是近似和模仿。</description>
      <content>&lt;p&gt;诺姆·乔姆斯基说：人的无知，可分为神秘和问题(mysteries &amp;amp; problems)。&lt;/p&gt;
&lt;p&gt;诚哉斯言，为了对抗无知，人的理性，也可分为归纳和演绎(induction &amp;amp; deduction)。&lt;/p&gt;
&lt;p&gt;前者推断神秘，后者解决问题。&lt;/p&gt;
&lt;p&gt;前者&lt;a href=&#34;(https://en.wikipedia.org/wiki/Complete_theory)&#34;&gt;完备&lt;/a&gt;而不可计算，后者&lt;a href=&#34;https://en.wikipedia.org/wiki/Computability_theory&#34;&gt;可计算&lt;/a&gt;而不完备。&lt;/p&gt;
&lt;h2 id=&#34;何为问题&#34;&gt;何为问题？&lt;/h2&gt;
&lt;p&gt;何为问题？本文中指代可求解问题——有良好定义的输入、输出，存在确定性算法能逐步计算，最终得到正确结果，比如&lt;a href=&#34;https://en.wikipedia.org/wiki/Eight_queens_puzzle&#34;&gt;八皇后问题&lt;/a&gt;，&lt;a href=&#34;https://en.wikipedia.org/wiki/Hamiltonian_path_problem&#34;&gt;哈密顿路径问题&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;何为神秘&#34;&gt;何为神秘？&lt;/h2&gt;
&lt;p&gt;何为神秘？本文中指代不可求解问题——或需要无限计算资源，或具有不可判定性质（不可判定本质上就是不可计算），比如图灵提出的&lt;a href=&#34;https://en.wikipedia.org/wiki/Undecidable_problem&#34;&gt;不可判定&lt;/a&gt;的&lt;a href=&#34;https://en.wikipedia.org/wiki/Halting_problem&#34;&gt;停机问题&lt;/a&gt;。哥德尔构造的『真但不可证』的&lt;a href=&#34;https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems&#34;&gt;哥德尔语句&lt;/a&gt;。又如纯粹贝叶斯主义的置信度计算和预测必须同时考虑无限个预测性理论，现实计算机不仅处理不了计算量的无限性，还处理不了各个算法分支的不可终止性——并不是所有计算都会终止，有的甚至越算越复杂，永不止歇。&lt;/p&gt;
&lt;h2 id=&#34;何为演绎&#34;&gt;何为演绎？&lt;/h2&gt;
&lt;p&gt;何为演绎？本文中指代&lt;a href=&#34;https://en.wikipedia.org/wiki/Deductive_reasoning&#34;&gt;演绎推理&lt;/a&gt;。欧几里得几何、初等算术、图灵机、&lt;a href=&#34;https://en.wikipedia.org/wiki/Lambda_calculus&#34;&gt;λ演算&lt;/a&gt;等形式化系统（或者说演绎装置），就是由一组公理和一系列推理规则所组成，允许从公理推导出新理论的系统。&lt;/p&gt;
&lt;p&gt;数理逻辑和演绎推理具有天然的美感。我们小时候学欧式几何时，应该都赞叹过这种形式主义美感：从简洁的五条公理，竟然能推导出复杂、庞大、深邃而又确切无疑正确的体系。演绎似乎是真理之钥。&lt;/p&gt;
&lt;p&gt;自然而然地，历史上有数学家希望将所有数学理论建立在一组有限而完备的公理基础上，提供这些公理是一致的证明。1920年代，形式主义派的领导者希尔伯特希望建立一个形式化的证明体系，用严格遵循运算规则的符号逻辑语言表述所有数学陈述，给当时正面临多种&lt;a href=&#34;https://en.wikipedia.org/wiki/Foundations_of_mathematics#Foundational_crisis&#34;&gt;悖论危机&lt;/a&gt;的数学一个安全的基础。这就是&lt;a href=&#34;https://en.wikipedia.org/wiki/Hilbert%27s_program&#34;&gt;希尔伯特计划&lt;/a&gt;，一个人类理性对抗神秘的伟大尝试。希尔伯特说：『我们必须知道，我们也终将知道！』这是形式主义者的战斗宣言，他们坚信：数学具有完备性(在形式化数学里，所有数学陈述可证)、一致性（在形式化数学里，不存在矛盾）、可判定性（总有算法能判定某个数学陈述的真假）、保守性（证明不依赖于理想对象，如不可数集合）。只要这4点假设为真，无论现实多么宏大广博，真理如何高不可攀，人类只要持有演绎推理之钥，就能证明一切真理。&lt;/p&gt;
&lt;p&gt;然而希尔伯特计划还未开始，就被哥德尔、图灵、邱奇终结。1930年代，哥德尔提出“广义递归函数”，图灵构造“图灵机”，邱奇提出“λ演算”，三人各自独立地形式化了可计算性：λ可计算性等价于图灵机可计算性，图灵机可计算性又等价于广义递归性。&lt;a href=&#34;https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems#First_incompleteness_theorem&#34;&gt;哥德尔不完备定理&lt;/a&gt;，证明了(1)一个蕴含初等算术的形式系统，若一致，则不完备。(2)该系统的一致性不能在系统内部证明。图灵提出停机问题，证明了我们无法判定图灵机是否会停止。停机问题的不可判定性蕴含了数学的不可判定性。一旦停机问题可判定，图灵机就能用来证明任意数学问题，因为理论上我们可以编写一个图灵机程序从公理开始构造出所有定理，每个定理生成后检查一下是不是目标问题——比如孪生素数猜想——如果是，图灵机程序终止，如果不是，则不停机。&lt;/p&gt;
&lt;h2 id=&#34;何为归纳&#34;&gt;何为归纳？&lt;/h2&gt;
&lt;p&gt;何为归纳？本文中指代&lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_inference&#34;&gt;统计推断&lt;/a&gt;，泛指是使用数据分析来推断概率分布属性的过程。&lt;/p&gt;
&lt;p&gt;统计学有贝叶斯学派和频率学派之争。贝叶斯学派认为待估计的模型参数是一个具有模型主观性的随机变量，而用来估计模型参数的数据反过来是确定的常数，讨论观测数据的概率分布才是没有意义的，概率被解释为模型在现有知识基础上对一件事情发生的相信程度。频率学派认为事件本身具有某种客观的随机性，概率是一个确定的值，讨论概率的分布没有意义，概率只是重复实验次数趋近于无穷后频率的极限值。贝叶斯理论最早起源于1763年，此后被频率学派以居高临下的姿态视作已经足够好的正统方法的补充，或者干脆是当成某种有趣但错误的尝试。直到上个世纪末才重新被学界重视。部分原因是计算机取代计算器，成为新的计算工具，克服了贝叶斯方法在计算上的困难。在通用电子计算机出现之前，频率学派对某些特定问题做出特殊假设的做法显然更经济，往往也相当有效，只不过这些人为假设难以证伪也难以证明。&lt;/p&gt;
&lt;p&gt;归根到底，频率学派的归纳推断只能勉强算是贝叶斯推断的近似，还有一部分近似是错误的近似。CERN通过大型强子对撞机发现希格斯玻色子，实际上就是依据：假设不存在希格斯玻色子，观测数据的p值&amp;lt;0.00003%，这个阈值看起来足够低，因此CERN的研究者以及整个人类社会就接受了希格斯玻色子的存在。这个0.0003%的阈值是哪里来的呢？和人文社科研究中那些p值&amp;lt;5%一样都来源于直觉和人为假设。所谓的科学，不过是建立在如此薄弱的基础之上。即使在物理学实验这种极端严谨的场合，基于p值的统计分析依然会造成假象。2003年的五夸克态因p值被宣布发现，然而又因原始实验无法复现而被否定。&lt;a href=&#34;https://www.nature.com/articles/506150a&#34;&gt;Regina Nuzzo在Nature杂志上对p值滥用进行了批判&lt;/a&gt;，通过实例证明科学研究中普遍存在p值统计学显著结果不可重现的问题。&lt;/p&gt;
&lt;p&gt;1960年代，纯粹贝叶斯主义者所罗门诺夫超越了哥德尔不完备理论，超越了邱奇-图灵论题，提出了贝叶斯理论的计算形式化，即&lt;a href=&#34;https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference&#34;&gt;所罗门诺夫归纳推断理论&lt;/a&gt;，并证明了该理论具有完备性和不可计算性。所罗门诺夫给出了一个惊人的观察：完备性和可计算性，不可得兼。任何理论，只要是可计算的，就不能检出所有规律。而像所罗门诺夫归纳推断理论这样具有完备性的理论，则必然是不可计算的。&lt;/p&gt;
&lt;p&gt;所罗门诺夫理论的完备性意味着贝叶斯公式（这里是简化表示）P(θ|d) = P(d|θ)P(θ)/P(d)几乎可以被认为是真理公式。演绎推理则是贝叶斯推理在真假确定场景下的正确特例：当P(d) = 1时，概率语言中P(θ|d) = P(d|θ)P(θ)/P(d) 就自然退化成经典逻辑语言d =&amp;gt; θ，即d蕴含θ了。&lt;/p&gt;
&lt;h2 id=&#34;规则的尽头是贝叶斯&#34;&gt;规则的尽头是贝叶斯&lt;/h2&gt;
&lt;p&gt;下图是摘自&lt;a href=&#34;https://plato.stanford.edu/entries/computability&#34;&gt;Computability and Complexity&lt;/a&gt;，最早出自Descriptive Complexity(Immerman 1999)的可计算性与复杂度世界图，自上而下，逐渐从神秘过渡到问题。
&lt;img src=&#34;https://cmbbq.github.io/img/CaC.jpeg&#34; alt=&#34;cac&#34;&gt;&lt;/p&gt;
&lt;p&gt;当我们用if else条件判断，用计算机程序语言，无论如何努力，都难以编写出效果让人满意的程序时。我们往往是在面对神秘，而非问题。&lt;/p&gt;
&lt;p&gt;这种场景下，经典逻辑的手段已经穷尽，演绎明确不是真理之钥，但我们可以观想所罗门诺夫之妖。&lt;/p&gt;
&lt;p&gt;所罗门诺夫之妖所喻的纯粹贝叶斯主义固然具有不可计算性，却可以被各种手段近似模拟，典型的结构就是深度神经网络，用数十亿、数百亿、数千亿个参数/权重，用臃肿笨拙地方式去模拟贝叶斯推断。&lt;/p&gt;
&lt;p&gt;在决策和博弈型任务上，强化学习对比基于规则的方法，取得了惊人的效果。目前的强化学习基本上能求解任意人类游戏/博弈，DeepMind的星际争霸AI AlphaStar击败MaNa，Dota2的OpenAI Five击败OG，围棋的AlphaGo击败柯洁。这是此前基于规则的方法几乎不可能实现的。&lt;/p&gt;
&lt;p&gt;在理解和生成型任务上，大语言模型对比传统的NLP方法，达到了超出训练目标的额外效果，涌现出种种惊喜，比如单步逻辑推理能力，以及某些简单复合问题的逻辑推理能力。&lt;/p&gt;
&lt;p&gt;基于图灵机、λ演算的计算理论根基数十年岿然不动，一朝有变，则蕴含着更深刻的变革——我们正见证从图灵机的演绎推理到神经网络的统计推断的跳跃，从“可计算且不完备”到“不可计算且完备”的质变。&lt;/p&gt;
&lt;p&gt;前文&lt;a href=&#34;https://cmbbq.github.io/posts/reality-knowledge&#34;&gt;Knowledge is Embeddings of Reality&lt;/a&gt;中提到对音频的理解可以是不同深度，不同角度的，音乐信息检索可以用信号处理的基于规则的方法，也可以用resnet这样的深度神经网络，或whisper这样的大模型。理解愈深邃，愈不精确，应用也就愈发灵活，愈发贴近人类思维。这是理解的视角，若是从计算的视角讲，基于信号处理的系统是在做演绎推理，因此可计算性最强，但天然具有不完备性，无论我们给代码里增加多少个公理，增加多少执行路径，设计多么巧妙的手工特征，注入多么鬼斧神工的专家洞察，都总有覆盖不到的场景，甚至可以说只能覆盖到很小很小的一部分，因为音频数据来自于现实，原始数据具备极高的整体复杂度和细微结构精致度，超越了人类思考和编程的能力极限，人类专家设计的音频特征能考虑抗噪，具备简单的鲁棒性，就已经是sota了。而基于深度模型、乃至大模型的方法则是对贝叶斯推断的近似，放弃精确性、增加计算成本的同时，从演绎转变成了归纳，从收敛走向开放，哪怕仅仅是对贝叶斯的近似，也换取了一定程度上的完备性——不局限于简单的抗噪、抗变调，开始在旋律、情绪、风格，甚至歌词的语言逻辑层面上考虑音频的相似度。&lt;/p&gt;
&lt;h2 id=&#34;认知计算人性化与完备化的革命&#34;&gt;认知计算：人性化与完备化的革命&lt;/h2&gt;
&lt;p&gt;基于推理的算法，本质是为形式化系统（λ演算、图灵机）手工编写公理（先验数据，如魔数、掩码表、配置文件）或规则（传统的条件控制与算术计算）进行正确、高效但不完备的演绎推理。&lt;/p&gt;
&lt;p&gt;基于推断的算法，本质是使用形式化系统（计算神经网络依然是形式化系统，其公理变成了更庞大的模型权重，规则变成了矩阵乘加、归一化、激活函数等数学算子的组合）对&lt;a href=&#34;https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference&#34;&gt;所罗门诺夫归纳推断理论&lt;/a&gt;（形式化的纯粹贝叶斯主义）进行拙劣、错误，但有用的近似。&lt;/p&gt;
&lt;p&gt;统计推断，模型推理，英文中都是inference，中文用词却不一，不知是巧合还是集体无意识暗合真理：推断是归纳，推理则是演绎。纯粹贝叶斯主义的所罗门诺夫归纳推断近乎全知，不可计算，无穷尽，不停机，而我们用模型权重为公理，用矩阵乘加、批正态化、激活函数组合为规则写出来的模型推理程序本质上还是基于演绎推理，训练或者说优化、学习的过程亦然。因此在计算机科学的AI应用语境下我们把inference称为推理完全合理，正如在统计学语境下我们把inference称为推断，也是无比合理的。&lt;/p&gt;
&lt;p&gt;所谓认知计算，就是用演绎推理的凡俗躯壳容纳纯粹贝叶斯主义全知魂灵的神降，补齐符号逻辑、规则系统和信号处理手段在“人性化(align to humanity)”、“完备化(pin down reality)”上的缺憾。&lt;/p&gt;
&lt;p&gt;可以断言，尽管AI学界和业界浮躁且浮夸，受传统学科乃至计算系统传统领域的研究者诟病，但深度学习本身绝非niche应用，更非Metaverse、Web3那种VC追逐的玩具和噱头。所有计算领域——互联网服务、数据中心应用、智能终端、游戏AI，都将受益于认知计算进步，人类文明很有可能也会进入新的阶段。&lt;/p&gt;
&lt;p&gt;近未来人类文明在基础物理、材料、能源领域或难有寸进，但在认知科学上构筑出辉煌的认知计算体系几乎是确定性的，因为认知计算所需的硬件进步不构成瓶颈，至少不会像对撞机、深空旅行、可控核聚变那样受到时空尺度和材料极限的约束。毕竟考虑即使是小小一只蜜蜂不足2立方厘米的大脑，就能拥有超越人类目前基于规则和导航辅助的先进算法的复杂3D环境寻路避障能力和天生的贝叶斯推断能力，亿万年生物演化的自然结构已经存在，我们不必担心这种结构根本不存在：它就在那，我们所需做的仅仅是近似和模仿。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Distributed Computing Systems At Scale</title>
      <link>https://cmbbq.github.io/posts/distributed-computing-systems/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/distributed-computing-systems/</guid>
      <description>Distributed Computing Systems At Scale 分布式计算系统完成规模化的跳跃后，核心挑战是正确性和系统效率。前者对应共识抽象，后者对应性能工程。
Consensus Abstraction 分布式系统的正确性实际上蕴含了稳定性、一致性、可用性等理想属性。 分布式天然意味着并发执行、进程失败、不可靠的消息传输，这些因素都使正确性在各种常见的分布式应用语境下变得极为困难：
并发意味着执行轨迹空间维度爆炸，和(进程数*步数)的阶乘成正比。很难保证每个执行路径都是对的。 进程失败可分为：崩溃（进程没了，网卡或CPU故障）、丢请求（没崩，网络阻塞或服务降级导致临时不可用）、恢复后崩溃（反复重启，最终恢复到正确状态，往往基于日志）、拜占庭失败（不可预料，来自宇宙射线或恶意攻击）。 消息传输面临篡改、丢包、重传、乱序等风险，即使是可靠网络协议也不完全可靠。 共识是指多个进程就某个问题（2-general问题、复制状态机保证全局序的问题、分布式事务）最终形成一致决定。理想的共识算法应具备安全性(safety)和活性(liveliness)。安全性包括validity（只有一个提议值被选择）、agreement（正确的节点保持一致）、integrity（每个节点最多选一次）。活性则是指可终止，每个正确节点最终总会选择一个值，而不是hang住。
如果不考虑安全性的话，TCP其实就是一个最简单、粗糙、有效的共识协议，TCP协议本质是让通信的两个进程对连接的状态达成了共识，才能可靠地通信。比如进程关闭连接，就会把本地状态改为TIME_WAIT，等几分钟后才真正释放资源，以确保对方收到ACK，用简单的等待，达成高概率的共识，避免双方开始下一轮连接后收到旧报文而出错。
如果追求完全安全，简单的场景是只对一个值进行商议和决策，即单值共识。
单值共识前提下的最简场景则是拥有完美失败检测能力，泛洪共识（只有获得所有进程提议才做出决定）、等级统一共识就是这种场景下的简单解决方案。
若只具备最终失败检测能力，则需引入代次变更概念，用代次共识（往往是Paxos变种）去解决。Paxos是唯一已知的completely-safe &amp;amp; largely-live的容错共识算法。2PC其实也是一种代次共识，不过2PC里协调者宕机就会全局阻塞，是safe but not live的典型。Paxos甚至可以通过将代次(round)的偏序关系从&amp;lt;改成整除，来实现多维的先后代次，从而兼具2PC的功能。
如果再把单值共识推广到序列共识（在分布式存储中更常用，因为log-structured store的写操作是一个append-only序列，而非单值），则从Paxos进化为某种Multi-Paxos，或Raft。Raft本质上解决了复制状态机问题，而序列共识问题恰好可以归约为复制状态机问题。
如果再考虑拜占庭失败，则需BFT、PBFT、PoW。
Performance Engineering 性能的重要性毋庸置疑，端上应用（语音识别、智能客服）的延迟够不够低直接决定项目是否成立，而大规模的IDC应用(搜广推、存储、在线推理、训练)的吞吐性能则直接和金钱与环保挂钩。
计算系统的性能与分布式密不可分。因为几乎所有计算系统都是分布式的，包括单机，单卡，单设备，因为单机（in-node）仍然有网络结构，比如NVLink和NVSwitch，甚至Xeon CPU都是众核通过总线连起来的，网卡、SSD等设备，拆开来看都是网联设备（networked device）。通过分区、并行、软硬件协同、减少不必要的分层获取性能的方法是普适的。
如果存在基线，性能工程的主要工作就是优化、迭代，和渐进式创新。 当基线不够好时，性能工程(performance engineering)是发散的：跨越层次的，不同抽象层次、不同的模块都有可能成为瓶颈，不应存在优化盲点，必要时还需反分层，比如将整个IO路径放到用户态，比如暴露hypervisor状态给vm以便高效调度。 当基线已经非常好时，性能路径(performance path)则是收敛的：明确指向软硬件协同，毕竟性能最终来源于对硬件资源的量体裁衣和极限压榨，如果协同也足够好，那就只能指向硬件更新：更大更强的N卡、TPUv4这样的DSA、OCS这样的光学路由以及更进一步的AI DC建设。
如果是全新的问题，或对已有问题产生了非凡洞察，对其施加更有效结构，就产生了新的架构，新的计算和IO形态：
大模型时代来临后，模型参数本身就已经放不进顶级的GPU，更不用说神经网络计算所需的额外内存了，这就使原本在系统效率和正确性之间平衡的非常好的Parameter Server架构面临挑战，不得不用全新的架构和策略解决大模型训练的问题：Node内（8卡组一个Node）的权重切分和模型并行，算子内的数据和指令并行，子图切分、算子摆布和流水线并行（8个Node做8级流水）、16组8*8做Batch16的数据并行，于是产生了Alpa over Ray训练大模型的解决方案。这又间接导致带宽需求激增，以至于需要Google的光学交换机OCS这样的网络解决方案，避免光电模块和解析报文计算的开销——光交换劣势在于切换慢，但训练周期里数据交换的主要流量路由是固定的，用来建设AI DC正好规避了这个缺点。 KVM的作者根据hypervisor在NUMA系统上的性能瓶颈的经验，很容易就发现应用层共享内存的并行模型是有缺陷的，在众核时代尤其严重，因此创造了seastar。利用hardware locality，避免线程切换、数据拷贝、NUMA远端内存访问，对访存密集型CPU-bound应用来说，是非常重要的性能路径。 </description>
      <content>&lt;h1 id=&#34;distributed-computing-systems-at-scale&#34;&gt;Distributed Computing Systems At Scale&lt;/h1&gt;
&lt;p&gt;分布式计算系统完成规模化的跳跃后，核心挑战是正确性和系统效率。前者对应共识抽象，后者对应性能工程。&lt;/p&gt;
&lt;h2 id=&#34;consensus-abstraction&#34;&gt;Consensus Abstraction&lt;/h2&gt;
&lt;p&gt;分布式系统的正确性实际上蕴含了稳定性、一致性、可用性等理想属性。
分布式天然意味着并发执行、进程失败、不可靠的消息传输，这些因素都使正确性在各种常见的分布式应用语境下变得极为困难：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;并发意味着执行轨迹空间维度爆炸，和(进程数*步数)的阶乘成正比。很难保证每个执行路径都是对的。&lt;/li&gt;
&lt;li&gt;进程失败可分为：崩溃（进程没了，网卡或CPU故障）、丢请求（没崩，网络阻塞或服务降级导致临时不可用）、恢复后崩溃（反复重启，最终恢复到正确状态，往往基于日志）、拜占庭失败（不可预料，来自宇宙射线或恶意攻击）。&lt;/li&gt;
&lt;li&gt;消息传输面临篡改、丢包、重传、乱序等风险，即使是可靠网络协议也不完全可靠。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;共识是指多个进程就某个问题（2-general问题、复制状态机保证全局序的问题、分布式事务）最终形成一致决定。理想的共识算法应具备安全性(safety)和活性(liveliness)。安全性包括validity（只有一个提议值被选择）、agreement（正确的节点保持一致）、integrity（每个节点最多选一次）。活性则是指可终止，每个正确节点最终总会选择一个值，而不是hang住。&lt;/p&gt;
&lt;p&gt;如果不考虑安全性的话，TCP其实就是一个最简单、粗糙、有效的共识协议，TCP协议本质是让通信的两个进程对连接的状态达成了共识，才能可靠地通信。比如进程关闭连接，就会把本地状态改为TIME_WAIT，等几分钟后才真正释放资源，以确保对方收到ACK，用简单的等待，达成高概率的共识，避免双方开始下一轮连接后收到旧报文而出错。&lt;/p&gt;
&lt;p&gt;如果追求完全安全，简单的场景是只对一个值进行商议和决策，即单值共识。&lt;/p&gt;
&lt;p&gt;单值共识前提下的最简场景则是拥有完美失败检测能力，泛洪共识（只有获得所有进程提议才做出决定）、等级统一共识就是这种场景下的简单解决方案。&lt;/p&gt;
&lt;p&gt;若只具备最终失败检测能力，则需引入代次变更概念，用代次共识（往往是Paxos变种）去解决。Paxos是唯一已知的completely-safe &amp;amp; largely-live的容错共识算法。2PC其实也是一种代次共识，不过2PC里协调者宕机就会全局阻塞，是safe but not live的典型。Paxos甚至可以通过将代次(round)的偏序关系从&amp;lt;改成整除，来实现多维的先后代次，从而兼具2PC的功能。&lt;/p&gt;
&lt;p&gt;如果再把单值共识推广到序列共识（在分布式存储中更常用，因为log-structured store的写操作是一个append-only序列，而非单值），则从Paxos进化为某种Multi-Paxos，或Raft。Raft本质上解决了复制状态机问题，而序列共识问题恰好可以归约为复制状态机问题。&lt;/p&gt;
&lt;p&gt;如果再考虑拜占庭失败，则需BFT、PBFT、PoW。&lt;/p&gt;
&lt;h2 id=&#34;performance-engineering&#34;&gt;Performance Engineering&lt;/h2&gt;
&lt;p&gt;性能的重要性毋庸置疑，端上应用（语音识别、智能客服）的延迟够不够低直接决定项目是否成立，而大规模的IDC应用(搜广推、存储、在线推理、训练)的吞吐性能则直接和金钱与环保挂钩。&lt;/p&gt;
&lt;p&gt;计算系统的性能与分布式密不可分。因为几乎所有计算系统都是分布式的，包括单机，单卡，单设备，因为单机（in-node）仍然有网络结构，比如NVLink和NVSwitch，甚至Xeon CPU都是众核通过总线连起来的，网卡、SSD等设备，拆开来看都是网联设备（networked device）。通过分区、并行、软硬件协同、减少不必要的分层获取性能的方法是普适的。&lt;/p&gt;
&lt;p&gt;如果存在基线，性能工程的主要工作就是优化、迭代，和渐进式创新。
当基线不够好时，性能工程(performance engineering)是发散的：跨越层次的，不同抽象层次、不同的模块都有可能成为瓶颈，不应存在优化盲点，必要时还需反分层，比如将整个IO路径放到用户态，比如暴露hypervisor状态给vm以便高效调度。
当基线已经非常好时，性能路径(performance path)则是收敛的：明确指向软硬件协同，毕竟性能最终来源于对硬件资源的量体裁衣和极限压榨，如果协同也足够好，那就只能指向硬件更新：更大更强的N卡、TPUv4这样的DSA、OCS这样的光学路由以及更进一步的AI DC建设。&lt;/p&gt;
&lt;p&gt;如果是全新的问题，或对已有问题产生了非凡洞察，对其施加更有效结构，就产生了新的架构，新的计算和IO形态：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大模型时代来临后，模型参数本身就已经放不进顶级的GPU，更不用说神经网络计算所需的额外内存了，这就使原本在系统效率和正确性之间平衡的非常好的Parameter Server架构面临挑战，不得不用全新的架构和策略解决大模型训练的问题：Node内（8卡组一个Node）的权重切分和模型并行，算子内的数据和指令并行，子图切分、算子摆布和流水线并行（8个Node做8级流水）、16组8*8做Batch16的数据并行，于是产生了Alpa over Ray训练大模型的解决方案。这又间接导致带宽需求激增，以至于需要Google的光学交换机OCS这样的网络解决方案，避免光电模块和解析报文计算的开销——光交换劣势在于切换慢，但训练周期里数据交换的主要流量路由是固定的，用来建设AI DC正好规避了这个缺点。&lt;/li&gt;
&lt;li&gt;KVM的作者根据hypervisor在NUMA系统上的性能瓶颈的经验，很容易就发现应用层共享内存的并行模型是有缺陷的，在众核时代尤其严重，因此创造了seastar。利用hardware locality，避免线程切换、数据拷贝、NUMA远端内存访问，对访存密集型CPU-bound应用来说，是非常重要的性能路径。&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Knowledge is Embeddings of Reality</title>
      <link>https://cmbbq.github.io/posts/reality-knowledge/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/reality-knowledge/</guid>
      <description>现实有无限维度，现实的背后还有潜在现实，结构的边缘会涌现出新的深层结构。
人类对现实的感知、认知、理解，本质上是将高维现实投射到一个低维表示上，这种低维表示在统计学和机器学习中被称为embedding（嵌入），没错，知识就是将现实在人类有限的认知空间的一个嵌入，紧凑、浅薄且片面。
举个具体的例子，人眼观察到的彩虹呈现七色。
但这只是对无限广博的现实的最初观察，是最直观的认知。
人类通过对人体生理学的研究，发现七色感知是一个更深层的潜在现实的涌现结构：多种光感受器的叠加反应。
人类颜色感知系统对红绿蓝敏感的三种光感受器，叠加反应产生的三个峰和四个谷决定了人类能感知到7种颜色。 鸟类由于拥有对紫外线敏感的光感受器，就能感知到9种颜色，因此可以看到九色彩虹。
无论人类的七色彩虹还是鸟类的九色彩虹，都只是知识（或者说embedding），而不是现实。
人类通过数学工具和电磁学研究，进一步认识到光谱是连续的，包括无线电波、微波、热、红外线、可见光、紫外线等。
这接近现实了吗？不，这依然只是浅薄的embedding。。
人类通过量子物理学的研究，发现现有的电磁学依旧是一个更深层的潜在现实的涌现模型——电磁波是量子粒子流的一种表现形式：光子。
光子的认知，折射出更复杂的现实，因为一个光子同时拥有无限的位置和频率，直到被观察到才能确定其位置或能量，需用海森堡不确定性原理、薛定谔方程来描述，这依旧不是真理，不是终极，仍然是在现实之上所施加的结构(structure imposed upon reality)。
类似地，现实中的一首歌曲，看似简单平凡，实际上却拥有无穷维度，我们可以从任意一个角度去理解它，将它嵌入到任意一个向量空间。
假设用户手机的传感器捕捉了一段音频，将其上传到平台上，消重（时频图的mask特征）、指纹(时频图的landmark特征)、翻唱（浅层cnn）、哼唱（whisper大模型）、多模态大模型应用分别捕捉了这段音频的某种特征，产生了不同层次的知识（embedding），理解愈发深邃，愈发不精确，应用也就愈发灵活，愈发贴近人类思维。
后端的消重服务或指纹服务将表示音频的信号采样数据通过fft计算得到时频图，再基于时频图提取某种mask特征或landmark特征，用于直接刻画其能量分布特征，然后与音频特征库里存着的现有特征做对比就可以找到匹配的音频，这就是音频消重和指纹识别的过程：对信号做简单数学处理，生成的就是高度特化、具体、紧凑、确定、可解释的知识，表示这种知识的embedding或不具有抗噪性（mask特征），或具有少许抗噪性（landmark特征），但无论如何都只适用于原曲匹配，不知何为翻唱，何为remix，何为二创，何为串烧。
翻唱识别服务能通过一个学习了歌曲旋律的神经网络对音频信号进行处理，产出一定程度上表达旋律特征的embedding，将曲库里的翻唱embedding存入向量数据库，通过KNN/ANN召回的结果往往不限于原曲，还能容忍变调、音色上的改变。
哼唱识别服务可以基于Whisper这样的音频大模型实现，这种深层网络学习了人类的自然语言，因此对音频信号处理时，提取了表达歌词信息的embedding。基于这样的embedding，可以在跑调非常严重，且没有伴奏的情况下，依然能识别出用户在哼什么歌。
考虑未来的多模态大模型，更是可以直接将embedding存在transformer的ffn层，不必借助外部的向量数据库或倒排索引，直接在模型内部刻画embedding之间的复杂关系，让更多概念之间产生关联：识别出歌曲的意境、歌唱者的情绪、歌词内容的图片，甚至联系用户交互的上下文，衍生出更客制化、场景相关的输出。
人工智能领域用transformer简单搭建的大模型拥有出人意料的逻辑能力，昭示着人类意识的本质或许原本就极端肤浅：将知识内化、知识关联后，用天生的脑回路做贝叶斯推理，就产生了逻辑，诚然现有的结构远不如人脑高效，但有和无的界限已经被突破。
足够好的计算神经网络可以被视作神经科学的涌现模型。 一如往昔。 一如神经科学是细胞生物学的涌现。 一如细胞生物学是分子生物学的涌现。 一如分子生物学是物理化学的涌现。 一如物理化学是量子物理的涌现。</description>
      <content>&lt;p&gt;现实有无限维度，现实的背后还有潜在现实，结构的边缘会涌现出新的深层结构。&lt;/p&gt;
&lt;p&gt;人类对现实的感知、认知、理解，本质上是将高维现实投射到一个低维表示上，这种低维表示在统计学和机器学习中被称为embedding（嵌入），没错，知识就是将现实在人类有限的认知空间的一个嵌入，紧凑、浅薄且片面。&lt;/p&gt;
&lt;p&gt;举个具体的例子，人眼观察到的彩虹呈现七色。&lt;/p&gt;
&lt;p&gt;但这只是对无限广博的现实的最初观察，是最直观的认知。&lt;/p&gt;
&lt;p&gt;人类通过对人体生理学的研究，发现七色感知是一个更深层的潜在现实的涌现结构：多种光感受器的叠加反应。&lt;/p&gt;
&lt;p&gt;人类颜色感知系统对红绿蓝敏感的三种光感受器，叠加反应产生的三个峰和四个谷决定了人类能感知到7种颜色。
&lt;img src=&#34;https://cmbbq.github.io/img/color.png&#34; alt=&#34;color&#34;&gt;&lt;/p&gt;
&lt;p&gt;鸟类由于拥有对紫外线敏感的光感受器，就能感知到9种颜色，因此可以看到九色彩虹。&lt;/p&gt;
&lt;p&gt;无论人类的七色彩虹还是鸟类的九色彩虹，都只是知识（或者说embedding），而不是现实。&lt;/p&gt;
&lt;p&gt;人类通过数学工具和电磁学研究，进一步认识到光谱是连续的，包括无线电波、微波、热、红外线、可见光、紫外线等。&lt;/p&gt;
&lt;p&gt;这接近现实了吗？不，这依然只是浅薄的embedding。。&lt;/p&gt;
&lt;p&gt;人类通过量子物理学的研究，发现现有的电磁学依旧是一个更深层的潜在现实的涌现模型——电磁波是量子粒子流的一种表现形式：光子。&lt;/p&gt;
&lt;p&gt;光子的认知，折射出更复杂的现实，因为一个光子同时拥有无限的位置和频率，直到被观察到才能确定其位置或能量，需用海森堡不确定性原理、薛定谔方程来描述，这依旧不是真理，不是终极，仍然是在现实之上所施加的结构(structure imposed upon reality)。&lt;/p&gt;
&lt;p&gt;类似地，现实中的一首歌曲，看似简单平凡，实际上却拥有无穷维度，我们可以从任意一个角度去理解它，将它嵌入到任意一个向量空间。&lt;/p&gt;
&lt;p&gt;假设用户手机的传感器捕捉了一段音频，将其上传到平台上，消重（时频图的mask特征）、指纹(时频图的landmark特征)、翻唱（浅层cnn）、哼唱（whisper大模型）、多模态大模型应用分别捕捉了这段音频的某种特征，产生了不同层次的知识（embedding），理解愈发深邃，愈发不精确，应用也就愈发灵活，愈发贴近人类思维。&lt;/p&gt;
&lt;p&gt;后端的消重服务或指纹服务将表示音频的信号采样数据通过fft计算得到时频图，再基于时频图提取某种mask特征或landmark特征，用于直接刻画其能量分布特征，然后与音频特征库里存着的现有特征做对比就可以找到匹配的音频，这就是音频消重和指纹识别的过程：对信号做简单数学处理，生成的就是高度特化、具体、紧凑、确定、可解释的知识，表示这种知识的embedding或不具有抗噪性（mask特征），或具有少许抗噪性（landmark特征），但无论如何都只适用于原曲匹配，不知何为翻唱，何为remix，何为二创，何为串烧。&lt;/p&gt;
&lt;p&gt;翻唱识别服务能通过一个学习了歌曲旋律的神经网络对音频信号进行处理，产出一定程度上表达旋律特征的embedding，将曲库里的翻唱embedding存入向量数据库，通过KNN/ANN召回的结果往往不限于原曲，还能容忍变调、音色上的改变。&lt;/p&gt;
&lt;p&gt;哼唱识别服务可以基于Whisper这样的音频大模型实现，这种深层网络学习了人类的自然语言，因此对音频信号处理时，提取了表达歌词信息的embedding。基于这样的embedding，可以在跑调非常严重，且没有伴奏的情况下，依然能识别出用户在哼什么歌。&lt;/p&gt;
&lt;p&gt;考虑未来的多模态大模型，更是可以直接将embedding存在transformer的ffn层，不必借助外部的向量数据库或倒排索引，直接在模型内部刻画embedding之间的复杂关系，让更多概念之间产生关联：识别出歌曲的意境、歌唱者的情绪、歌词内容的图片，甚至联系用户交互的上下文，衍生出更客制化、场景相关的输出。&lt;/p&gt;
&lt;p&gt;人工智能领域用transformer简单搭建的大模型拥有出人意料的逻辑能力，昭示着人类意识的本质或许原本就极端肤浅：将知识内化、知识关联后，用天生的脑回路做贝叶斯推理，就产生了逻辑，诚然现有的结构远不如人脑高效，但有和无的界限已经被突破。&lt;/p&gt;
&lt;p&gt;足够好的计算神经网络可以被视作神经科学的涌现模型。
一如往昔。
一如神经科学是细胞生物学的涌现。
一如细胞生物学是分子生物学的涌现。
一如分子生物学是物理化学的涌现。
一如物理化学是量子物理的涌现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/em.png&#34; alt=&#34;em&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Federated Learning</title>
      <link>https://cmbbq.github.io/posts/federated-learning/</link>
      <pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/federated-learning/</guid>
      <description>联邦学习 联邦学习由McMahan于2016年提出，指的是许多移动设备在一个中央服务器的编排下协作训练模型，保持训练数据离散，避免对用户数据进行收集，仅将客户端模型更新上传中央服务器汇总成新的全局模型的机器学习模式。
联邦学习的naive实现如下：
中央服务器选定一些clients，让这些client下载模型。 每个client根据自己的数据计算更新。 每个client将更新（即新的完整模型）上传到中央服务器。 中央服务器用某种方式（比如取平均）聚合这些模型得到一个全局模型。 应对上传模型开销大的问题 大量文献对联邦学习进行了讨论。Federated Learning: Strategies For Improving Communication Efficiency指出联邦学习的naive实现的第3步很容易出现通信瓶颈，这是首当其冲的问题，提出了降低上行链路通信成本的两种方法：结构化更新，缩略更新。
联邦学习的问题可以被形式化地表述为：学习模型中的参数。全连接层的参数可用一个实数矩阵（这只是为了简化问题，所以讨论单个矩阵）表示 W ∈ R^(d1×d2)，shape为(#input × #output)，d1和d2表示输出维度和输入维度。卷积层kernel是4d tensor(#input × width × height × #output) ，需reshape到 (#input × width × height) × #output 。
用W(t)表示当前回合(t)的模型，W(i,t)表示本地更新后的模型，所谓更新就是H(i,t) = W(i,t) - W(t)。中央服务器可聚合得到新模型：W(t+1) = W(t) + η(t)H(t)，其中H(t) = Sum(H(i,t))/N，η(t)是learning rate。
结构化更新 结构化更新是指直接从一个用少量变量参数化的受限空间里学习更新，而不是学习整个模型的更新。
所谓结构化更新，也就是impose structure on updates，论文里提出两种结构，一种是低秩矩阵，另一种是随机掩码。
在结构化更新之低秩矩阵方法中，本地模型的更新H(i,t)必须是一个rank &amp;lt; k的低秩矩阵，这里k是一个固定的数字。将H(i,t)表示为H(i,t)=A(i,t)B(i,t)，其中A(i,t)∈ R^(d1×k)，B(i,t)∈ R^(k×d2)，在后续计算中，随机生成一个A(i,t)视为常数，然后优化B(i,t)。这样A的数据就不必上传，可以坍塌成一个随机种子，只需上传B(i,t)。这种优化，本质上是利用降维的压缩技术，使用矩阵将原始数据进行降维处理，然后使用重构矩阵将降维后的数据重新构建为原始数据。 A(i,t)每回合都为每个client随机生成一次。立刻就得到d1/k的上传开销减少。固定B，训练A，或者同时训练A和B都试过，效果不如固定A，训练B。对此的解释是A可视作重构矩阵（从变换后的向量中重新构建出原始向量），B是投影矩阵（将一个向量投影到另一个向量的子空间上）。固定A训练B实际上相当于解决这样一个问题：给定一个随机的重构矩阵，什么样的投影矩阵可以恢复最多的信息？
在结构化更新之随机掩码方法中，本地模型的更新H(i,t)必须是用某个预定义的随机掩码生成的稀疏矩阵。同样是每回合对每个client重新生成一次。稀疏掩码可以坍塌成一个随机种子，因此只需上传H(i,t)的非零值和种子。
缩略更新 缩略(sketched)更新，学习一个完整模型更新，学成之后，再通过有损的量化、随机旋转、子采样将其压缩后再发给服务器。
量化：将权重做概率量化，用更小的标量类型充当原始权重的unbiased estimator。
随机旋转：其实就是用一个随机正交矩阵乘一下，防止大部分数据都是0的情形，导致量化效果差。
子采样：原本上传的是H(i,t)，subsampling之后只上传一个随机子集。
完全去中心化联邦学习在各个领域面临的挑战 Advances and Open Problems in Federated Learning中讨论了完全去中心化联邦学习在算法、隐私、安全、工程等个层面的挑战:</description>
      <content>&lt;h1 id=&#34;联邦学习&#34;&gt;联邦学习&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.05629.pdf&#34;&gt;联邦学习&lt;/a&gt;由McMahan于2016年提出，指的是许多移动设备在一个中央服务器的编排下协作训练模型，保持训练数据离散，避免对用户数据进行收集，仅将客户端模型更新上传中央服务器汇总成新的全局模型的机器学习模式。&lt;/p&gt;
&lt;p&gt;联邦学习的naive实现如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;中央服务器选定一些clients，让这些client下载模型。&lt;/li&gt;
&lt;li&gt;每个client根据自己的数据计算更新。&lt;/li&gt;
&lt;li&gt;每个client将更新（即新的完整模型）上传到中央服务器。&lt;/li&gt;
&lt;li&gt;中央服务器用某种方式（比如取平均）聚合这些模型得到一个全局模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;应对上传模型开销大的问题&#34;&gt;应对上传模型开销大的问题&lt;/h2&gt;
&lt;p&gt;大量文献对联邦学习进行了讨论。&lt;a href=&#34;https://arxiv.org/pdf/1610.05492.pdf&#34;&gt;Federated Learning: Strategies For Improving Communication Efficiency&lt;/a&gt;指出联邦学习的naive实现的第3步很容易出现通信瓶颈，这是首当其冲的问题，提出了降低上行链路通信成本的两种方法：结构化更新，缩略更新。&lt;/p&gt;
&lt;p&gt;联邦学习的问题可以被形式化地表述为：学习模型中的参数。全连接层的参数可用一个实数矩阵（这只是为了简化问题，所以讨论单个矩阵）表示 W ∈ R^(d1×d2)，shape为(#input × #output)，d1和d2表示输出维度和输入维度。卷积层kernel是4d tensor(#input × width × height × #output) ，需reshape到 (#input × width × height) × #output 。&lt;/p&gt;
&lt;p&gt;用W(t)表示当前回合(t)的模型，W(i,t)表示本地更新后的模型，所谓更新就是H(i,t) = W(i,t) - W(t)。中央服务器可聚合得到新模型：W(t+1) = W(t) + η(t)H(t)，其中H(t) = Sum(H(i,t))/N，η(t)是learning rate。&lt;/p&gt;
&lt;h3 id=&#34;结构化更新&#34;&gt;结构化更新&lt;/h3&gt;
&lt;p&gt;结构化更新是指直接从一个用少量变量参数化的受限空间里学习更新，而不是学习整个模型的更新。&lt;/p&gt;
&lt;p&gt;所谓结构化更新，也就是impose structure on updates，论文里提出两种结构，一种是低秩矩阵，另一种是随机掩码。&lt;/p&gt;
&lt;p&gt;在结构化更新之低秩矩阵方法中，本地模型的更新H(i,t)必须是一个rank &amp;lt; k的低秩矩阵，这里k是一个固定的数字。将H(i,t)表示为H(i,t)=A(i,t)B(i,t)，其中A(i,t)∈ R^(d1×k)，B(i,t)∈ R^(k×d2)，在后续计算中，随机生成一个A(i,t)视为常数，然后优化B(i,t)。这样A的数据就不必上传，可以坍塌成一个随机种子，只需上传B(i,t)。这种优化，本质上是利用降维的压缩技术，使用矩阵将原始数据进行降维处理，然后使用重构矩阵将降维后的数据重新构建为原始数据。
A(i,t)每回合都为每个client随机生成一次。立刻就得到d1/k的上传开销减少。固定B，训练A，或者同时训练A和B都试过，效果不如固定A，训练B。对此的解释是A可视作重构矩阵（从变换后的向量中重新构建出原始向量），B是投影矩阵（将一个向量投影到另一个向量的子空间上）。固定A训练B实际上相当于解决这样一个问题：给定一个随机的重构矩阵，什么样的投影矩阵可以恢复最多的信息？&lt;/p&gt;
&lt;p&gt;在结构化更新之随机掩码方法中，本地模型的更新H(i,t)必须是用某个预定义的随机掩码生成的稀疏矩阵。同样是每回合对每个client重新生成一次。稀疏掩码可以坍塌成一个随机种子，因此只需上传H(i,t)的非零值和种子。&lt;/p&gt;
&lt;h3 id=&#34;缩略更新&#34;&gt;缩略更新&lt;/h3&gt;
&lt;p&gt;缩略(sketched)更新，学习一个完整模型更新，学成之后，再通过有损的量化、随机旋转、子采样将其压缩后再发给服务器。&lt;/p&gt;
&lt;p&gt;量化：将权重做概率量化，用更小的标量类型充当原始权重的unbiased estimator。&lt;/p&gt;
&lt;p&gt;随机旋转：其实就是用一个随机正交矩阵乘一下，防止大部分数据都是0的情形，导致量化效果差。&lt;/p&gt;
&lt;p&gt;子采样：原本上传的是H(i,t)，subsampling之后只上传一个随机子集。&lt;/p&gt;
&lt;h2 id=&#34;完全去中心化联邦学习在各个领域面临的挑战&#34;&gt;完全去中心化联邦学习在各个领域面临的挑战&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1912.04977.pdf&#34;&gt;Advances and Open Problems in Federated Learning&lt;/a&gt;中讨论了完全去中心化联邦学习在算法、隐私、安全、工程等个层面的挑战:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;中央服务器可能成为瓶颈和单点故障风险点。由此产生p2p/完全去中心化的设计思路。&lt;/li&gt;
&lt;li&gt;完全去中心化的算法需应对client可用性和网络稳定性的局限。&lt;/li&gt;
&lt;li&gt;设计一个试图达到最快收敛速度的模型平均策略是困难的。&lt;/li&gt;
&lt;li&gt;去中心化的场景会导致算法易受恶意攻击、不可靠数据或标注的威胁。&lt;/li&gt;
&lt;li&gt;client的通信带宽和电量不足，，将已有的压缩算法移植到移动端比较困难。&lt;/li&gt;
&lt;li&gt;隐私问题：如何防止一个client重建另一个client的隐私数据。&lt;/li&gt;
&lt;li&gt;如何实现的问题：区块链作为分布式账簿本质上是一个最终一致的复制状态机。不过以太坊这样的区块链上的数据是公开的，如果要适用于联邦学习，还需要改造。&lt;/li&gt;
&lt;li&gt;cross-silo场景（多个组织或公司一起训一个模型，但数据不能直接共享，比如多个银行一起训一个fraud detection模型）对数据进行分区，并增加incentive机制。&lt;/li&gt;
&lt;li&gt;通信和压缩瓶颈。&lt;/li&gt;
&lt;li&gt;公平性：联邦学习引入了新的bias来源——设备型号、地理位置、活动模式、本地数据集大小等。&lt;/li&gt;
&lt;li&gt;安全性计算问题：如何应对恶意服务器？如何应对外部攻击？&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;non-iid数据分布问题&#34;&gt;Non-IID数据分布问题&lt;/h3&gt;
&lt;p&gt;Non-IID(independent &amp;amp; identically distributed) data问题：样本的统计属性没有均匀分布，对于任何client-partitioned数据集来说都是常见的。&lt;/p&gt;
&lt;p&gt;论文中给出了多种non-identical client分布（考虑对特征x，标签y进行有监督学习，(x,y)~Pi(x,y)即为client i的本地分布，P(x,y) = P(y|x)P(x) = P(x|y)P(y)）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature distribution skew(covariate shift)，即不同client的P(y|x)相同，但特征的边际分布P(x)不同。比如笔迹识别中不同用户写相同的字，但笔法、书写习惯还是不同。&lt;/li&gt;
&lt;li&gt;Label distribution skew(prior probability shift)，即不同client的P(x|y)相同，但标签的边际分布P(y)不同。比如澳洲的日常动物识别应用，标签里就会频繁出现袋鼠，其他地区则不会。&lt;/li&gt;
&lt;li&gt;Same label, different features(concept drift)，即不同client的P(y)相同，但条件分布P(x|y)不同，相同标签在不同client上对应到了不同的特征。比如豪宅，在香港和加州尺度是不同的。&lt;/li&gt;
&lt;li&gt;Same features, different label(concept shift)，即不同client的P(x)相同，但条件分布P(y|x)不同，相同特征被标注成立不同标签。比如有人把熊猫标注为宠物，也有人把熊猫标注为猛兽。&lt;/li&gt;
&lt;li&gt;Quantity skew or unbalancedness，不同的client的数据量差异大。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;违反independence同样常见，因为client的分布很容易收到触发训练的约束条件的影响：比如很多训练是利用夜间睡眠时间跑的，那就导致往往一个经度的地区的clients更容易遇到一起。&lt;/p&gt;
&lt;p&gt;应对Non-IID数据，一个可行的方法是用一个不涉及隐私数据的全局共享小数据集做数据增强。此外，还可以限制一个用户每天能做的贡献上限，避免量上面的不平衡。此外，某些场景还可以把Non-IID从bug转化为feature，就训一个本地客制化的专用模型出来，提供个性化服务，而不是最终产生一个全局模型。文章后面还介绍了Non-IID数据集上的优化算法和收敛速率。&lt;/p&gt;
&lt;h3 id=&#34;应对隐私问题split-learning&#34;&gt;应对隐私问题：Split Learning&lt;/h3&gt;
&lt;p&gt;Split Learning是模型执行路径层面的横切，同时适用于训练和推理：最简单场景是让每个client一直前向算到某个特定的cut layer停下，将cut layer的输出（smashed data）传给中央服务器或peer接着算，于是就完成了无需数据共享就实现的前向传播。类似地，梯度反向传播是从最后一层到cut layer停下，仅将cut layer的梯度回传给client。这样整个过程中其他节点都不会直接访问本地数据。&lt;/p&gt;
&lt;p&gt;考虑到cut layer的权重本身也能一定程度上反映底层的数据现实，Split Learning是否能提供形式化的隐私承诺仍然是个开放问题。&lt;/p&gt;
&lt;h2 id=&#34;应对通信延迟高的问题&#34;&gt;应对通信延迟高的问题&lt;/h2&gt;
&lt;p&gt;还有一个关键问题——高通信延迟，由于无线和长距离传输的特性而无法回避，但在之前的论文中没被很好地address。&lt;a href=&#34;https://dga.hanlab.ai/assets/neurips21_dga.pdf&#34;&gt;Delayed Gradient Averaging: Tolerate the
Communication Latency in Federated Learning&lt;/a&gt;一文提出了一种延迟进行梯度平均的算法，用16节点是树莓派集群模拟现实世界中的移动节点和无线网络环境做了个实验，经验性地证明应用延迟梯度平均可以使联邦学习过程容忍高网络延迟，同时还不牺牲准确度。&lt;/p&gt;
&lt;p&gt;In-center环境下，同一个机柜的延迟&amp;lt;1us，同机房则是ms级别。无线环境大概是20ms，跨洋连接则至少100ms。在解决带宽问题后，延迟就成为最大瓶颈。这篇论文提出的DGA(Delayed Gradient Aggregation)算法的核心思路是延迟梯度平均到未来的某个迭代，即模型更新时接收过时的平均梯度，从而允许通信和计算流水线化。论文将问题形式化为：最小化随机函数的和。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/DGA.png&#34; alt=&#34;DGA&#34;&gt;&lt;/p&gt;
&lt;p&gt;N表示client数量，fi表示client i的stochastic损失函数。随机变量ζi关联一个mini-batch样本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/DGA2.png&#34; alt=&#34;DGA2&#34;&gt;&lt;/p&gt;
&lt;p&gt;算法的主要思路是允许averaging通信过程中做本地更新（averaging通信和本地更新并行，）。FedAvg中clients在每轮结束发送参数到彼此，等averaging结束再开启下一轮。DGA里把averaging barrier延迟到了后续迭代(iteration，指的是本地更新的迭代)。因此clients可以立刻开启下一轮(round，指的是最外层循环，即一轮更新)。第一轮下收到外部信息时迭代已经发生了D次，延迟了D个迭代后进行梯度修正。理想情况下不存在通信延迟，D=0时，DGA恢复成最初的FedAvg。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;clients在第t轮彼此发更新。&lt;/li&gt;
&lt;li&gt;clients在本地更新后继续用最新的本地参数继续本地更新。(averaging通信延迟 &amp;gt; 单次甚至若干次本地更新)&lt;/li&gt;
&lt;li&gt;当其他client的第t轮信息到达，则client已进行了D次额外本地更新。&lt;/li&gt;
&lt;li&gt;将本地t轮梯度替换为接受到的平均梯度。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在最宽泛的场景下（延迟极高），延迟梯度可能要几个轮次之后才能抵达。这就需要将延迟参数D表示为D = sK + r，其中s&amp;gt;=0, r &amp;lt;=K。DGA仍能保证不同client只在最近D个梯度上是不同的，t-D轮之前的梯度都是一样的。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>On Transparency</title>
      <link>https://cmbbq.github.io/posts/on-transparency/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-transparency/</guid>
      <description>透明度是软件工程中长期被忽略的理想属性，是目前互联网行业技术管理体系中的薄弱环节。本文的透明度主要是指组件内部实现对研发团队的可见度、可解释性和可掌控程度，或者说白盒指数。
一个项目自上而下依赖的第三方黑盒越少，各个抽象层次上诸多组件对研发团队就越是透明的。高透明度意味着高度可维护，高度hackable，随时可拆卸，定位到任何抽象层次上存在性能瓶颈都能毫无桎梏地zoom in，然后修改、重构。习惯性地使用第三方依赖，哪怕这个依赖是得到广泛应用，甚至接近行业标准的高声望项目，依然会注入实现需求不完全匹配的风险。
高性能计算的一个重要启发式设计原则(heuristic)即特化(specialization)，特化显然包括软件特化——基于新的真实需求创造最直接贴合需求的新解决方案(direct solutions to the needs)，即造轮。
国内互联网同行们对造轮持过分谨慎态度，将其视为anti-pattern，遇到问题时立刻开始技术选型，对使用第三方依赖形成了不假思索的路径依赖。缺乏洞察问题的新颖性和独特性的敏锐度和饥渴，自然故步自封于技术选型，而无法做到真正的技术创新——计算系统的创新是根植于真实需求，对非结构化的现实需求施加结构，创造新的计算和存储形态。就以全文检索为例，美团外卖搜索用ES，微信聊天搜索用SQLite，都是没有选择自研高性能检索引擎，导致业务场景和经典解决方案出现错配，后期遇到了性能瓶颈后的所谓优化方案也只不过是对第三方库进行魔改，让它更贴合原本的需求罢了。比如lucene压根就不是in-memory索引，以至于外卖店家商品搜索这么小规模的场景都不能保证实时性。再说SQLite，默认当然不支持拆表+并行搜索，针对大库做简单的拆表和scatter-gather并行搜索提升性能其实是理所当然的解决方案。
如此视造轮为畏途实让人唏嘘，在性能上碰壁后，再钻进别人的故纸堆中修修补补，浪费的精力，产生额外的痛苦，远远超过当初用C++/Rust实现一个小而美的in-memory search index(例如pisa)。自研检索引擎用极少的代码量就可以击败lucene、sqlite、postgres，也可以很轻松地支持多线程实时索引更新，而不必牺牲索引性能，还可以针对现代物理机进行深度cache优化、减少核间通信和远端内存访问，将现代硬件的性能在检索这种访存密集应用上发挥到极致。</description>
      <content>&lt;p&gt;透明度是软件工程中长期被忽略的理想属性，是目前互联网行业技术管理体系中的薄弱环节。本文的透明度主要是指组件内部实现对研发团队的可见度、可解释性和可掌控程度，或者说&lt;a href=&#34;https://en.wikipedia.org/wiki/White_box_(software_engineering)&#34;&gt;白盒&lt;/a&gt;指数。&lt;/p&gt;
&lt;p&gt;一个项目自上而下依赖的第三方黑盒越少，各个抽象层次上诸多组件对研发团队就越是透明的。高透明度意味着高度可维护，高度hackable，随时可拆卸，定位到任何抽象层次上存在性能瓶颈都能毫无桎梏地zoom in，然后修改、重构。习惯性地使用第三方依赖，哪怕这个依赖是得到广泛应用，甚至接近行业标准的高声望项目，依然会注入实现需求不完全匹配的风险。&lt;/p&gt;
&lt;p&gt;高性能计算的一个重要启发式设计原则(heuristic)即特化(specialization)，特化显然包括软件特化——基于新的真实需求创造最直接贴合需求的新解决方案(direct solutions to the needs)，即造轮。&lt;/p&gt;
&lt;p&gt;国内互联网同行们对造轮持过分谨慎态度，将其视为anti-pattern，遇到问题时立刻开始技术选型，对使用第三方依赖形成了不假思索的路径依赖。缺乏洞察问题的新颖性和独特性的敏锐度和饥渴，自然故步自封于技术选型，而无法做到真正的技术创新——计算系统的创新是根植于真实需求，对非结构化的现实需求施加结构，创造新的计算和存储形态。就以全文检索为例，&lt;a href=&#34;https://tech.meituan.com/2022/11/17/elasicsearch-optimization-practice-based-on-run-length-encoding.html&#34;&gt;美团外卖搜索用ES&lt;/a&gt;，&lt;a href=&#34;https://zhuanlan.zhihu.com/p/608082104&#34;&gt;微信聊天搜索用SQLite&lt;/a&gt;，都是没有选择自研高性能检索引擎，导致业务场景和经典解决方案出现错配，后期遇到了性能瓶颈后的所谓优化方案也只不过是对第三方库进行魔改，让它更贴合原本的需求罢了。比如lucene压根就不是in-memory索引，以至于外卖店家商品搜索这么小规模的场景都不能保证实时性。再说SQLite，默认当然不支持拆表+并行搜索，针对大库做简单的拆表和scatter-gather并行搜索提升性能其实是理所当然的解决方案。&lt;/p&gt;
&lt;p&gt;如此视造轮为畏途实让人唏嘘，在性能上碰壁后，再钻进别人的故纸堆中修修补补，浪费的精力，产生额外的痛苦，远远超过当初用C++/Rust实现一个小而美的in-memory search index(例如&lt;a href=&#34;https://github.com/pisa-engine/pisa&#34;&gt;pisa&lt;/a&gt;)。自研检索引擎用极少的代码量就可以击败lucene、sqlite、postgres，也可以很轻松地支持多线程实时索引更新，而不必牺牲索引性能，还可以针对现代物理机进行深度cache优化、减少核间通信和远端内存访问，将现代硬件的性能在检索这种访存密集应用上发挥到极致。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>String Lookups Reduce to Parsing</title>
      <link>https://cmbbq.github.io/posts/string-lookups-could-reduce-to-parsing/</link>
      <pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/string-lookups-could-reduce-to-parsing/</guid>
      <description>标题即结论：字符串查找问题可归约为解析问题。
这个结论源于近期一个有趣的观察：用ragel写高性能的ascii protocol parser，本质上是利用nfa转dfa提升性能，这和toplingdb中将同一层各个sst对应的trie（本质是dfa）合并成一个dfa(大量dfa-&amp;gt;nfa-&amp;gt;1dfa)的思路是同构的。
上述同构隐隐蕴含着一个reduction：字符串查找和字符串解析，本质都用尽可能紧凑的结构和高效的算法从字符流中抽取状态，lookups可以视作一类特殊（且模式相当规则）的parsing。LSM key lookup更是比较特殊的海量索引的key range无overlap的场景，存在大量可以轻松合并的DFA。因此龙书中的大量DFA转NFA转DFA算法可以派上用场。
KV Store的in-memory key index，可以是红黑树，可以是skiplist，可以是hashmap，可以说patricia trie(一种radix tree变种)，也可以是toplingdb中的NestLoudsTrie。这种结构我们同样可以在路由表实现中看到。字符串索引，归根到底是字符串lookup结构。正如路由表实现可以通过把所有routes合并到一个DFA里（很多routes都包含regex），kv数据库也把Trie这种特殊的dfa（Trie的状态转移图是树，树是一种无向图，多了个任意两结点只由一个边连接的约束）做多索引合并，每个索引对应的key range还不重叠（LSM特性），因此合并速度非常快，合并后的DFA表示起来也简单、紧凑，详见自动机算法在数据库索引中的应用，我在作者的文章下面追问了一下DFA合并的触发条件和DFA合并开销，作者的答复是compaction/flush时触发，在整个lsm更新过程中占比很小，也不涉及多线程，无需考虑线程安全。</description>
      <content>&lt;p&gt;标题即结论：字符串查找问题可归约为解析问题。&lt;/p&gt;
&lt;p&gt;这个结论源于近期一个有趣的观察：用ragel写高性能的ascii protocol parser，本质上是利用&lt;a href=&#34;https://en.wikipedia.org/wiki/Nondeterministic_finite_automata&#34;&gt;nfa&lt;/a&gt;转&lt;a href=&#34;https://en.wikipedia.org/wiki/Deterministic_finite_automaton&#34;&gt;dfa&lt;/a&gt;提升性能，这和&lt;a href=&#34;https://github.com/topling/toplingdb&#34;&gt;toplingdb&lt;/a&gt;中将同一层各个sst对应的trie（本质是dfa）合并成一个dfa(大量dfa-&amp;gt;nfa-&amp;gt;1dfa)的思路是同构的。&lt;/p&gt;
&lt;p&gt;上述同构隐隐蕴含着一个reduction：字符串查找和字符串解析，本质都用尽可能紧凑的结构和高效的算法从字符流中抽取状态，lookups可以视作一类特殊（且模式相当规则）的parsing。LSM key lookup更是比较特殊的海量索引的key range无overlap的场景，存在大量可以轻松合并的DFA。因此龙书中的大量DFA转NFA转DFA算法可以派上用场。&lt;/p&gt;
&lt;p&gt;KV Store的in-memory key index，可以是红黑树，可以是skiplist，可以是hashmap，可以说patricia trie(一种&lt;a href=&#34;https://en.wikipedia.org/wiki/Radix_tree&#34;&gt;radix tree&lt;/a&gt;变种)，也可以是toplingdb中的NestLoudsTrie。这种结构我们同样可以在路由表实现中看到。字符串索引，归根到底是字符串lookup结构。正如路由表实现可以通过把所有routes合并到一个DFA里（很多routes都包含regex），kv数据库也把Trie这种特殊的dfa（Trie的状态转移图是树，树是一种无向图，多了个任意两结点只由一个边连接的约束）做多索引合并，每个索引对应的key range还不重叠（LSM特性），因此合并速度非常快，合并后的DFA表示起来也简单、紧凑，详见&lt;a href=&#34;https://zhuanlan.zhihu.com/p/628057993&#34;&gt;自动机算法在数据库索引中的应用&lt;/a&gt;，我在作者的文章下面追问了一下DFA合并的触发条件和DFA合并开销，作者的答复是compaction/flush时触发，在整个lsm更新过程中占比很小，也不涉及多线程，无需考虑线程安全。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Paradigms of Generic Programming: Archetype, Ducktype, Subtype</title>
      <link>https://cmbbq.github.io/posts/paradigms-of-generic-programming/</link>
      <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/paradigms-of-generic-programming/</guid>
      <description>泛型不等价于模板 什么是泛型编程？提起generic programming，很多人都会立刻想到template，但模板编程只是泛型编程的一种范式。最初发明&amp;quot;generic programming&amp;quot;这种说法的Alexander Stepanov反复强调过： generic programming is not about how to use template。第一个版本的STL尽管名字里带template，实际上是基于scheme实现的。
要泛型，就必须有规约 规约的制定和对规约的遵循是泛型编程的基础。 任何泛型编程都需要一定规约，有规约才能让一种抽象适配多个具体实体。没有规约，那写具体实现的人都不知道遵循什么规范，实现什么接口，满足什么条件，泛型编程自然就无从谈起。
规约：在JavaScript里是prototype；在Swift里是protocol；在Rust里是trait；在C++模板编程里是concept或者SFINAE。
遵循规约：在JavaScript里是运行时将具体对象关联到某个原型对象；在Swift里是用类似继承的冒号让具体类型conforms to某些protocol；在Rust里是用impl SomeTrait for SomeStruct语法显式地为某个类型提供某个trait的实现；在C++里模板实参无需特殊语法声明遵循模板形参，但要心照不宣地遵循模板代码的要求。
三种规约，三种范式 根据规约不同，可命名泛型编程的三种范式：Archetype, Ducktype, Subtype。
Archetype：以类型的类型(type-of-type)或者说协议(protocol)、接口(interface)为规约。
Rust trait: &amp;ldquo;defines shared behavior&amp;rdquo; Carbon interface: &amp;ldquo;defines an API that a given type can implement&amp;rdquo; Swift protocol: &amp;ldquo;defines a blueprint of methods, properties, and other requirements&amp;rdquo; C++ type erasure idiom: &amp;ldquo;captures the concept shared among all the concrete types&amp;rdquo; Ducktype：基于模板进行文本替换的结构化规约。
模板本质上就是编译期ducktyping，不能提前独立进行类型和语法检查，只有实例化之后才能做类型和语法检查，能鸭子叫，编译通过，叫不出鸭子叫，编译报错。 C++20中模板结构化规约是concept, 此前则是SFINAE技巧，或者干脆不成文：要么因循旧例(iterable的T一定要有begin和end)，要么心照不宣（自己写的代码，只有自己懂，无需对外公开规约）。 由于规约本身并非类型，无法用普通容器存储遵循规约的一系列具体类型，只能用类型推导的tuple-like容器。 Subtype: 以基类为规约。</description>
      <content>&lt;h2 id=&#34;泛型不等价于模板&#34;&gt;泛型不等价于模板&lt;/h2&gt;
&lt;p&gt;什么是泛型编程？提起generic programming，很多人都会立刻想到template，但模板编程只是泛型编程的一种范式。最初发明&amp;quot;generic programming&amp;quot;这种说法的Alexander Stepanov反复强调过：
generic programming is not about how to use template。第一个版本的STL尽管名字里带template，实际上是基于scheme实现的。&lt;/p&gt;
&lt;h2 id=&#34;要泛型就必须有规约&#34;&gt;要泛型，就必须有规约&lt;/h2&gt;
&lt;p&gt;规约的制定和对规约的遵循是泛型编程的基础。
任何泛型编程都需要一定规约，有规约才能让一种抽象适配多个具体实体。没有规约，那写具体实现的人都不知道遵循什么规范，实现什么接口，满足什么条件，泛型编程自然就无从谈起。&lt;/p&gt;
&lt;p&gt;规约：在JavaScript里是prototype；在Swift里是protocol；在Rust里是trait；在C++模板编程里是concept或者SFINAE。&lt;/p&gt;
&lt;p&gt;遵循规约：在JavaScript里是运行时将具体对象关联到某个原型对象；在Swift里是用类似继承的冒号让具体类型conforms to某些protocol；在Rust里是用impl SomeTrait for SomeStruct语法显式地为某个类型提供某个trait的实现；在C++里模板实参无需特殊语法声明遵循模板形参，但要心照不宣地遵循模板代码的要求。&lt;/p&gt;
&lt;h2 id=&#34;三种规约三种范式&#34;&gt;三种规约，三种范式&lt;/h2&gt;
&lt;p&gt;根据规约不同，可命名泛型编程的三种范式：Archetype, Ducktype, Subtype。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Archetype&lt;/strong&gt;：以类型的类型(type-of-type)或者说协议(protocol)、接口(interface)为规约。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://doc.rust-lang.org/book/ch10-02-traits.html&#34;&gt;Rust trait&lt;/a&gt;: &amp;ldquo;defines shared behavior&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/carbon-language/carbon-lang/blob/trunk/docs/design/generics/terminology.md#interface&#34;&gt;Carbon interface&lt;/a&gt;: &amp;ldquo;defines an API that a given type can implement&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.swift.org/swift-book/documentation/the-swift-programming-language/protocols/&#34;&gt;Swift protocol&lt;/a&gt;: &amp;ldquo;defines a blueprint of methods, properties, and other requirements&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://davekilian.com/cpp-type-erasure.html&#34;&gt;C++ type erasure idiom&lt;/a&gt;: &amp;ldquo;captures the concept shared among all the concrete types&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ducktype&lt;/strong&gt;：基于模板进行文本替换的结构化规约。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模板本质上就是编译期ducktyping，不能提前独立进行类型和语法检查，只有实例化之后才能做类型和语法检查，能鸭子叫，编译通过，叫不出鸭子叫，编译报错。&lt;/li&gt;
&lt;li&gt;C++20中模板结构化规约是concept, 此前则是SFINAE技巧，或者干脆不成文：要么因循旧例(iterable的T一定要有begin和end)，要么心照不宣（自己写的代码，只有自己懂，无需对外公开规约）。&lt;/li&gt;
&lt;li&gt;由于规约本身并非类型，无法用普通容器存储遵循规约的一系列具体类型，只能用类型推导的tuple-like容器。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Subtype&lt;/strong&gt;: 以基类为规约。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;子类系统是面向对象语言最普及的泛型编程范式，往往基于class hierarchy +『虚表存放函数指针』+『对象内置虚指针』或『callsite胖指针』实现。&lt;/li&gt;
&lt;li&gt;子类系统固然是简单自然的多态，但毕竟subtyping有一丢丢的性能开销，不是零成本抽象，而且难以非侵入式地让一个新接口适配已有代码。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本节命名的三种范式名称恰好都以type结尾，一方面是因为这样比较帅，另一方面是因为（对C++/Rust这种抽象层次的语言来说）编程本身就是在打造一个个类型，泛型编程就是在打造一个个类型规约+遵循规约的类型。Archetype和Subtype范式中，类型规约恰好就是一种抽象类型，所以这两种范式写起来更简单自然。Ducktype范式（C++模板）中，类型规约是结构化规约，可以是语言实体(concept)，也可以心照不宣，无论如何都不是类型。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Paradigms&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Archetype&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Ducktype&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Subtype&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;具体语言实例&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Swift protocol, Carbon interface, Rust trait&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;C++ template(constrained or not), Rust generic&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;C++/Java/Python class hierarchy&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;在哪里写泛型代码？&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;泛型函数&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;函数模板&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;子类方法&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;承载泛型代码的语言实体&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;普通函数，只不过是以规约类型为参数&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;模板文本&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;普通函数，不过函数指针被语言特性暗中与callsite指针绑定了&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;规约&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;定义一些类的方法或属性共性的协议类型&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;模板参数应符合的一组需求闭集，可以是成文约束Concept或SFINAE，也可以是不成文约束&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;面向对象语言继承图中的某个基类的虚函数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;承载规约的语言实体&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;类，类型擦除之后的共性类，类型的类型，本质上仍然是类型&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;对文本替换的placeholder的结构化约束。即使约束了，我们也无法对placeholder做类型/语法检查&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;被基类列入必要功能列表的函数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;遵循规约的语言实体&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;具体的类&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;用于模板实例化的模板参数&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;子类&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;何时做name lookup决议？&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;早绑定，允许独立编译&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;晚绑定，实例化时，不用的话就一直不绑定&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;早绑定，允许独立编译&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;何时做类型检查？&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;独立编译时&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;实例化后&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;独立编译时&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;是否允许支持动态绑定？&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;是&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;否&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;是&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;如何将已有泛型接口在新的类型上进行扩展？&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;允许基于已有类型创造一个数据表示兼容的新类型，只不过规约变了，允许它实现某个新的规约，或提供与原类型的已实现的某个规约提供不同的实现。&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;为新偏特化场景写新的函数模板即可扩展，可用Concept或SFINAE修订重载决议规则。也可以在函数模板里用某个约定好的函数名、成员变量名、关联类型作为客制点，新的类型只需实现这些客制点。&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;继承，子类数据表示可能会变&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;archetype范式&#34;&gt;Archetype范式&lt;/h2&gt;
&lt;p&gt;Archetype在Rust中是语言的核心机制——trait，要写好Rust的泛型代码，就要习惯于基于archetype编程。相比与DuckType、SubType，Archetype范式天然有一些优势。&lt;/p&gt;
&lt;h3 id=&#34;generic-code-is-just-normal-code&#34;&gt;Generic code is just normal code&lt;/h3&gt;
&lt;p&gt;和模板相比，&amp;ldquo;Generic code is just normal code&amp;quot;是Archetype范式最大优势，也是Rust语言相比C++的巨大优势。让非专家也能写出高度抽象，同时还零成本，具有高度可复用性的泛型代码。&lt;/p&gt;
&lt;p&gt;C++基于模板文本替换的泛型代码和普通类型的具体实现代码，在写法上有一定区别，编译链接也有区别。
这是因为C++中基于concept模板，其实是没办法提前编译、提前语法检查，只能在实例化之后再做语法检查。所以是一种难度相当高的编程范式，能用模板写C++库的一般是业界专家，普通人很难掌握，也没有足够动机去掌握。&lt;/p&gt;
&lt;p&gt;Rust基于trait的泛型代码则和普通类型的具体实现代码，在写法上基本没区别，编译链接方式也一样。
这是因为Rust中的trait是一种archetype，type-of-type，是规定一组类型应该具备什么接口的元类型，无论它多么抽象多么特殊，究其本质仍是一种类型。只要是类型，就可以单独提前编译，就可以被提前语法检查。&lt;/p&gt;
&lt;h3 id=&#34;adapting-erases-interoperability&#34;&gt;Adapting Erases Interoperability&lt;/h3&gt;
&lt;p&gt;和继承相比，&amp;ldquo;Adapting rather than extending a type&amp;quot;是Archetype范式的优势之一。
Subtype范式在实践中不能保证所有类型继承自同一个基类，比如说对第三方的代码没有控制权，或者说这个类型不是个class，而是int, float这种内置类型。
Archetype范式中不仅可以为自己的类型提供多种Archetype adaption，或使自己的代码遵循第三方Archetype，还可以为第三方类型提供自己的Archetype实现——前两点还好，这最后一点是Subtype范式做不到的，只能加个丑陋的wrapper，不仅工作量特别大，而且容易出错。&lt;/p&gt;
&lt;p&gt;有人会问，允许修改已有的类型是不是比较危险？这是一种误解。继承是修改，因此是危险的。Adapt（或者说override，newtype）其实不是修改，而是新增。
继承改变了类型的数据表示，Adapt机制则不改变类型的数据表示，只为其新增接口——换一种说法，override Archetype for T机制实际上是为已有类型T新建了一个遵循规约Archetype的入口。&lt;/p&gt;
&lt;h3 id=&#34;archetypes-in-c&#34;&gt;Archetypes in C++&lt;/h3&gt;
&lt;p&gt;有些人会argue，C++无所不能，的确C++也可以实现archetype范式，比如std::function，以及其他类型擦除。但是基于现有语法写出来的泛型代码和普通代码之间还是没那么像。One has to drastically change the programming style in order to &amp;ldquo;go generic&amp;rdquo;. 实现archetype范式在C++中相对困难。但即使如此，archetype范式的固有优势还是让某些标准或准标准选择了它，比如std::function, std::any, boost::any_range。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>On NCO</title>
      <link>https://cmbbq.github.io/posts/on-nco/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-nco/</guid>
      <description>凸优化收敛时间一般是polynomial的，线性规划和最小二乘就是凸优化的特例。
非凸优化non-convex optimization是一种至少np-hard的问题，不存在通用解法。想要确定问题是否有解，局部最优是否全局最优，或目标函数是否有界都会随着变量和约束数目指数爆炸blow up，局部优化手段对算法参数敏感，又高度依赖initial guess，这使局部非凸优化more-art-than-technology，相比而言线性规划是毫无art可言的。
深度神经网络作为通用函数拟合器，最重要的作用是拟合非凸函数，因为复杂问题一般不可以用凸函数拟合。ChatGPT这类生成式模型就是对target和input间互信息的非凸优化。怎么训好模型，目前依然是一个art。
随机梯度下降stochastic gradient descent(SGD)被证明可以收敛于凸函数、可微和利普希茨连续函数，但还不能确定在非凸函数上的效果，SGD收敛缓慢，还不一定达到局部最优，更不一定达到全局最优。如果选择一个足够靠近全局最优的点，或许可以用SGD收敛到全局最优，但这一方面耗时间，另一方面只适用于特殊场景。对于深度神经网络来说，一旦陷入错误的局部最优，就要用不同的初始化配置或加入额外的梯度更新噪音。如果遇到鞍点，则需找到海森矩阵或计算下降方向。如果陷入低梯度区域，则需batchnorm，或使用relu做激活函数。如果因高曲率而使得steps过大，则应使用adaptive step size或限制梯度step尺度。此外，如果超参有问题，还需要用各种超参优化的方法。总之，目前深度学习的NCO还是处于art的阶段。</description>
      <content>&lt;p&gt;凸优化收敛时间一般是polynomial的，线性规划和最小二乘就是凸优化的特例。&lt;/p&gt;
&lt;p&gt;非凸优化non-convex optimization是一种至少np-hard的问题，不存在通用解法。想要确定问题是否有解，局部最优是否全局最优，或目标函数是否有界都会随着变量和约束数目指数爆炸blow up，局部优化手段对算法参数敏感，又高度依赖initial guess，这使局部非凸优化more-art-than-technology，相比而言线性规划是毫无art可言的。&lt;/p&gt;
&lt;p&gt;深度神经网络作为通用函数拟合器，最重要的作用是拟合非凸函数，因为复杂问题一般不可以用凸函数拟合。ChatGPT这类生成式模型就是对target和input间互信息的非凸优化。怎么训好模型，目前依然是一个art。&lt;/p&gt;
&lt;p&gt;随机梯度下降stochastic gradient descent(SGD)被证明可以收敛于凸函数、可微和利普希茨连续函数，但还不能确定在非凸函数上的效果，SGD收敛缓慢，还不一定达到局部最优，更不一定达到全局最优。如果选择一个足够靠近全局最优的点，或许可以用SGD收敛到全局最优，但这一方面耗时间，另一方面只适用于特殊场景。对于深度神经网络来说，一旦陷入错误的局部最优，就要用不同的初始化配置或加入额外的梯度更新噪音。如果遇到鞍点，则需找到海森矩阵或计算下降方向。如果陷入低梯度区域，则需batchnorm，或使用relu做激活函数。如果因高曲率而使得steps过大，则应使用adaptive step size或限制梯度step尺度。此外，如果超参有问题，还需要用各种超参优化的方法。总之，目前深度学习的NCO还是处于art的阶段。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>A Decade of Tussle between CPU and GPU</title>
      <link>https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/</guid>
      <description>GPU和CPU方法的边界何在？ 做AI应用的架构工作，遇到新的计算密集任务的第一问往往是：这应该上GPU，还是NUMA物理机？
这个问题可以归约为On the Limits of GPU Acceleration(2010)中提出的问题：&amp;ldquo;大致相同能耗的前提下，能和不能用GPU有效加速计算的边界在哪？&amp;rdquo; Vuduc的这个研究分析了3种具有代表性复杂行为和访存不规则性的计算：稀疏线性系统的迭代求解器、稀疏矩阵的乔列斯基分解、快速多极子算法，得出的结论是——大致地说，良好优化的GPU实现在相同能耗下和良好优化的CPU实现在性能上是相仿的。可见当年用GPGPU加速还是一个普遍得不偿失的工作，毕竟要付出剧烈变更编程模型的代价。
GPU和CPU的价格-性能趋势 上述结论在2010年成立，如今13年过去了，GPU固然飞速发展（无论是硬件、软件，还是生态、应用、资金投入），但GPU发展速度是否已经甩开CPU了呢？
摩尔定律是两年翻倍，而黄氏定律则是宣称通过软硬件协同能达到1.08年翻倍。
甚至如果我们考虑成本因素，根据经验数据，GPU FLOP/s每刀的增长速度是和CPU相仿的。这和2023年工业界的经验是吻合的——CPU在大多数应用语境下依然是成本更低的选择。</description>
      <content>&lt;h2 id=&#34;gpu和cpu方法的边界何在&#34;&gt;GPU和CPU方法的边界何在？&lt;/h2&gt;
&lt;p&gt;做AI应用的架构工作，遇到新的计算密集任务的第一问往往是：这应该上GPU，还是NUMA物理机？&lt;/p&gt;
&lt;p&gt;这个问题可以归约为&lt;a href=&#34;https://www.usenix.org/legacy/event/hotpar10/tech/full_papers/main.pdf&#34;&gt;On the Limits of GPU Acceleration(2010)&lt;/a&gt;中提出的问题：&amp;ldquo;大致相同能耗的前提下，能和不能用GPU有效加速计算的边界在哪？&amp;rdquo; Vuduc的这个研究分析了3种具有代表性复杂行为和访存不规则性的计算：稀疏线性系统的迭代求解器、稀疏矩阵的乔列斯基分解、快速多极子算法，得出的结论是——大致地说，良好优化的GPU实现在相同能耗下和良好优化的CPU实现在性能上是相仿的。可见当年用GPGPU加速还是一个普遍得不偿失的工作，毕竟要付出剧烈变更编程模型的代价。&lt;/p&gt;
&lt;h2 id=&#34;gpu和cpu的价格-性能趋势&#34;&gt;GPU和CPU的价格-性能趋势&lt;/h2&gt;
&lt;p&gt;上述结论在2010年成立，如今13年过去了，GPU固然飞速发展（无论是硬件、软件，还是生态、应用、资金投入），但GPU发展速度是否已经甩开CPU了呢？&lt;/p&gt;
&lt;p&gt;摩尔定律是两年翻倍，而&lt;a href=&#34;https://en.wikipedia.org/wiki/Huang%27s_law&#34;&gt;黄氏定律&lt;/a&gt;则是宣称通过软硬件协同能达到1.08年翻倍。&lt;/p&gt;
&lt;p&gt;甚至如果我们考虑成本因素，根据&lt;a href=&#34;https://epochai.org/blog/trends-in-gpu-price-performance&#34;&gt;经验数据&lt;/a&gt;，GPU FLOP/s每刀的增长速度是和CPU相仿的。这和2023年工业界的经验是吻合的——CPU在大多数应用语境下依然是成本更低的选择。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/4.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar1&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/5.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar2&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/6.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar3&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>On ABI</title>
      <link>https://cmbbq.github.io/posts/on-abi/</link>
      <pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-abi/</guid>
      <description>前注：ABI在本文中特指系统语言的ABI，这里系统语言，即system programming language，指的是C，C++，Rust这样应用于系统编程的的编译语言。有时候我们也会讨论常用库或基础库的ABI，比如gcc5就把std::basic_string和std::list的实现改了，自然也就影响了跨版本的ABI兼容性，这种层面的ABI兼容性虽然也是一个工程上的疑难杂症，但不在本文的讨论范围内，毕竟源码都不同了。本文讨论ABI兼容性的是同一份源码在不同ISA、操作系统、编译器上对应二进制产物的兼容性。
系统语言的ABI、应用层软件的API、微处理器架构的ISA都是描述各自抽象层次互操作能力的接口。
ABI的独特之处在于它本身就是繁重的机制实现，而非轻量的协议声明——实现和声明之间有概念上的分野，这已经体现在法律上：复制ABI是赤裸裸的抄袭，而复制API或ISA则属于fair use。你显然可以合法地为他人写的API/ISA出书做注，Google实现自己的Java时合法复制了Oracle的Java API，无需license。
ABI之所以复杂，就是因为它处于两个抽象层次之间。ABI中的很大一部分内容既要向上对语言标准负责，向下要对ISA负责，必须遵循两个方向的约束，将复杂度留给自身，从而成就语言标准和ISA这两个抽象契约的简洁、干净，和人类友好。
ABI中的重要组成部分之一calling convention受到ISA的约束，i386中用cdecl, stdcall, fastcall, vectorcall, thiscall, amd64中用systemv, msnative, vectorcall，arm32用aapcs，arm64用aapcs64。ABI中也有很大一部分是指令集无关的（ISA-agnostic），比如name mangling、class layout。这些机制往往足够底层，又与其他指令集相关的部分有强耦合，加上历史因素，往往也只有一小部分能被写入语言标准。
如果ISA差异足够大，维护不同的ABI无疑是必要的，强行用一套ABI兼容不仅不自然，也不高效。以早期的Windows为例，微软就为早先的i386（IA-32）和后来的Intel IA-64提供了两套ABI。再后来，AMD赢得64位战争，Intel也遵循了前向兼容IA-32的amd64，又名x86_64, x64，IA-64就式微了。
ABI以ISA提供的指令、寄存器、内存管理能力为构件（building block），详尽且精确地描述如何实现一个系统语言在特定硬件、操作系统、编译器下的执行模型，并允许分开编译的产物之间能互操作。例如C ABI需定义调用约定（calling convention）、基础数据类型的表示、聚合数据类型的内存布局；C++03 ABI需额外定义异常机制、RTTI信息存储、虚表布局和动态绑定机制、重载函数/运算符/模板实例化所需的名字重置（name mangling）；C++11 ABI需进一步定义lambda的实现、自动类型推导机制等新增语言机制。
无论是C，还是C++都没有在语言层面制定官方的ABI标准，毕竟ABI标准本身也限制了实现的自由。目前最主流的Itanium C++ ABI号称被许多操作系统采用，能适配多数微处理器架构，被大多数编译器实现，包括gcc和clang这两个主要玩家。但值得注意的是，被多方支持，不意味着多方支持的是完全相同的东西。同样打着Itanium ABI名号，clang在arm32 linux上编译的C++库能给amd64 windows上的程序使用吗？显然不行，因为最基础的calling convention，甚至基础数据类型表示都不同。Itanium C++ ABI即使实现大一统，积极意义也仅限于允许一些依赖CPU无关规则的危险技巧（比如修改vtable，这种操作我们现在只能称之为hack）在相当多的环境下通用罢了——只要C++语言标准不将ABI纳入讨论范围，对ABI做出假设的技巧永远是危险的，想要创造新的语言机制，就必须在C++标准层面推进某种ABI共识。
那么系统语言是否应该对某种基于特定ABI实现的语言特性进行标准化呢？从C++的发展历史来看，这种做法已经有了先例，而且是相当危险冒失的。将非零抽象开销的dynamic exception、rtti引入C++客观上导致了社区分裂，有相当一部分C++使用者至今依然选择-fno-exeptions或-fno-rtti。近期的提案Zero-Overhead Deterministic Exceptions: Catching Values给出了一个对exception ABI进行改动的零开销异常机制，是对历史错误的亡羊补牢。系统语言的语言标准应审慎地只对ISA无关且零抽象开销的ABI规则进行标准化，以便在此基础上创造新的语言机制或为应用层开发提供便利。</description>
      <content>&lt;p&gt;前注：ABI在本文中特指系统语言的ABI，这里系统语言，即system programming language，指的是C，C++，Rust这样应用于系统编程的的编译语言。有时候我们也会讨论常用库或基础库的ABI，比如gcc5就把std::basic_string和std::list的实现改了，自然也就影响了跨版本的ABI兼容性，这种层面的ABI兼容性虽然也是一个工程上的疑难杂症，但不在本文的讨论范围内，毕竟源码都不同了。本文讨论ABI兼容性的是同一份源码在不同ISA、操作系统、编译器上对应二进制产物的兼容性。&lt;/p&gt;
&lt;p&gt;系统语言的ABI、应用层软件的API、微处理器架构的ISA都是描述各自抽象层次互操作能力的接口。&lt;/p&gt;
&lt;p&gt;ABI的独特之处在于它本身就是繁重的机制实现，而非轻量的协议声明——实现和声明之间有概念上的分野，这已经体现在法律上：复制ABI是赤裸裸的抄袭，而复制API或ISA则属于fair use。你显然可以合法地为他人写的API/ISA出书做注，Google实现自己的Java时合法复制了Oracle的Java API，无需license。&lt;/p&gt;
&lt;p&gt;ABI之所以复杂，就是因为它处于两个抽象层次之间。ABI中的很大一部分内容既要向上对语言标准负责，向下要对ISA负责，必须遵循两个方向的约束，将复杂度留给自身，从而成就语言标准和ISA这两个抽象契约的简洁、干净，和人类友好。&lt;/p&gt;
&lt;p&gt;ABI中的重要组成部分之一calling convention受到ISA的约束，i386中用cdecl, stdcall, fastcall, vectorcall, thiscall, amd64中用systemv, msnative, vectorcall，arm32用aapcs，arm64用aapcs64。ABI中也有很大一部分是指令集无关的（ISA-agnostic），比如name mangling、class layout。这些机制往往足够底层，又与其他指令集相关的部分有强耦合，加上历史因素，往往也只有一小部分能被写入语言标准。&lt;/p&gt;
&lt;p&gt;如果ISA差异足够大，维护不同的ABI无疑是必要的，强行用一套ABI兼容不仅不自然，也不高效。以早期的Windows为例，微软就为早先的i386（IA-32）和后来的Intel IA-64提供了两套ABI。再后来，AMD赢得64位战争，Intel也遵循了前向兼容IA-32的amd64，又名x86_64, x64，IA-64就式微了。&lt;/p&gt;
&lt;p&gt;ABI以ISA提供的指令、寄存器、内存管理能力为构件（building block），详尽且精确地描述如何实现一个系统语言在特定硬件、操作系统、编译器下的执行模型，并允许分开编译的产物之间能互操作。例如C ABI需定义调用约定（calling convention）、基础数据类型的表示、聚合数据类型的内存布局；C++03 ABI需额外定义异常机制、RTTI信息存储、虚表布局和动态绑定机制、重载函数/运算符/模板实例化所需的名字重置（name mangling）；C++11 ABI需进一步定义lambda的实现、自动类型推导机制等新增语言机制。&lt;/p&gt;
&lt;p&gt;无论是C，还是C++都没有在语言层面制定官方的ABI标准，毕竟ABI标准本身也限制了实现的自由。目前最主流的Itanium C++ ABI号称被许多操作系统采用，能适配多数微处理器架构，被大多数编译器实现，包括gcc和clang这两个主要玩家。但值得注意的是，被多方支持，不意味着多方支持的是完全相同的东西。同样打着Itanium ABI名号，clang在arm32 linux上编译的C++库能给amd64 windows上的程序使用吗？显然不行，因为最基础的calling convention，甚至基础数据类型表示都不同。Itanium C++ ABI即使实现大一统，积极意义也仅限于允许一些依赖CPU无关规则的危险技巧（比如修改vtable，这种操作我们现在只能称之为hack）在相当多的环境下通用罢了——只要C++语言标准不将ABI纳入讨论范围，对ABI做出假设的技巧永远是危险的，想要创造新的语言机制，就必须在C++标准层面推进某种ABI共识。&lt;/p&gt;
&lt;p&gt;那么系统语言是否应该对某种基于特定ABI实现的语言特性进行标准化呢？从C++的发展历史来看，这种做法已经有了先例，而且是相当危险冒失的。将非零抽象开销的dynamic exception、rtti引入C++客观上导致了社区分裂，有相当一部分C++使用者至今依然选择-fno-exeptions或-fno-rtti。近期的提案&lt;a href=&#34;https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2021/p2232r0.html&#34;&gt;Zero-Overhead Deterministic Exceptions: Catching Values&lt;/a&gt;给出了一个对exception ABI进行改动的零开销异常机制，是对历史错误的亡羊补牢。系统语言的语言标准应审慎地只对ISA无关且零抽象开销的ABI规则进行标准化，以便在此基础上创造新的语言机制或为应用层开发提供便利。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>A Taxonomy of Stateful Distributed Systems</title>
      <link>https://cmbbq.github.io/posts/a-taxonomy-of-stateful-distributed-systems/</link>
      <pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/a-taxonomy-of-stateful-distributed-systems/</guid>
      <description>CAP theorem的讨论范围是狭窄的 在分布式系统领域，CAP theorem广泛被引用，而且经常被用以指导超出它讨论边界的问题。
被形式化证明（见Brewer&amp;rsquo;s Conjecture and the Feasibility of Consistent, Available, Partition-​Tolerant Web Services）的CAP theorem其实只局限在read-write storage场景：一个只包含get、set(x)两种操作的存储系统，这种系统被称为register。
在异步网络（指的是消息传递时间无界）中实现register是无法同时满足下列属性的：
Availability：所有发往register的请求最终都能完成。这和大多数真实系统的定义是不同的，因为真实系统不要求100%完成请求，只要保证SLA够高，同时又往往有一定时间约束，超时就会返回timeout错误。 Consistency：所有读、写操作均是linearizable的：B操作在A操作之后成功执行，则B看到的系统状态不能比A完成时的系统状态更旧。 Partition tolerance：允许网络丢包。 Partition tolerance默认是满足的，因此CAP-availability、CAP-consistency两种属性二选一。 离开了形式化证明的场景后，CAP theorem还有指导意义吗？答案是否定的——除非重新定义availability和consistency，将其推广到更通用的场景：从单对象单操作到多对象多操作的事务系统。 重新定义availability和consistency后的分布式系统分类学虽然和CAP theorem非常类似，但不能称之为CAP theorem。
一致性 更通用的“一致性”应定义为“并发系统中共享状态更新的可见性”。 现代微处理器、分布式系统、数据库的共性是——它们都是存在共享数据的并发系统。 当我们讨论一致性（consistency）时，可能指的是微处理器架构和系统编程领域的一致性模型，也可能指的是分布式系统领域的副本一致性，还可能指的是数据库领域的事务隔离性。这些领域的抽象层次不同，共同点是讨论的系统都是并发系统。
一致性模型（consistency model）是用于描述微处理器架构领域的多核并发场景下，各个处理器被允许的乱序程度——乱序的约束越少，效率越高，并发程序正确性越难保证。
最强的strict consistency指的是任何写在任何时钟周期都立刻对任何处理器可见，显然不能推广到分布式系统领域。 次之的sequential consistency指的是写操作顺序对于每个副本而言都是一致的，即各进程内部的program order一致，而不同进程执行的顺序可以不一致。这个概念最初也是Lamport在讨论multi-processor computer如何正确执行并发程序时提出的。和分布式系统的副本一致性无关。C++中std::memory_order_seq_cst即可保证线程内部的program order。 更宽松的causal consistency指的是写操作中有依赖关系的那一部分的顺序是一致的，即各进程中的dependency order一致。现代CPU基本上都是out-of-order流水线，在保证dependency order这个底线后，能多乱序就多乱序。在C++中用std::memory_order_consume的load(A)和std::memory_order_acquire的store(B)配合，即可保证这个store之前所有写操作中load(A)依赖的那一部分对load(A)是可见的。如果每个依赖都保证Release-Consume ordering，则依赖链就有序，整体上可满足causal consistency。 除了上述几个著名模型外，还有几十个不同方法、领域中应用的一致性模型，下图就包含了非事务分布式存储系统中种种一致性模型（详见Consistency in Non-Transactional Distributed Storage Systems）。 并发程序显然可以很容易推广到分布式复制状态机，只是增加了网络延迟。因此，一致性模型可以推广应用到分布式系统中的副本一致性。以sequential consistency为例，增加了实时约束后就是分布式系统领域中更被广泛引用的linearizability，指的是单个被复制对象上的单个操作满足：A是写操作，B是对副本的读，A happened-before（因果律上的先于，见https://en.wikipedia.org/wiki/Happened-before） B，则A的写对B的读总是可见。比较一下C++的sequentially consistent ordering定义：everything that happened-before a store in one thread becomes a visible side effect in the thread that did a load。二者是一致的。</description>
      <content>&lt;h2 id=&#34;cap-theorem的讨论范围是狭窄的&#34;&gt;CAP theorem的讨论范围是狭窄的&lt;/h2&gt;
&lt;p&gt;在分布式系统领域，CAP theorem广泛被引用，而且经常被用以指导超出它讨论边界的问题。&lt;/p&gt;
&lt;p&gt;被形式化证明（见&lt;a href=&#34;https://users.ece.cmu.edu/~adrian/731-sp04/readings/GL-cap.pdf&#34;&gt;Brewer&amp;rsquo;s Conjecture and the Feasibility of Consistent, Available, Partition-​Tolerant Web Services&lt;/a&gt;）的CAP theorem其实只局限在read-write storage场景：一个只包含get、set(x)两种操作的存储系统，这种系统被称为register。&lt;/p&gt;
&lt;p&gt;在异步网络（指的是消息传递时间无界）中实现register是无法同时满足下列属性的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Availability：所有发往register的请求最终都能完成。这和大多数真实系统的定义是不同的，因为真实系统不要求100%完成请求，只要保证SLA够高，同时又往往有一定时间约束，超时就会返回timeout错误。&lt;/li&gt;
&lt;li&gt;Consistency：所有读、写操作均是linearizable的：B操作在A操作之后成功执行，则B看到的系统状态不能比A完成时的系统状态更旧。&lt;/li&gt;
&lt;li&gt;Partition tolerance：允许网络丢包。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Partition tolerance默认是满足的，因此CAP-availability、CAP-consistency两种属性二选一。
离开了形式化证明的场景后，CAP theorem还有指导意义吗？答案是否定的——除非重新定义availability和consistency，将其推广到更通用的场景：从单对象单操作到多对象多操作的事务系统。
重新定义availability和consistency后的分布式系统分类学虽然和CAP theorem非常类似，但不能称之为CAP theorem。&lt;/p&gt;
&lt;h2 id=&#34;一致性&#34;&gt;一致性&lt;/h2&gt;
&lt;p&gt;更通用的“一致性”应定义为“并发系统中共享状态更新的可见性”。
现代微处理器、分布式系统、数据库的共性是——它们都是存在共享数据的并发系统。
当我们讨论一致性（consistency）时，可能指的是微处理器架构和系统编程领域的一致性模型，也可能指的是分布式系统领域的副本一致性，还可能指的是数据库领域的事务隔离性。这些领域的抽象层次不同，共同点是讨论的系统都是并发系统。&lt;/p&gt;
&lt;p&gt;一致性模型（consistency model）是用于描述微处理器架构领域的多核并发场景下，各个处理器被允许的乱序程度——乱序的约束越少，效率越高，并发程序正确性越难保证。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最强的strict consistency指的是任何写在任何时钟周期都立刻对任何处理器可见，显然不能推广到分布式系统领域。&lt;/li&gt;
&lt;li&gt;次之的sequential consistency指的是写操作顺序对于每个副本而言都是一致的，即各进程内部的program order一致，而不同进程执行的顺序可以不一致。这个概念最初也是Lamport在讨论multi-processor computer如何正确执行并发程序时提出的。和分布式系统的副本一致性无关。C++中std::memory_order_seq_cst即可保证线程内部的program order。&lt;/li&gt;
&lt;li&gt;更宽松的causal consistency指的是写操作中有依赖关系的那一部分的顺序是一致的，即各进程中的dependency order一致。现代CPU基本上都是out-of-order流水线，在保证dependency order这个底线后，能多乱序就多乱序。在C++中用std::memory_order_consume的load(A)和std::memory_order_acquire的store(B)配合，即可保证这个store之前所有写操作中load(A)依赖的那一部分对load(A)是可见的。如果每个依赖都保证Release-Consume ordering，则依赖链就有序，整体上可满足causal consistency。&lt;/li&gt;
&lt;li&gt;除了上述几个著名模型外，还有几十个不同方法、领域中应用的一致性模型，下图就包含了非事务分布式存储系统中种种一致性模型（详见&lt;a href=&#34;https://arxiv.org/pdf/1512.00168.pdf&#34;&gt;Consistency in Non-Transactional Distributed Storage Systems&lt;/a&gt;）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/1.png&#34; alt=&#34;Consistency1&#34;&gt;&lt;/p&gt;
&lt;p&gt;并发程序显然可以很容易推广到分布式复制状态机，只是增加了网络延迟。因此，一致性模型可以推广应用到分布式系统中的副本一致性。以sequential consistency为例，增加了实时约束后就是分布式系统领域中更被广泛引用的linearizability，指的是单个被复制对象上的单个操作满足：A是写操作，B是对副本的读，A happened-before（因果律上的先于，见https://en.wikipedia.org/wiki/Happened-before） B，则A的写对B的读总是可见。比较一下C++的sequentially consistent ordering定义：everything that happened-before a store in one thread becomes a visible side effect in the thread that did a load。二者是一致的。&lt;/p&gt;
&lt;p&gt;必须注意，迄今为止讨论的对象仅限单个被复制到不同副本中的对象上的单个操作，分布式存储不可能只存一个对象，有很多分布式存储支持事务或BatchWrite，涉及到多个对象上的多个操作。将单对象上单操作的可见性推广到多对象上多操作也不困难——事务的ACID隔离级别本质上就是将单个共享对象上单个操作的可见性推广到多个对象上一组操作上。下图的左侧就是数据库领域熟知的隔离级别，和分布式系统、微处理器架构、多核编程一样，乱序的约束越少，效率越高，并发程序正确性越难保证。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/2.png&#34; alt=&#34;Consistency2&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;可用性&#34;&gt;可用性&lt;/h2&gt;
&lt;p&gt;更通用的“可用性”应定义为“在施加某种约束后，系统仍最终能响应所有请求，无论网络分区持续多久”。
比CAP theorem更完善的分布式系统分类学可参考&lt;a href=&#34;https://arxiv.org/pdf/1302.0309.pdf&#34;&gt;Highly Available Transactions: Virtues and Limitations&lt;/a&gt;。
这篇论文里给出了新的可用性定义：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;High availability：每个用户向运行中的系统发的请求，最终都会得到回复，无论网络分区持续多久。这就是CAP-availability，或者说traditional availability的标准定义。&lt;/li&gt;
&lt;li&gt;Sticky availability：每当用户事务在一个数据库状态（该状态反应了之前该用户所有操作）拷贝上执行，最终都会得到回复，无论网络分区持续多久。 这比CAP-availability要求更高。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;只追求high availability，用户可以访问系统中的任何一个replica，不同操作在不同replica上响应也没关系；但若追求sticky availability，用户则需要保证连续的若干操作总是在同一个replica上。比如Dynamo这种multi-writer的分布式存储，不能写一会儿A节点，再写一会儿B节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Transactional availability：分布式系统文献的一致性模型大多考虑的都是单对象上单个操作的场景，而数据库文献中关注的是事务：多个对象上多个操作合起来称为一个事务。显然CAP-availability定义也不适用于事务。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;事务的replica availability：事务能为它需要访问的各个对象联系到至少一个replica。这个要求是比CAP-availability低的。&lt;/li&gt;
&lt;li&gt;事务的liveliness：假设我们让每个事务都abort，就可以保证100%的及时响应，完美实现CAP-Availability，但又有什么意义呢？因此还需要保证尽可能让事务commit，而不是abort。&lt;/li&gt;
&lt;li&gt;因此，最终给出的transactional availability定义是：对事务中每个数据都保证replica availability，并且最终能够在N次retry内commit成功，或internal abort（由事务自己主动选择的abort，而非系统实现将其abort）。&lt;/li&gt;
&lt;li&gt;更进一步还可以给出sticky transactional availability的定义：如果系统能保证sticky availability，则能保证transactional availability。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据这种定义，可以将现有的事务系统的consistency（隔离级别）与其availability进行比较，得出下图中的结果：在新的分类学中，availability要求越高，consistency要求越宽松是成立的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/3.png&#34; alt=&#34;Availability&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Digital Representations of Sound</title>
      <link>https://cmbbq.github.io/posts/digital-representations-of-sound/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/digital-representations-of-sound/</guid>
      <description>声音的本质是振动。
对于人耳来说，空气分子的振动抵达鼓膜时就会引起鼓膜振动——鼓膜顾名思义就是一层薄膜结构，被空气分子撞击就如同击鼓——带动鼓膜后的听小骨振动，传入内耳，再引发卵圆窗振动，卵圆窗这层薄膜之后是充满液体的耳蜗管道，这些液体随着卵圆窗振动而流动，冲刷耳蜗上的毛细胞的纤毛，从而产生生物电信号，经过神经传入大脑，就形成了听觉。
对于电容式麦克风来说，当有声音时，两块金属极板就开始振动，其间的距离会产生变化，电压形成了对空气分子振幅的模拟，因此这种电压信号被称为模拟信号。只有离散的数据才能被计算机存储、计算，因此要形成数字音频，还需要声卡的模数转换电路将连续的模拟信号采样后转化为离散的数字信号。这是一个连续曲线转化成柱状图的过程，采样率决定了柱的粗细，bit depth则决定柱高度的精度。如果bit depth为N，则振幅的取值范围为2^N，音乐App里标注的16-bit, 24-bit, 32-bit就指的是bit depth，显然bit depth越高，振幅的表示越精确。比特率bit rate = sample rate * bit depth，即每秒用多少bit来表示音频数据。
PGC和UGC的差别主要是信噪比，毕竟录音的时候连手机都不能带，空调、电灯都可能影响波形电平稳定。UGC可以想象，直播或者念稿子，噪声的平均振幅往往还比背景音乐更大。音频从32bit到16bit是一个量化的过程，不过无论采样精度多高，都终归是对现实的一个量化，量化error引入量化噪声，所以任何数字音频都是有底噪的——白噪声在各频率上都是一样的能量，由于能量（响度）较低，被称为noise floor，表现为微弱的嘶嘶声。
音频时域数据的纵坐标为振幅，横坐标为时间，可绘制成波形图——其震动频率决定音高，平均振幅决定响度，具体波形和这种其他因素决定音色。
时域与频域是对信号波的两个观察面。时域是真实世界唯一存在的域，频域则是对时域的数学构造。任何时域信号都可以表示为不同频率的正弦波信号的叠加。
常见的音频格式包括wav、mdi、mp3、mp3pro、wma、realaudio、audible、aac、ogg vorbis、ape、flac。不同的音频编码有不同的目标，在压缩（降低传输所需信道带宽）和质量（对人耳来说）之间做tradeoff。
各种格式的数字音频都能转成采样率8000Hz的PCM格式音频，PCM(pulse code modulation)是最简单的时域编码方式，就是对信号的离散和量化（通常是对数量化）。1/8000s长度的帧即构成了PCM音频在时域上的最小单位，每个帧包含channel数目个采样点，如果channel数目为1，则帧大小就等于bit depth。我们可以把PCM音频切分成多个chunk，每个chunk有固定大小（比如包含1024个帧），chunk与chunk之间必须有重合（比如256个帧）。之所以要有些重合，是为了对抗Time Skew（假如不做重合，查询音频的chunk的起始帧和库中的音频各个chunk的起始帧有一定offset就查询不到了）。
接下来只需对各个chunk进行1维离散傅立叶变换（fftw_plan_dft_1d）就能得到各个chunk的频域信息（如下图，横坐标为频率，纵坐标为能量）。
再将这些帧的频谱按时间顺序拼起来（可以有一些重合）就形成一个y轴为频率，x轴为时间，z轴为能量的三维表面（也可以用平面彩图表示，z轴的高度换成色彩来表示能量），这就形成了频谱图（spectrogram）。
有了频谱图，就可以尝试从较长的数据中提炼出简短的信息作为这段音频的指纹了。在频谱图上可以逐帧找到各帧上显著的高能频率点（salient peaks，能量超过一定阈值，且比周围所有点都高）——这些点本身已经可以作为音频的指纹特征了，只是不够鲁棒，因为点与点之间是无关的，满屏都是噪点的情况下就会有误匹配。一个改进方案是这些点右侧划定一个目标区域，在区域内找出一些点，形成几个pair，这些pair叫做landmark特征，即[t1, f1, t2, f2]，其抗噪能力增强了很多。</description>
      <content>&lt;p&gt;声音的本质是振动。&lt;/p&gt;
&lt;p&gt;对于人耳来说，空气分子的振动抵达鼓膜时就会引起鼓膜振动——鼓膜顾名思义就是一层薄膜结构，被空气分子撞击就如同击鼓——带动鼓膜后的听小骨振动，传入内耳，再引发卵圆窗振动，卵圆窗这层薄膜之后是充满液体的耳蜗管道，这些液体随着卵圆窗振动而流动，冲刷耳蜗上的毛细胞的纤毛，从而产生生物电信号，经过神经传入大脑，就形成了听觉。&lt;/p&gt;
&lt;p&gt;对于电容式麦克风来说，当有声音时，两块金属极板就开始振动，其间的距离会产生变化，电压形成了对空气分子振幅的模拟，因此这种电压信号被称为模拟信号。只有离散的数据才能被计算机存储、计算，因此要形成数字音频，还需要声卡的模数转换电路将连续的模拟信号采样后转化为离散的数字信号。这是一个连续曲线转化成柱状图的过程，采样率决定了柱的粗细，bit depth则决定柱高度的精度。如果bit depth为N，则振幅的取值范围为2^N，音乐App里标注的16-bit, 24-bit, 32-bit就指的是bit depth，显然bit depth越高，振幅的表示越精确。比特率bit rate = sample rate * bit depth，即每秒用多少bit来表示音频数据。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/audio1.png&#34; alt=&#34;audio1&#34;&gt;&lt;/p&gt;
&lt;p&gt;PGC和UGC的差别主要是信噪比，毕竟录音的时候连手机都不能带，空调、电灯都可能影响波形电平稳定。UGC可以想象，直播或者念稿子，噪声的平均振幅往往还比背景音乐更大。音频从32bit到16bit是一个量化的过程，不过无论采样精度多高，都终归是对现实的一个量化，量化error引入量化噪声，所以任何数字音频都是有底噪的——白噪声在各频率上都是一样的能量，由于能量（响度）较低，被称为noise floor，表现为微弱的嘶嘶声。&lt;/p&gt;
&lt;p&gt;音频时域数据的纵坐标为振幅，横坐标为时间，可绘制成波形图——其震动频率决定音高，平均振幅决定响度，具体波形和这种其他因素决定音色。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/wave.jpeg&#34; alt=&#34;wave&#34;&gt;&lt;/p&gt;
&lt;p&gt;时域与频域是对信号波的两个观察面。时域是真实世界唯一存在的域，频域则是对时域的数学构造。任何时域信号都可以表示为不同频率的正弦波信号的叠加。&lt;/p&gt;
&lt;p&gt;常见的音频格式包括wav、mdi、mp3、mp3pro、wma、realaudio、audible、aac、ogg vorbis、ape、flac。不同的音频编码有不同的目标，在压缩（降低传输所需信道带宽）和质量（对人耳来说）之间做tradeoff。&lt;/p&gt;
&lt;p&gt;各种格式的数字音频都能转成采样率8000Hz的PCM格式音频，PCM(pulse code modulation)是最简单的时域编码方式，就是对信号的离散和量化（通常是对数量化）。1/8000s长度的帧即构成了PCM音频在时域上的最小单位，每个帧包含channel数目个采样点，如果channel数目为1，则帧大小就等于bit depth。我们可以把PCM音频切分成多个chunk，每个chunk有固定大小（比如包含1024个帧），chunk与chunk之间必须有重合（比如256个帧）。之所以要有些重合，是为了对抗Time Skew（假如不做重合，查询音频的chunk的起始帧和库中的音频各个chunk的起始帧有一定offset就查询不到了）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/timeskew.png&#34; alt=&#34;ts&#34;&gt;&lt;/p&gt;
&lt;p&gt;接下来只需对各个chunk进行1维离散傅立叶变换（fftw_plan_dft_1d）就能得到各个chunk的频域信息（如下图，横坐标为频率，纵坐标为能量）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spec.png&#34; alt=&#34;sp&#34;&gt;&lt;/p&gt;
&lt;p&gt;再将这些帧的频谱按时间顺序拼起来（可以有一些重合）就形成一个y轴为频率，x轴为时间，z轴为能量的三维表面（也可以用平面彩图表示，z轴的高度换成色彩来表示能量），这就形成了频谱图（spectrogram）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spec2.png&#34; alt=&#34;sp&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/spec3.png&#34; alt=&#34;sp&#34;&gt;&lt;/p&gt;
&lt;p&gt;有了频谱图，就可以尝试从较长的数据中提炼出简短的信息作为这段音频的指纹了。在频谱图上可以逐帧找到各帧上显著的高能频率点（salient peaks，能量超过一定阈值，且比周围所有点都高）——这些点本身已经可以作为音频的指纹特征了，只是不够鲁棒，因为点与点之间是无关的，满屏都是噪点的情况下就会有误匹配。一个改进方案是这些点右侧划定一个目标区域，在区域内找出一些点，形成几个pair，这些pair叫做landmark特征，即[t1, f1, t2, f2]，其抗噪能力增强了很多。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spec4.png&#34; alt=&#34;sp&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
