<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>en on Cmbbq&#39;s Encyclopedia</title>
    <link>https://cmbbq.github.io/tags/en/</link>
    <description>Recent content in en on Cmbbq&#39;s Encyclopedia</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://cmbbq.github.io/tags/en/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Practical Performance Engineering of Memory-Bound Systems</title>
      <link>https://cmbbq.github.io/posts/performance-engineering/</link>
      <pubDate>Mon, 29 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/performance-engineering/</guid>
      <description>[WIP]
Work Reduction vs Hardware Enablement Optimization can be divided into work reduction and hardware enablement.
Work, in terms of computer programming, is basically a gross measure of how much stuff the program needs to do.
The idea of work optimization is to reduce the amount of stuff the program needs to do. Commonly employed techniques include: approximation, tail-recursion elimination, coarsening/refining recursion, inlining, loop fusion, loop unrolling, hoisting, short-circuiting, common-subexpression elimination, compile-time initialization and evaluation, exploiting sparsity, caching, pre-computation, and bit hacks.</description>
      <content>&lt;p&gt;[WIP]&lt;/p&gt;
&lt;h1 id=&#34;work-reduction-vs-hardware-enablement&#34;&gt;Work Reduction vs Hardware Enablement&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Optimization can be divided into work reduction and hardware enablement.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Work, in terms of computer programming, is basically a gross measure of how much stuff the program needs to do.&lt;/p&gt;
&lt;p&gt;The idea of work optimization is to reduce the amount of stuff the program needs to do. Commonly employed techniques include: approximation, tail-recursion elimination, coarsening/refining recursion, inlining, loop fusion, loop unrolling, hoisting, short-circuiting, common-subexpression elimination, compile-time initialization and evaluation, exploiting sparsity, caching, pre-computation, and bit hacks.&lt;/p&gt;
&lt;p&gt;While reducing work undoubtedly serves as an essential heuristic for reducing overall running time, it&amp;rsquo;s not the sole determinant of the running time; it doesn&amp;rsquo;t capture the whole picture of computer programming, leaving the intricate nature of computer hardware unaddressed.&lt;/p&gt;
&lt;p&gt;To thoroughly investigate architectural improvements that unlock hardware potential, we must delve into numerous aspects in hardware and micro-architecture: the ISA, pipeline stages, superscalar processing, out-of-order execution, paging, caching, vectorization, speculation, hardware prefetching, branch prediction, etc.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Historically computer architecture leverages either locality or parallelism to enhance performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To exploit locality, the memory hierarchy(registers-&amp;gt;L1/L2/L3 caches-&amp;gt;local DRAM-&amp;gt;remote DRAM-&amp;gt;PMem-&amp;gt;SSD) was made deeper for hiding performance issue; hardware prefetchers and branch predictors were used to predict immediate accesses and move data or instructions closer to processors. As programmers, we are working on one layer obove the computer architecture layer, what we can do is to write NUMA-aware, cache-aligned, and perhaps vectorized code with regular data access patterns and proper software pre-fetching.&lt;/p&gt;
&lt;p&gt;To exploit parallelism, superscalar out-of-order pipelines, breaking ops into micro-ops, vector hardware, multi-core were invented. Correspondingly, we need to keep all these things busy by using techniques like bit tricks, ILP(Instruction-level parallelism), AVX/SSE, AMX, multithread/multi-process programming and offloading to accelerators like DPUs or GPUs.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s delve deeper into ILP, as it is more connected with the processor Î¼-arch - aspects such as superscalar processing, pipeline stages, out-of-order execution, data bypassing, register renaming, and so on. To exploit it programmatically, we can use separate functional units, write likely/unlikely hints for branch prediction, and break dependency in the data-flow graph beforehand to reduce data hazards.&lt;/p&gt;
&lt;h1 id=&#34;lessons-learnt-from-prior-work&#34;&gt;Lessons Learnt from Prior Work&lt;/h1&gt;
&lt;p&gt;[understand your program. measure its Arithmetic Intensity]&lt;/p&gt;
&lt;h1 id=&#34;interesting-heuristics-for-future-work&#34;&gt;Interesting Heuristics for Future Work&lt;/h1&gt;
&lt;h2 id=&#34;employing-quantitative-approaches&#34;&gt;Employing Quantitative Approaches&lt;/h2&gt;
&lt;p&gt;[off-cpu flame graph]&lt;/p&gt;
&lt;h2 id=&#34;playing-with-dram-as-if-we-were-handling-slow-devices&#34;&gt;Playing with DRAM as if We were Handling Slow Devices&lt;/h2&gt;
&lt;p&gt;As &lt;code&gt;DRAM&lt;/code&gt; access has become the de-facto bottleneck for many of today&amp;rsquo;s datacenter applications, an intuitive idea to optimize these applications would be shamelessly copying prior wisdom on dealing with slow devices like &lt;code&gt;PMem&lt;/code&gt;s or &lt;code&gt;SSD&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PMem&lt;/code&gt;s offer only $\frac{1}{14}$~$\frac{1}{3}$ of DRAMs&amp;rsquo; bandwidth. &lt;code&gt;SSD&lt;/code&gt;s are typically 1~2 orders of magnitude slower than DRAMs. To overcome their slowness, many data structures were crafted specifically for them.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Dashtable&lt;/code&gt;[Lu et al., 2020]&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, for example, was targeting scalable hasing on the once fashionable &lt;code&gt;PMem&lt;/code&gt;s. Hasing on &lt;code&gt;PMem&lt;/code&gt;s sounds somewhat niche. But in reality, if any being could survive &lt;code&gt;PMem&lt;/code&gt;&amp;rsquo;s hellish bandwidth, you can definitely count on it for arbitrary memory-bound application. In 2022, Intel killed off its &lt;code&gt;PMem&lt;/code&gt; business&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. &lt;code&gt;PMem&lt;/code&gt; died. Yet the &lt;code&gt;Dash&lt;/code&gt; methodology still thrives. Today we use it in regular &lt;code&gt;DRAM&lt;/code&gt; setups. Check this out, the &lt;a href=&#34;https://github.com/dragonflydb/dragonfly&#34;&gt;DragonFly&lt;/a&gt; project. It&amp;rsquo;s based on &lt;code&gt;Dash&lt;/code&gt; and claims to be the modern replacement for Redis and Memcached, delivering 25x more throughput.&lt;/p&gt;
&lt;p&gt;The core idea behind &lt;code&gt;Dashtable&lt;/code&gt; is trading space for time, or more specifically, each bucket paying a little bit extra space in metadata to buy (1)faster bucket probing with fingerprints of keys and (2)lightweight optimistic concurrency control with version locks, (3)stashing overflow records, which helps to alleviate segment splits caused by unbalanced bucket loads.&lt;/p&gt;
&lt;p&gt;Another noticeable development is that modern-day system language &lt;code&gt;Rust&lt;/code&gt;, has opted for &lt;code&gt;B-tree&lt;/code&gt;s over &lt;code&gt;Red-black tree&lt;/code&gt;s for its ordered maps. The 50+ years old &lt;code&gt;B-tree&lt;/code&gt;, once targeting disk storage, might find its place in today&amp;rsquo;s memory-bound applications. That is interesting because the &lt;code&gt;Red-black tree&lt;/code&gt;s was deemed memory-efficient and is still widely in use as the default implementation of &lt;code&gt;C++&lt;/code&gt;&amp;rsquo;s &lt;code&gt;std::map&lt;/code&gt;. Yet &lt;code&gt;B-tree&lt;/code&gt; somehow beats &lt;code&gt;Red-black tree&lt;/code&gt; on modern servers.&lt;/p&gt;
&lt;p&gt;So what happened to modern hardware? Well, it mostly involves the memory wall problem, which refers to the increasing gap between processor and memory speed. For decades, the memory wall problem has never been resolved, but only mitigated by introducing deeper and deeper memory hierarchies. In today&amp;rsquo;s architectures, L3 become much slower than L1/L2, and &lt;code&gt;DRAM&lt;/code&gt;s should be labeled as dangerously slow as disks were in the 1980s perspective.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;B. Lu, X. Hao, T. Wang, and E. Lo. Dash: Scalable Hashing on Persistent Memory. CoRR abs/2003.07302, 2020. &lt;a href=&#34;https://arxiv.org/pdf/2003.07302.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datacenterdynamics.com/en/news/intel-kills-off-optane-memory-writes-off-559-million-inventory/&#34;&gt;Intel kills off Optane Memory, writes off $559 million inventory&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Dash: Scalable Hashing</title>
      <link>https://cmbbq.github.io/posts/dash/</link>
      <pubDate>Fri, 26 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/dash/</guid>
      <description>The main focus of the Dash paper was on the once fashionable persistent memory, but in reality, any memory bandwidth-limited scenario can benefit from it. With Intel killing off its pmem business, the significance of the Dash approach has shifted to regular DRAM applications.
Dynamic Hashing Dashtable, the proposed scalable hashtable, evolves from extendible hashing.
Extendible hashing is a hash system that uses the first $N$ bits of hashed values to look up buckets in a trie-structured directory.</description>
      <content>&lt;p&gt;The main focus of the Dash paper was on the once fashionable &lt;code&gt;persistent memory&lt;/code&gt;, but in reality, any &lt;code&gt;memory bandwidth&lt;/code&gt;-limited scenario can benefit from it. With Intel killing off its &lt;code&gt;pmem&lt;/code&gt; business, the significance of the &lt;code&gt;Dash&lt;/code&gt; approach has shifted to regular DRAM applications.&lt;/p&gt;
&lt;h1 id=&#34;dynamic-hashing&#34;&gt;Dynamic Hashing&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;Dashtable&lt;/code&gt;, the proposed scalable hashtable, evolves from &lt;code&gt;extendible hashing&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Extendible hashing&lt;/code&gt; is a hash system that uses the first $N$ bits of hashed values to look up buckets in a trie-structured &lt;code&gt;directory&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;directory&lt;/code&gt; with &lt;code&gt;global depth&lt;/code&gt; $N$ can hold $2^N$ buckets. It means $N$ is the key size that maps the &lt;code&gt;directory&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Each bucket also has a &lt;code&gt;local depth&lt;/code&gt; $M(M \le N)$, which is the key size that previously mapped the &lt;code&gt;directory&lt;/code&gt;. Any bucket having a &lt;code&gt;local depth&lt;/code&gt; $M = N$ is pointed-to by exactly one &lt;code&gt;directory&lt;/code&gt; entry. Any bucket having a &lt;code&gt;local depth&lt;/code&gt; $M \lt N$ is pointed-to by more than one &lt;code&gt;directory&lt;/code&gt; entries.&lt;/p&gt;
&lt;p&gt;The minimal $N$ needed to ensure every item has a unique bucket index is 1 for 2 items. The minimal $N = 2$  for 4 items. Everytime a new  item added into a bucket, if the number of items in the bucket exceeds a certain threshold, a rehashing operation happens by splitting the bucket into 2 parts. Hence rehashing in this scheme doesn&amp;rsquo;t have to stop the world and do a full-table scan &amp;amp; copy, but instead is done incremental.&lt;/p&gt;
&lt;p&gt;Similar to &lt;code&gt;extendible hashing&lt;/code&gt;, &lt;code&gt;linear hashing&lt;/code&gt; also uses a &lt;code&gt;directory&lt;/code&gt; to orgranize and address buckets. The distinction lies in split control. In &lt;code&gt;linear hashing&lt;/code&gt;, a split typically occurs only if the load factor exceeds a threshold and the bucket to be split is chosen in a &amp;ldquo;linear&amp;rdquo; manner.&lt;/p&gt;
&lt;h1 id=&#34;dash-for-extendible-hashing&#34;&gt;Dash for Extendible Hashing&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/dash_eh.png&#34; alt=&#34;dash_eh&#34;&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;Dash-EH&lt;/code&gt;, each &lt;code&gt;directory&lt;/code&gt; entry points to a &lt;code&gt;segment&lt;/code&gt; which consists of a fixed number of normal buckets and stash buckets. A &lt;code&gt;segment&lt;/code&gt; can be viewed as a sub-hashtable of constant size. The so-called stash buckets shares the same layout as the normal buckets, responsible for storing overflow records.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/dash_eh_bucket.png&#34; alt=&#34;dash_eh&#34;&gt;&lt;/p&gt;
&lt;p&gt;The core idea of &lt;code&gt;Dash-EH&lt;/code&gt; is to pay a little bit extra space in metadata to buy faster probing with fingerprints and lightweight concurrency control with version locks.&lt;/p&gt;
&lt;p&gt;Inside a &lt;code&gt;Dash-EH&lt;/code&gt; bucket, as shown in the figure above, the first 32 bytes are metadata, including version lock, counter, alloc bitmap, membership bitmap for load balancing, and 18 one-byte fingerprints for bucket probing(14 for slots in the bucket, 4 for overflow records originally hashed to this bucket). It is followed by $16(Bytes) \times 14 (records) = 224 Bytes$ payload, which stores 14 16-byte records.&lt;/p&gt;
&lt;h2 id=&#34;fingerprinting&#34;&gt;Fingerprinting&lt;/h2&gt;
&lt;p&gt;Bucket probing, which refers to searching for a slot in a bucket, is a basic operation of hashtables, needed by &lt;code&gt;search&lt;/code&gt;, &lt;code&gt;insert&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; operations to locate a particular key. Traditionally probing requires a linear scan, which is naturally slow on &lt;code&gt;PMem&lt;/code&gt; and could be completely unnecessary when a searched key doesn&amp;rsquo;t exist. &lt;code&gt;Dash-EH&lt;/code&gt; employs fingerprinting to reduce unnecessary scans. Fingerprints are the least significant byte of hashes of keys. To probe for a key, the probing thread first checks if any fingerprint in the bucket&amp;rsquo;s metadata matches the key&amp;rsquo;s, so it can skip buckets without any fingerprint match.&lt;/p&gt;
&lt;p&gt;Fingerprinting primarily benefits negative(key-not-found) &lt;code&gt;search&lt;/code&gt;es. The &lt;code&gt;Dash&lt;/code&gt; paper also claims fingerprinting enables using larger buckets spanning more than 2 cachelines. But I have to take it with a grain of salt. The paper itself uses a 256B setup. So does the DragonFly implementation&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. In theory, larger buckets can indeed tolerate more collisions and improve the load factor; however, this may come at the cost of compromising locality to a certain degree - you don&amp;rsquo;t want to load multiple times for a single bucket access in a hashtable.&lt;/p&gt;
&lt;h2 id=&#34;bucket-load-balancing&#34;&gt;Bucket Load Balancing&lt;/h2&gt;
&lt;p&gt;Segmentation reduces cache misses on &lt;code&gt;directory&lt;/code&gt; by reducing its size. In the &lt;code&gt;extendible hashing&lt;/code&gt; scheme, if any bucket in a segment is full, the entire segment needs to be split, even though other buckets might have much free space.&lt;/p&gt;
&lt;p&gt;To prevent frequent segment splits, the &lt;code&gt;Dash-EH&lt;/code&gt; algorithm design incorporates bucket load balancing. For an &lt;code&gt;insert&lt;/code&gt; operation, &lt;code&gt;Dash-EH&lt;/code&gt; probes both bucket $B_b$ and $B_{b+1}$, and then inserts into the bucket that is less full. If both $B_b$ and $B_{b+1}$ are full, &lt;code&gt;Dash-EH&lt;/code&gt; tries to displace a &amp;ldquo;native record&amp;rdquo; from $B_{b+1}$ to $B_{b+2}$, or move a &amp;ldquo;rebalanced record&amp;rdquo; from $B_b$ back to $B_{b-1}$ where it originally belongs.&lt;/p&gt;
&lt;p&gt;The per-bucket membership bitmap is used to decide whether a record is rebalanced or native. If a bit is set in the membership bitmap, then the corresponding key was not directly hashing into this bucket(native) but placed here due to re-balancing(rebalanced).&lt;/p&gt;
&lt;p&gt;If both &lt;code&gt;insert&lt;/code&gt; and &lt;code&gt;displacement&lt;/code&gt; failed, &lt;code&gt;Dash-EH&lt;/code&gt; turns to the last resort - stashing. Each segment has a fixed number of stash buckets to hold these overflow records. Probing stash buckets introduces significant overhead to negative &lt;code&gt;search&lt;/code&gt;es and &lt;code&gt;insert&lt;/code&gt;s(needs uniqueness check). To address this issue, each normal bucket reserves certain metadata fields. 4 overflow fingerprints are reserved for overflow records stored in stsh buckets. A overflow bit indicates if there exists an overflow at all. So if there isn&amp;rsquo;t an overflow in a bucket, the &lt;code&gt;search&lt;/code&gt;/&lt;code&gt;insert&lt;/code&gt; operation doesn&amp;rsquo;t have to probe stash buckets. Anyway, it is still advisable to maintain a small number of stash buckets. The paper claims &amp;ldquo;using 2â4 stash buckets per segment can improve load factor to over 90% without imposing significant overhead&amp;rdquo;. In Dragonfly&amp;rsquo;s Dashtable, each segment has 56 regular buckets, and 4 stash buckets.&lt;/p&gt;
&lt;h2 id=&#34;lightweight-concurrency-control&#34;&gt;Lightweight Concurrency Control&lt;/h2&gt;
&lt;p&gt;The lightweight concurrency control in &lt;code&gt;Dash-EH&lt;/code&gt; naturally scales well on today&amp;rsquo;s &lt;code&gt;many-core&lt;/code&gt; architectures, out-performing traiditional bucket-level shared locks.&lt;/p&gt;
&lt;p&gt;Write operations follow traditional bucket-level locking to lock the affected buckets, using CAS over a lock bit. If a write is done, the writer thread resets the lock bit and increments the per-bucket version number by one.&lt;/p&gt;
&lt;p&gt;On the other hand, read operations are designed to be lock-free. Before a read, the reader thread first fetches a snapshot of the lock word, waits until the lock is released, then proceeds to read without holding any lock. After reading, it will check the lock word again to verify the version number stays unchanged. If the version is changed, it retries the entire operation.&lt;/p&gt;
&lt;h1 id=&#34;dash-for-linear-hashing&#34;&gt;Dash for Linear Hashing&lt;/h1&gt;
&lt;p&gt;The Dash paper also presents &lt;code&gt;Dash-LH&lt;/code&gt;, a Dash-enabled linear hashing approach built upon building blocks used in &lt;code&gt;Dash-EH&lt;/code&gt;, such as balanced &lt;code&gt;insert&lt;/code&gt;/&lt;code&gt;displacement&lt;/code&gt;, fingerprinting and optimistic concurrency; they are pretty much orthogonal after all. The main difference is that &lt;code&gt;Dash-LH&lt;/code&gt; split the segment pointed to by a pointer in a linear manner.&lt;/p&gt;
&lt;p&gt;Traditional &lt;code&gt;linear hashing&lt;/code&gt; links overflow records with a linklist. In &lt;code&gt;Dash-LH&lt;/code&gt;, it&amp;rsquo;s done more cache-friendly with stash buckets. It still needs to chain these stash buckets though. Still it&amp;rsquo;s much better than chaining individual records.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/dragonflydb/dragonfly/blob/main/docs/dashtable.md&#34;&gt;Dashtable in Dragonfly&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
  </channel>
</rss>
