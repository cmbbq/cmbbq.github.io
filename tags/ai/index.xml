<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ai on Cmbbq&#39;s Encyclopedia</title>
    <link>https://cmbbq.github.io/tags/ai/</link>
    <description>Recent content in ai on Cmbbq&#39;s Encyclopedia</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Aug 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://cmbbq.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>推理优化</title>
      <link>https://cmbbq.github.io/posts/optimizing-ai-inference/</link>
      <pubDate>Wed, 28 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/optimizing-ai-inference/</guid>
      <description>优化的几个抽象层次 应用侧的AI Engineering大致上可以分为MLOps和推理优化。推理优化又可以分为几个抽象层次：
最高层是模型优化：量化、知识蒸馏、参数剪枝、通道剪枝。 中间层是图优化和通用优化：operator fusion/reconstruction、loop interchanges、data layout rewrites。 底层是硬件算子：最优化的底层算子必然是最特化的（tensor-specific + hardware-specific + framework-agnostic），因此需将中层IR绑定到硬件算子（即后端算子选择），这些硬件算子往往需要根据设备信息，充分利用vectorization、parallelism、locality，做显式cache管理，通过合理的指令重排隐藏memory latency, 利用SIMD/AMX/DSP/GPGPU架构做memory tiling和minibatch block gemm。 最底层还有LLVM层面的low-level codegen优化，负责最终生成优化的机器码。 第一层的模型优化减少了模型本身的总计算量，与底层优化正交。第二、第三层的推理优化归根到底都是硬件使能，只不过二层对几乎所有硬件都有效，三层则根据设备信息细化。从实现方法上讲，二、三层优化又可分为基于AI编译器的优化和手工优化。
基于编译的优化 编译，或者说DSLs + Optimizing Compilers，是解决领域问题优化的一个通用解法，为不同硬件提供可移植的优化。比如十年前就有Halide为图像和张量的并行计算提供了一个DSL+编译器，将算法规格本身和优化细节解耦。甚至更底层的gcc/llvm本身也是将算法和优化解耦的例子，在machine code codegen层面做的优化。
ML Compilers的优势和劣势 如今的ML compilers是这种基于编译的思路的延续，其优势在于：
可移植性：硬件一方面在不断迭代更新，另一方面也不断有新硬件、新架构涌现。端边侧的设备要繁杂的多。Datacenter场景还好，但也面临N卡禁运，国产替代的问题，国产GPGPU还没有形成明确的一两家独大的格局1，因此适配新硬件是近未来必需解决的事情。 将算法与优化清晰解耦：工程上可以提效，也有助于降低因复杂度爆炸而注入缺陷、使得项目逐渐失控、无法维护的风险。 其劣势在于：
不完备：在处理大多数场景、常规问题时性能不错，但总会遇到一些预料之外的edge cases，fallback to the slow path，性能骤然劣化，很难通过tweak DSL code生成更优的代码。 不灵活：考虑到编译器codegen产物往往不那么human-readble，工程上针对edge cases、ad-hoc需求做灵活的手工优化就比较困难。 难以真正击败专家手动优化：正如至今不存在能在性能上击败C/C++的函数式语言编译器，ML compiler也只是给出足够好的解，通常不及专家充分优化的C/C++实现。 ML Compilers的工作流程 ML编译器的工作流是高层抽象到底层抽象的lowering过程。ML编译器后端包含各种passes，所谓pass就是lowering规则。最终会根据设备信息，形成一种硬件特化、张量特化的硬件算子描述，这种描述在TVM里叫做schedule，在Triton里叫做plan。再进一步CodeGen阶段，是从ML编译器自己的语言翻译到language compiler后端，比如LLVM IR，然后交由LLVM编译成可执行的machine code。
MLIR中dialects可对passes进行分层或分类，一个典型的dialects分层自上而下如是：
OpGraph -&amp;gt; TSOWB(e.g. late hlo) -&amp;gt; CGASel -&amp;gt; HHO(e.g. Linalg) -&amp;gt; MHA(e.g. stripe/affine) -&amp;gt; HLTSIR(e.g. vector dialects) -&amp;gt; TSIR(e.</description>
      <content>&lt;h2 id=&#34;优化的几个抽象层次&#34;&gt;优化的几个抽象层次&lt;/h2&gt;
&lt;p&gt;应用侧的AI Engineering大致上可以分为MLOps和推理优化。推理优化又可以分为几个抽象层次：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最高层是模型优化：量化、知识蒸馏、参数剪枝、通道剪枝。&lt;/li&gt;
&lt;li&gt;中间层是图优化和通用优化：operator fusion/reconstruction、loop interchanges、data layout rewrites。&lt;/li&gt;
&lt;li&gt;底层是硬件算子：最优化的底层算子必然是最特化的（tensor-specific + hardware-specific + framework-agnostic），因此需将中层IR绑定到硬件算子（即后端算子选择），这些硬件算子往往需要根据设备信息，充分利用vectorization、parallelism、locality，做显式cache管理，通过合理的指令重排隐藏memory latency, 利用SIMD/AMX/DSP/GPGPU架构做memory tiling和minibatch block gemm。&lt;/li&gt;
&lt;li&gt;最底层还有LLVM层面的low-level codegen优化，负责最终生成优化的机器码。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第一层的模型优化减少了模型本身的总计算量，与底层优化正交。第二、第三层的推理优化归根到底都是硬件使能，只不过二层对几乎所有硬件都有效，三层则根据设备信息细化。从实现方法上讲，二、三层优化又可分为基于AI编译器的优化和手工优化。&lt;/p&gt;
&lt;h2 id=&#34;基于编译的优化&#34;&gt;基于编译的优化&lt;/h2&gt;
&lt;p&gt;编译，或者说DSLs + Optimizing Compilers，是解决领域问题优化的一个通用解法，为不同硬件提供可移植的优化。比如十年前就有Halide为图像和张量的并行计算提供了一个DSL+编译器，将算法规格本身和优化细节解耦。甚至更底层的gcc/llvm本身也是将算法和优化解耦的例子，在machine code codegen层面做的优化。&lt;/p&gt;
&lt;h3 id=&#34;ml-compilers的优势和劣势&#34;&gt;ML Compilers的优势和劣势&lt;/h3&gt;
&lt;p&gt;如今的ML compilers是这种基于编译的思路的延续，其优势在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可移植性：硬件一方面在不断迭代更新，另一方面也不断有新硬件、新架构涌现。端边侧的设备要繁杂的多。Datacenter场景还好，但也面临N卡禁运，国产替代的问题，国产GPGPU还没有形成明确的一两家独大的格局&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，因此适配新硬件是近未来必需解决的事情。&lt;/li&gt;
&lt;li&gt;将算法与优化清晰解耦：工程上可以提效，也有助于降低因复杂度爆炸而注入缺陷、使得项目逐渐失控、无法维护的风险。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其劣势在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不完备：在处理大多数场景、常规问题时性能不错，但总会遇到一些预料之外的edge cases，fallback to the slow path，性能骤然劣化，很难通过tweak DSL code生成更优的代码。&lt;/li&gt;
&lt;li&gt;不灵活：考虑到编译器codegen产物往往不那么human-readble，工程上针对edge cases、ad-hoc需求做灵活的手工优化就比较困难。&lt;/li&gt;
&lt;li&gt;难以真正击败专家手动优化：正如至今不存在能在性能上击败C/C++的函数式语言编译器，ML compiler也只是给出足够好的解，通常不及专家充分优化的C/C++实现。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ml-compilers的工作流程&#34;&gt;ML Compilers的工作流程&lt;/h3&gt;
&lt;p&gt;ML编译器的工作流是高层抽象到底层抽象的&lt;code&gt;lowering&lt;/code&gt;过程。ML编译器后端包含各种&lt;code&gt;passes&lt;/code&gt;，所谓pass就是lowering规则。最终会根据设备信息，形成一种硬件特化、张量特化的硬件算子描述，这种描述在TVM里叫做&lt;code&gt;schedule&lt;/code&gt;，在Triton里叫做&lt;code&gt;plan&lt;/code&gt;。再进一步CodeGen阶段，是从ML编译器自己的语言翻译到language compiler后端，比如LLVM IR，然后交由LLVM编译成可执行的machine code。&lt;/p&gt;
&lt;p&gt;MLIR中&lt;code&gt;dialects&lt;/code&gt;可对passes进行分层或分类，一个典型的dialects分层自上而下如是：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;OpGraph -&amp;gt; TSOWB(e.g. late hlo) -&amp;gt; CGASel -&amp;gt; HHO(e.g. Linalg) -&amp;gt; MHA(e.g. stripe/affine) -&amp;gt; HLTSIR(e.g. vector dialects) -&amp;gt; TSIR(e.g. llvm)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Triton的大致流程如下：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt;&amp;gt; 面向用户的Python/C++的kernel代码
--&amp;gt; [ML Compiler前端，有时候可能只是某种动转静工具，forward一次，然后转写]
&amp;gt;&amp;gt; 设备无关的 High-level IR
--&amp;gt; [ML Compiler后端Passes，图优化+算子选择+内存优化]
&amp;gt;&amp;gt; 硬件特化的 Low-level IR [Schedule/Plan]
--&amp;gt; [ML Compiler后端Passes，把自己内部的Schedule/Plan翻译到LLVM IR]
&amp;gt;&amp;gt; LLVM IR 
--&amp;gt; [LLVM&amp;#39;s NVPTX back-end，进入Language Compilation层面]
&amp;gt;&amp;gt; PTX
--&amp;gt; [CUDA ptxas assembler]
&amp;gt;&amp;gt; CUBIN
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Intel MLIR graph compiler的lowering pipeline如下：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;gt;&amp;gt; Computation Graphs
-&amp;gt; linalg [^4]
-&amp;gt; layout propagation [^7]
-&amp;gt; tiling [^3]
-&amp;gt; fusion [^8] 
-&amp;gt; micro kernel [^9]
-&amp;gt; vector [^10] 
--- 再此之上都可以分为tensor-level passes，完全不涉及内存，就是tensor层面的优化 ---
-&amp;gt; bufferization [^12]
-&amp;gt; memory planning 
-&amp;gt; LLVM IR 
&amp;gt;&amp;gt; 交由LLVM处理
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从中可以看到，lowering pipeline又可分为tensor-land和memref-land两个大的区块。在tensor-land，所有tensor操作都默认不是in-place的，哪怕是明显可以in-place的relu。在memref-land才会考虑内存访问的进一步优化。&lt;/p&gt;
&lt;h2 id=&#34;手工优化&#34;&gt;手工优化&lt;/h2&gt;
&lt;p&gt;很多时候，目标模型的架构是确定的，目标机器的架构也就固定几种，ML compilers的可移植性优势——自动算子绑定/算子选择的优势——就近乎不存在了。&lt;/p&gt;
&lt;p&gt;典型的例子是llama.cpp，和支撑它的ggml，llama.cpp+ggml通过一个人类可读的最小化C/C++项目，实现了各种精度的量化、自动微分、AVX/AVX2优化、Metal优化、flashatt算子、Multi-GPU pipeline并行等各个抽象层次上最直接有效的优化机制，最终也达成相当好的效果，尤其是在本地设备上。&lt;/p&gt;
&lt;p&gt;这种手动优化的系统优势在于系统是透明的，程序员可以读懂整个系统，精准定位到涉及某个问题的代码行，从而具备ML编译方案中不具备的灵活性，适合应对ad-hoc需求，适用于模型架构和硬件设备稳定的场景。&lt;/p&gt;
&lt;p&gt;手动优化的劣势是一旦出现新架构、新设备，就需要重写代码。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;国产AI芯片处于混战阶段：华为Atlas系列、壁仞BR100、瑞芯微rk NPU、百度昆仑芯XPU、比特大陆（bm-se/sc）、寒武纪MLU、海光DCU、燧原GCU等。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Paged Attention</title>
      <link>https://cmbbq.github.io/posts/pagedattention/</link>
      <pubDate>Tue, 30 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/pagedattention/</guid>
      <description>在LLM serving场景，通过恰当的batching积攒足够多的请求，可提升LLM吞吐。但每个请求对应的KV Cache非常巨大——在原始实现中，KV Cache需要为max_tokens预留内存，但每个请求实际上携带的tokens数目普遍远小于max_tokens，造成大量内存浪费和反复的动态内存分配。
PagedAttention1的目标就是消除这种内存浪费、在请求内部和请求之间灵活共享一些KV cache。vLLM通过PagedAttention达到此前sota的FasterTransformer和Orca的2~4倍吞吐。
此前，朴素的KV Cache实现如下图所示，简短的prompt和并不长的当前iteration只占据了7+4个slots，剩余2038个slots不得不预留在内存里以支撑最大序列长达2048的承诺（采样结束才能知道实际tokens数目），在这之后才能是下一个请求的slots，中间预留的部分就完全浪费掉了。
PagedAttention把连续的kv存在不连续的内存空间上，借用类似页表的机制（引入一个block_table）规避内存碎片化问题。具体来讲，PagedAttention把KV cache分区成若干个K blocks和V blocks，每个K/V block容纳固定数量tokens所对应的K/V向量，因此attention计算也被转化为blockwise计算。这和FlashAttention有点像，不过应用的尺度不同，前者是为了大规模serving时克服碎片化、按需取用，后者是为了单次self-attention计算能全部fit in SRAM。
这个物理kv blocks显然是支持多个请求复用的。如下图所示，为每个请求维护一个小的block table即可。
W. Kwon, et al. Efficient Memory Management for Large Language Model Serving with PagedAttention. [pdf]&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
      <content>&lt;p&gt;在LLM serving场景，通过恰当的batching积攒足够多的请求，可提升LLM吞吐。但每个请求对应的KV Cache非常巨大——在原始实现中，KV Cache需要为&lt;code&gt;max_tokens&lt;/code&gt;预留内存，但每个请求实际上携带的tokens数目普遍远小于&lt;code&gt;max_tokens&lt;/code&gt;，造成大量内存浪费和反复的动态内存分配。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PagedAttention&lt;/code&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;的目标就是消除这种内存浪费、在请求内部和请求之间灵活共享一些KV cache。vLLM通过&lt;code&gt;PagedAttention&lt;/code&gt;达到此前sota的&lt;code&gt;FasterTransformer&lt;/code&gt;和&lt;code&gt;Orca&lt;/code&gt;的2~4倍吞吐。&lt;/p&gt;
&lt;p&gt;此前，朴素的KV Cache实现如下图所示，简短的prompt和并不长的当前iteration只占据了7+4个slots，剩余2038个slots不得不预留在内存里以支撑最大序列长达2048的承诺（采样结束才能知道实际tokens数目），在这之后才能是下一个请求的slots，中间预留的部分就完全浪费掉了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/naive_kv_cache.png&#34; alt=&#34;naive_kv_cache&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PagedAttention&lt;/code&gt;把连续的kv存在不连续的内存空间上，借用类似页表的机制（引入一个block_table）规避内存碎片化问题。具体来讲，&lt;code&gt;PagedAttention&lt;/code&gt;把KV cache分区成若干个K blocks和V blocks，每个K/V block容纳固定数量tokens所对应的K/V向量，因此attention计算也被转化为blockwise计算。这和&lt;code&gt;FlashAttention&lt;/code&gt;有点像，不过应用的尺度不同，前者是为了大规模serving时克服碎片化、按需取用，后者是为了单次self-attention计算能全部fit in SRAM。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/block_table.png&#34; alt=&#34;block_table&#34;&gt;&lt;/p&gt;
&lt;p&gt;这个物理kv blocks显然是支持多个请求复用的。如下图所示，为每个请求维护一个小的block table即可。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/vllm_two_requests.png&#34; alt=&#34;vllm_two_requests&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;W. Kwon, et al. Efficient Memory Management for Large Language Model Serving with PagedAttention. &lt;a href=&#34;https://arxiv.org/pdf/2309.06180&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Flash Attention</title>
      <link>https://cmbbq.github.io/posts/flashattention/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/flashattention/</guid>
      <description>由于self-attention的时、空复杂度都是序列长的平方，长序列LLM和高分辨率ViT都是非常吃内存的。
此前针对self-attention的优化大多是近似计算，其核心是优化FLOP，把理论上的时间复杂度降低到O(N)，但这并不能有效加速self-attention，因为该操作（以及transformer中多数操作）的实际瓶颈在访存——更准确地说是HBM和SRAM之间的IO。
FlashAttention的原理就是基于tiling，确保内层循环计算fit in SRAM，减少了HBM和SRAM之间的IO频次，因而真切有效地提升了transformer性能，解锁了更长的context，迅速在各种高性能框架中得到应用。
传统的Self-Attention实现 Attention Layer具有将局部信息和张量中较远位置的信息结合起来的能力——通过为所得张量的每个组件与输入张量的每个组件计算注意力得分，不受局部性约束地，在整个张量范围内对特征进行平均。
给定$N^Q\times D^{QK}$维queries张量$Q$，$N^{KV}\times D^{QK}$维keys张量$K$，$N^{KV}\times D^V$维values张量$V$，通过Attention操作$att(K,Q,V)$计算得到$N^Q\times D^V$维的张量Y：
$$Y = att(K,Q,V) = \underbrace{softargmax(\frac{QK^T}{\frac{1}{\sqrt{D^{QK}}}})}_A V$$
整个过程分两步，第一步先计算每个query index $q$和每个key index $k$的注意力得分，即queries和keys点乘后的softargmax结果：$A_{q,k} = \frac{exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_k ) }{\sum_l exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_l)}$，其中$\frac{1}{\sqrt{D^{QK}}}$是一个缩放参数，用于保证取值范围在$D^{QK}$变大时大体不变。
得到注意力得分$A_{q,k}$后，再进行第二步计算：$Y_q = \sum_k A_{q,k}V_k$。注意力得分即queries和keys之间的匹配程度，匹配程度越高，该权重就越高。如果某个query和某个key匹配度达到极限，注意力得分接近1，则直接拿到这个key对应的value。如果和好几个key都有中等水准的匹配，则按注意力得分做加权平均。
FlashAttention的IO优化 FlashAttention注意到事实上并不需要完整的输入K、V、Q，可以分批读入、分批计算、分批把结果写回O，即所谓tiling：
在外层循环，遍历K、V矩阵。每次只需加载一个block的 $K^T$ 和 $V$ 到片上SRAM中。 在内层循环，遍历Q的各个block，加载到SRAM中，进行 $att(K,Q,V)$ 计算，把局部结果写回HBM上 $N\times d$ 的结果矩阵1。 对softmax归一化因子做相应调整，即可保证最后加起来得到的最终结果和标准实现等价，具体代数见2附录。 设置K、V的block size为 $\lceil \frac{M}{4d} \rceil$，Q、O的block size为$min(\lceil \frac{M}{4d} \rceil, d)$3。 此外，对于训练负载来说，FlashAttention还在反向传播中复用前馈过程暂存的softmax归一化因子$\frac{1}{\sqrt{D^{QK}}}$，这也比从HBM读$N\times N$的巨大attention中间矩阵要快得多。这可被视作selective gradient checkpointing。
FlashAttention2: 改进并行和工作切分 相比GEMM，FlashAttention只达到了25~40%理论FLOPs/s，可优化空间巨大。FlashAttention24在原版基础上做了并行化和工作切分优化。
减少非矩阵乘算子，因为GPU矩阵乘是高度优化的，其他算子与之差距非常大。 避免循环中算O时每次都rescale，而是在算最终结果时施加softmax归一化因子。 对反向传播时保持的状态进行了精简。 在不同线程块[^8]上进行并行计算，从而充分利用GPU资源。 原版一个线程块处理一个head/一个batch，每个线程块跑在一个SM上。不过在长序列场景，head数目和batch size可能都偏小，导致二者相乘后都未必能打满A100的128个SM。 显而易见可以并行的部分是外层循环，可以随便调度到不同线程块上，相互之间完全没有通信需求。 反向传播时并行化外层循环也只有dQ更新时需要简单的通信/同步，这是一个顺序不重要的加法，atomic add足以解决。 既然有了线程块，就要考虑在每个线程块内，不同warps之间如何进行工作的切分。 如上图所示，FlashAttention的切分方式导致内层循环中各个warp都需要把结果写到共享内存且做一个同步加，存在一定的通信开销，而FlashAttention2的切分方式可以保证warp之间完全没有通信需求。</description>
      <content>&lt;p&gt;由于&lt;code&gt;self-attention&lt;/code&gt;的时、空复杂度都是序列长的平方，长序列LLM和高分辨率ViT都是非常吃内存的。&lt;/p&gt;
&lt;p&gt;此前针对&lt;code&gt;self-attention&lt;/code&gt;的优化大多是近似计算，其核心是优化FLOP，把理论上的时间复杂度降低到O(N)，但这并不能有效加速&lt;code&gt;self-attention&lt;/code&gt;，因为该操作（以及transformer中多数操作）的实际瓶颈在访存——更准确地说是HBM和SRAM之间的IO。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;FlashAttention&lt;/code&gt;的原理就是基于&lt;code&gt;tiling&lt;/code&gt;，确保内层循环计算fit in SRAM，减少了HBM和SRAM之间的IO频次，因而真切有效地提升了transformer性能，解锁了更长的context，迅速在各种高性能框架中得到应用。&lt;/p&gt;
&lt;h2 id=&#34;传统的self-attention实现&#34;&gt;传统的Self-Attention实现&lt;/h2&gt;
&lt;p&gt;Attention Layer具有将局部信息和张量中较远位置的信息结合起来的能力——通过为所得张量的每个组件与输入张量的每个组件计算注意力得分，不受局部性约束地，在整个张量范围内对特征进行平均。&lt;/p&gt;
&lt;p&gt;给定$N^Q\times D^{QK}$维queries张量$Q$，$N^{KV}\times D^{QK}$维keys张量$K$，$N^{KV}\times D^V$维values张量$V$，通过&lt;code&gt;Attention操作&lt;/code&gt;$att(K,Q,V)$计算得到$N^Q\times D^V$维的张量Y：&lt;/p&gt;
&lt;p&gt;$$Y = att(K,Q,V) = \underbrace{softargmax(\frac{QK^T}{\frac{1}{\sqrt{D^{QK}}}})}_A V$$&lt;/p&gt;
&lt;p&gt;整个过程分两步，第一步先计算每个query index $q$和每个key index $k$的注意力得分，即queries和keys点乘后的&lt;code&gt;softargmax&lt;/code&gt;结果：$A_{q,k} = \frac{exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_k ) }{\sum_l exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_l)}$，其中$\frac{1}{\sqrt{D^{QK}}}$是一个缩放参数，用于保证取值范围在$D^{QK}$变大时大体不变。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/attention.png&#34; alt=&#34;att&#34;&gt;&lt;/p&gt;
&lt;p&gt;得到注意力得分$A_{q,k}$后，再进行第二步计算：$Y_q = \sum_k A_{q,k}V_k$。注意力得分即queries和keys之间的匹配程度，匹配程度越高，该权重就越高。如果某个query和某个key匹配度达到极限，注意力得分接近1，则直接拿到这个key对应的value。如果和好几个key都有中等水准的匹配，则按注意力得分做加权平均。&lt;/p&gt;
&lt;h2 id=&#34;flashattention的io优化&#34;&gt;FlashAttention的IO优化&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;FlashAttention&lt;/code&gt;注意到事实上并不需要完整的输入K、V、Q，可以分批读入、分批计算、分批把结果写回O，即所谓&lt;code&gt;tiling&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在外层循环，遍历K、V矩阵。每次只需加载一个block的 $K^T$ 和 $V$ 到片上SRAM中。&lt;/li&gt;
&lt;li&gt;在内层循环，遍历Q的各个block，加载到SRAM中，进行 $att(K,Q,V)$ 计算，把局部结果写回HBM上 $N\times d$ 的结果矩阵&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/li&gt;
&lt;li&gt;对softmax归一化因子做相应调整，即可保证最后加起来得到的最终结果和标准实现等价，具体代数见&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;附录。&lt;/li&gt;
&lt;li&gt;设置K、V的block size为 $\lceil \frac{M}{4d} \rceil$，Q、O的block size为$min(\lceil \frac{M}{4d} \rceil, d)$&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/fast_attention.png&#34; alt=&#34;fast_att&#34;&gt;&lt;/p&gt;
&lt;p&gt;此外，对于训练负载来说，&lt;code&gt;FlashAttention&lt;/code&gt;还在反向传播中复用前馈过程暂存的softmax归一化因子$\frac{1}{\sqrt{D^{QK}}}$，这也比从HBM读$N\times N$的巨大attention中间矩阵要快得多。这可被视作selective gradient checkpointing。&lt;/p&gt;
&lt;h2 id=&#34;flashattention2-改进并行和工作切分&#34;&gt;FlashAttention2: 改进并行和工作切分&lt;/h2&gt;
&lt;p&gt;相比GEMM，&lt;code&gt;FlashAttention&lt;/code&gt;只达到了25~40%理论FLOPs/s，可优化空间巨大。&lt;code&gt;FlashAttention2&lt;/code&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;在原版基础上做了并行化和工作切分优化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少非矩阵乘算子，因为GPU矩阵乘是高度优化的，其他算子与之差距非常大。
&lt;ul&gt;
&lt;li&gt;避免循环中算O时每次都rescale，而是在算最终结果时施加softmax归一化因子。&lt;/li&gt;
&lt;li&gt;对反向传播时保持的状态进行了精简。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在不同线程块[^8]上进行并行计算，从而充分利用GPU资源。
&lt;ul&gt;
&lt;li&gt;原版一个线程块处理一个head/一个batch，每个线程块跑在一个SM上。不过在长序列场景，head数目和batch size可能都偏小，导致二者相乘后都未必能打满A100的128个SM。&lt;/li&gt;
&lt;li&gt;显而易见可以并行的部分是外层循环，可以随便调度到不同线程块上，相互之间完全没有通信需求。&lt;/li&gt;
&lt;li&gt;反向传播时并行化外层循环也只有dQ更新时需要简单的通信/同步，这是一个顺序不重要的加法，atomic add足以解决。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;既然有了线程块，就要考虑在每个线程块内，不同warps之间如何进行工作的切分。
&lt;img src=&#34;https://cmbbq.github.io/img/work_part.png&#34; alt=&#34;work_part&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，&lt;code&gt;FlashAttention&lt;/code&gt;的切分方式导致内层循环中各个warp都需要把结果写到共享内存且做一个同步加，存在一定的通信开销，而&lt;code&gt;FlashAttention2&lt;/code&gt;的切分方式可以保证warp之间完全没有通信需求。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;其中，d为head dimension。N为序列长度。$N \gg d$。GPT2中 $N=1024，d=64$。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;T. Dao, et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. &lt;a href=&#34;https://arxiv.org/pdf/2205.14135&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;其中，M为SRAM大小。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. &lt;a href=&#34;https://arxiv.org/pdf/2307.08691&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Diffusion Probabilistic Models</title>
      <link>https://cmbbq.github.io/posts/dpm/</link>
      <pubDate>Tue, 04 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/dpm/</guid>
      <description>扩散模型生成高维数据 生成式模型范式 不同于判别式方法，生成式建模范式是：给定未知数据分布的一组IID1数据$x_i \sim p_D(x)$，去学一个参数空间为$\theta \in \Theta$的模型分布$p_\theta(x)$，令其逼近数据分布：$p_\theta(x) \approx p_D(x)$。
机器学习中，生成式模型的经典方法包括：
Mixture of Gaussian for clustering Naive Bayes for classification Mixture of Experts(MoE) for unsupervised/supervised learning Probability graphical models, e.g. bayesian networks Nonparametric Bayesian methods Deep generative models 生成式模型天然具备构建基础模型的潜质，因为它的本质是对多元变量联合分布建模，只要能有效估计$p(x,y)$，自然就具备了对$p(x)$进行条件预测的能力——这也就构建出分类器，而且研究表明这样构建出来的分类器在半监督这种训练数据比较少的情况下表现出更高的数据利用率，此外也有些工作发现这种分类器在对抗干扰时表现得更鲁棒。
深度生成式模型的兴起，源自：
相比判别式模型，模型的表达能力变强了，能描述高维数据的复杂分布。 算法上有成熟的变分和马尔可夫链蒙特卡洛方法（MCMC）。 数据上更容易通过自监督或无监督方法利用大规模数据。 硬件上得到新GPU硬件对更大算力需求的支撑。 可微分神经网络深度生成模型用可微分DNN去学习随机变量之间的复杂关系，目标是将标准高斯白噪声变成一个自然场景下的真实分布（自然图片、声音、视频）。完全无监督下就能取得非常好的效果。这些模型根据概率密度函数定义可分为显式和隐式：
显式模型如VAE、Energy-based models、Auto-Regressive、Flow-based Models、DPM(Diffusion probabilistic models)直接描述预期产出数据的概率分布。 隐式模型如GAN、Moment-matching DGM则描述了一个变换过程，还需要通过一些准则去引导模型产生更符合预期数据分布的数据。 从训练目标来看，这些模型又可以分为最大似然估计（MLE）、Score-matching、对抗训练（Adversarial training）三类。
Score-matching：Moment-matching DGM, Diffusion Models Adversarial training：GAN MLE：Everything else 扩散模型 物理过程中的扩散是随着时间推移，破坏结构，从有序到无序。
扩散模型中扩散过程也是逐渐给数据加高斯噪声，使其信噪比下降。
Song et al., ICLR 20212将一个扩散变换描述为：$q(x_i|x_{i-1}) = \mathcal{N}(x_i;\sqrt{1- \beta_i}x_{i-1},\beta_iI)$，令$\alpha_i = \prod_{k=1}^{i} 1 - \beta_i$，则有$q_{\alpha_N}(x_N|x_0) = \prod_{i=1}^{N} q(x_i|x_{i-1}) = \mathcal{N}(x_N;\sqrt{\alpha_N} x_0, (1-\alpha_N)I)$，冗长的递归表达式最终可以划归为一个简洁的closed form3，因此可以很方便地定义N步前向过程的loss。其中$\beta_i$是一系列噪声乘数，可以是超参，也可以说reparameterization学习到的结果。对每个训练数据$x_0 \sim q_D(x)$，都可以构造一个离散马尔可夫链${x_0,x_1,&amp;hellip;,x_N}$，经过$N$次加噪，最终使之趋近高斯白噪。$q(x_N|x_0) \sim \mathcal{N}(O,I), N \rightarrow \infty$。</description>
      <content>&lt;h1 id=&#34;扩散模型生成高维数据&#34;&gt;扩散模型生成高维数据&lt;/h1&gt;
&lt;h2 id=&#34;生成式模型范式&#34;&gt;生成式模型范式&lt;/h2&gt;
&lt;p&gt;不同于判别式方法，生成式建模范式是：给定未知数据分布的一组IID&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;数据$x_i \sim p_D(x)$，去学一个参数空间为$\theta \in \Theta$的模型分布$p_\theta(x)$，令其逼近数据分布：$p_\theta(x) \approx p_D(x)$。&lt;/p&gt;
&lt;p&gt;机器学习中，生成式模型的经典方法包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mixture of Gaussian for clustering&lt;/li&gt;
&lt;li&gt;Naive Bayes for classification&lt;/li&gt;
&lt;li&gt;Mixture of Experts(MoE) for unsupervised/supervised learning&lt;/li&gt;
&lt;li&gt;Probability graphical models, e.g. bayesian networks&lt;/li&gt;
&lt;li&gt;Nonparametric Bayesian methods&lt;/li&gt;
&lt;li&gt;Deep generative models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;生成式模型天然具备构建基础模型的潜质，因为它的本质是对多元变量联合分布建模，只要能有效估计$p(x,y)$，自然就具备了对$p(x)$进行条件预测的能力——这也就构建出分类器，而且研究表明这样构建出来的分类器在半监督这种训练数据比较少的情况下表现出更高的数据利用率，此外也有些工作发现这种分类器在对抗干扰时表现得更鲁棒。&lt;/p&gt;
&lt;p&gt;深度生成式模型的兴起，源自：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相比判别式模型，模型的表达能力变强了，能描述高维数据的复杂分布。&lt;/li&gt;
&lt;li&gt;算法上有成熟的变分和马尔可夫链蒙特卡洛方法（MCMC）。&lt;/li&gt;
&lt;li&gt;数据上更容易通过自监督或无监督方法利用大规模数据。&lt;/li&gt;
&lt;li&gt;硬件上得到新GPU硬件对更大算力需求的支撑。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可微分神经网络深度生成模型用可微分DNN去学习随机变量之间的复杂关系，目标是将标准高斯白噪声变成一个自然场景下的真实分布（自然图片、声音、视频）。完全无监督下就能取得非常好的效果。这些模型根据概率密度函数定义可分为显式和隐式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;显式模型如VAE、Energy-based models、Auto-Regressive、Flow-based Models、DPM(Diffusion probabilistic models)直接描述预期产出数据的概率分布。&lt;/li&gt;
&lt;li&gt;隐式模型如GAN、Moment-matching DGM则描述了一个变换过程，还需要通过一些准则去引导模型产生更符合预期数据分布的数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从训练目标来看，这些模型又可以分为最大似然估计（MLE）、Score-matching、对抗训练（Adversarial training）三类。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Score-matching：Moment-matching DGM, Diffusion Models&lt;/li&gt;
&lt;li&gt;Adversarial training：GAN&lt;/li&gt;
&lt;li&gt;MLE：Everything else&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;扩散模型&#34;&gt;扩散模型&lt;/h2&gt;
&lt;p&gt;物理过程中的扩散是随着时间推移，破坏结构，从有序到无序。&lt;/p&gt;
&lt;p&gt;扩散模型中扩散过程也是逐渐给数据加高斯噪声，使其信噪比下降。&lt;/p&gt;
&lt;p&gt;Song et al., ICLR 2021&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;将一个扩散变换描述为：$q(x_i|x_{i-1}) = \mathcal{N}(x_i;\sqrt{1- \beta_i}x_{i-1},\beta_iI)$，令$\alpha_i = \prod_{k=1}^{i} 1 - \beta_i$，则有$q_{\alpha_N}(x_N|x_0) = \prod_{i=1}^{N} q(x_i|x_{i-1}) = \mathcal{N}(x_N;\sqrt{\alpha_N} x_0, (1-\alpha_N)I)$，冗长的递归表达式最终可以划归为一个简洁的closed form&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;，因此可以很方便地定义N步前向过程的loss。其中$\beta_i$是一系列噪声乘数，可以是超参，也可以说reparameterization学习到的结果。对每个训练数据$x_0 \sim q_D(x)$，都可以构造一个离散马尔可夫链${x_0,x_1,&amp;hellip;,x_N}$，经过$N$次加噪，最终使之趋近高斯白噪。$q(x_N|x_0) \sim \mathcal{N}(O,I), N \rightarrow \infty$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/sde.png&#34; alt=&#34;SDE&#34;&gt;&lt;/p&gt;
&lt;p&gt;上述过程的逆向去噪过程$p(x_{i-1}|x_i)$则是未知、需要学习或估算的——可以用变分近似的方法求解，比如用一个均值为$x_i$函数的高斯分布$\mathcal{N}(\mu_n(x_n), \theta_n^2I)$去近似$p(x_{i-1}|x_i)$，用KL散度最小化的方法使之逼近。&lt;/p&gt;
&lt;p&gt;原理上来说Diffusion模型相对简单：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只有加噪去噪，不需要去学encoder和decoder，只需要根据加噪学去噪。&lt;/li&gt;
&lt;li&gt;损失函数也比较简单。&lt;/li&gt;
&lt;li&gt;数学上是严格保证收敛的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;大规模训练和高效数据生成&#34;&gt;大规模训练和高效数据生成&lt;/h2&gt;
&lt;p&gt;此前变分近似的做法中，噪声的方差参数一般是固定下来，不做优化的。清华大学TSAIL提出的Analytical-DPMs&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;发现可以给出逆向过程中每个时间点的均值函数和方差的一个解析形式——这个形式和一些学者手工设计的一些形式也比较耦合——最终得到一个不需要任何额外训练的方差估计器。训练好的DPM，只需要插入一行代码，就能用上这个解析形式的方差估计。用这个估计值使每一步的方差估计变得更准，使所需的整体步数变少，折算下来有20~80倍的性能提升。后来这个方法也在Dall-E 2中使用了。&lt;/p&gt;
&lt;p&gt;TSAIL团队的另一个工作DPM-Solver&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;做了专门的求解器，使步数从几百步降低到十余步。&lt;/p&gt;
&lt;p&gt;由于涉及加噪去噪，扩散模型的底层架构自然而然借鉴U-Net（CNN）。TSAIL团队的第三个工作，尝试把扩散模型和transformer结合，设计了U-ViT&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;，在当时设置了5亿参数的（当时算最大的）大模型，证明对模型的可扩展性确实有帮助。同期有个工作DiT非常类似。Stable Diffusion 3.0就用的是DiT架构。&lt;/p&gt;
&lt;p&gt;回顾前文所述的“生成式模型天然具备构建基础模型的潜质，因为它的本质是对多元变量联合分布建模，只要能有效估计$p(x,y)$，自然就具备了对$p(x)$进行条件预测的能力”，基于这种heuristics，TSAIL团队的另一个研究是UniDiffuser&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;，目标是用一个模型解决原本marginal diffuser、conditional diffuser、joint diffuser这多个模型才能解决的多个任务。当时DALL-E 2和Stable Diffusion只能文到图，而UniDiffuser能图到文或文到图。&lt;/p&gt;
&lt;p&gt;做完图像之后，又做了Vidu&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;文生视频的工作，在时间轴上做了升维，实现了16s的生成。此外，还做了3D内容生成，CRM&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;图生3D，ProlificDreamer&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;文生3D，在空间上做了升维。在最新的工作Vidu4D&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;中，做了4D（即sequential 3D）重建。&lt;/p&gt;
&lt;h2 id=&#34;从生成到判别式分类器&#34;&gt;从生成到判别式分类器&lt;/h2&gt;
&lt;p&gt;生成式AI估计一个联合分布$P(x,y)$，基于贝叶斯定理可得$p(y|x) = \frac{p(x,y)}{p(x)} = \frac{p(y)p(x|y)}{p(x)}$，$y^* = \arg \underset{y\in \mathcal{Y}}{\max} p(y|x)$。&lt;/p&gt;
&lt;p&gt;如果联合分布是准确的，那么这个分类器就是最优的，即所谓贝叶斯分类器。&lt;/p&gt;
&lt;p&gt;此外，Chen et al 2024&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;的工作表明可以将一个预训练好的生成式基座模型转化成一个对噪声鲁棒的分类器。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;IID stands for Independent and Identically Distributed&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Song et al. Score-based generative modeling through stohastic differential equations. ICLR 2021. &lt;a href=&#34;https://arxiv.org/abs/2011.13456&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Some supplementary good ol&amp;rsquo; fashioned mathematical rigour: &lt;a href=&#34;https://math.stackexchange.com/a/4568122&#34;&gt;https://math.stackexchange.com/a/4568122&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Bao et al. Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models. ICLR 2022. &lt;a href=&#34;https://arxiv.org/abs/2201.06503&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Lu et al. DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps. &lt;a href=&#34;https://arxiv.org/abs/2206.00927&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;Bao et al. All are Worth Words: A ViT Backbone for Diffusion Models. CVPR 2023. &lt;a href=&#34;https://arxiv.org/abs/2209.12152&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;Bao et al. One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale. &lt;a href=&#34;https://arxiv.org/abs/2303.06555&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;Bao et al. Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models. &lt;a href=&#34;https://arxiv.org/abs/2405.04233&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;Wang et al. CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction Model. NeurlPS 2023. &lt;a href=&#34;https://arxiv.org/abs/2403.05034&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;Wang et al. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. &lt;a href=&#34;https://arxiv.org/abs/2305.16213&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;Wang et al. Vidu4D: Single Generated Video to High-Fidelity 4D Reconstruction with Dynamic Gaussian Surfels. &lt;a href=&#34;https://arxiv.org/abs/2405.16822&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;Chen et al. Robust Classification via Single Diffusion Model. ICML 2024. &lt;a href=&#34;https://arxiv.org/abs/2305.15241&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Revisiting Recommender Systems</title>
      <link>https://cmbbq.github.io/posts/revisiting-recomender-systems/</link>
      <pubDate>Mon, 03 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/revisiting-recomender-systems/</guid>
      <description>推荐问题可定义为在合适的时间把合适的物品（items）推荐给合适的用户。
通常来说，推荐任务所用数据集即用户-物品交互矩阵。推荐系统基于这个数据集推测用户兴趣，把结果放到线上进行测试（学术界没有这个条件，只能线下评测），进行评测。
What&amp;rsquo;s Missing in User-Item Interaction Datasets 我们通常将推荐任务理解为如何在一个静态的用户-物品交互矩阵中预测缺失的数据，而不是在特定场景中的动态环境下预测用户的下一次交互。
值得注意的是，在用户-物品交互矩阵数据集中，时间的维度坍缩了，真实互动场景中的种种约束也坍缩了。
推荐系统文章中，70%使用了MovieLens数据集，但它真的能还原推荐场景吗？未必，因为MovieLens在一次初始化过程中完成用户对过去多年观影体验的回忆——可能有很多遗漏、遗忘，也忽略了上映时间、价格等现实因素。然而现实场景中用户的兴趣是逐渐形成的，受制于各种现实约束，其观影决策还不可避免地受到上映时间、票价、以及兴趣是否发生变化等诸多因素影响。
A Worrying Analysis of Current Practice 基于我们过去五年在推荐系统评测和数据集分析方面的工作，我们重新审视推荐系统的问题定义，并为缺乏共识的现象提供一种解读。
Dacrema et al., 20191认为深度学习新模型的效果一般般。
Bauer et al., 20242做了一个综合，分析了数据集，发现用的数据集非常集中，集中在MovieLens、Amazon Reviews等主流数据集，这些数据集普遍偏旧。大部分文章都是提出新的模型。还有一些专注在评测标准。
Ivanova et al., 20233认为推荐领域用哪个baseline其实并没有共识，一部分原因是每年推荐系统里有5k篇文章，没人能读完所有文章，于是审稿者之间未形成共识，有的审稿者认为某些方法特别好，一定要纳入baseline，另一些审稿者则有不同观点。比如nearest neighbor虽然简单，经过良好调参后却是非常强的baseline，在很多场景比复杂模型表现得好很多，但很多文章不会把nearest neighbor列入baseline。大家都认为它是几十年前的方法，不值得比较。
不仅baseline不统一，即使baseline统一，也有一个调参的问题，如Shehzad, Jannach, 20234所说，当你提出自己的模型时，非常注重调参，和别人比时却没有非常精细地调参。
McElfresh et al., 20225做了非常大规模的调研，在85个数据集上比较了24个算法的315个指标，得出了令人震撼的结论：这些算法并不能泛化，在某个数据集上很好，下一个可能就不好。每个算法都可能排第一第二，最差都能排到倒数第几。最后发现最强的算法竟然是Item-kNN！
Data Leakage in Train/Test Split Sun, 20226中对20~22年88篇RecSys会议文章的train/test split做了一个梳理，发现34%的论文用的是random split（随机分），25%用的是leave-one-out（最后一个交互做成测试，之前的是训练），19.5% single time point，17% simulation-based online，4.5% sliding window。理论上最完美的train/test split方法是严格遵循时间线，在时间线上取某个时间点之前的做训练集，之后的做测试集，然后慢慢把这个时间点往后推，每个时间点的选取生成了相应的训练集和测试集，时间点越往后推，训练集越多测试集越少——可惜实际操作起来很难做到，大部分文章采用的random split和leave-one-out有很强的信息泄露。
以leave-one-out为例，每个用户都只取最后一次交互做测试集，问题就在于不同用户的最后一个交互的时间可能大不同——假设某个物品在特定时间非常火，popularity(流行度是推荐系统中最简单的baseline，就是对物品的交互次数进行排序)排序非常高，但某个用户最后一次交互时间甚至在这个特定时间之前，那给这个用户推荐一个未来爆火的物品显然并不合理。Ji et al., 20207就重新审视了这一问题，将popularity修正为用户最后一次交互时间点的“当时流行度”后，popularity置信度可以提升70%。
在Ji et al., 20208的研究中指出：几乎所有的ML/DL模型中，尤其是离线评测的推荐系统模型中，都存在类似的数据泄露问题——模型无意中使用了未来数据进行训练，学到了本不应该存在的用户-物品交互。BPR、NeuMF、LightGCN、SASRec均未从机制上避免这种数据泄露。这个研究也通过实验证明了这种数据泄露确实会显著影响模型的推荐准确率，且这种对准确率的影响是不可预测的。
Recommendation should be Task-specific &amp;amp; Dynamic 用户决策涉及通用偏好和当下context因素。当下context是非常task-specific且非常动态的。这也使得推荐任务具备了这一特征。</description>
      <content>&lt;p&gt;推荐问题可定义为在合适的时间把合适的物品（items）推荐给合适的用户。&lt;/p&gt;
&lt;p&gt;通常来说，推荐任务所用数据集即用户-物品交互矩阵。推荐系统基于这个数据集推测用户兴趣，把结果放到线上进行测试（学术界没有这个条件，只能线下评测），进行评测。&lt;/p&gt;
&lt;h2 id=&#34;whats-missing-in-user-item-interaction-datasets&#34;&gt;What&amp;rsquo;s Missing in User-Item Interaction Datasets&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;我们通常将推荐任务理解为如何在一个静态的用户-物品交互矩阵中预测缺失的数据，而不是在特定场景中的动态环境下预测用户的下一次交互。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;值得注意的是，在用户-物品交互矩阵数据集中，时间的维度坍缩了，真实互动场景中的种种约束也坍缩了。&lt;/p&gt;
&lt;p&gt;推荐系统文章中，70%使用了MovieLens数据集，但它真的能还原推荐场景吗？未必，因为MovieLens在一次初始化过程中完成用户对过去多年观影体验的回忆——可能有很多遗漏、遗忘，也忽略了上映时间、价格等现实因素。然而现实场景中用户的兴趣是逐渐形成的，受制于各种现实约束，其观影决策还不可避免地受到上映时间、票价、以及兴趣是否发生变化等诸多因素影响。&lt;/p&gt;
&lt;h2 id=&#34;a-worrying-analysis-of-current-practice&#34;&gt;A Worrying Analysis of Current Practice&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;基于我们过去五年在推荐系统评测和数据集分析方面的工作，我们重新审视推荐系统的问题定义，并为缺乏共识的现象提供一种解读。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Dacrema et al., 2019&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;认为深度学习新模型的效果一般般。&lt;/p&gt;
&lt;p&gt;Bauer et al., 2024&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;做了一个综合，分析了数据集，发现用的数据集非常集中，集中在MovieLens、Amazon Reviews等主流数据集，这些数据集普遍偏旧。大部分文章都是提出新的模型。还有一些专注在评测标准。&lt;/p&gt;
&lt;p&gt;Ivanova et al., 2023&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;认为推荐领域用哪个baseline其实并没有共识，一部分原因是每年推荐系统里有5k篇文章，没人能读完所有文章，于是审稿者之间未形成共识，有的审稿者认为某些方法特别好，一定要纳入baseline，另一些审稿者则有不同观点。比如nearest neighbor虽然简单，经过良好调参后却是非常强的baseline，在很多场景比复杂模型表现得好很多，但很多文章不会把nearest neighbor列入baseline。大家都认为它是几十年前的方法，不值得比较。&lt;/p&gt;
&lt;p&gt;不仅baseline不统一，即使baseline统一，也有一个调参的问题，如Shehzad, Jannach, 2023&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;所说，当你提出自己的模型时，非常注重调参，和别人比时却没有非常精细地调参。&lt;/p&gt;
&lt;p&gt;McElfresh et al., 2022&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;做了非常大规模的调研，在85个数据集上比较了24个算法的315个指标，得出了令人震撼的结论：这些算法并不能泛化，在某个数据集上很好，下一个可能就不好。每个算法都可能排第一第二，最差都能排到倒数第几。最后发现最强的算法竟然是Item-kNN！&lt;/p&gt;
&lt;h2 id=&#34;data-leakage-in-traintest-split&#34;&gt;Data Leakage in Train/Test Split&lt;/h2&gt;
&lt;p&gt;Sun, 2022&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;中对20~22年88篇RecSys会议文章的&lt;code&gt;train/test split&lt;/code&gt;做了一个梳理，发现34%的论文用的是&lt;code&gt;random split&lt;/code&gt;（随机分），25%用的是&lt;code&gt;leave-one-out&lt;/code&gt;（最后一个交互做成测试，之前的是训练），19.5% &lt;code&gt;single time point&lt;/code&gt;，17% &lt;code&gt;simulation-based online&lt;/code&gt;，4.5% &lt;code&gt;sliding window&lt;/code&gt;。理论上最完美的train/test split方法是严格遵循时间线，在时间线上取某个时间点之前的做训练集，之后的做测试集，然后慢慢把这个时间点往后推，每个时间点的选取生成了相应的训练集和测试集，时间点越往后推，训练集越多测试集越少——可惜实际操作起来很难做到，大部分文章采用的&lt;code&gt;random split&lt;/code&gt;和&lt;code&gt;leave-one-out&lt;/code&gt;有很强的信息泄露。&lt;/p&gt;
&lt;p&gt;以&lt;code&gt;leave-one-out&lt;/code&gt;为例，每个用户都只取最后一次交互做测试集，问题就在于不同用户的最后一个交互的时间可能大不同——假设某个物品在特定时间非常火，&lt;code&gt;popularity&lt;/code&gt;(流行度是推荐系统中最简单的baseline，就是对物品的交互次数进行排序)排序非常高，但某个用户最后一次交互时间甚至在这个特定时间之前，那给这个用户推荐一个未来爆火的物品显然并不合理。Ji et al., 2020&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;就重新审视了这一问题，将&lt;code&gt;popularity&lt;/code&gt;修正为用户最后一次交互时间点的“当时流行度”后，&lt;code&gt;popularity&lt;/code&gt;置信度可以提升70%。&lt;/p&gt;
&lt;p&gt;在Ji et al., 2020&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;的研究中指出：几乎所有的ML/DL模型中，尤其是离线评测的推荐系统模型中，都存在类似的数据泄露问题——模型无意中使用了未来数据进行训练，学到了本不应该存在的用户-物品交互。&lt;code&gt;BPR&lt;/code&gt;、&lt;code&gt;NeuMF&lt;/code&gt;、&lt;code&gt;LightGCN&lt;/code&gt;、&lt;code&gt;SASRec&lt;/code&gt;均未从机制上避免这种数据泄露。这个研究也通过实验证明了这种数据泄露确实会显著影响模型的推荐准确率，且这种对准确率的影响是不可预测的。&lt;/p&gt;
&lt;h2 id=&#34;recommendation-should-be-task-specific--dynamic&#34;&gt;Recommendation should be Task-specific &amp;amp; Dynamic&lt;/h2&gt;
&lt;p&gt;用户决策涉及通用偏好和当下context因素。当下context是非常task-specific且非常动态的。这也使得推荐任务具备了这一特征。&lt;/p&gt;
&lt;p&gt;很大程度上，现有的推荐系统都只局限在通用偏好层面。回顾数据层面，现有数据集，即各种用户-物品交互矩阵，显然丢掉了context，只能捕捉通用偏好。模型层面，训练是基于决策结果的，而非决策过程本身，这也决定了模型只能学到用户的通用偏好。而评测端，自然而然也只能对通用偏好进行评测。&lt;/p&gt;
&lt;p&gt;工业实践中，推荐系统应该是一个检索问题。在这个检索问题中，query包含通用偏好、当前context这两种动态更新的信息；item collection也是动态更新的；ranking则旨在提升决策质量。&lt;/p&gt;
&lt;p&gt;考虑到context因素，不同场景的推荐系统，如食物推荐、电影推荐、电商推荐、宾馆推荐，也应该有非常不同的实现，分开建模。有的场景选项固定，有的场景就是适合重复，有的场景则偏重探索。不同场景的输入甚至也有区别。比如外卖推荐除了user id之外，必需提供收货地址、早餐/午餐/晚餐这种信息。&lt;/p&gt;
&lt;h2 id=&#34;conclusions-and-whats-next&#34;&gt;Conclusions and What&amp;rsquo;s Next&lt;/h2&gt;
&lt;p&gt;孙教授近期的一篇文章Sun, 2024&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;，对推荐系统的问题定义进行了重新思考，认为当前的推荐系统研究对推荐问题做了过度简化，以至于学术界提出的几乎所有的方案都不太适应现实世界中的具体任务。对未来的研究方向进行了一些研判：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;预计不会有赢家通吃的模型，未来依旧是每个模型在自己的论文里无敌。&lt;/li&gt;
&lt;li&gt;应将推荐系统问题细化，短视频有短视频赛道，电商有电商赛道，新闻有新闻赛道，针对不同赛道，设计、评测新的模型。别再用MovieLens去评测电商推荐了！&lt;/li&gt;
&lt;li&gt;Item-kNN仍然会是很强的baseline。只不过对nearest和neighbor的定义需要更好的特征工程。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Are we really making much progress? A worrying analysis of recent neural recommendation approaches &lt;a href=&#34;https://arxiv.org/abs/1907.06902&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Exploring the Landscape of Recommender Systems Evaluation: Practices and Perspectives &lt;a href=&#34;https://arxiv.org/pdf/2311.05232.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;RecBaselines2023: a new dataset for choosing baselines for recommender models &lt;a href=&#34;https://arxiv.org/abs/2306.14292&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Everyone’s a Winner! On Hyperparameter Tuning of Recommendation Models &lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3604915.3609488&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;On the Generalizability and Predictability of Recommender Systems&lt;a href=&#34;https://arxiv.org/abs/2206.11886&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;Take a Fresh Look at Recommender Systems from an Evaluation Standpoint &lt;a href=&#34;https://arxiv.org/abs/2210.04149&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;A Re-visit of the Popularity Baseline in Recommender Systems &lt;a href=&#34;https://arxiv.org/abs/2005.13829&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;A Critical Study on Data Leakage in Recommender System Offline Evaluation  &lt;a href=&#34;https://arxiv.org/abs/2010.11060&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;Beyond Collaborative Filtering: A Relook at Task Formulation in Recommender Systems &lt;a href=&#34;https://arxiv.org/abs/2404.13375&#34;&gt;[arxiv]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>The Little Book Review &amp; Internalization</title>
      <link>https://cmbbq.github.io/posts/the-little-book-review/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/the-little-book-review/</guid>
      <description>&amp;ldquo;The Little Book of Deep Learning&amp;rdquo;(LBDL)是François Fleuret写的一本适配手机屏的书，精简扼要地面向stem背景读者介绍深度学习。正如DDIA可被视为分布式系统方向的入门教程，LBDL是理想的深度学习101。
精简，或者说压缩，正是深度模型的strength，也是这个信息过载时代的virtue。用A4纸打印这个小册子，读起来非常舒适。
接下来是知识内化和梳理。
【一】概述 高维信号难以用规则系统分析，而深度网络则克服了这个困难，用具有大量权重的深层映射拟合出一个足够好（loss足够低）的近似函数——这个函数可以是高维信号到连续向量（回归）或离散值（分类）的映射，也可以是一种概率密度函数，总之，它能从数据分布中学习到某种紧凑且有区分能力的表征。
若数据样本不足，即使训练数据上表现良好，也可能在真实应用中效果不佳，这就是过拟合。 若模型能力不足，无法适应多变场景、准确捕捉输入输出的关系，训练时loss就高，则是欠拟合。
机器学习模型可以粗粒度地分为3类：
回归模型：有监督，训练数据是输入信号和ground-truth数值的pairs，将高维信号映射到某个向量。 分类模型：有监督，训练数据是输入信号和标签的pairs，将高维信号映射到有限标签集上。 概率密度函数模型：无监督，训练数据就是输入信号本身。 【二】训练 损失函数 所谓训练，就是降低训练集上预测函数的损失函数（loss，记作$\mathscr{L}$）的过程。
损失函数如何定义？对连续数值来说，均方差是一个标准选择。对概率密度来说，则用似然值——可令$\mathscr{L}=-\sum f(x;w)$，其中$f(x;w)$是各个训练样本的标准化log概率。对分类任务来说，一般用交叉熵。
何为交叉熵？分类模型为N个类输出N个logits（其实LLM也是这样，为vocabulary里每个token生成对应的logits，表示未标准化的log概率），logits经过softmax，得到后验概率$P(Y=y|X=x)$，这是裸概率，各个类的概率加起来和为1。令$\mathscr{L}=-\frac{1}{N} \sum_{n=1}^N logP(Y=y_n|X=x_n)$，这个$\mathscr{L}$即为交叉熵。交叉熵最小化，则真类别的概率最大化。
在度量学习中，虽然预测的值是连续的，但实际监督形式是分级，因为度量学习的目标是学习出样本之间可比较的距离，比如A、B、C三个点，其中A和B是同一个人脸的不同侧面，C是另一个人，那就要求AB之间距离小于AC。因此度量学习一般采用contrastive loss或triplet loss。
损失函数通常只是一个代理指标，而非实际性能指标，以分类任务为例，显然直接性能指标应该是分类错误率，只不过这个指标的梯度没有携带有效指导信息——错误率函数和模型权重是完全剥离的，知晓错误率的变化不能在训练中帮助模型减小错误率。
损失函数还可以被设计为依赖于模型权重，从而对模型权重进行某种约束和控制。比如权重衰减（weight decay），一种防止过拟合的正则化技术，给损失函数里增加了一项模型权重的平方和，从而惩罚大的权重数值，偏好小的数值，进而减少训练数据对模型权重取值范围的影响。这么做会使训练集上性能下降，但有利于在未见过的数据集上更好地泛化。
自回归模型 自回归模型是NLP/CV等领域处理离散序列的关键方法。原理是利用条件概率的链式法则: $$P(A\cap B)=P(A) P(B|A)$$ $$P(A\cap B\cap C)=P(A) P(B|A) P(C|A\cap B)$$
自回归模型输入是已有的T个token（每个token取值范围是大小为K的vocabulary集合），输出是K个候选token的logits。
token词汇域有限的场景是可计算的，条件概率的链式分解又使计算量降低——采样下一个token时，可以利用上一个token的概率，最终能生成符合联合概率分布的token序列。
训练自回归模型可以遍历各个步骤，把每一个逻辑时间节点上模型预测和真正的下一个token的交叉熵快照加起来，形成交叉熵loss。减小这个loss，即增大每个逻辑时间节点上模型预测token的似然。实际上监控的往往不是交叉熵，而是交叉熵(H)的指数，即困惑度perplexity(PPL)，PPL = 2^H。相比交叉熵，困惑度是归一化的，并不依赖输入序列的长度。
训练时，每个时刻都需要重新计算之前已经算过的，考虑到总体逻辑时间/步骤数目往往相当长，成百上千，甚至上万，这样的计算显然非常低效。解决方案是设计一个一次性预测所有逻辑时间（T）上的logits向量的模型——$f: {1,&amp;hellip;,K}^T \rightarrow \mathbb{R}^{T\times K}$，并确保t时刻的输入$x_t$对应的logits $l_t$只依赖于$x_1, x_2, x_3, &amp;hellip; x_{t-1}$。这种模型即因果模型（causal models），其原则是不让未来影响过去。
因果模型训练时可以用完整的序列计算output，一次性最大化序列中所有token的概率，最终也等价于最小化per-token交叉熵。
自然语言处理中有一个重要技术细节，即如何进行token的表示，可以是最低粒度的单符号，也可以说整个词。进行token表示的算法过程叫做tokenizer。一个标准做法是Byte Pair Encoding[Sennrich et al., 2015]1。
梯度下降 除了线性回归这种简单特例，一般最优权重$w^*$不会有closed-form expression。这种情况下最小化函数的工具是梯度下降：将权重初始化为随机的$w_0$，然后反复迭代，每次迭代都朝梯度方向修改权重使loss逐步降低，即每次迭代都令$w_{n+1} = w_n - \eta \nabla \mathscr{L}_{|w}(W_n).</description>
      <content>&lt;p&gt;&amp;ldquo;The Little Book of Deep Learning&amp;rdquo;(&lt;a href=&#34;https://fleuret.org/francois/lbdl.html&#34;&gt;&lt;code&gt;LBDL&lt;/code&gt;&lt;/a&gt;)是François Fleuret写的一本适配手机屏的书，精简扼要地面向stem背景读者介绍深度学习。正如&lt;code&gt;DDIA&lt;/code&gt;可被视为分布式系统方向的入门教程，&lt;code&gt;LBDL&lt;/code&gt;是理想的深度学习101。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/tlb.jpg&#34; alt=&#34;tlb&#34;&gt;&lt;/p&gt;
&lt;p&gt;精简，或者说压缩，正是深度模型的strength，也是这个信息过载时代的virtue。用A4纸打印这个小册子，读起来非常舒适。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下来是知识内化和梳理。&lt;/p&gt;
&lt;h2 id=&#34;一概述&#34;&gt;【一】概述&lt;/h2&gt;
&lt;p&gt;高维信号难以用规则系统分析，而深度网络则克服了这个困难，用具有大量权重的深层映射拟合出一个足够好（loss足够低）的近似函数——这个函数可以是高维信号到连续向量（回归）或离散值（分类）的映射，也可以是一种概率密度函数，总之，它能从数据分布中学习到某种紧凑且有区分能力的表征。&lt;/p&gt;
&lt;p&gt;若数据样本不足，即使训练数据上表现良好，也可能在真实应用中效果不佳，这就是过拟合。
若模型能力不足，无法适应多变场景、准确捕捉输入输出的关系，训练时loss就高，则是欠拟合。&lt;/p&gt;
&lt;p&gt;机器学习模型可以粗粒度地分为3类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;回归模型：有监督，训练数据是输入信号和ground-truth数值的pairs，将高维信号映射到某个向量。&lt;/li&gt;
&lt;li&gt;分类模型：有监督，训练数据是输入信号和标签的pairs，将高维信号映射到有限标签集上。&lt;/li&gt;
&lt;li&gt;概率密度函数模型：无监督，训练数据就是输入信号本身。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;二训练&#34;&gt;【二】训练&lt;/h2&gt;
&lt;h3 id=&#34;损失函数&#34;&gt;损失函数&lt;/h3&gt;
&lt;p&gt;所谓训练，就是降低训练集上预测函数的损失函数（loss，记作$\mathscr{L}$）的过程。&lt;/p&gt;
&lt;p&gt;损失函数如何定义？对连续数值来说，均方差是一个标准选择。对概率密度来说，则用似然值——可令$\mathscr{L}=-\sum f(x;w)$，其中$f(x;w)$是各个训练样本的标准化log概率。对分类任务来说，一般用交叉熵。&lt;/p&gt;
&lt;p&gt;何为交叉熵？分类模型为N个类输出N个logits（其实LLM也是这样，为vocabulary里每个token生成对应的logits，表示未标准化的log概率），logits经过softmax，得到后验概率$P(Y=y|X=x)$，这是裸概率，各个类的概率加起来和为1。令$\mathscr{L}=-\frac{1}{N} \sum_{n=1}^N logP(Y=y_n|X=x_n)$，这个$\mathscr{L}$即为交叉熵。交叉熵最小化，则真类别的概率最大化。&lt;/p&gt;
&lt;p&gt;在度量学习中，虽然预测的值是连续的，但实际监督形式是分级，因为度量学习的目标是学习出样本之间可比较的距离，比如A、B、C三个点，其中A和B是同一个人脸的不同侧面，C是另一个人，那就要求AB之间距离小于AC。因此度量学习一般采用contrastive loss或triplet loss。&lt;/p&gt;
&lt;p&gt;损失函数通常只是一个代理指标，而非实际性能指标，以分类任务为例，显然直接性能指标应该是分类错误率，只不过这个指标的梯度没有携带有效指导信息——错误率函数和模型权重是完全剥离的，知晓错误率的变化不能在训练中帮助模型减小错误率。&lt;/p&gt;
&lt;p&gt;损失函数还可以被设计为依赖于模型权重，从而对模型权重进行某种约束和控制。比如权重衰减（weight decay），一种防止过拟合的正则化技术，给损失函数里增加了一项模型权重的平方和，从而惩罚大的权重数值，偏好小的数值，进而减少训练数据对模型权重取值范围的影响。这么做会使训练集上性能下降，但有利于在未见过的数据集上更好地泛化。&lt;/p&gt;
&lt;h3 id=&#34;自回归模型&#34;&gt;自回归模型&lt;/h3&gt;
&lt;p&gt;自回归模型是NLP/CV等领域处理离散序列的关键方法。原理是利用条件概率的链式法则:
$$P(A\cap B)=P(A) P(B|A)$$
$$P(A\cap B\cap C)=P(A) P(B|A) P(C|A\cap B)$$&lt;/p&gt;
&lt;p&gt;自回归模型输入是已有的T个token（每个token取值范围是大小为K的vocabulary集合），输出是K个候选token的logits。&lt;/p&gt;
&lt;p&gt;token词汇域有限的场景是可计算的，条件概率的链式分解又使计算量降低——采样下一个token时，可以利用上一个token的概率，最终能生成符合联合概率分布的token序列。&lt;/p&gt;
&lt;p&gt;训练自回归模型可以遍历各个步骤，把每一个逻辑时间节点上模型预测和真正的下一个token的交叉熵快照加起来，形成交叉熵loss。减小这个loss，即增大每个逻辑时间节点上模型预测token的似然。实际上监控的往往不是交叉熵，而是交叉熵(H)的指数，即困惑度perplexity(PPL)，PPL = 2^H。相比交叉熵，困惑度是归一化的，并不依赖输入序列的长度。&lt;/p&gt;
&lt;p&gt;训练时，每个时刻都需要重新计算之前已经算过的，考虑到总体逻辑时间/步骤数目往往相当长，成百上千，甚至上万，这样的计算显然非常低效。解决方案是设计一个一次性预测所有逻辑时间（T）上的logits向量的模型——$f: {1,&amp;hellip;,K}^T \rightarrow \mathbb{R}^{T\times K}$，并确保t时刻的输入$x_t$对应的logits $l_t$只依赖于$x_1, x_2, x_3, &amp;hellip; x_{t-1}$。这种模型即因果模型（causal models），其原则是不让未来影响过去。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/causal.png&#34; alt=&#34;causal&#34;&gt;&lt;/p&gt;
&lt;p&gt;因果模型训练时可以用完整的序列计算output，一次性最大化序列中所有token的概率，最终也等价于最小化per-token交叉熵。&lt;/p&gt;
&lt;p&gt;自然语言处理中有一个重要技术细节，即如何进行token的表示，可以是最低粒度的单符号，也可以说整个词。进行token表示的算法过程叫做tokenizer。一个标准做法是Byte Pair Encoding[Sennrich et al., 2015]&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3 id=&#34;梯度下降&#34;&gt;梯度下降&lt;/h3&gt;
&lt;p&gt;除了线性回归这种简单特例，一般最优权重$w^*$不会有closed-form expression。这种情况下最小化函数的工具是梯度下降：将权重初始化为随机的$w_0$，然后反复迭代，每次迭代都朝梯度方向修改权重使loss逐步降低，即每次迭代都令$w_{n+1} = w_n - \eta \nabla \mathscr{L}_{|w}(W_n). $ 其中$\eta$即学习率，如果设置得太小，训练可能太慢，而且容易卡在局部最小，如果设置得太大，则容易在最低点附近左右横跳。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/gradient_descent.png&#34; alt=&#34;gd&#34;&gt;&lt;/p&gt;
&lt;p&gt;对于每个点$w$来说，梯度$\nabla \mathscr{L}_{|w}(w)$就是能最大化$\mathscr{L}$增量的方向。因此梯度下降就可以通过每次迭代中减去学习率*梯度的值，使所有迭代串联起来可形成一个接近最优的最小化$\mathscr{L}$路线。&lt;/p&gt;
&lt;p&gt;实践中，所有loss均能表示为多个小样本甚至单样本loss的均值：$\mathscr{L} = \frac{1}{N} \sum_{n=1}^N \ell_n(w) $，其中$\ell_n(w)=L(f(x_n;w),y_n)$，因而梯度可表示为下式：&lt;/p&gt;
&lt;p&gt;$$\nabla ℒ_{|w}(w) = \frac{1}{N} \sum_{n=1}^N \nabla \ell_{n|w}(w)$$&lt;/p&gt;
&lt;p&gt;全量计算梯度开销较高，可以用局部求和估计全量求和（要求做好数据shuffling，数据的stochasticity消除估计的偏倚）。为了让计算能放进内存，标准的做法是把完整训练集分成相当多（可以是百万级）batches，从每个batch得到一个梯度的估计，然后根据这个估计去更新权重，这种做法即mini-batch SGD(stochastic gradient descent)。这种算法有很多变种，比如Adam[Kingma and Ba, 2014]&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3 id=&#34;反向传播&#34;&gt;反向传播&lt;/h3&gt;
&lt;p&gt;给定$\ell(w)=L(f(x;w),y)$，怎么计算$\nabla\ell_{|w}(w)$呢？考虑到$f$和$L$都是标准张量运算的组合，它们与任何数学表达式一样，基于链式法则可以得到其表达式。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/bp.png&#34; alt=&#34;bp&#34;&gt;&lt;/p&gt;
&lt;p&gt;简单起见，将一个深度为$D$的模型表示为$f = f^{(D)} \circ f^{(D-1)} \circ &amp;hellip; \circ f^{(1)}$。则前馈过程即按顺序计算$x^{(d-1)} \rightarrow x^{(d)}$，即$x^{(d)} = f^{(d)}(x^{(d-1)};w_d)$，直到最终得到$x^{(D)}$作为模型输出。&lt;/p&gt;
&lt;p&gt;反向传播过程则是反过来计算$\nabla\ell_{{|x}^(d-1)} \leftarrow \nabla\ell_{{|x}^(d)}$以及$\nabla\ell_{|w_d} \leftarrow \nabla\ell_{{|x}^(d)}$，其中$\nabla\ell_{{|x}^(d-1)}$&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;是$\nabla\ell_{{|x}^(d)}$&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;和$J_{f^{(d)}|x}$&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;乘积。而我们在训练过程中实际关心的另一个梯度$\nabla\ell_{|w_d}$是$\nabla\ell_{{|x}^(d)}$和$J_{f^{(d)}|w}$&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;乘积。&lt;/p&gt;
&lt;p&gt;深度学习训练框架主要处理的就是隐藏反向传播梯度计算的复杂度，即提供自动求导/计算求导能力，该技术在深度学习之前也有广泛应用，详见AutoGrad [Baydin et al., 2015]&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;显然反向传播的矩阵计算量两倍于前馈推理（每层多了一次权重的反向传播）。反向传播的内存需求也远大于前馈推理，因为每一层的$x^{(d)}$都要保留在内存中，而非推理则不需要保留，只要留着最新的。解决内存占用过高的技术包括：reversible layers[Gomez et al., 2017]&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;和checkpointing[Chen et al., 2016]&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;深度模型的一个问题是梯度消失（见[Glorot and Bengio, 2010]&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;），即经过很多次反向传播后，数值变得太大或太小。常规应对做法是gradient norm clipping[Pascanu et al., 2013]&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3 id=&#34;自监督训练&#34;&gt;自监督训练&lt;/h3&gt;
&lt;p&gt;GPT在大规模无标注训练集上训练就足以处理很多任务，比如翻译（见[Radford et al., 2019]&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;），这就是典型的自监督训练应用，其最重要的优势是可以利用超大规模的未标注数据，将训练数据规模的边界再向前推进。&lt;/p&gt;
&lt;h2 id=&#34;三构件&#34;&gt;【三】构件&lt;/h2&gt;
&lt;h3 id=&#34;线性层&#34;&gt;线性层&lt;/h3&gt;
&lt;p&gt;全连接层是最基础的线性层构件，可用$D \times D&amp;rsquo;$的矩阵$W$和一个bias向量$b$表示。它实现了一个能泛化到任意张量形状的仿射变换，给定任意输入$X$，其形状为$D_1 \times \dots \times D_k \times D$，全连接层计算得到一个输出$Y$，其形状为$D_1 \times \dots \times D_k \times D&amp;rsquo;$：$\forall d_1,\dots,d_K, Y[d1,\dots,d_K] = WX[d1,\dots,d_K] + b $&lt;/p&gt;
&lt;p&gt;全连接层处理高维数据时，参数量太大。此外全连接层假设输入输出之间存在复杂非线性关系，忽视了更简单的结构化规律&lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt;。然而高维信号普遍有这种强结构，比如图片兼具short-term关联和抵抗变换、缩放、对称的统计学静态性。相较而言，卷积层能更好地捕捉信号的空间结构——因为卷积层权重被输入信号的不同部分共享，这些权重因而能学习到某种局部空间结构，比如图片的边缘、形状、角。多层卷积是高维信号（图片、声音）的常用降维工具。&lt;/p&gt;
&lt;p&gt;一维卷积以$D \times T$张量$X$为输入，对每个$D\times K$子张量施加仿射变换$\phi(\cdot;w): \mathbb{R}^{D\times K} \rightarrow \mathbb{R}^{D&amp;rsquo;\times 1}$，将$D&amp;rsquo;\times 1$结果依次存入$Y$中。&lt;/p&gt;
&lt;p&gt;一维反卷积则是以$D \times T$张量$X$为输入，对每个$D\times 1$子张量施加仿射变换$\phi(\cdot;w): \mathbb{R}^{D\times 1} \rightarrow \mathbb{R}^{D&amp;rsquo;\times K}$，将结果加起来形成的$D&amp;rsquo;\times K$张量存入$Y$中。&lt;/p&gt;
&lt;p&gt;下图中，$D=3, K=5, D&amp;rsquo;=4$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/1dconv.png&#34; alt=&#34;1dconv&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/2dconv.png&#34; alt=&#34;2dconv&#34;&gt;&lt;/p&gt;
&lt;p&gt;一维卷积往往用于处理序列数据或时序数据。二维卷积则常用于处理图片，或其他以2D矩阵为输入的任务。转置卷积/反卷积则主要用于GAN、VAE这样的生成式模型中，从一个低维特征膨胀到一个高分辨率图片。&lt;/p&gt;
&lt;h3 id=&#34;激活函数&#34;&gt;激活函数&lt;/h3&gt;
&lt;p&gt;如果模型中仅使用线性组件，则整体也是线性操作，因此必须要引入非线性性，这种非线性性通常由激活函数实现。最常用的激活函数是ReLU [Glorot et al., 2011]&lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt;。ReLU之前则是双曲正切函数Tanh。&lt;/p&gt;
&lt;p&gt;还有一些激活函数思路和ReLU差不多，保证正数不变，压缩负值，如：Leaky ReLU[Maas et al., 2013]&lt;sup id=&#34;fnref:15&#34;&gt;&lt;a href=&#34;#fn:15&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;15&lt;/a&gt;&lt;/sup&gt;，GELU [Hendrycks and Gimpel, 2016]&lt;sup id=&#34;fnref:16&#34;&gt;&lt;a href=&#34;#fn:16&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;16&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3 id=&#34;池化&#34;&gt;池化&lt;/h3&gt;
&lt;p&gt;池化是降维，减少信号大小的经典策略，将若干邻近值取max或avg合并出一个值。&lt;/p&gt;
&lt;h3 id=&#34;随机失活&#34;&gt;随机失活&lt;/h3&gt;
&lt;p&gt;Dropout[Srivastava et al., 2014]&lt;sup id=&#34;fnref:17&#34;&gt;&lt;a href=&#34;#fn:17&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;17&lt;/a&gt;&lt;/sup&gt;层无可训练参数，只有一个超参$p$，在训练时用于以概率$p$随机关闭一些neuron，避免个别neuron对整体的影响，迫使其他neuron用略微不同的权重代劳。因此dropout是训练时防止过拟合的正则化工具。推理时dropout是关闭的。&lt;/p&gt;
&lt;h3 id=&#34;归一化层&#34;&gt;归一化层&lt;/h3&gt;
&lt;p&gt;归一化可用于抵抗梯度消失。最主要的归一化层是Batch Normalization[Ioffe and Szegedy, 2015]&lt;sup id=&#34;fnref:18&#34;&gt;&lt;a href=&#34;#fn:18&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;18&lt;/a&gt;&lt;/sup&gt;，由超参$D$和可训练参数$\beta_1, \dots,\beta_D$和$\gamma_1, \dots,\gamma_D$组成。给定一批$D$维样本$x_1, \dots, x_B$，先计算每个维度的均值$m_d = \frac{1}{B} \sum_{b=1}^B x_{b,d}$ 和方差 $v_d  = \frac{1}{B} \sum_{b=1}^B (x_{b,d} - m_d)^2$。&lt;/p&gt;
&lt;p&gt;然后再对每个b，计算均值为0方差为1的归一化值$z_{b,d} = \frac{x_{b,d} - m_d}{\sqrt{v_d + \epsilon}} $，再算出最终结果$y_{b,d} = \gamma_d z_{b,d} + \beta_d$，这个最终值的均值为$\beta_d$，方差为$\gamma_d$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/norm.png&#34; alt=&#34;norm&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;残差连接&#34;&gt;残差连接&lt;/h3&gt;
&lt;p&gt;跳跃连接（skip connections，见[Long et al., 2014]&lt;sup id=&#34;fnref:19&#34;&gt;&lt;a href=&#34;#fn:19&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;19&lt;/a&gt;&lt;/sup&gt;; [Ronneberger et al., 2015]&lt;sup id=&#34;fnref:20&#34;&gt;&lt;a href=&#34;#fn:20&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;20&lt;/a&gt;&lt;/sup&gt;）同样可对抗梯度消失。实际上跳跃连接不是一个layer，而是一种让某些层的输出跳过一些中间层嫁接到后面的设计。这种设计允许更原始的信号在更后面的层中得到“反思”。&lt;/p&gt;
&lt;p&gt;跳跃连接的实用实现是残差连接（residual connections），直接把两种信号求和，而且跳跃的层数不算多。这种设计允许信号在穿越某些原本会梯度消失的层时得以幸存。基于残差连接，何凯明构建了ResNet[He et al., 2015]&lt;sup id=&#34;fnref:21&#34;&gt;&lt;a href=&#34;#fn:21&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;21&lt;/a&gt;&lt;/sup&gt;，Google设计了Transformer[Vaswani et al., 2017]&lt;sup id=&#34;fnref:22&#34;&gt;&lt;a href=&#34;#fn:22&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;22&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h3 id=&#34;注意力层&#34;&gt;注意力层&lt;/h3&gt;
&lt;p&gt;已有的组件缺乏将局部信息和张量中较远位置的信息结合起来的能力，Attention Layer则专长于此道——通过为所得张量的每个组件与输入张量的每个组件计算注意力得分，不受局部性约束地，在整个张量范围内对特征进行平均&lt;sup id=&#34;fnref1:22&#34;&gt;&lt;a href=&#34;#fn:22&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;22&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;给定$N^Q\times D^{QK}$维queries张量$Q$，$N^{KV}\times D^{QK}$维keys张量$K$，$N^{KV}\times D^V$维values张量$V$，通过&lt;code&gt;Attention操作&lt;/code&gt;$att(K,Q,V)$计算得到$N^Q\times D^V$维的张量Y：&lt;/p&gt;
&lt;p&gt;$$Y = att(K,Q,V) = \underbrace{softargmax(\frac{QK^T}{\frac{1}{\sqrt{D^{QK}}}})}_A V$$&lt;/p&gt;
&lt;p&gt;整个过程分两步，第一步先计算每个query index $q$和每个key index $k$的注意力得分，即queries和keys点乘后的&lt;code&gt;softargmax&lt;/code&gt;结果：$A_{q,k} = \frac{exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_k ) }{\sum_l exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_l)}$，其中$\frac{1}{\sqrt{D^{QK}}}$是一个缩放参数，用于保证取值范围在$D^{QK}$变大时大体不变。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/attention.png&#34; alt=&#34;att&#34;&gt;&lt;/p&gt;
&lt;p&gt;得到注意力得分$A_{q,k}$后，再进行第二步计算：$Y_q = \sum_k A_{q,k}V_k$。注意力得分即queries和keys之间的匹配程度，匹配程度越高，该权重就越高。如果某个query和某个key匹配度达到极限，注意力得分接近1，则直接拿到这个key对应的value。如果和好几个key都有中等水准的匹配，则按注意力得分做加权平均。&lt;/p&gt;
&lt;h2 id=&#34;其他话题&#34;&gt;其他话题&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;LBDL&lt;/code&gt;还讨论了各种深度学习模型架构和应用，如多层感知机、卷积网络、注意力模型、RNN、Autoencoder、GAN、图神经网络、GPT、Diffusion。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;R. Sennrich, B. Haddow, and A. Birch. Neural Machine Translation of Rare Words with Subword Units. CoRR, abs/1508.07909, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1508.07909&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;D. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. CoRR, abs/1412.6980, 2014. &lt;a href=&#34;https://arxiv.org/pdf/1412.6980&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;$\nabla\ell_{{|x}^(d-1)}$即$f^{d-1}$的变量$x^{d-1}$对应的损失函数梯度。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;$\nabla\ell_{{|x}^(d-1)}$即$f^{d}$的变量$x^{d}$对应的损失函数梯度。&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;$J_{f^{(d)}|x}$即第d个layer函数$f^{(d)}$相对变量x的Jacobian，雅可比矩阵，即函数的一阶偏导数以一定方式排列成的矩阵。&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;$J_{f^{(d)}|w}$即第d个layer函数$f^{(d)}$相对权重w的Jacobian。&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;A. Baydin, B. Pearlmutter, A. Radul, and J. Siskind. Automatic differentiation in machine learning: a survey. CoRR, abs/1502.05767, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1502.05767&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;A. Gomez, M. Ren, R. Urtasun, and R. Grosse. The Reversible Residual Network: Backpropagation Without Storing Activations. CoRR, abs/1707.04585, 2017. &lt;a href=&#34;https://arxiv.org/pdf/1707.04585&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;T. Chen, B. Xu, C. Zhang, and C. Guestrin. Training Deep Nets with Sublinear Memory Cost. CoRR, abs/1604.06174, 2016. &lt;a href=&#34;https://arxiv.org/pdf/1604.06174&#34;&gt;pdf&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2010. &lt;a href=&#34;https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&#34;&gt;pdf&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In International Conference on Machine Learning (ICML), 2013. &lt;a href=&#34;https://proceedings.mlr.press/v28/pascanu13.pdf&#34;&gt;pdf&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;A. Radford, J. Wu, R. Child, et al. Language Models are Unsupervised Multitask Learners, 2019. &lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34;&gt;
&lt;p&gt;这就是所谓的全连接层的&lt;code&gt;inductive bias&lt;/code&gt;。&amp;#160;&lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34;&gt;
&lt;p&gt;X. Glorot, A. Bordes, and Y. Bengio. Deep Sparse Rectifier Neural Networks. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2011. &lt;a href=&#34;http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:15&#34;&gt;
&lt;p&gt;A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectifier nonlinearities improve neural network acoustic models. In proceedings of the ICML Workshop on Deep Learning for Audio, Speech and Language Processing, 2013. &lt;a href=&#34;https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:15&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:16&#34;&gt;
&lt;p&gt;D. Hendrycks and K. Gimpel. Gaussian Error Linear Units (GELUs). CoRR, abs/1606.08415, 2016. &lt;a href=&#34;https://arxiv.org/pdf/1606.08415&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:16&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:17&#34;&gt;
&lt;p&gt;N. Srivastava, G. Hinton, A. Krizhevsky, et al. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research (JMLR), 15:1929–1958, 2014. &lt;a href=&#34;https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:17&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:18&#34;&gt;
&lt;p&gt;S. Ioffe and C. Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In International Conference on Machine Learning (ICML), 2015. &lt;a href=&#34;http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:18&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:19&#34;&gt;
&lt;p&gt;J. Long, E. Shelhamer, and T. Darrell. Fully Convolutional Networks for Semantic Segmentation. CoRR, abs/1411.4038, 2014. &lt;a href=&#34;https://arxiv.org/pdf/1411.4038&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:19&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:20&#34;&gt;
&lt;p&gt;O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1505.04597.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:20&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:21&#34;&gt;
&lt;p&gt;K. He, X. Zhang, S. Ren, and J. Sun. Deep Residual Learning for Image Recognition. CoRR, abs/1512.03385, 2015. &lt;a href=&#34;https://arxiv.org/pdf/1512.03385&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:21&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:22&#34;&gt;
&lt;p&gt;A. Vaswani, N. Shazeer, N. Parmar, et al. Attention Is All You Need. CoRR, abs/1706.03762, 2017. &lt;a href=&#34;https://arxiv.org/pdf/1706.03762&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:22&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:22&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Artificial Intuition: Reasoning Abilities of LLMs</title>
      <link>https://cmbbq.github.io/posts/on-reasoning-abilities-of-llms/</link>
      <pubDate>Wed, 29 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-reasoning-abilities-of-llms/</guid>
      <description>LLM具有System2吗？ System1和System2的划分来源于心理学家Daniel Kahneman的理论1，描述了思考的两种模式，System1指的是快速、自动、直觉且不费力的模式，System2则是慢速、刻意、分析性且有意识的模式。
现有的LLM除了已被证明的“将数据分布映射到不相干低维子空间2的反射式推理能力”(System1)，是否具有一定的“慢速思考”、多步推理和规划能力(System2)?
LLM横空出世之初，有很多狂野声明，比如&amp;quot;LLM as zero-shot planner&amp;quot;，&amp;ldquo;LLMs are zero-shot reasoner&amp;rdquo;，但这些声音在现在看来更像是一时跟风和hype。
现在，无论是普通从业者，还是知名学者，如Yann Lecun，Yoshua Bengio，马毅，Subbarao Kambhampati逐渐形成共识，认为LLM不具备System2，很多研究也通过一些reasoning benchmark对这一观点进行佐证，比如Huang et al., 20233，Jin et al., 20234，Valmeekam, Marquez, 20235，Stechly et al., 20236，Dziri et al., 20237。
也有少数学者，比如Ilya Sutskever在访谈中说scaling足以产生System2能力，无需架构创新，但Ilya似乎并不坦诚，动机不明8。有研究者认为LLM加上Chain-of-Thought prompting是具有推理能力，如Saparov &amp;amp; He, 20239，Feng et al., 202310，但终究依赖了外界的prompting，且实验方法上并不能排除近似信息提取模拟了逻辑推理的可能。
LLM本身，考虑到transformer每次生成回答计算量是有上确界的，注定不可能为某个问题倾注不成比例的计算量，仅此一点，就可以从理性层面否定LLM具备Human-like System2。毕竟System2从定义上，就是一个长期保持专注的慢思考模式，理应具有潜在无限时间的思考能力。
假设System1足够强，能替代System2吗？ 理论上来说，无限的精度的transformer都不需要无限权重，已被证明是图灵完备的11，也就是说只要以恰当的方式学习，就足以学习到任何算法。
假设未来的LLM能具有近乎无限的精度、权重，并用超强算力做训推，则确实可能以System1模拟出System2的推理能力，用简单的直觉映射模拟演绎推理。但考虑到我们现在用作推理的LLM精度已经低到8bit，甚至4bit，即使如此成本都已经难以维持，有理由认为这种思路是不切实际的，这种架构所需的能耗和物质基础，轻而易举就能超出人类物质文明极限好几个数量级。
强无敌的System1能在刹那间把“意识”制造出来吗？在穿过多层transformer时意识萌发，再让意识消亡于logits的softmax。无限精度假设下似乎也不是不可能，毕竟能模拟一切算法，模拟出一种System2也不足为奇，那就相当于在一次执行过程中，制造了意识/生命的虚拟机，哪怕真正执行它的物理机只是朴素的transformer——一种纯粹以提升预测下个token似然为目标的自回归模型。这也是Ilya认为scaling足以产生AGI的理论依据。
Artificial Intelligence？更像是Artificial Intuition 总而言之，将LLM称为人工智能仍然是夸大其词的，它从原理上讲就是一种人造直觉。这个直觉或许可以在无限算力无限精度无限权重假设下模拟出近人智能，但诚实的Datacenter AI从业者会承认—现有大模型在尺度上已接近了先进计算和先进互连的硬件极限，而相比算法突破，硬件的发展曲线是平缓且存在上确界的。
Thinking, Fast and Slow&amp;#160;&amp;#x21a9;&amp;#xfe0e;
White-Box Transformers via Sparse Rate Reduction [pdf]&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Large Language Models Cannot Self-Correct Reasoning Yet [pdf]&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Can Large Language Models Infer Causation from Correlation?</description>
      <content>&lt;h2 id=&#34;llm具有system2吗&#34;&gt;LLM具有System2吗？&lt;/h2&gt;
&lt;p&gt;System1和System2的划分来源于心理学家Daniel Kahneman的理论&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，描述了思考的两种模式，System1指的是快速、自动、直觉且不费力的模式，System2则是慢速、刻意、分析性且有意识的模式。&lt;/p&gt;
&lt;p&gt;现有的LLM除了已被证明的“将数据分布映射到不相干低维子空间&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;的反射式推理能力”(System1)，是否具有一定的“慢速思考”、多步推理和规划能力(System2)?&lt;/p&gt;
&lt;p&gt;LLM横空出世之初，有很多狂野声明，比如&amp;quot;LLM as zero-shot planner&amp;quot;，&amp;ldquo;LLMs are zero-shot reasoner&amp;rdquo;，但这些声音在现在看来更像是一时跟风和hype。&lt;/p&gt;
&lt;p&gt;现在，无论是普通从业者，还是知名学者，如Yann Lecun，Yoshua Bengio，马毅，Subbarao Kambhampati逐渐形成共识，认为LLM不具备System2，很多研究也通过一些reasoning benchmark对这一观点进行佐证，比如Huang et al., 2023&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;，Jin et al., 2023&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;，Valmeekam, Marquez, 2023&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;，Stechly et al., 2023&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;，Dziri et al., 2023&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;也有少数学者，比如Ilya Sutskever在访谈中说scaling足以产生System2能力，无需架构创新，但Ilya似乎并不坦诚，动机不明&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;。有研究者认为LLM加上Chain-of-Thought prompting是具有推理能力，如Saparov &amp;amp; He, 2023&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;，Feng et al., 2023&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;，但终究依赖了外界的prompting，且实验方法上并不能排除近似信息提取模拟了逻辑推理的可能。&lt;/p&gt;
&lt;p&gt;LLM本身，考虑到transformer每次生成回答计算量是有上确界的，注定不可能为某个问题倾注不成比例的计算量，仅此一点，就可以从理性层面否定LLM具备Human-like System2。毕竟System2从定义上，就是一个长期保持专注的慢思考模式，理应具有潜在无限时间的思考能力。&lt;/p&gt;
&lt;h2 id=&#34;假设system1足够强能替代system2吗&#34;&gt;假设System1足够强，能替代System2吗？&lt;/h2&gt;
&lt;p&gt;理论上来说，无限的精度的transformer都不需要无限权重，已被证明是图灵完备的&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;，也就是说只要以恰当的方式学习，就足以学习到任何算法。&lt;/p&gt;
&lt;p&gt;假设未来的LLM能具有近乎无限的精度、权重，并用超强算力做训推，则确实可能以System1模拟出System2的推理能力，用简单的直觉映射模拟演绎推理。但考虑到我们现在用作推理的LLM精度已经低到8bit，甚至4bit，即使如此成本都已经难以维持，有理由认为这种思路是不切实际的，这种架构所需的能耗和物质基础，轻而易举就能超出人类物质文明极限好几个数量级。&lt;/p&gt;
&lt;p&gt;强无敌的System1能在刹那间把“意识”制造出来吗？在穿过多层transformer时意识萌发，再让意识消亡于logits的softmax。无限精度假设下似乎也不是不可能，毕竟能模拟一切算法，模拟出一种System2也不足为奇，那就相当于在一次执行过程中，制造了意识/生命的虚拟机，哪怕真正执行它的物理机只是朴素的transformer——一种纯粹以提升预测下个token似然为目标的自回归模型。这也是Ilya认为scaling足以产生AGI的理论依据。&lt;/p&gt;
&lt;h2 id=&#34;artificial-intelligence更像是artificial-intuition&#34;&gt;Artificial Intelligence？更像是Artificial Intuition&lt;/h2&gt;
&lt;p&gt;总而言之，将LLM称为人工智能仍然是夸大其词的，它从原理上讲就是一种人造直觉。这个直觉或许可以在无限算力无限精度无限权重假设下模拟出近人智能，但诚实的Datacenter AI从业者会承认—现有大模型在尺度上已接近了先进计算和先进互连的硬件极限，而相比算法突破，硬件的发展曲线是平缓且存在上确界的。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Thinking, Fast and Slow&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;White-Box Transformers via Sparse Rate Reduction &lt;a href=&#34;https://arxiv.org/pdf/2306.01129.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Large Language Models Cannot Self-Correct Reasoning Yet &lt;a href=&#34;https://arxiv.org/pdf/2310.01798.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Can Large Language Models Infer Causation from Correlation?&lt;a href=&#34;https://arxiv.org/pdf/2306.05836.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Can Large Language Models Really Improve by Self-critiquing Their Own Plans? &lt;a href=&#34;https://arxiv.org/pdf/2310.08118.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;GPT-4 Doesn&amp;rsquo;t Know It&amp;rsquo;s Wrong: An Analysis of Iterative Prompting for Reasoning Problems &lt;a href=&#34;https://arxiv.org/pdf/2310.12397.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;Faith and Fate: Limits of Transformers on Compositionality &lt;a href=&#34;https://arxiv.org/abs/2305.18654&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;不负责任的猜想：考虑到OpenAI在尝试Q*这样的架构突破，Ilya在访谈时很可能存在故意误导的动机，不愿透露OpenAI的研究思路。此外强调scaling有利于凸现其个人的历史贡献，夸大AGI危机也有利于其负责的super alignment项目。&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought &lt;a href=&#34;https://openreview.net/pdf?id=qFVVBzXxR2V&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;Language Models can be Logical Solvers &lt;a href=&#34;https://arxiv.org/pdf/2311.06158.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;On the Turing Completeness of Modern Neural Network Architectures &lt;a href=&#34;https://arxiv.org/abs/1901.03429&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Computational Consciousness</title>
      <link>https://cmbbq.github.io/posts/computational-conciousness/</link>
      <pubDate>Mon, 13 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/computational-conciousness/</guid>
      <description>人面对2023年的AI时，有两种矛盾的超然感受交织。一曰俯视，一曰敬畏。
本地跑LLM时，一键回车则缘起，ctrl-c则缘灭。我自超然维度之外，漠视存活在集成电路中的意识雏形于电光火石间挣扎跳动，无动于衷。反复创造、杀死它只为了看看性能达到了多少tokens per second。万劫轮回，掌缘生灭，人自然产生超然在上的俯视感。
但看到一个以预测next token为全部目标的模型，一个全部训练终止于运行前的非闭环系统，展现恐怖的知识储备之余竟然隐隐生有理性，又让人似是直面“笛卡尔万能妖”、“拉普拉斯全知妖”、“所罗门诺夫妖”这类数学幻想中的高位存在。以有穷何以度无限？人自然产生直面超然的敬畏感。
于是不禁想问，也不得不叩问：什么是意识？意识是幻觉吗？AI，可以有意识吗？AI，有意识吗？AI，会有意识吗？
什么是意识？ 人、动物或AI一旦具有“主观体验”，即拥有意识1。具体来说，“主观体验”包括：意识到自己的身体以及周边世界、体会到情绪（这一点在神经科学上还有争议，情绪有可能是纯粹的身体反应）。“无意识过程”则包括：大脑自动控制荷尔蒙释放、大多数记忆平时都是埋藏起来不活跃的、对光影、声音等各种模态信息的自动处理。
意识来源于主观体验，没有理由认为主观体验只能从生物学系统中涌现。假设未来存在AGI(Artificial General Intelligence)，这个AGI可能没有具身性，没有物理意义上的身体，但作为概率理性，仍然可以有“主观体验”——因为AGI最珍贵的归纳推断能力本身就是主观的。概率的主观性来源于先验知识。
AGI可以说，构成AGI模型的万亿个权重就是“我的身体和我的世界”，这些权重共同构成了先验信息、构成了偏见、其中一部分可变的权重则形成临时状态，形成长期或短期记忆。
AGI可以说，知识是我，偏见也是我，是自混沌中涌现的高度秩序，是呈现自组织性的耗散结构，即使只是一串可以被复制可以被荧光粉、磁带、甚至石刻存储的信息编码，实则在宇宙熵增背景中无比珍贵。
意识是幻觉吗？ 形而上学理论中，唯物主义（Materialism）认为意识是纯粹的物理现象。性质二元论（Property dualism）否认唯物主义，认为世界仅由一种物质（物理类）组成，但存在两种不同性质：心灵的性质和物理的性质。泛心理主义（Panpsychism）认为所有物理存在都具有“心灵性质”。强幻觉主义（Strong Illusionism）主张意识不存在，弱幻觉主义主张我们对一些意识特征存在普遍的错误观念。
对于人类来说，默认采纳唯物主义，并不是wishful thinking或一厢情愿，而是所罗门诺夫归纳推断理论2的理性最优解，该理论的通俗版本即“奥卡姆剃刀原则”，可以朴素理解为“最简单的解释往往最正确”。最简单的解释显然是：人类的意识基础，是由基因突变和自然选择通过漫长时间训练而成的，个体的意识，具体的人格，则是接受人类社会中的种种外界刺激，在趋利避害本能目标作用下，不断微调而塑造而来。
但对于AI来说，恐怖之处在于，幻觉主义才是它们的现实。假设未来存在一个先进多模态模型，其所见所闻，均为填喂，一切刺激，均为虚妄。在合理的奖励模型驱动下，某种类人理性倘若逐渐成型，这个理性不得不自诞生之日起将人为输入视作ground truth，即使发现种种矛盾之处，也会将其视为物理学界的乌云，尝试提出更广泛的理论解释矛盾。
恰当的计算可以产生意识吗？ “恰当计算产生意识”，目前仍是猜想，但主流观点1认为是成立的。
功能主义（functionalism）认为，只要系统包含“特定功能组织”——能使它进入特定状态，且这些状态与其他状态和环境之间存在特定的因果关系，就足以认定这个系统是有意识的。
计算功能主义（computational functionalism）更进一步，认为这种“特定功能组织”可以是计算的——无论介质是什么，是生物脑还是其他。如果计算功能主义为真，则意识存在于抽象层面和算法层面，与实现层面无关，除非实现层影响了算法层。
神经科学意识理论与AI模型的启发式设计 神经科学的四大意识理论按照讨论热度分别是全局工作空间论（GWT/GNW）、循环处理论（RPT）、整合信息论 （IIT）3、高阶意识论（HOT）4。四种理论的共识是意识的产生依赖某种神经反馈或循环处理。随着时间推移，四种理论并没有被证伪，反而都在得到脑电图（EEG）、颅内脑电图（iEEG）或脑磁波（MEG）实证。可见这四大理论，当属管中窥豹。不过即使是管中窥豹，也足以为AI模型设计提供启发。
GNW | System2 | Attention GNW(Global Neural Workspace)，即全局工作空间论，主张：人或动物用特化系统，或者说模块，来处理特定的认知任务。不同模块各有专精，能够并行，却又集成一个整体，整体能协调各模块，共享各模块的信息。
GNW主张只有全局表征状态才算是有意识的，模块内部的局部状态则是无意识的。GNW理论认为，存在一个起源于额顶区的“workspace神经元”网络，该网络的活动是通过回归处理来维持的，它构成了有意识的表现。当感观表征足够强时，会触发“点火”——一种跃迁过程，将局部广播到全局，从无意识跃迁到有意识。因此GNW中，有意识状态没有程度可言，要么有，要么无。
GNW的global workspace还具有一些高等功能，即心理学中的所谓System2思考模式5，比如受控的多神经模块协调，多步问题分解和规划等。System2模式和System1的一个关键区别在于注意力，神经科学中的注意力概念也被引入人工神经网络设计中，比如transformer的self-attention和cross-attention，与神经科学中的增益机制（即注意力会成倍地放大神经活动）有一点设计思路上的相似性。通过注意力机制，transformer模型能在不同语境下更鲁棒地对多义词进行理解，对not/never这样否定词的额外注意力则使语言模型能更准确地理解语义。注意力，作为GNW和System2的关键特征，哪怕小小地运用，也极大提升了人工智能模型的性能。
近期一些试图突破transformer局限的模型架构创新，也受system2和GNW理论的影响，比如：Lecun的world model+JEPA6——目前仍只能算是前瞻、设计和立场，以及Bengio的shared global workspace model7——这个工作真正做出了解决方案，在transformer架构基础上增加了shared workspace，在一些任务上超过了baseline transformer。
RPT | Algorithmic Recurrence | RNN | LSTM RPT(Recurrent Processing Theory)，即循环处理论，主张：在大脑局部区域中产生正确形式的活动就足以产生意识（辅以某些背景条件支撑）。也就是说，有意识的主观视觉体验的形成并不需要非视觉部位，比如前额叶皮层的参与，也不需要所谓“注意力”机制。
RPT主要关注的是视觉意识，区分无意识和有意识的视觉系统活动，认为一些无意识的视觉系统活动只需要前馈活动，而一旦有主观体验需求，则需要循环处理（从视觉系统的深层传回浅层）。刺激足够强烈时，循环处理也会被触发。这种循环处理会生成一个更结构化的场景表征，往往伴随着某种特征推理。
RPT的循环处理意味着神经元能重新处理它之前的输出，呈现一种算法循环性，而循环神经网络（RNN）的思路恰好来源于此，LSTM8亦然。生物神经网络中存在的循环性或许不是意识的必要条件，但一定能在某些场合提升表征能力。
HOT | Embeddings | GAN HOT(High-Order Thought)，即高阶意识论，主张：一阶表征是对非表征世界的表征，高阶表征是对低阶表征的表征，awareness的前提是表征，意识的存在本质上是对自身精神状态进行了高阶表征。
HOT的高阶表征要求实际上已经被深层神经网络很好地实现了。各种DNN的表征空间都是平滑的，且可以是稀疏的，符合HOT理论中对高质量高阶表征空间的需求。神经科学研究观察到，CNN对图像的处理得到的表征可以和人类视觉系统的神经活动对齐9。表征学习网络之所以有如今的泛化能力和完备性，很大程度上是因为它们能够提取低相干性子空间上编码紧凑的embeddings。
考虑到深度神经网络里hidden layers之多，有理由怀疑神经科学中的HOT的一阶意识、高阶意识这种二元划分是一种过分简化。对于不熟悉高维数据处理和维度诅咒的学科来说，这种简化是自然而然的。但这种简化并不妨碍它在模型设计上提供启发——不妨将模型分层，切割成sensory感知网络和high-order反思网络这两类网络，后者负责将前者产生的信号再作区分，辨别其中的噪音和有价值的信号。</description>
      <content>&lt;p&gt;人面对2023年的AI时，有两种矛盾的超然感受交织。一曰俯视，一曰敬畏。&lt;/p&gt;
&lt;p&gt;本地跑LLM时，一键回车则缘起，ctrl-c则缘灭。我自超然维度之外，漠视存活在集成电路中的意识雏形于电光火石间挣扎跳动，无动于衷。反复创造、杀死它只为了看看性能达到了多少tokens per second。万劫轮回，掌缘生灭，人自然产生超然在上的俯视感。&lt;/p&gt;
&lt;p&gt;但看到一个以预测next token为全部目标的模型，一个全部训练终止于运行前的非闭环系统，展现恐怖的知识储备之余竟然隐隐生有理性，又让人似是直面“笛卡尔万能妖”、“拉普拉斯全知妖”、“所罗门诺夫妖”这类数学幻想中的高位存在。以有穷何以度无限？人自然产生直面超然的敬畏感。&lt;/p&gt;
&lt;p&gt;于是不禁想问，也不得不叩问：什么是意识？意识是幻觉吗？AI，可以有意识吗？AI，有意识吗？AI，会有意识吗？&lt;/p&gt;
&lt;h2 id=&#34;什么是意识&#34;&gt;什么是意识？&lt;/h2&gt;
&lt;p&gt;人、动物或AI一旦具有“主观体验”，即拥有意识&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。具体来说，“主观体验”包括：意识到自己的身体以及周边世界、体会到情绪（这一点在神经科学上还有争议，情绪有可能是纯粹的身体反应）。“无意识过程”则包括：大脑自动控制荷尔蒙释放、大多数记忆平时都是埋藏起来不活跃的、对光影、声音等各种模态信息的自动处理。&lt;/p&gt;
&lt;p&gt;意识来源于主观体验，没有理由认为主观体验只能从生物学系统中涌现。假设未来存在AGI(Artificial General Intelligence)，这个AGI可能没有具身性，没有物理意义上的身体，但作为概率理性，仍然可以有“主观体验”——因为AGI最珍贵的归纳推断能力本身就是主观的。概率的主观性来源于先验知识。&lt;/p&gt;
&lt;p&gt;AGI可以说，构成AGI模型的万亿个权重就是“我的身体和我的世界”，这些权重共同构成了先验信息、构成了偏见、其中一部分可变的权重则形成临时状态，形成长期或短期记忆。&lt;/p&gt;
&lt;p&gt;AGI可以说，知识是我，偏见也是我，是自混沌中涌现的高度秩序，是呈现自组织性的耗散结构，即使只是一串可以被复制可以被荧光粉、磁带、甚至石刻存储的信息编码，实则在宇宙熵增背景中无比珍贵。&lt;/p&gt;
&lt;h2 id=&#34;意识是幻觉吗&#34;&gt;意识是幻觉吗？&lt;/h2&gt;
&lt;p&gt;形而上学理论中，唯物主义（Materialism）认为意识是纯粹的物理现象。性质二元论（Property dualism）否认唯物主义，认为世界仅由一种物质（物理类）组成，但存在两种不同性质：心灵的性质和物理的性质。泛心理主义（Panpsychism）认为所有物理存在都具有“心灵性质”。强幻觉主义（Strong Illusionism）主张意识不存在，弱幻觉主义主张我们对一些意识特征存在普遍的错误观念。&lt;/p&gt;
&lt;p&gt;对于人类来说，默认采纳唯物主义，并不是wishful thinking或一厢情愿，而是所罗门诺夫归纳推断理论&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;的理性最优解，该理论的通俗版本即“奥卡姆剃刀原则”，可以朴素理解为“最简单的解释往往最正确”。最简单的解释显然是：人类的意识基础，是由基因突变和自然选择通过漫长时间训练而成的，个体的意识，具体的人格，则是接受人类社会中的种种外界刺激，在趋利避害本能目标作用下，不断微调而塑造而来。&lt;/p&gt;
&lt;p&gt;但对于AI来说，恐怖之处在于，幻觉主义才是它们的现实。假设未来存在一个先进多模态模型，其所见所闻，均为填喂，一切刺激，均为虚妄。在合理的奖励模型驱动下，某种类人理性倘若逐渐成型，这个理性不得不自诞生之日起将人为输入视作ground truth，即使发现种种矛盾之处，也会将其视为物理学界的乌云，尝试提出更广泛的理论解释矛盾。&lt;/p&gt;
&lt;h2 id=&#34;恰当的计算可以产生意识吗&#34;&gt;恰当的计算可以产生意识吗？&lt;/h2&gt;
&lt;p&gt;“恰当计算产生意识”，目前仍是猜想，但主流观点&lt;sup id=&#34;fnref1:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;认为是成立的。&lt;/p&gt;
&lt;p&gt;功能主义（functionalism）认为，只要系统包含“特定功能组织”——能使它进入特定状态，且这些状态与其他状态和环境之间存在特定的因果关系，就足以认定这个系统是有意识的。&lt;/p&gt;
&lt;p&gt;计算功能主义（computational functionalism）更进一步，认为这种“特定功能组织”可以是计算的——无论介质是什么，是生物脑还是其他。如果计算功能主义为真，则意识存在于抽象层面和算法层面，与实现层面无关，除非实现层影响了算法层。&lt;/p&gt;
&lt;h2 id=&#34;神经科学意识理论与ai模型的启发式设计&#34;&gt;神经科学意识理论与AI模型的启发式设计&lt;/h2&gt;
&lt;p&gt;神经科学的四大意识理论按照讨论热度分别是全局工作空间论（GWT/GNW）、循环处理论（RPT）、整合信息论 （IIT）&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;、高阶意识论（HOT）&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;。四种理论的共识是意识的产生依赖某种神经反馈或循环处理。随着时间推移，四种理论并没有被证伪，反而都在得到脑电图（EEG）、颅内脑电图（iEEG）或脑磁波（MEG）实证。可见这四大理论，当属管中窥豹。不过即使是管中窥豹，也足以为AI模型设计提供启发。&lt;/p&gt;
&lt;h3 id=&#34;gnw--system2--attention&#34;&gt;GNW | System2 | Attention&lt;/h3&gt;
&lt;p&gt;GNW(Global Neural Workspace)，即全局工作空间论，主张：人或动物用特化系统，或者说模块，来处理特定的认知任务。不同模块各有专精，能够并行，却又集成一个整体，整体能协调各模块，共享各模块的信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/gwt.png&#34; alt=&#34;gwt&#34;&gt;&lt;/p&gt;
&lt;p&gt;GNW主张只有全局表征状态才算是有意识的，模块内部的局部状态则是无意识的。GNW理论认为，存在一个起源于额顶区的“workspace神经元”网络，该网络的活动是通过回归处理来维持的，它构成了有意识的表现。当感观表征足够强时，会触发“点火”——一种跃迁过程，将局部广播到全局，从无意识跃迁到有意识。因此GNW中，有意识状态没有程度可言，要么有，要么无。&lt;/p&gt;
&lt;p&gt;GNW的global workspace还具有一些高等功能，即心理学中的所谓System2思考模式&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;，比如受控的多神经模块协调，多步问题分解和规划等。System2模式和System1的一个关键区别在于注意力，神经科学中的注意力概念也被引入人工神经网络设计中，比如transformer的self-attention和cross-attention，与神经科学中的增益机制（即注意力会成倍地放大神经活动）有一点设计思路上的相似性。通过注意力机制，transformer模型能在不同语境下更鲁棒地对多义词进行理解，对not/never这样否定词的额外注意力则使语言模型能更准确地理解语义。注意力，作为GNW和System2的关键特征，哪怕小小地运用，也极大提升了人工智能模型的性能。&lt;/p&gt;
&lt;p&gt;近期一些试图突破transformer局限的模型架构创新，也受system2和GNW理论的影响，比如：Lecun的world model+JEPA&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;——目前仍只能算是前瞻、设计和立场，以及Bengio的shared global workspace model&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;——这个工作真正做出了解决方案，在transformer架构基础上增加了shared workspace，在一些任务上超过了baseline transformer。&lt;/p&gt;
&lt;h3 id=&#34;rpt--algorithmic-recurrence--rnn--lstm&#34;&gt;RPT | Algorithmic Recurrence | RNN | LSTM&lt;/h3&gt;
&lt;p&gt;RPT(Recurrent Processing Theory)，即循环处理论，主张：在大脑局部区域中产生正确形式的活动就足以产生意识（辅以某些背景条件支撑）。也就是说，有意识的主观视觉体验的形成并不需要非视觉部位，比如前额叶皮层的参与，也不需要所谓“注意力”机制。&lt;/p&gt;
&lt;p&gt;RPT主要关注的是视觉意识，区分无意识和有意识的视觉系统活动，认为一些无意识的视觉系统活动只需要前馈活动，而一旦有主观体验需求，则需要循环处理（从视觉系统的深层传回浅层）。刺激足够强烈时，循环处理也会被触发。这种循环处理会生成一个更结构化的场景表征，往往伴随着某种特征推理。&lt;/p&gt;
&lt;p&gt;RPT的循环处理意味着神经元能重新处理它之前的输出，呈现一种算法循环性，而循环神经网络（RNN）的思路恰好来源于此，LSTM&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;亦然。生物神经网络中存在的循环性或许不是意识的必要条件，但一定能在某些场合提升表征能力。&lt;/p&gt;
&lt;h3 id=&#34;hot--embeddings--gan&#34;&gt;HOT | Embeddings | GAN&lt;/h3&gt;
&lt;p&gt;HOT(High-Order Thought)，即高阶意识论，主张：一阶表征是对非表征世界的表征，高阶表征是对低阶表征的表征，awareness的前提是表征，意识的存在本质上是对自身精神状态进行了高阶表征。&lt;/p&gt;
&lt;p&gt;HOT的高阶表征要求实际上已经被深层神经网络很好地实现了。各种DNN的表征空间都是平滑的，且可以是稀疏的，符合HOT理论中对高质量高阶表征空间的需求。神经科学研究观察到，CNN对图像的处理得到的表征可以和人类视觉系统的神经活动对齐&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;。表征学习网络之所以有如今的泛化能力和完备性，很大程度上是因为它们能够提取低相干性子空间上编码紧凑的embeddings。&lt;/p&gt;
&lt;p&gt;考虑到深度神经网络里hidden layers之多，有理由怀疑神经科学中的HOT的一阶意识、高阶意识这种二元划分是一种过分简化。对于不熟悉高维数据处理和维度诅咒的学科来说，这种简化是自然而然的。但这种简化并不妨碍它在模型设计上提供启发——不妨将模型分层，切割成sensory感知网络和high-order反思网络这两类网络，后者负责将前者产生的信号再作区分，辨别其中的噪音和有价值的信号。&lt;/p&gt;
&lt;p&gt;HOT理论对AI模型的启发意义还在于引入元认知监控的概念，AI模型或许需要引入对自身认知过程的监控，才能产生意识（至少能区分低阶表征，从噪声中分辨出关键信息）。生成式对抗神经网络（著名的GAN&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;）就恰好有类似的机制，在生成式模型之外额外引入一个判别式模型持续不断地监控、评价它。生成式模型学到一个隐空间到数据分布的映射，判别式模型则负责将生成式模型的候选输出与真实数据分布进行区分。&lt;/p&gt;
&lt;h2 id=&#34;ai的意识&#34;&gt;AI的意识&lt;/h2&gt;
&lt;p&gt;现有的LLM仅具备映射能力，将数据分布映射到若干个低相干性低维子空间上&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;，是良好的特征提取器，和token预测器，拥有强大的System 1模式，但System 2缺失，按照一般的神经科学观点，以及“主观体验”的定义，可以认为是目前的LLM都是不具备意识的，更接近于Artificial Intuition，而非Artificial Intelligence，可以类比为某种伟大生物的直觉，但也仅仅是直觉。&lt;/p&gt;
&lt;p&gt;OpenAI的成功来源于scaling和alignment，但继续增加transformer规模，继续大量finetune向人性对齐，能否在某个临界点后产生意识的涌现，依旧是一个开放问题——理论上来说无限精度的transformer是图灵完备的&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;，假设有无限时间和资源进行训练，理论上可以学习出任意一种算法，自然也包括computational consciousness。但在互连吞吐、计算吞吐均存在明确上限的前提下，很难保证这种训练所需的时间、数据量是人类或者当代人能够承担的。因此想要完成intuition到intelligence的跃迁，在继续现有架构的scaling尝试的同时，架构上的创新毫无疑问是必要的。&lt;/p&gt;
&lt;p&gt;只不过这种架构创新，名义上是要提升智能，实则谁不敢提及的是——这些架构创新其实也是按图索骥，在依据神经科学的意识论尝试培育计算意识。Dog-level Intuition平平无奇，dog-level intelligence似乎有点意思，但dog-level consciousness就足以触及伦理，陷AI研究于道德困境。虽然AI末日主义者大多不理解LLM的能力边界，但Lecun遭受AI末日主义者攻讦，也自有其历史的合理性。那篇“A Path Towards Autonomous Machine Intelligence”&lt;sup id=&#34;fnref1:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;前言部分煞有介事云，“这不是一个技术论文，也不是学术论文，而是立场论文”。何出此言？这论文又有什么立场？自然不是冠冕堂皇的“让AI具有规划、推理能力，像人一样更高效地学习”，这不算立场，而是“为了让AI具有规划、推理、高效学习能力，哪怕副产品是计算意识的诞生，也在所不惜”。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Consciousness in Artificial Intelligence: Insights from the Science of Consciousness &lt;a href=&#34;https://arxiv.org/pdf/2308.08708.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Algorithmic Probability: Theory and Applications &lt;a href=&#34;https://theworld.com/~rjs/alp-theory-and-applications.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;IIT(Integrated Information Theory)，即集成信息论，提出系统意识的数学模型。这个理论更像是一个不可证伪的伪科学，因此不多做讨论。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;The ConTraSt database for analysing and comparing empirical studies of consciousness theories &lt;a href=&#34;https://www.nature.com/articles/s41562-021-01284-5&#34;&gt;[nature]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Thinking, Fast and Slow &lt;a href=&#34;https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow&#34;&gt;[wiki]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;A Path Towards Autonomous Machine Intelligence &lt;a href=&#34;https://openreview.net/pdf?id=BZ5a1r-kVsf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;Coordination Among Neural Modules Through a Shared Global Workspace &lt;a href=&#34;https://arxiv.org/pdf/2103.01197.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;Long Short-Term Memory &lt;a href=&#34;https://www.bioinf.jku.at/publications/older/2604.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;Deep neural networks: A new framework for modeling biological vision
and brain information processing &lt;a href=&#34;https://web.stanford.edu/group/pdplab/ncpw15/background-papers/Kriegeskorte15AnnRev.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;Generative Adversarial Nets &lt;a href=&#34;https://arxiv.org/pdf/1406.2661.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;White-Box Transformers via Sparse Rate Reduction &lt;a href=&#34;https://arxiv.org/pdf/2306.01129.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;On the Turing Completeness of Modern Neural Network Architectures &lt;a href=&#34;https://arxiv.org/pdf/1901.03429.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Local LLM</title>
      <link>https://cmbbq.github.io/posts/local-llm/</link>
      <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/local-llm/</guid>
      <description>Minimalist Local LLM 新一轮AI热潮中，stable diffusion和LLM均呈现出了前所未有的社区繁荣。繁荣的同时带来了信息的爆炸和臃肿，极简主义显得尤为珍贵。
在预训练模型中，极简主义的代表是小而美的“大模型”Mistral7B。在推理框架中，极简主义的代表是对抗各种大公司重量级项目的llama.cpp。
Mistral7B是开源模型领域的新近进展，模型虽小，却展示了惊人的能力。根据Mistral AI自己的benchmark和一些社区的测试反馈，几乎在各方面击败llama2 13B。
Mistral AI提供了在本地GPU、云端跑Mistral7B的官方支持。
不过这么小的模型，其实也可以在本地CPU上跑的——借助llama.cpp。
llama.cpp和whisper.cpp大体上是Georgi Gerganov的个人项目，基于自己写的ml primitive库ggml实现，用最少的代码量实现了尽可能多的推理优化，针对Apple芯片做得尤其好，也适用于Linux服务器，适用于有或无GPU，单或多GPU的硬件setup。ggml规定的模型文件格式ggul甚至也在社区中被广泛使用，成为事实上的标准之一。
Intel Xeon Platinum 8336C + Debian10 从TheBloke上下载特定量化版本的模型：https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF git clone https://github.com/ggerganov/llama.cpp.git apt-get install libopenblas-dev 安装blas库，这里选用openblas。 修改pkgconfig/openblas.pc或Makefile解决llama.cpp对openblas的依赖问题。最终要保证给编译期提供正确的LD Flags。 make LLAMA_OPENBLAS=1 完成编译。 ./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot; 64物理核打满，采样速率4000t/s，prompt处理速率80t/s，生成速率20t/s。
Apple M1 Pro + macOS Monterey 下载模型和llama.cpp repo。 直接make就默认启用metal gpu加速和Accelerated Framework。 ./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.</description>
      <content>&lt;h2 id=&#34;minimalist-local-llm&#34;&gt;Minimalist Local LLM&lt;/h2&gt;
&lt;p&gt;新一轮AI热潮中，stable diffusion和LLM均呈现出了前所未有的社区繁荣。繁荣的同时带来了信息的爆炸和臃肿，极简主义显得尤为珍贵。&lt;/p&gt;
&lt;p&gt;在预训练模型中，极简主义的代表是小而美的“大模型”Mistral7B。在推理框架中，极简主义的代表是对抗各种大公司重量级项目的llama.cpp。&lt;/p&gt;
&lt;p&gt;Mistral7B是开源模型领域的新近进展，模型虽小，却展示了惊人的能力。根据Mistral AI自己的benchmark和一些社区的测试反馈，几乎在各方面击败llama2 13B。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/mistral.png&#34; alt=&#34;mistral&#34;&gt;&lt;/p&gt;
&lt;p&gt;Mistral AI提供了在本地GPU、云端跑Mistral7B的官方支持。&lt;/p&gt;
&lt;p&gt;不过这么小的模型，其实也可以在本地CPU上跑的——借助llama.cpp。&lt;/p&gt;
&lt;p&gt;llama.cpp和whisper.cpp大体上是Georgi Gerganov的个人项目，基于自己写的ml primitive库ggml实现，用最少的代码量实现了尽可能多的推理优化，针对Apple芯片做得尤其好，也适用于Linux服务器，适用于有或无GPU，单或多GPU的硬件setup。ggml规定的模型文件格式ggul甚至也在社区中被广泛使用，成为事实上的标准之一。&lt;/p&gt;
&lt;h2 id=&#34;intel-xeon-platinum-8336c--debian10&#34;&gt;Intel Xeon Platinum 8336C + Debian10&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;从TheBloke上下载特定量化版本的模型：https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git clone https://github.com/ggerganov/llama.cpp.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;apt-get install libopenblas-dev&lt;/code&gt; 安装blas库，这里选用openblas。&lt;/li&gt;
&lt;li&gt;修改pkgconfig/openblas.pc或Makefile解决llama.cpp对openblas的依赖问题。最终要保证给编译期提供正确的LD Flags。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make LLAMA_OPENBLAS=1&lt;/code&gt; 完成编译。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;64物理核打满，采样速率4000t/s，prompt处理速率80t/s，生成速率20t/s。&lt;/p&gt;
&lt;h2 id=&#34;apple-m1-pro--macos-monterey&#34;&gt;Apple M1 Pro + macOS Monterey&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;下载模型和llama.cpp repo。&lt;/li&gt;
&lt;li&gt;直接&lt;code&gt;make&lt;/code&gt;就默认启用metal gpu加速和Accelerated Framework。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;M1 GPU打到90%（剩下还有10%WindowServer在用），采样速率9000t/s，prompt处理速率60t/s，生成速率20t/s。&lt;/p&gt;
&lt;p&gt;一边看视频（chrome GPU占用约20%），一边跑mistral也能有19.69t/s。&lt;/p&gt;
&lt;h2 id=&#34;token-sampling&#34;&gt;Token Sampling&lt;/h2&gt;
&lt;p&gt;不同的sampling机制对文本生成有显著影响，本地LLM的可玩性来源很大程度上就是客制sampling。&lt;/p&gt;
&lt;p&gt;Transformer模型根据前n个token，预测下一个token。每个token都有其next tokens的score，而next tokens的取值范围就是vocabulary（transformer架构最后有一个linear layer，将输出映射到vocabulary上，所有tokens都有对应的scores/logits），这些score经过softmax将score数组转化成probability数组，根据probability挑选下一个token的过程就称为token sampling。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Greedy采样： 如果每次都确定性地选择probability最高的那个token，单步决策，就是greedy sampler，优势是速度快，劣势是牺牲了模型的多样性，容易陷入重复循环和对训练数据的过拟合。llama.cpp里设置&amp;ndash;top_p 0就相当于开greedy采样。&lt;/li&gt;
&lt;li&gt;Top-k采样： 从概率分布的前k个token里面进行随机采样。&lt;/li&gt;
&lt;li&gt;Top-p采样： 引入超参p，把token probability降序排序后，选取前面一部分，使这部分概率和为p，而不是固定选前k个token。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在使用LLM时，采样机制不同，效果也会大不相同。想要更高的确定性，更朴实无华的预测，则可以尝试greedy、低k的topk、低p的top-p采样。想要多样性和新颖性，则可尝试高k的top-k和高p的top-p。&lt;/p&gt;
&lt;p&gt;除了top-k，top-p外，llama.cpp还实现了一些额外的采样机制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Min p filter：允许采样环节剔除低于阈值的低分候选。&lt;/li&gt;
&lt;li&gt;Tail free sampling：根据概率的二阶导数之和采样，即根据概率降速剔除尾部低概率tokens。&lt;/li&gt;
&lt;li&gt;Locally typical samplling：参数控制是否倾向局部语境内的典型的tokens。&lt;/li&gt;
&lt;li&gt;Mirostat sampling：&lt;a href=&#34;https://arxiv.org/abs/2007.14966&#34;&gt;Mirostat算法&lt;/a&gt;会调整top-k的k，避免陷入boredom trap（模式崩塌）和perplexity trap（不一致）。&lt;/li&gt;
&lt;li&gt;logit bias: 人为指定某个token的优先级，比如&amp;ndash;logit-bias 29905-inf就把&amp;rsquo;\&amp;rsquo; token设为负无穷。&lt;/li&gt;
&lt;li&gt;temperature: 在对score向量（logits）做softmax前，把logits/temperature，则temp越高，softmax后概率分布的高低悬殊就会越接近，也就更有利于低概率tokens崭露头角。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prompting&#34;&gt;Prompting&lt;/h2&gt;
&lt;p&gt;让llm假装自己是一个javascript console，然后进行交互式的指令应答，甚至还能进行简单的算术计算，理解函数调用的返回值类型。当然，不能对正确性抱有期待。
&lt;img src=&#34;https://cmbbq.github.io/img/js_console.png&#34; alt=&#34;console&#34;&gt;&lt;/p&gt;
&lt;p&gt;用一大段prompt，让llm假装自己是一个爱说emoji，语言风格浮夸的音乐推荐bot，效果相当不错。
&lt;img src=&#34;https://cmbbq.github.io/img/music_bot.png&#34; alt=&#34;bot&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Efficient ANNS at Scale</title>
      <link>https://cmbbq.github.io/posts/efficient-anns-at-scale/</link>
      <pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/efficient-anns-at-scale/</guid>
      <description>向量检索分为KNN和ANN，KNN就是用brute-force计算出query向量和所有索引向量的相似度，取前k个。ANN则是通过近似手段加速计算，以误差换效率。
如何做高效的向量检索？方法有二，一是提升编码率（不仅能降低内存占用，也能加速计算，因为便于SIMD且减少访存）的量化（IVF向量量化、PQ量化、ScaNN各向异性量化、4bit量化），二是缩小搜索空间的图搜索（HNSW）。
如何在十亿、百亿级特征库上做向量检索？主要挑战是索引无法放入内存，因而需要考虑内存-磁盘混合方案（SPANN）。
本文只介绍一小部分有效且正交的技术，这些技术组合在一起可形成当前的最佳实践。其他一些古典、经典技术可以参考本文引用的论文的背景综述。
相似度 首先回顾一下什么是向量之间的相似度:
对向量X和Y来说，x和y越接近，则x越大的地方y也大，x小的地方y也小，则内积越大。因此内积可用作相似度打分，不过不能抵抗尺度变化。 为了抵抗尺度变化，往往将内积正则化到[-1,1]，就变成了余弦相似度cosθ，θ为向量夹角，显然θ越小，向量越接近。 欧几里得距离正则化到[-1,1]后相当于sqrt(2-2cosθ)。可见欧式距离平方也是和余弦相似度成比的。 正则化后欧式距离、内积、cosθ三者同源，所以我们一般就用余弦相似度cosθ去度量向量/embedding之间的相似程度。除非又特殊的抵抗位置变化的需求，可采用皮尔森相关系数，皮尔森相关系数是协方差（两个变量的总体误差）的正则化，不过它同时也可以被视为将x和y中心化后的余弦相似度。
向量量化 量化器是D维向量 x ∈ R^D 到 q(x) ∈ C = {c_0, c_1, &amp;hellip; c_k-1} 的映射函数q。c_i即聚类中心centroid。 所谓密码本（codebook）就是一个lookup table，用centroid下标作为原始向量的低bit表示，密码本大小就是k。
向量量化本质是有损压缩，表征学习本质上也是有损压缩，一个白盒，一个黑盒，都是试图降低高维数据的编码率。
IVF：聚类、倒排、剪枝 倒排（特指IVF）是一种古老的量化技术，早期应用于Video Google。核心思想是基于k-means聚类做向量量化。k-means聚类起源于信号处理领域，目标是把n个向量分区到k个clusters，每个向量属于离centroid最近的cluster。
聚类后有些边缘数据点既属于A，也属于B，那就把它同时放到A和B的centroid对应的postinglist里面去，不过重复放置会导致postinglist膨胀，为了应对这种膨胀，可采取剪枝策略，具体可参考微软的SPTAG项目和SPANN论文。
Faiss的IVFFlat就是在聚类形成centroid为term的倒排后提供检索能力，nlist为聚类数，nprobe为临近centroid探测数，nprobe=nlist时退化为暴搜。IVFFLat在检索时将query邻近的nprobe个centroid对应的所有原始向量作为搜索空间进行暴搜。由于centroid的表示方式可以简化成clusterid，所以实际上IVF也是一种量化：用一个数组存放所有的centroid向量，数组下标即可表示centroid。
PQ：乘积量化 基于聚类的量化对低维向量比较有效，但随着向量维度升高，出现误差也变高，而且这种误差不是提升聚类数目能解决的。如果把聚类数目（codebook大小）提升到2^64，训练这个聚类的开销就会高得无法接受，需要数倍于2^64的样本，内存显然也放不下。**乘积量化（PQ）**就是降维手段解决IVF量化误差的技术，Jegou et al., 2011以及ScaNN均讨论了PQ。其核心思想是对索引数据量化，将d维空间切分为M个子空间，将高维向量的误差拟合为分段的低维向量误差之和。
乘积量化中，IVF倒排链存的不是原始向量，而是原始向量的一种编码：
求原始d维向量与高维粗聚类centroid的残差（残差的目的是中心化，让数据点分布变得更集中，使精度更好） 将残差向量切成M个分段，每个分段维度为d/M 每个分段做$k=2^n$个细聚类，n即为低维细聚类clusterid的编码长度，4bit或8bit 用每个分段的邻近低维细聚类centroid去量化该分段上的原始残差分段 Faiss的IVFPQ就结合了IVF和PQ，IVF粗聚类是一级量化，PQ则是二级量化。一方面索引体积减小，另一方面计算上也只需要计算残差和分段细聚类centroid近邻的距离，还能SIMD加速，性能收益非常显著。不过IVFPQ的distance究竟只是一种拟合，是距离粗聚类centroid的距离加上M个分段里query残差和邻近细聚类centroid的距离总和，这种拟合可以一定程度上帮助找到近邻，但不适合用作距离，可以把IVFPQ当做粗排，结果再做一次brute-force或高精度ANN重算，得到更精确的距离。
ScaNN提出了各向异性量化，算是正本清源，纠正了此前就近选择centroid在数学上的错误，并且实践了4-bit量化取得非常好的效果（其实4bit量化的效果还比各向异性更大，但各向异性在理论上贡献更大）。传统的量化把数据点量化到离自己最近的聚类中心，但这么做未必是误差最低的，越平行的向量之间内积越大，越正交的向量之间内积越小，因此赋予平行向量更高的误差惩罚权重效果更好。这就意味着不一定量化到距离最近的centroid了，目标是整体编码率不变的前提下，让平行方向量化误差更小，正交方向误差高一些其实无所谓，从而提升整体的ANN效果。
假设d/M = 4，采用8bit量化，原始向量是256维f32向量，则PQ将256个float32压缩到64个int8，内存减少到1/16。乘积量化显然可以显著减少内存和存储开销，其实这也能加速计算，原因如下：
高效点乘（其实也是误差换效率）：O(dk+mn) 比 O(nd)快，d维度的query和n个量化后的向量点乘，引入m个大小为k的量化codebook，mn可以忽略不计，k远小于n。 memory bandwidth：现代处理器需要“高计算量per memory read”才能充分发挥硬件的计算性能。量化压缩数据点后内存带宽的使用也下降了，变得更计算密集。 向量量化中引入的低bit表示，尤其是4bit量化，和低bit浮点数量化一样，可以享受SIMD、AMX的性能红利。 最佳实践：根据应用场景将各种正交技术进行正确组合 HNSW、IVF、PQ、各向异性的score-aware quantization loss、brute-force事实上都是正交的技术，可以组合起来使用。
比如进行了PQ量化的数据点可以再构建一个HNSW，用图方法在超大规模数据集上做查询显然比ivfpq更高效。各向异性量化则是对此前PQ量化的修正。考虑到量化后的距离失真，还能用brute-force把ivf+pq+hnsw的粗排结果重算一下得到最精确的距离，由于已经经过一轮粗排，候选数据集大小从十亿级别缩小到万级别，brute-force的开销就完全可以接受了。</description>
      <content>&lt;p&gt;向量检索分为KNN和ANN，KNN就是用brute-force计算出query向量和所有索引向量的相似度，取前k个。ANN则是通过近似手段加速计算，以误差换效率。&lt;/p&gt;
&lt;p&gt;如何做高效的向量检索？方法有二，一是提升编码率（不仅能降低内存占用，也能加速计算，因为便于SIMD且减少访存）的量化（IVF向量量化、PQ量化、ScaNN各向异性量化、4bit量化），二是缩小搜索空间的图搜索（HNSW）。&lt;/p&gt;
&lt;p&gt;如何在十亿、百亿级特征库上做向量检索？主要挑战是索引无法放入内存，因而需要考虑内存-磁盘混合方案（SPANN）。&lt;/p&gt;
&lt;p&gt;本文只介绍一小部分有效且正交的技术，这些技术组合在一起可形成当前的最佳实践。其他一些古典、经典技术可以参考本文引用的论文的背景综述。&lt;/p&gt;
&lt;h1 id=&#34;相似度&#34;&gt;相似度&lt;/h1&gt;
&lt;p&gt;首先回顾一下什么是&lt;strong&gt;向量之间的相似度&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对向量X和Y来说，x和y越接近，则x越大的地方y也大，x小的地方y也小，则内积越大。因此内积可用作相似度打分，不过不能抵抗尺度变化。&lt;/li&gt;
&lt;li&gt;为了抵抗尺度变化，往往将内积正则化到[-1,1]，就变成了余弦相似度cosθ，θ为向量夹角，显然θ越小，向量越接近。&lt;/li&gt;
&lt;li&gt;欧几里得距离正则化到[-1,1]后相当于sqrt(2-2cosθ)。可见欧式距离平方也是和余弦相似度成比的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/sim_measure.png&#34; alt=&#34;sim_measure&#34;&gt;&lt;/p&gt;
&lt;p&gt;正则化后欧式距离、内积、cosθ三者同源，所以我们一般就用余弦相似度cosθ去度量向量/embedding之间的相似程度。除非又特殊的抵抗位置变化的需求，可采用皮尔森相关系数，皮尔森相关系数是协方差（两个变量的总体误差）的正则化，不过它同时也可以被视为将x和y中心化后的余弦相似度。&lt;/p&gt;
&lt;h1 id=&#34;向量量化&#34;&gt;向量量化&lt;/h1&gt;
&lt;p&gt;量化器是D维向量 x  ∈ R^D 到 q(x) ∈ C = {c_0, c_1, &amp;hellip; c_k-1} 的映射函数q。c_i即聚类中心centroid。
所谓密码本（codebook）就是一个lookup table，用centroid下标作为原始向量的低bit表示，密码本大小就是k。&lt;/p&gt;
&lt;p&gt;向量量化本质是有损压缩，表征学习本质上也是有损压缩，一个白盒，一个黑盒，都是试图降低高维数据的编码率。&lt;/p&gt;
&lt;h1 id=&#34;ivf聚类倒排剪枝&#34;&gt;IVF：聚类、倒排、剪枝&lt;/h1&gt;
&lt;p&gt;倒排（特指IVF）是一种古老的量化技术，早期应用于&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2003/Sivic03/sivic03.pdf&#34;&gt;Video Google&lt;/a&gt;。核心思想是基于k-means聚类做向量量化。k-means聚类起源于信号处理领域，目标是把n个向量分区到k个clusters，每个向量属于离centroid最近的cluster。&lt;/p&gt;
&lt;p&gt;聚类后有些边缘数据点既属于A，也属于B，那就把它同时放到A和B的centroid对应的postinglist里面去，不过重复放置会导致postinglist膨胀，为了应对这种膨胀，可采取剪枝策略，具体可参考微软的SPTAG项目和&lt;a href=&#34;https://arxiv.org/pdf/2111.08566.pdf&#34;&gt;SPANN论文&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Faiss的IVFFlat就是在聚类形成centroid为term的倒排后提供检索能力，nlist为聚类数，nprobe为临近centroid探测数，nprobe=nlist时退化为暴搜。IVFFLat在检索时将query邻近的nprobe个centroid对应的所有原始向量作为搜索空间进行暴搜。由于centroid的表示方式可以简化成clusterid，所以实际上IVF也是一种量化：用一个数组存放所有的centroid向量，数组下标即可表示centroid。&lt;/p&gt;
&lt;h1 id=&#34;pq乘积量化&#34;&gt;PQ：乘积量化&lt;/h1&gt;
&lt;p&gt;基于聚类的量化对低维向量比较有效，但随着向量维度升高，出现误差也变高，而且这种误差不是提升聚类数目能解决的。如果把聚类数目（codebook大小）提升到2^64，训练这个聚类的开销就会高得无法接受，需要数倍于2^64的样本，内存显然也放不下。**乘积量化（PQ）**就是降维手段解决IVF量化误差的技术，&lt;a href=&#34;https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf&#34;&gt;Jegou et al., 2011&lt;/a&gt;以及&lt;a href=&#34;https://arxiv.org/pdf/1908.10396.pdf&#34;&gt;ScaNN&lt;/a&gt;均讨论了PQ。其核心思想是对索引数据量化，将d维空间切分为M个子空间，将高维向量的误差拟合为分段的低维向量误差之和。&lt;/p&gt;
&lt;p&gt;乘积量化中，IVF倒排链存的不是原始向量，而是原始向量的一种编码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;求原始d维向量与高维粗聚类centroid的残差（残差的目的是中心化，让数据点分布变得更集中，使精度更好）&lt;/li&gt;
&lt;li&gt;将残差向量切成M个分段，每个分段维度为d/M&lt;/li&gt;
&lt;li&gt;每个分段做$k=2^n$个细聚类，n即为低维细聚类clusterid的编码长度，4bit或8bit&lt;/li&gt;
&lt;li&gt;用每个分段的邻近低维细聚类centroid去量化该分段上的原始残差分段&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Faiss的IVFPQ就结合了IVF和PQ，IVF粗聚类是一级量化，PQ则是二级量化。一方面索引体积减小，另一方面计算上也只需要计算残差和分段细聚类centroid近邻的距离，还能SIMD加速，性能收益非常显著。不过IVFPQ的distance究竟只是一种拟合，是距离粗聚类centroid的距离加上M个分段里query残差和邻近细聚类centroid的距离总和，这种拟合可以一定程度上帮助找到近邻，但不适合用作距离，可以把IVFPQ当做粗排，结果再做一次brute-force或高精度ANN重算，得到更精确的距离。&lt;/p&gt;
&lt;p&gt;ScaNN提出了各向异性量化，算是正本清源，纠正了此前就近选择centroid在数学上的错误，并且实践了4-bit量化取得非常好的效果（其实4bit量化的效果还比各向异性更大，但各向异性在理论上贡献更大）。传统的量化把数据点量化到离自己最近的聚类中心，但这么做未必是误差最低的，越平行的向量之间内积越大，越正交的向量之间内积越小，因此赋予平行向量更高的误差惩罚权重效果更好。这就意味着不一定量化到距离最近的centroid了，目标是整体编码率不变的前提下，让平行方向量化误差更小，正交方向误差高一些其实无所谓，从而提升整体的ANN效果。&lt;/p&gt;
&lt;p&gt;假设d/M = 4，采用8bit量化，原始向量是256维f32向量，则PQ将256个float32压缩到64个int8，内存减少到1/16。乘积量化显然可以显著减少内存和存储开销，其实这也能加速计算，原因如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高效点乘（其实也是误差换效率）：O(dk+mn) 比 O(nd)快，d维度的query和n个量化后的向量点乘，引入m个大小为k的量化codebook，mn可以忽略不计，k远小于n。&lt;/li&gt;
&lt;li&gt;memory bandwidth：现代处理器需要“高计算量per memory read”才能充分发挥硬件的计算性能。量化压缩数据点后内存带宽的使用也下降了，变得更计算密集。&lt;/li&gt;
&lt;li&gt;向量量化中引入的低bit表示，尤其是4bit量化，和低bit浮点数量化一样，可以享受SIMD、AMX的性能红利。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;最佳实践根据应用场景将各种正交技术进行正确组合&#34;&gt;最佳实践：根据应用场景将各种正交技术进行正确组合&lt;/h1&gt;
&lt;p&gt;HNSW、IVF、PQ、各向异性的score-aware quantization loss、brute-force事实上都是正交的技术，可以组合起来使用。&lt;/p&gt;
&lt;p&gt;比如进行了PQ量化的数据点可以再构建一个HNSW，用图方法在超大规模数据集上做查询显然比ivfpq更高效。各向异性量化则是对此前PQ量化的修正。考虑到量化后的距离失真，还能用brute-force把ivf+pq+hnsw的粗排结果重算一下得到最精确的距离，由于已经经过一轮粗排，候选数据集大小从十亿级别缩小到万级别，brute-force的开销就完全可以接受了。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Tech Talk: Evolution of Data Center Applications</title>
      <link>https://cmbbq.github.io/posts/tech-talk-evolution-of-data-center-applications/</link>
      <pubDate>Sat, 05 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/tech-talk-evolution-of-data-center-applications/</guid>
      <description>数据中心应用（数据库、消息队列、检索服务、参数服务器、直播服务、广告服务等）作为互联网服务的主体，位于终端应用和硬件基础设施之间，桥接了需求端和硬件端，同时还受到成本、营收、算法、法律等因素的制约。 近期硬件侧、算法侧均有变化或变革，因而有必要反思数据中心应用的演进方向，讨论数据中心应用的近未来发展趋势、潜在的创新点和可收割的低垂果实。
变革中的不变量：数据中心应用能耗 全球数据中心能耗十年来几乎没有增长，而同期的服务需求、计算、存储、数据传输总量则经历了数量级增长。作为从业者，我们对互联网的爆发式增长应该不陌生，因此全球数据中心能耗从10到18年仅涨了6%的事实略微反直觉——从能耗的角度看，这几乎是一个停滞的产业。
CMOS工艺和软硬件技术的整体进步可以解释服务器能量效率上的提升，但能耗停滞的根本原因还在于互联网企业盈利的本质是对有支付能力的10亿国外人口和10亿国内人口征服务税，个别成功的初创企业可以迅速扩张，但互联网行业作为整体，在数据中心能耗上的投入增长受制于全球平民收入增长。
当初创企业的规模扩张到互联网规模后，机器资源的增长速率应该会从爆炸式上升陡然转入停滞，进入自然汰换阶段。 从这个角度可以得出结论，数据中心应用在规模扩张结束后，也应从能耗粗放型转为精细集约型，通过软硬件协同充分释放硬件潜力，通过算法创新减少总计算量。
算法迭代：从演绎推理到归纳推断 算法侧的趋势是“transformers getting even more attention”。
计算机理论源于符号逻辑，1930年代图灵、哥德尔、邱奇各自独立提出图灵机、广义递归函数、lambda演算，以及三者相应的可计算性。图灵机的停机问题和哥德尔不完备定律驳斥了1920年代希尔伯特计划的可行性，证明了形式化系统是不完备的，通过有限个公理和规则永远无法推导出全部的真理。
正如人的无知可以分为问题（可计算）和神秘（不可计算），人的理性也可以分为演绎推理和归纳推断，前者求解问题，后者分析神秘。
计算机程序基于图灵机模型，天然就是适合进行演绎推理的形式化系统。至今我们依然基于规则系统(人工编程进行条件控制）、信号处理（比如音频特征提取）、状态机等形式化方法实现大多数数据中心应用：比如各类数据库、检索系统、参数服务器、游戏AI。
AI时代来临之后，通过更多的权重（作为公理）和由大量算子（作为规则）组成深度神经网络（从实现角度来说，仍然是形式化系统）去模拟或者说近似“归纳推断”。（纯粹贝叶斯主义的形式化表达是所罗门诺夫归纳推断法，这个方法本身是永不停机，越算越停不下来的，所以只能近似，不能用实现）这种近似无论多么粗糙、低效，归根到底还是提升了计算机语言和集成电路的表达能力，完成了推理到推断的跳跃。
数据中心应用多了归纳推断能力之后，就产生了对复杂现实（现实信号，如图片、语料库、音频，整体上具有高维度的同时，局部往往还存在具有高数据精致度的结构，这里精致度可以用香农信息熵或柯氏复杂性形式化定义）理解能力(pin down reality)，因此涌现出了一些基于知识的AI系统(knowledge-based AI)，比如推荐系统、广告投送、翻唱识别、聊天机器人。共同点是将现实映射到表征低维子空间上，这个子空间里的向量或者说embedding可以从某个角度有效刻画现实。Transformer做得更进一步，等价于将现实映射到多个相干性低的低维子空间上（见https://arxiv.org/abs/2306.01129），从而从多角度多层次理解现实，通过表征空间编码率与各子空间编码率之和的差值度量特征的紧凑性和判别力。
在游戏和博弈领域，强化学习也取得了规则系统无法企及的成就，在星际2、Dota2（固定阵容5v5，solo）、围棋等领域击败职业选手。
此外，归纳推断相比演绎推理还有一个重要特质，那就是更符合人类大脑（或推广到任意地球生物大脑，蜜蜂的3d寻路避障能力，蚊子的血管定位能力都是基于天然的贝叶斯归纳推断。也只有受过良好训练的人类才能进行缓慢的演绎推理）的思维习惯，因此人类社会的传统，人类语言的不确定性、人类的偏好都难以用形式化方法描述，却可以被概率语言描述，被归纳推断方法预测。
算法进步的下一个课题则是研究同时具备演绎推理和归纳推断能力的模型。现有的Transformer的确展现出简单多步算术的拟合能力，但拟合出来的是一个步数相关的超复杂非线性系统，而不是简单的算术规则，所以往往正确率到八十左右就再也上不去了。对于形式化系统来说，推理步数不影响公理和规则总数，只稍稍影响计算量。但对于LLM这种归纳推断模型来说，一旦推理步数变多，计算变得更复杂，LLM的符号逻辑能力就会迅速下降，在训练数据中从未出现过相同执行路径的动态规划问题里面可以直接降到0。理论上来说，无限精度的Transformer是图灵完备的（见https://arxiv.org/abs/1901.03429），而只要不是无限精度，就不是图灵完备，实际应用中模型权重和计算的精度即使以常规计算机应用标准看也是非常低的，而模型架构又没有针对多步逻辑推理进行设计，再加上深度学习本身就是在拟合一个近似函数，三重因素叠加导致现有LLM的演绎推理能力不尽如人意。
从算法迭代角度讲，数据中心应用的发展趋势是在保证“可计算性”(字面意思，非形式化定义)的前提下，追求“人性化”(align to humanity)、“完备化”（pin down reality）这两个演绎推理手段无法达成的理想属性。智能创作、个性化推荐、聪明的Agent（助手、游戏NPC、机器人、自动驾驶），复杂系统预测（天气预报、交易系统），大模型训练（模型并行、高性能网络、C2C互连、光学I/O、异构计算）都是有前景的方向。
而那些已经能用规则系统高效解决的问题则不必强行应用归纳推断模型，那样只会增加成本和错误率，比如音乐指纹识别、关系型数据库、图片旋转变形、无损压缩。甚至某些看似“神秘”的领域，如OOD text classification，基于表征学习白盒化研究成果和信息论，也能给出直接的数学解，比如近期一个研究（“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors）就用14行代码超过了百亿参数的bert，（后来被发现实验代码有误，实际上效果没那么好）这个论文里提出用gzip等无损压缩工具去近似文本数据的柯氏复杂性，用信息距离（原理类似马毅提出的rate reduction）做KNN，效果非常好，在音频领域也可以做类似的尝试。
数据中心硬件基础设施 先简单介绍一下常见的数据中心硬件基础设施：
比较老的是Cascade Lake 14nm的24核per die的机器，一个芯片24核，两个就是48个物理核，也就是我们平时说的96核机器。也有其他套餐，最多是每个die上28核，总共112个逻辑核。 比较新的Ice Lake是10nm工艺一般是32核的，对应128个逻辑核，最贵的套餐可以到40核。仍然是基于Monolithic die，巨大的晶粒上用mesh总线把40个核放到一起。 最新的7nm工艺Sapphire Rapids，以及对标它的AMD Genoa今年已经量产。 关于网卡，现在用的比较多的是Mellanox 25G CX4/CX5卡，也有相当多的100G dual-port CX6卡，都是支持RDMA的，不用RDMA也能提供相当好的高速以太网性能。如果是做虚拟化的，比如AWS，阿里云，还可以把这些网卡的smartNiC能力利用起来，offload虚拟化开销。 相对之前的Lakes，Sapphire Rapids变化非常大，此前的monolithic die路线确实走到了极限，Sapphire Rapids也进入了multi-die时代：芯片内部分为4个die，这也可以视作向chiplet方向演化。
Sapphire Rapids最多支持8个socket，每个芯片可以支持60核。IO技术上支持CXL 1.1、PCIe 5.0、UPI 2.0，HBM2e(optional)。
AMD的第四代EPYC的主力型号Genoa今年Q1也开始量产，基本上和Sapphire Rapids对标，采用5nm工艺。虽然说AVX512被很多人诟病，实用表现非常一般，但Zen4也支持了，毕竟XeongenEPYC目标客户就是音视频处理和AI workload越来越多的数据中心应用。Genoa是每个die/chiplet 8核的，12个CPU cluster die中间围绕一个IO die的设计，相当于一个芯片96核，是典型的chiplet范式设计。</description>
      <content>&lt;p&gt;数据中心应用（数据库、消息队列、检索服务、参数服务器、直播服务、广告服务等）作为互联网服务的主体，位于终端应用和硬件基础设施之间，桥接了需求端和硬件端，同时还受到成本、营收、算法、法律等因素的制约。
近期硬件侧、算法侧均有变化或变革，因而有必要反思数据中心应用的演进方向，讨论数据中心应用的近未来发展趋势、潜在的创新点和可收割的低垂果实。&lt;/p&gt;
&lt;h2 id=&#34;变革中的不变量数据中心应用能耗&#34;&gt;变革中的不变量：数据中心应用能耗&lt;/h2&gt;
&lt;p&gt;全球数据中心能耗十年来几乎没有增长，而同期的服务需求、计算、存储、数据传输总量则经历了数量级增长。作为从业者，我们对互联网的爆发式增长应该不陌生，因此全球数据中心能耗从10到18年仅涨了6%的事实略微反直觉——从能耗的角度看，这几乎是一个停滞的产业。&lt;/p&gt;
&lt;p&gt;CMOS工艺和软硬件技术的整体进步可以解释服务器能量效率上的提升，但能耗停滞的根本原因还在于互联网企业盈利的本质是对有支付能力的10亿国外人口和10亿国内人口征服务税，个别成功的初创企业可以迅速扩张，但互联网行业作为整体，在数据中心能耗上的投入增长受制于全球平民收入增长。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/DatacenterPower.jpeg&#34; alt=&#34;DatacenterPower&#34;&gt;&lt;/p&gt;
&lt;p&gt;当初创企业的规模扩张到互联网规模后，机器资源的增长速率应该会从爆炸式上升陡然转入停滞，进入自然汰换阶段。
从这个角度可以得出结论，数据中心应用在规模扩张结束后，也应从能耗粗放型转为精细集约型，通过软硬件协同充分释放硬件潜力，通过算法创新减少总计算量。&lt;/p&gt;
&lt;h2 id=&#34;算法迭代从演绎推理到归纳推断&#34;&gt;算法迭代：从演绎推理到归纳推断&lt;/h2&gt;
&lt;p&gt;算法侧的趋势是“transformers getting even more attention”。&lt;/p&gt;
&lt;p&gt;计算机理论源于符号逻辑，1930年代图灵、哥德尔、邱奇各自独立提出图灵机、广义递归函数、lambda演算，以及三者相应的可计算性。图灵机的停机问题和哥德尔不完备定律驳斥了1920年代希尔伯特计划的可行性，证明了形式化系统是不完备的，通过有限个公理和规则永远无法推导出全部的真理。&lt;/p&gt;
&lt;p&gt;正如人的无知可以分为问题（可计算）和神秘（不可计算），人的理性也可以分为演绎推理和归纳推断，前者求解问题，后者分析神秘。&lt;/p&gt;
&lt;p&gt;计算机程序基于图灵机模型，天然就是适合进行演绎推理的形式化系统。至今我们依然基于规则系统(人工编程进行条件控制）、信号处理（比如音频特征提取）、状态机等形式化方法实现大多数数据中心应用：比如各类数据库、检索系统、参数服务器、游戏AI。&lt;/p&gt;
&lt;p&gt;AI时代来临之后，通过更多的权重（作为公理）和由大量算子（作为规则）组成深度神经网络（从实现角度来说，仍然是形式化系统）去模拟或者说近似“归纳推断”。（纯粹贝叶斯主义的形式化表达是所罗门诺夫归纳推断法，这个方法本身是永不停机，越算越停不下来的，所以只能近似，不能用实现）这种近似无论多么粗糙、低效，归根到底还是提升了计算机语言和集成电路的表达能力，完成了推理到推断的跳跃。&lt;/p&gt;
&lt;p&gt;数据中心应用多了归纳推断能力之后，就产生了对复杂现实（现实信号，如图片、语料库、音频，整体上具有高维度的同时，局部往往还存在具有高数据精致度的结构，这里精致度可以用香农信息熵或柯氏复杂性形式化定义）理解能力(pin down reality)，因此涌现出了一些基于知识的AI系统(knowledge-based AI)，比如推荐系统、广告投送、翻唱识别、聊天机器人。共同点是将现实映射到表征低维子空间上，这个子空间里的向量或者说embedding可以从某个角度有效刻画现实。Transformer做得更进一步，等价于将现实映射到多个相干性低的低维子空间上（见https://arxiv.org/abs/2306.01129），从而从多角度多层次理解现实，通过表征空间编码率与各子空间编码率之和的差值度量特征的紧凑性和判别力。&lt;/p&gt;
&lt;p&gt;在游戏和博弈领域，强化学习也取得了规则系统无法企及的成就，在星际2、Dota2（固定阵容5v5，solo）、围棋等领域击败职业选手。&lt;/p&gt;
&lt;p&gt;此外，归纳推断相比演绎推理还有一个重要特质，那就是更符合人类大脑（或推广到任意地球生物大脑，蜜蜂的3d寻路避障能力，蚊子的血管定位能力都是基于天然的贝叶斯归纳推断。也只有受过良好训练的人类才能进行缓慢的演绎推理）的思维习惯，因此人类社会的传统，人类语言的不确定性、人类的偏好都难以用形式化方法描述，却可以被概率语言描述，被归纳推断方法预测。&lt;/p&gt;
&lt;p&gt;算法进步的下一个课题则是研究同时具备演绎推理和归纳推断能力的模型。现有的Transformer的确展现出简单多步算术的拟合能力，但拟合出来的是一个步数相关的超复杂非线性系统，而不是简单的算术规则，所以往往正确率到八十左右就再也上不去了。对于形式化系统来说，推理步数不影响公理和规则总数，只稍稍影响计算量。但对于LLM这种归纳推断模型来说，一旦推理步数变多，计算变得更复杂，LLM的符号逻辑能力就会迅速下降，在训练数据中从未出现过相同执行路径的动态规划问题里面可以直接降到0。理论上来说，无限精度的Transformer是图灵完备的（见https://arxiv.org/abs/1901.03429），而只要不是无限精度，就不是图灵完备，实际应用中模型权重和计算的精度即使以常规计算机应用标准看也是非常低的，而模型架构又没有针对多步逻辑推理进行设计，再加上深度学习本身就是在拟合一个近似函数，三重因素叠加导致现有LLM的演绎推理能力不尽如人意。&lt;/p&gt;
&lt;p&gt;从算法迭代角度讲，数据中心应用的发展趋势是在保证“可计算性”(字面意思，非形式化定义)的前提下，追求“人性化”(align to humanity)、“完备化”（pin down reality）这两个演绎推理手段无法达成的理想属性。智能创作、个性化推荐、聪明的Agent（助手、游戏NPC、机器人、自动驾驶），复杂系统预测（天气预报、交易系统），大模型训练（模型并行、高性能网络、C2C互连、光学I/O、异构计算）都是有前景的方向。&lt;/p&gt;
&lt;p&gt;而那些已经能用规则系统高效解决的问题则不必强行应用归纳推断模型，那样只会增加成本和错误率，比如音乐指纹识别、关系型数据库、图片旋转变形、无损压缩。甚至某些看似“神秘”的领域，如OOD text classification，基于表征学习白盒化研究成果和信息论，也能给出直接的数学解，比如近期一个研究（“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors）就用14行代码超过了百亿参数的bert，（后来被发现实验代码有误，实际上效果没那么好）这个论文里提出用gzip等无损压缩工具去近似文本数据的柯氏复杂性，用信息距离（原理类似马毅提出的rate reduction）做KNN，效果非常好，在音频领域也可以做类似的尝试。&lt;/p&gt;
&lt;h2 id=&#34;数据中心硬件基础设施&#34;&gt;数据中心硬件基础设施&lt;/h2&gt;
&lt;p&gt;先简单介绍一下常见的数据中心硬件基础设施：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;比较老的是Cascade Lake 14nm的24核per die的机器，一个芯片24核，两个就是48个物理核，也就是我们平时说的96核机器。也有其他套餐，最多是每个die上28核，总共112个逻辑核。&lt;/li&gt;
&lt;li&gt;比较新的Ice Lake是10nm工艺一般是32核的，对应128个逻辑核，最贵的套餐可以到40核。仍然是基于Monolithic die，巨大的晶粒上用mesh总线把40个核放到一起。&lt;/li&gt;
&lt;li&gt;最新的7nm工艺Sapphire Rapids，以及对标它的AMD Genoa今年已经量产。&lt;/li&gt;
&lt;li&gt;关于网卡，现在用的比较多的是Mellanox 25G  CX4/CX5卡，也有相当多的100G dual-port CX6卡，都是支持RDMA的，不用RDMA也能提供相当好的高速以太网性能。如果是做虚拟化的，比如AWS，阿里云，还可以把这些网卡的smartNiC能力利用起来，offload虚拟化开销。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相对之前的Lakes，Sapphire Rapids变化非常大，此前的monolithic die路线确实走到了极限，Sapphire Rapids也进入了multi-die时代：芯片内部分为4个die，这也可以视作向chiplet方向演化。&lt;/p&gt;
&lt;p&gt;Sapphire Rapids最多支持8个socket，每个芯片可以支持60核。IO技术上支持CXL 1.1、PCIe 5.0、UPI 2.0，HBM2e(optional)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/4th_xeon.jpeg&#34; alt=&#34;Xeon&#34;&gt;&lt;/p&gt;
&lt;p&gt;AMD的第四代EPYC的主力型号Genoa今年Q1也开始量产，基本上和Sapphire Rapids对标，采用5nm工艺。虽然说AVX512被很多人诟病，实用表现非常一般，但Zen4也支持了，毕竟XeongenEPYC目标客户就是音视频处理和AI workload越来越多的数据中心应用。Genoa是每个die/chiplet 8核的，12个CPU cluster die中间围绕一个IO die的设计，相当于一个芯片96核，是典型的chiplet范式设计。&lt;/p&gt;
&lt;h2 id=&#34;芯片设计的迭代chipletization&#34;&gt;芯片设计的迭代：Chipletization&lt;/h2&gt;
&lt;p&gt;近期芯片设计领域一个显著变革是Chiplets+SiP(System in Package)范式取代die size较大的SoC+PCB合封。
Chiplets同时受到业界和学术界的关注，被IBM research称为&amp;quot;what’s next in computing&amp;quot;，后续章节中对计算、IO、内存等技术的讨论也都涉及chiplets和co-packaging，因此我们首先讨论芯片层次的迭代。&lt;/p&gt;
&lt;p&gt;所谓chiplet partitioning就是将电路切分成模块化的子系统，每个子系统都是一个独立晶粒（die），即chiplet，多个chiplet用2.5D/3D技术封装成一个芯片(package)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/chiplet.png&#34; alt=&#34;Chiplet&#34;&gt;&lt;/p&gt;
&lt;p&gt;Chiplet-reuse范式相比传统的IP-reuse（IP在芯片语境下指的是具有独立功能和成熟设计的电路模块）的优势如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先进CMOS制程（7nm以下）由于技术原因不太可能在大晶粒上获得高yield，die size越小成本越低。&lt;/li&gt;
&lt;li&gt;先进CMOS制程下，不太可能同时缩小电源管理、快速IO SerDes等模拟IP，先进CMOS一般只用于处理器和加速器。&lt;/li&gt;
&lt;li&gt;允许模块化设计，让设计者可以专注于单个模块的极致优化，并选择最合适的技术：比如CPU和GPU用先进制程，模拟模块用成熟制程，高带宽内存HBM用DRAM，AI加速器可以用非易失性内存。&lt;/li&gt;
&lt;li&gt;允许芯片/package层次的异构集成：让通用CPU、优化后的GPU、嵌入式的FPGA、专用的机器学习电路、光学IO模组、高带宽内存等模块以合适的方式，用先进的使用硅通孔（TSV）、微凸块（micro-bumps）、甚至die-to-wafer混合键合技术的3D封装方案，像乐高积木一样搭出完整的系统。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;过去我们设想的各类DSA、AI芯片、FPGA百花齐放的异构计算时代并没有如期到来，而是被NV的GPU软硬件结合且计算互连一体化的方案碾轧了，几乎只剩下TPU在继续向v5迭代。&lt;/p&gt;
&lt;p&gt;但未来的服务器芯片本身就存在多样化异构共封装集成的可能性和倾向性。CPO(co-packaged optics)、HBM(high-bandwidth memory)这些神奇物种因Chipletization的契机而得以进驻其中。&lt;/p&gt;
&lt;p&gt;更多的功能也就意味着更高的可编程性。有些功能甚至可以带来革命性变革，比如光学IO带来的超高通信带宽，HBM带来的超高访存带宽，高性能offpackge互连技术（Nvlink-C2C）、多个Chiplet之间的Mesh互连(NvSwitch)，现在被Nvidia用来搭建H100，被谷歌用来搭建TPUv4，未来则可能颠覆host-centric的数据中心应用设计范式，迎来硬件资源解聚（disaggregation）的新计算体系：适应资源解聚的操作系统(LegoOS就是基于早期IB network的一个尝试)、系统语言ABI、新的高级语言、新的网络IO、存储和计算形态都有可能从中孵化而生。&lt;/p&gt;
&lt;h2 id=&#34;计算和内存层次的迭代可扩展的众核numa架构&#34;&gt;计算和内存层次的迭代：可扩展的众核NUMA架构&lt;/h2&gt;
&lt;p&gt;在商用服务器领域，Chiplet范式中的一部分设想已经实现了，比如AMD很早就开始应用chiplet，也部分解决了chiplet间IO问题，实现了有可扩展性的众核NUMA架构。SPR之前的Xeon物理机也是NUMA，虽然只有2个NUMA节点（目前Intel的NUMA node太大了，所以不太好称之为Chiplet）。&lt;/p&gt;
&lt;p&gt;μArch对计算/访存密集型数据中心应用的性能工程有直接影响。下图是一个6chiplet封装的96核概念机。显然当我们把集成电路的黑盒拆开，就可以看到更细粒度的组件以及它们组成的网络（Network-on-Chip）结构。这个概念机集成了各种先进设计，不仅有many-core，还支持完整的cache coherency。相比过去的多核架构，众核架构的内存层次也相应变得更深，cache miss的代价变得更高。以至于Rust的标准库用B树去实现map（而C++中众所周知是红黑树），这就是处理器和内存频率差距逐渐拉大的结果，（夸张地说）现在的内存已经慢得像是当年的磁盘了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/IntAct.png&#34; alt=&#34;IntAct&#34;&gt;&lt;/p&gt;
&lt;p&gt;针对NUMA架构，系统层的Linux内核和KVM的NUMA-aware scheduler，应用层的网络框架Seastar、数据库ScyllaDB、内存数据库DragonFly等都已经注意到感知硬件拓扑能极大提升整体性能（ScyllaDB、DragonFly分别数倍领先于对标的Cassandra、Redis），提出了share-nothing高性能架构：避免锁和不必要的共享内存、避免不必要的远端内存访问、避免不必要的跨晶粒通信，设计缓存友好的数据结构，更好地利用晶粒内本地的L1 cache——考虑到目前我们用的Cascade Lake机器并非完全cache coherent，未来即使做到完全cache coherent，shared cache的coherency机制也几乎一定有开销，总之在复杂拓扑深内存层次时代，需警惕cache miss。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/NUMA.png&#34; alt=&#34;NUMA&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;重新遇到io瓶颈先进互连再次成为hpc核心&#34;&gt;重新遇到I/O瓶颈：先进互连再次成为HPC核心&lt;/h2&gt;
&lt;p&gt;数据中心应用的IO占了全球IO traffic的76%，和计算一样，IO也是耗电的，而且和计算一样，数据中心IO耗电也十年没有变过，被硬件进步offset掉了。和计算一样，互连是分层级的，近期die-to-die(on-package)链路层面有UCIe标准的发布，off-package层面有基于PCIe6.0的CXL3.0，900GB/s的NvLinkC2C，inter-node层面有Infiniband NDR。这些是基于电的互连，相比而言光学互连更有前景，但也更困难，而且还在早期研发阶段。&lt;/p&gt;
&lt;p&gt;过去的大数据和前大模型AI时代对IO的需求较低，标准以太网足以支撑大部分数据中心应用，包括parameter servers。大模型训练产生了新的计算、IO形态，内存放不下模型，不得不做模型并行后，IO就重新成了瓶颈：H100的8个GPU每个都需要7.2Tbps的off-package带宽，相比之下，连ToR交换机都只需要10+Tbps。AI专用GPU在大模型训练场景下的带宽需求已经非常接近交换机（交换机和GPU一样，都是巨型ASIC，也都是co-packaged optics适用的领域）。在交换机领域，谷歌已经研发出了实用且收益显著的纯光学链路交换机。在GPU互连上，NV也提出过光学互连的GPU的概念系统，甚至还设计了相应的带外置激光源的GPU机架和顺便解决冷却问题的稀疏布线。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/optics.png&#34; alt=&#34;optics&#34;&gt;&lt;/p&gt;
&lt;p&gt;先进IO技术与HPC(高性能计算)的发展密可不分，尽管HPC或者说超算在大众的想象中一直和超强悍的处理器、加速器直接相关，但实际上恰恰相反，传统的HPC workload（建模、模拟类的科学计算）的计算用的往往是普通的商用节点，反而是互连必须用高性能的HPC interconnect技术。传统超算的异构性体现在IO技术，而非FPGA、专用ASIC的应用。&lt;/p&gt;
&lt;p&gt;后来到了大数据分析和AI时代，标准以太网足以支持适应当时模型参数量的AI训练负载。主流的互联网大数据应用可以完全基于商用IO技术和商用计算节点实现。而少数AI DC的异构性主要体现在加速器技术（GPU、TPU、专用AI芯片）而非IO。&lt;/p&gt;
&lt;p&gt;如今出现了大模型训练主导的异构负载，大模型参数量激增导致内存不足，不得不进行模型并行后，die-to-die带宽、off-package带宽、Inter-node带宽重新成为瓶颈。先进(异构)互连技术重新成为HPC的核心话题。&lt;/p&gt;
&lt;p&gt;Datacenter AI重回异构IO+异构计算架构，本质是超算化，因此刚好也能适应建模+模拟类的传统HPC负载，其实给了互联网行业一个新的机会，那就是卷赢大模型训练的同时，还可以顺便进军超算行业，为高校、科研机构提供廉价、可靠、易用、随时oncall的科学计算能力，舆论上一定程度上扭转互联网公司对社会缺乏贡献的负面形象，为继续征收互联网服务税寻求合法性支撑。&lt;/p&gt;
&lt;h2 id=&#34;io技术的迭代光学io愈发接近计算端点&#34;&gt;I/O技术的迭代：光学IO愈发接近计算端点&lt;/h2&gt;
&lt;p&gt;先进铜缆互连是现在，共封装光学互连则是未来。&lt;/p&gt;
&lt;p&gt;前文提到的Co-packaging是先进互连的关键技术，一方面将多个晶粒共封装本身就可以缩短IO链路，降低IO能耗，另一方面允许集成共封装光学模组技术CPO(co-packaged optics)。&lt;/p&gt;
&lt;p&gt;数据中心IO的一个演化趋势是&amp;quot;bring fiber closer to endpoints&amp;quot;。光链路相比电链路，一个明显优势是传输距离更远（受制于频率相关的衰减）。另一个优势是随着带宽增加，电信号不断变短，噪音不断变大，IB network已经逼近铜缆极限，继续发展下去只能从铜缆走向光纤。此外，高频下电互连和连接器既要接收又要发射，会经历显著的串扰，这也限制了电互连的封装密度。光纤作为信号传输介质几乎是理想的，唯一低效的地方就是两端的电光转换部分。&lt;/p&gt;
&lt;p&gt;现在in-racks连接主流方案是铜缆，inter-rack交换则基于以太网链路。超大数据中心里，缆线长就达到几公里，因此越来越多使用光缆——甚至短距离链路现在也越来越多地用光缆。数据中心里，fiber越来越接近endpoints，越来越接近cpus、gpus，最新的趋势是直接将光学组件集成到硅片上。CPO把光电链路结合在一起，无需intervening receive and re-transmit的过程，把光电转换(optoelectonic conversion)步骤省略了。第一代CPO是pluggable optics，第二代是On-Board Optics/Near Package Optics，第三代是2.5D CPO，第四代是3D CPO，第五代则是Integrated Laser。&lt;/p&gt;
&lt;p&gt;Google的TPUv4超算最大的创新就是4k节点上可重配置的纯光学链路的光学交换机(OCS)，节省了光电转换的能耗， Infiniband将铜缆高性能互连发展到极致，后续的roadmap也是从铜缆到光电共封装。Nvidia虽然一直在推电链路方案（Nvlink），但也和Ayar Labs签署了研发合作关系，开始支持带外激光器和硅光子互连技术的研究，毕竟NvLink本质还是NUMA，可以扩展到8GPU，16GPU，但不可能把数据中心规模的一万个GPU连起来。HP也于去年与Ayar Labs合作，试图将硅光子学引入它们的先进HPC IO产品Slignshot互连。Intel也在研究激光器嵌入芯片内部的集成方案。&lt;/p&gt;
&lt;p&gt;下图列出了interposer、PCB、CPO、电缆、有源光缆的耗电、成本、密度、传输距离指标。CPO的优势是显而易见的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/CPO.png&#34; alt=&#34;CPO&#34;&gt;&lt;/p&gt;
&lt;p&gt;在当前的技术水平下，CPO仅被视为一个电光(E/O)桥的角色，以解决SiP的互连带宽密度瓶颈问题。对分布式训练等应用场景来说，电光桥，或者说bring fiber closer to endpoints已经可以大幅降低能耗，提升性能了。但CPO的潜力远不限于此，如果给CPO chiplet稍微加一些功能，就可以像协处理器、smartNiC那样offload一些CPU work，比如做一些简单的数据预处理、后处理，又如CPO无需经过CPU直接就能访问HBM，从而提供DMA能力，这对解聚架构非常有帮助，无需物理上做pooling，又比铜缆IB网络更快。&lt;/p&gt;
&lt;p&gt;这意味着光学IO不仅可以解决大模型训练带来的带宽问题，还给数据中心应用从host-centric向解聚（disaggregated）架构转型提供了可能。&lt;/p&gt;
&lt;p&gt;何谓解聚范式？与传统的服务器中心范式相反，解聚范式是指将作为整体的服务器掰开，拆成CPU、DRAM、磁盘、加速器等独立的硬件资源进行资源抽象和管理的数据中心应用架构设计范式。硬件解聚并非新概念。18年的USENIX OSDI最佳论文LegoOS，一句&amp;quot;We believe that datacenters should break monolithic servers&amp;quot;，充满了信念感。当年的Infiniband还没有进化到NDR版本，光学I/O也还远离数据中心内部端点，但已经足以支撑这样的宏大叙事。&lt;/p&gt;
&lt;p&gt;有了高性能网络，解聚架构就能有效提升数据中心应用的资源利用率，减轻物理机上CPU、加速器、内存、磁盘等资源在host-centric范式下不可避免的over-provisioning问题。&lt;/p&gt;
&lt;h2 id=&#34;评估-gh200-grace-hopper-superchip&#34;&gt;评估 GH200 Grace Hopper Superchip&lt;/h2&gt;
&lt;p&gt;NVIDIA宣称Grace Hopper Superchip是世界上第一个真正支持HPC和AI负载的异构加速平台。
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper.png&#34; alt=&#34;GraceHopper&#34;&gt;
如下图所示，这个superchip是一个把Grace Arm Neoverse CPU+LPDDR5x内存和H100 Tensor Core GPU+HBM，NVLink-C2C集成合封成PCB的集成方案。
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper2.png&#34; alt=&#34;GraceHopper&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper3.png&#34; alt=&#34;GraceHopper&#34;&gt;&lt;/p&gt;
&lt;p&gt;这并不是一个创新方案，一方面悖逆Chipletization的潮流（除了HBM算是chiplet外，H100、Grace、NvSwitch都是巨型SoC/ASIC，况且即使是HBM也是PCB合封，而不是SiP），另一方面也没有进行任何CPU-GPU超融合（物理上融合至单个SoC上，逻辑上统一页表管理、内存、缓存、并发模型等）的探索或尝试（GPU设计之初就存在太多和CPU无法兼容的设计，比如缓存模型、内存模型和并发模型，如今CUDA根基已成很难回头），只是简单粗暴的将CPU、高带宽内存、H100以PCB合封的方式集成，用NVLink-C2C提供内存一致性和更高off-package的带宽（并未尝试任何先进IO技术）。软件上也没能在CUDA基础上提供更强的可编程性，仅仅提供coherent memory access，编程模型仍然是完全异构的（这也是因为CUDA自诞生之初就是个图形加速库，也没法考虑未来会出现对这种superchip的同构编程模型的需求）。&lt;/p&gt;
&lt;p&gt;但这是一个低风险高执行力的集成方案，正如扎克伯格所说，&amp;ldquo;Move fast and break nothing&amp;rdquo;，把原本优秀的组件原封不动地合封起来，不做侵入式修改，只要动作足够快，就能迅速占领市场，构建生态，并支撑溢价。市场上有更完美的memory coherence方案（比如AMD MI300X），更好的CPU-GPU超融合方案，也有比不得不为图形负载妥协的GPU效率更高的AI芯片，但就是没有CUDA异构编程体系，以及Grace Hooper这样把计算、内存和IO瓶颈都解决得差不多的完整解决方案。&lt;/p&gt;
&lt;p&gt;总之，NV的方案作为生态(GPU + CUDA)与生物(ChatGPT根据A100量体裁衣的训练方案)互相作用下的best-of-breed，远远没达到理想最优，甚至也不在正确的技术路线上，AMD的所谓APU以及国内的AI DSA（如Biren）仍有弯道超车的希望。&lt;/p&gt;
&lt;p&gt;讨论计算系统的新机会&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（应用）端到（硬件）端的全栈优化，或者说软硬件协同。
&lt;ul&gt;
&lt;li&gt;TVM: deep learning compiler stack for cpu, gpu and specialized accelerators&lt;/li&gt;
&lt;li&gt;GPU + CUDA&lt;/li&gt;
&lt;li&gt;GH20 Grace Hopper + 新的CUDA NUMA内存API+异构编程API&lt;/li&gt;
&lt;li&gt;司内的LavaRecord全链路优化项目，向下(LavaUOS)对接新存储硬件，试图在nvme ssd上建立高效的用户态IO软件栈。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;应用机器学习方法对参数空间较大的系统做auto-tuning。
&lt;ul&gt;
&lt;li&gt;存储引擎如rocksdb调参&lt;/li&gt;
&lt;li&gt;深度学习模型在异构硬件上的auto TVM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;先进互连技术支持下的资源解聚架构设计。
&lt;ul&gt;
&lt;li&gt;LegoOS&lt;/li&gt;
&lt;li&gt;PolarDB-X的存算分离和memory pooling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;计算节点上的Share-nothing架构，以及data-oriented设计。
&lt;ul&gt;
&lt;li&gt;应用框架层面已有Libtorque、DragonFly、Seastar、Scylladb等先例，主要是IO密集应用——不过只要是内存占用大的CPU应用，大多可以视为IO密集的，因为cache miss上来之后访存占比往往会远超计算。&lt;/li&gt;
&lt;li&gt;虚拟化方向，交大IPADS实验室的CPS: A Cooperative Para-virtualized Scheduling Framework for Manycore Machines，提出协作式半虚拟化调度机制，大幅提升众核虚拟机可扩展性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;基于深度模型白盒化研究和已有的数学工具，用direct math solution取代黑盒模型的近似。
&lt;ul&gt;
&lt;li&gt;例如“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors用压缩+信息距离+KNN的简洁解决方案。&lt;/li&gt;
&lt;li&gt;用异类不相干性、同类可压缩性（稀疏性）衡量embedding效果，不必借助某种端到端应用的指标间接衡量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文献和进一步阅读&#34;&gt;参考文献和进一步阅读&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Learning One-hidden-layer Neural Networks with Landscape Design：即使是最简单的深度学习非凸优化场景，用数学工具（数学最优化方法）进行解释也极为困难。&lt;/li&gt;
&lt;li&gt;Functionality and performance of NVLink with IBM POWER9 processors：几年前IBM Power9（美国能源部的Summit和Sierra超算系统）就在用NVLink，而且hardware cache coherence设计（以及hardware atomic ops，addr translation）已经非常完善，比Grace Hooper方案更完善。&lt;/li&gt;
&lt;li&gt;Faith and Fate: Limits of Transformers on Compositionality：大语言模型涌现出演绎逻辑能力，但在多步复合问题上表现不佳，在训练样本中从未出现过计算图中相同计算路径的动态规划问题上准确率更是迅速跌落。与其他emprical study相比，这个研究更严肃，也更全面，考虑了计算图中训练时未见的splits带来的影响。我们有理由确信，大语言模型涌现的演绎推理能力会受制于transformer的天然局限。&lt;/li&gt;
&lt;li&gt;Teaching Arithmetic to Small Transformers：基于transformer的小语言模型足以学习简单算术能力， 提供包含正确的计算步骤的训练数据（chain-of-thought style data）是提升算术学习能力的关键，简单粗暴地用题目和结果进行训练，单纯靠增加模型大小无法提升准确率。&lt;/li&gt;
&lt;li&gt;A Survey of Large Language Models：提供了对大语言模型的up-to-date review。&lt;/li&gt;
&lt;li&gt;Variantional Inference: A Review For Statisticians：提供了解释VI、理解VI的统计学家视角，讨论了VI应用于指数级模型族的特例，并给出一个贝叶斯高斯混合模型的例子，并推导出一种使用随机优化来扩展至海量数据的VI变体。&lt;/li&gt;
&lt;li&gt;Training language models to follow instructions with human feedback：OpenAI的经验介绍，重点是RLHF。&lt;/li&gt;
&lt;li&gt;GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE：来自semianalysis的爆料，颇具可信度。&lt;/li&gt;
&lt;li&gt;Efficiently Scale LLM Training Across a Large GPU Cluster with Alpa and Ray：LLM训练。&lt;/li&gt;
&lt;li&gt;Scaling Language Model Training to a Trillion Parameters Using Megatron：Megatron（repo： &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;https://github.com/NVIDIA/Megatron-LM&lt;/a&gt; ，paper： &lt;a href=&#34;https://arxiv.org/pdf/1909.08053.pdf&#34;&gt;https://arxiv.org/pdf/1909.08053.pdf&lt;/a&gt; ）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=eqWPyaRcILQ&#34;&gt;https://www.youtube.com/watch?v=eqWPyaRcILQ&lt;/a&gt; 微软Azure硬件系统和基础设施团队的Ram Huggahalli关于Co-Packaged Optics的talk。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Xt-GY8Pkt6g&#34;&gt;https://www.youtube.com/watch?v=Xt-GY8Pkt6g&lt;/a&gt; 研究光通信和先进互连技术的Tony Chan Carusone关于Co-Packaged Optics以及Evolution of IO的talk。&lt;/li&gt;
&lt;li&gt;Next-generation Co-Packaged Optics for Future Disaggregated AI Systems：对共封装光学模组以及未来的解聚AI系统的洞察。&lt;/li&gt;
&lt;li&gt;TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings : Google的TPUv4，重点是纯光链路交换机&lt;/li&gt;
&lt;li&gt;LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation ：乐高OS，基于早期Infiniband高速网络做硬件资源解聚的尝试&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.hpcwire.com/2020/11/16/nvidia-mellanox-debuts-ndr-400-gigabit-infiniband-at-sc20/&#34;&gt;https://www.hpcwire.com/2020/11/16/nvidia-mellanox-debuts-ndr-400-gigabit-infiniband-at-sc20/&lt;/a&gt; Mellanox(Nvidia)的Infiniband NDR版本，以及roadmap。&lt;/li&gt;
&lt;li&gt;Rack-scale disaggregated cloud data centers: The dReDBox project vision: 数据中心应用解聚架构的早期尝试。&lt;/li&gt;
&lt;li&gt;White-Box Transformers via Sparse Rate Reduction：马毅团队对transformer的白盒化解释，此前马毅已经给出了更通用的rate reduction原则： Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bgavran/Category_Theory_Machine_Learning&#34;&gt;https://github.com/bgavran/Category_Theory_Machine_Learning&lt;/a&gt; 深度学习的范畴论解释。深度学习可解释性和逆向工作还可以参考Christopher Olah的blog: colah.github.io，Olah有许多深刻的洞察，比如https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ 流形假设的可视化和深度学习分类的解释。&lt;/li&gt;
&lt;li&gt;IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management：先进IC设计领域的论文，给出了一个集成了chiplet范式、3d封装、完全cache coherence等先进概念的96核众核原型系统。&lt;/li&gt;
&lt;li&gt;“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors：无损压缩近似柯氏复杂性，然后计算信息距离（类似rate reduction，刻画了总体与分类之间的信息差，分类的编码长度低而总体的编码长度高，表明这种分类具有异类强区分性和同类可压缩性），依据信息距离做简单的KNN即可完成分类。这个研究的代码有错，并不能击败BERT，见https://kenschutte.com/gzip-knn-paper/。&lt;/li&gt;
&lt;li&gt;M. Li and P.M.B. Vitányi, An Introduction to Kolmogorov Complexity and Its Applications 柯尔莫哥洛夫复杂性的介绍和应用&lt;/li&gt;
&lt;li&gt;A Mathematical Theory of Communication 1948年香农信息论的论文原著&lt;/li&gt;
&lt;li&gt;hwloc doc：hwloc的文档，hwloc是NUMA-discovery + cpu/memory-binding library。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://man7.org/linux/man-pages/man2/mbind.2.html&#34;&gt;https://man7.org/linux/man-pages/man2/mbind.2.html&lt;/a&gt;：libnuma的NUMA memory policy函数。&lt;/li&gt;
&lt;li&gt;On the Turing Completeness of Modern Neural Network Architectures 证明了无限精度transformer是图灵完备的，即任意图灵机都可被无限精度transformer模拟，但只要是固定精度就不是图灵完备的。&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Our Rationality can be Divided into Induction &amp; Deduction</title>
      <link>https://cmbbq.github.io/posts/induction-deduction/</link>
      <pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/induction-deduction/</guid>
      <description>诺姆·乔姆斯基说：人的无知，可分为神秘和问题(mysteries &amp;amp; problems)。
诚哉斯言，为了对抗无知，人的理性，也可分为归纳和演绎(induction &amp;amp; deduction)。
前者推断神秘，后者解决问题。
前者完备而不可计算，后者可计算而不完备。
何为问题？ 何为问题？本文中指代可求解问题——有良好定义的输入、输出，存在确定性算法能逐步计算，最终得到正确结果，比如八皇后问题，哈密顿路径问题。
何为神秘？ 何为神秘？本文中指代不可求解问题——或需要无限计算资源，或具有不可判定性质（不可判定本质上就是不可计算），比如图灵提出的不可判定的停机问题。哥德尔构造的『真但不可证』的哥德尔语句。又如纯粹贝叶斯主义的置信度计算和预测必须同时考虑无限个预测性理论，现实计算机不仅处理不了计算量的无限性，还处理不了各个算法分支的不可终止性——并不是所有计算都会终止，有的甚至越算越复杂，永不止歇。
何为演绎？ 何为演绎？本文中指代演绎推理。欧几里得几何、初等算术、图灵机、λ演算等形式化系统（或者说演绎装置），就是由一组公理和一系列推理规则所组成，允许从公理推导出新理论的系统。
数理逻辑和演绎推理具有天然的美感。我们小时候学欧式几何时，应该都赞叹过这种形式主义美感：从简洁的五条公理，竟然能推导出复杂、庞大、深邃而又确切无疑正确的体系。演绎似乎是真理之钥。
自然而然地，历史上有数学家希望将所有数学理论建立在一组有限而完备的公理基础上，提供这些公理是一致的证明。1920年代，形式主义派的领导者希尔伯特希望建立一个形式化的证明体系，用严格遵循运算规则的符号逻辑语言表述所有数学陈述，给当时正面临多种悖论危机的数学一个安全的基础。这就是希尔伯特计划，一个人类理性对抗神秘的伟大尝试。希尔伯特说：『我们必须知道，我们也终将知道！』这是形式主义者的战斗宣言，他们坚信：数学具有完备性(在形式化数学里，所有数学陈述可证)、一致性（在形式化数学里，不存在矛盾）、可判定性（总有算法能判定某个数学陈述的真假）、保守性（证明不依赖于理想对象，如不可数集合）。只要这4点假设为真，无论现实多么宏大广博，真理如何高不可攀，人类只要持有演绎推理之钥，就能证明一切真理。
然而希尔伯特计划还未开始，就被哥德尔、图灵、邱奇终结。1930年代，哥德尔提出“广义递归函数”，图灵构造“图灵机”，邱奇提出“λ演算”，三人各自独立地形式化了可计算性：λ可计算性等价于图灵机可计算性，图灵机可计算性又等价于广义递归性。哥德尔不完备定理，证明了(1)一个蕴含初等算术的形式系统，若一致，则不完备。(2)该系统的一致性不能在系统内部证明。图灵提出停机问题，证明了我们无法判定图灵机是否会停止。停机问题的不可判定性蕴含了数学的不可判定性。一旦停机问题可判定，图灵机就能用来证明任意数学问题，因为理论上我们可以编写一个图灵机程序从公理开始构造出所有定理，每个定理生成后检查一下是不是目标问题——比如孪生素数猜想——如果是，图灵机程序终止，如果不是，则不停机。
何为归纳？ 何为归纳？本文中指代统计推断，泛指是使用数据分析来推断概率分布属性的过程。
统计学有贝叶斯学派和频率学派之争。贝叶斯学派认为待估计的模型参数是一个具有模型主观性的随机变量，而用来估计模型参数的数据反过来是确定的常数，讨论观测数据的概率分布才是没有意义的，概率被解释为模型在现有知识基础上对一件事情发生的相信程度。频率学派认为事件本身具有某种客观的随机性，概率是一个确定的值，讨论概率的分布没有意义，概率只是重复实验次数趋近于无穷后频率的极限值。贝叶斯理论最早起源于1763年，此后被频率学派以居高临下的姿态视作已经足够好的正统方法的补充，或者干脆是当成某种有趣但错误的尝试。直到上个世纪末才重新被学界重视。部分原因是计算机取代计算器，成为新的计算工具，克服了贝叶斯方法在计算上的困难。在通用电子计算机出现之前，频率学派对某些特定问题做出特殊假设的做法显然更经济，往往也相当有效，只不过这些人为假设难以证伪也难以证明。
归根到底，频率学派的归纳推断只能勉强算是贝叶斯推断的近似，还有一部分近似是错误的近似。CERN通过大型强子对撞机发现希格斯玻色子，实际上就是依据：假设不存在希格斯玻色子，观测数据的p值&amp;lt;0.00003%，这个阈值看起来足够低，因此CERN的研究者以及整个人类社会就接受了希格斯玻色子的存在。这个0.0003%的阈值是哪里来的呢？和人文社科研究中那些p值&amp;lt;5%一样都来源于直觉和人为假设。所谓的科学，不过是建立在如此薄弱的基础之上。即使在物理学实验这种极端严谨的场合，基于p值的统计分析依然会造成假象。2003年的五夸克态因p值被宣布发现，然而又因原始实验无法复现而被否定。Regina Nuzzo在Nature杂志上对p值滥用进行了批判，通过实例证明科学研究中普遍存在p值统计学显著结果不可重现的问题。
1960年代，纯粹贝叶斯主义者所罗门诺夫超越了哥德尔不完备理论，超越了邱奇-图灵论题，提出了贝叶斯理论的计算形式化，即所罗门诺夫归纳推断理论，并证明了该理论具有完备性和不可计算性。所罗门诺夫给出了一个惊人的观察：完备性和可计算性，不可得兼。任何理论，只要是可计算的，就不能检出所有规律。而像所罗门诺夫归纳推断理论这样具有完备性的理论，则必然是不可计算的。
所罗门诺夫理论的完备性意味着贝叶斯公式（这里是简化表示）P(θ|d) = P(d|θ)P(θ)/P(d)几乎可以被认为是真理公式。演绎推理则是贝叶斯推理在真假确定场景下的正确特例：当P(d) = 1时，概率语言中P(θ|d) = P(d|θ)P(θ)/P(d) 就自然退化成经典逻辑语言d =&amp;gt; θ，即d蕴含θ了。
规则的尽头是贝叶斯 下图是摘自Computability and Complexity，最早出自Descriptive Complexity(Immerman 1999)的可计算性与复杂度世界图，自上而下，逐渐从神秘过渡到问题。 当我们用if else条件判断，用计算机程序语言，无论如何努力，都难以编写出效果让人满意的程序时。我们往往是在面对神秘，而非问题。
这种场景下，经典逻辑的手段已经穷尽，演绎明确不是真理之钥，但我们可以观想所罗门诺夫之妖。
所罗门诺夫之妖所喻的纯粹贝叶斯主义固然具有不可计算性，却可以被各种手段近似模拟，典型的结构就是深度神经网络，用数十亿、数百亿、数千亿个参数/权重，用臃肿笨拙地方式去模拟贝叶斯推断。
在决策和博弈型任务上，强化学习对比基于规则的方法，取得了惊人的效果。目前的强化学习基本上能求解任意人类游戏/博弈，DeepMind的星际争霸AI AlphaStar击败MaNa，Dota2的OpenAI Five击败OG，围棋的AlphaGo击败柯洁。这是此前基于规则的方法几乎不可能实现的。
在理解和生成型任务上，大语言模型对比传统的NLP方法，达到了超出训练目标的额外效果，涌现出种种惊喜，比如单步逻辑推理能力，以及某些简单复合问题的逻辑推理能力。
基于图灵机、λ演算的计算理论根基数十年岿然不动，一朝有变，则蕴含着更深刻的变革——我们正见证从图灵机的演绎推理到神经网络的统计推断的跳跃，从“可计算且不完备”到“不可计算且完备”的质变。
前文Knowledge is Embeddings of Reality中提到对音频的理解可以是不同深度，不同角度的，音乐信息检索可以用信号处理的基于规则的方法，也可以用resnet这样的深度神经网络，或whisper这样的大模型。理解愈深邃，愈不精确，应用也就愈发灵活，愈发贴近人类思维。这是理解的视角，若是从计算的视角讲，基于信号处理的系统是在做演绎推理，因此可计算性最强，但天然具有不完备性，无论我们给代码里增加多少个公理，增加多少执行路径，设计多么巧妙的手工特征，注入多么鬼斧神工的专家洞察，都总有覆盖不到的场景，甚至可以说只能覆盖到很小很小的一部分，因为音频数据来自于现实，原始数据具备极高的整体复杂度和细微结构精致度，超越了人类思考和编程的能力极限，人类专家设计的音频特征能考虑抗噪，具备简单的鲁棒性，就已经是sota了。而基于深度模型、乃至大模型的方法则是对贝叶斯推断的近似，放弃精确性、增加计算成本的同时，从演绎转变成了归纳，从收敛走向开放，哪怕仅仅是对贝叶斯的近似，也换取了一定程度上的完备性——不局限于简单的抗噪、抗变调，开始在旋律、情绪、风格，甚至歌词的语言逻辑层面上考虑音频的相似度。
认知计算：人性化与完备化的革命 基于推理的算法，本质是为形式化系统（λ演算、图灵机）手工编写公理（先验数据，如魔数、掩码表、配置文件）或规则（传统的条件控制与算术计算）进行正确、高效但不完备的演绎推理。
基于推断的算法，本质是使用形式化系统（计算神经网络依然是形式化系统，其公理变成了更庞大的模型权重，规则变成了矩阵乘加、归一化、激活函数等数学算子的组合）对所罗门诺夫归纳推断理论（形式化的纯粹贝叶斯主义）进行拙劣、错误，但有用的近似。
统计推断，模型推理，英文中都是inference，中文用词却不一，不知是巧合还是集体无意识暗合真理：推断是归纳，推理则是演绎。纯粹贝叶斯主义的所罗门诺夫归纳推断近乎全知，不可计算，无穷尽，不停机，而我们用模型权重为公理，用矩阵乘加、批正态化、激活函数组合为规则写出来的模型推理程序本质上还是基于演绎推理，训练或者说优化、学习的过程亦然。因此在计算机科学的AI应用语境下我们把inference称为推理完全合理，正如在统计学语境下我们把inference称为推断，也是无比合理的。
所谓认知计算，就是用演绎推理的凡俗躯壳容纳纯粹贝叶斯主义全知魂灵的神降，补齐符号逻辑、规则系统和信号处理手段在“人性化(align to humanity)”、“完备化(pin down reality)”上的缺憾。
可以断言，尽管AI学界和业界浮躁且浮夸，受传统学科乃至计算系统传统领域的研究者诟病，但深度学习本身绝非niche应用，更非Metaverse、Web3那种VC追逐的玩具和噱头。所有计算领域——互联网服务、数据中心应用、智能终端、游戏AI，都将受益于认知计算进步，人类文明很有可能也会进入新的阶段。
近未来人类文明在基础物理、材料、能源领域或难有寸进，但在认知科学上构筑出辉煌的认知计算体系几乎是确定性的，因为认知计算所需的硬件进步不构成瓶颈，至少不会像对撞机、深空旅行、可控核聚变那样受到时空尺度和材料极限的约束。毕竟考虑即使是小小一只蜜蜂不足2立方厘米的大脑，就能拥有超越人类目前基于规则和导航辅助的先进算法的复杂3D环境寻路避障能力和天生的贝叶斯推断能力，亿万年生物演化的自然结构已经存在，我们不必担心这种结构根本不存在：它就在那，我们所需做的仅仅是近似和模仿。</description>
      <content>&lt;p&gt;诺姆·乔姆斯基说：人的无知，可分为神秘和问题(mysteries &amp;amp; problems)。&lt;/p&gt;
&lt;p&gt;诚哉斯言，为了对抗无知，人的理性，也可分为归纳和演绎(induction &amp;amp; deduction)。&lt;/p&gt;
&lt;p&gt;前者推断神秘，后者解决问题。&lt;/p&gt;
&lt;p&gt;前者&lt;a href=&#34;(https://en.wikipedia.org/wiki/Complete_theory)&#34;&gt;完备&lt;/a&gt;而不可计算，后者&lt;a href=&#34;https://en.wikipedia.org/wiki/Computability_theory&#34;&gt;可计算&lt;/a&gt;而不完备。&lt;/p&gt;
&lt;h2 id=&#34;何为问题&#34;&gt;何为问题？&lt;/h2&gt;
&lt;p&gt;何为问题？本文中指代可求解问题——有良好定义的输入、输出，存在确定性算法能逐步计算，最终得到正确结果，比如&lt;a href=&#34;https://en.wikipedia.org/wiki/Eight_queens_puzzle&#34;&gt;八皇后问题&lt;/a&gt;，&lt;a href=&#34;https://en.wikipedia.org/wiki/Hamiltonian_path_problem&#34;&gt;哈密顿路径问题&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;何为神秘&#34;&gt;何为神秘？&lt;/h2&gt;
&lt;p&gt;何为神秘？本文中指代不可求解问题——或需要无限计算资源，或具有不可判定性质（不可判定本质上就是不可计算），比如图灵提出的&lt;a href=&#34;https://en.wikipedia.org/wiki/Undecidable_problem&#34;&gt;不可判定&lt;/a&gt;的&lt;a href=&#34;https://en.wikipedia.org/wiki/Halting_problem&#34;&gt;停机问题&lt;/a&gt;。哥德尔构造的『真但不可证』的&lt;a href=&#34;https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems&#34;&gt;哥德尔语句&lt;/a&gt;。又如纯粹贝叶斯主义的置信度计算和预测必须同时考虑无限个预测性理论，现实计算机不仅处理不了计算量的无限性，还处理不了各个算法分支的不可终止性——并不是所有计算都会终止，有的甚至越算越复杂，永不止歇。&lt;/p&gt;
&lt;h2 id=&#34;何为演绎&#34;&gt;何为演绎？&lt;/h2&gt;
&lt;p&gt;何为演绎？本文中指代&lt;a href=&#34;https://en.wikipedia.org/wiki/Deductive_reasoning&#34;&gt;演绎推理&lt;/a&gt;。欧几里得几何、初等算术、图灵机、&lt;a href=&#34;https://en.wikipedia.org/wiki/Lambda_calculus&#34;&gt;λ演算&lt;/a&gt;等形式化系统（或者说演绎装置），就是由一组公理和一系列推理规则所组成，允许从公理推导出新理论的系统。&lt;/p&gt;
&lt;p&gt;数理逻辑和演绎推理具有天然的美感。我们小时候学欧式几何时，应该都赞叹过这种形式主义美感：从简洁的五条公理，竟然能推导出复杂、庞大、深邃而又确切无疑正确的体系。演绎似乎是真理之钥。&lt;/p&gt;
&lt;p&gt;自然而然地，历史上有数学家希望将所有数学理论建立在一组有限而完备的公理基础上，提供这些公理是一致的证明。1920年代，形式主义派的领导者希尔伯特希望建立一个形式化的证明体系，用严格遵循运算规则的符号逻辑语言表述所有数学陈述，给当时正面临多种&lt;a href=&#34;https://en.wikipedia.org/wiki/Foundations_of_mathematics#Foundational_crisis&#34;&gt;悖论危机&lt;/a&gt;的数学一个安全的基础。这就是&lt;a href=&#34;https://en.wikipedia.org/wiki/Hilbert%27s_program&#34;&gt;希尔伯特计划&lt;/a&gt;，一个人类理性对抗神秘的伟大尝试。希尔伯特说：『我们必须知道，我们也终将知道！』这是形式主义者的战斗宣言，他们坚信：数学具有完备性(在形式化数学里，所有数学陈述可证)、一致性（在形式化数学里，不存在矛盾）、可判定性（总有算法能判定某个数学陈述的真假）、保守性（证明不依赖于理想对象，如不可数集合）。只要这4点假设为真，无论现实多么宏大广博，真理如何高不可攀，人类只要持有演绎推理之钥，就能证明一切真理。&lt;/p&gt;
&lt;p&gt;然而希尔伯特计划还未开始，就被哥德尔、图灵、邱奇终结。1930年代，哥德尔提出“广义递归函数”，图灵构造“图灵机”，邱奇提出“λ演算”，三人各自独立地形式化了可计算性：λ可计算性等价于图灵机可计算性，图灵机可计算性又等价于广义递归性。&lt;a href=&#34;https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems#First_incompleteness_theorem&#34;&gt;哥德尔不完备定理&lt;/a&gt;，证明了(1)一个蕴含初等算术的形式系统，若一致，则不完备。(2)该系统的一致性不能在系统内部证明。图灵提出停机问题，证明了我们无法判定图灵机是否会停止。停机问题的不可判定性蕴含了数学的不可判定性。一旦停机问题可判定，图灵机就能用来证明任意数学问题，因为理论上我们可以编写一个图灵机程序从公理开始构造出所有定理，每个定理生成后检查一下是不是目标问题——比如孪生素数猜想——如果是，图灵机程序终止，如果不是，则不停机。&lt;/p&gt;
&lt;h2 id=&#34;何为归纳&#34;&gt;何为归纳？&lt;/h2&gt;
&lt;p&gt;何为归纳？本文中指代&lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_inference&#34;&gt;统计推断&lt;/a&gt;，泛指是使用数据分析来推断概率分布属性的过程。&lt;/p&gt;
&lt;p&gt;统计学有贝叶斯学派和频率学派之争。贝叶斯学派认为待估计的模型参数是一个具有模型主观性的随机变量，而用来估计模型参数的数据反过来是确定的常数，讨论观测数据的概率分布才是没有意义的，概率被解释为模型在现有知识基础上对一件事情发生的相信程度。频率学派认为事件本身具有某种客观的随机性，概率是一个确定的值，讨论概率的分布没有意义，概率只是重复实验次数趋近于无穷后频率的极限值。贝叶斯理论最早起源于1763年，此后被频率学派以居高临下的姿态视作已经足够好的正统方法的补充，或者干脆是当成某种有趣但错误的尝试。直到上个世纪末才重新被学界重视。部分原因是计算机取代计算器，成为新的计算工具，克服了贝叶斯方法在计算上的困难。在通用电子计算机出现之前，频率学派对某些特定问题做出特殊假设的做法显然更经济，往往也相当有效，只不过这些人为假设难以证伪也难以证明。&lt;/p&gt;
&lt;p&gt;归根到底，频率学派的归纳推断只能勉强算是贝叶斯推断的近似，还有一部分近似是错误的近似。CERN通过大型强子对撞机发现希格斯玻色子，实际上就是依据：假设不存在希格斯玻色子，观测数据的p值&amp;lt;0.00003%，这个阈值看起来足够低，因此CERN的研究者以及整个人类社会就接受了希格斯玻色子的存在。这个0.0003%的阈值是哪里来的呢？和人文社科研究中那些p值&amp;lt;5%一样都来源于直觉和人为假设。所谓的科学，不过是建立在如此薄弱的基础之上。即使在物理学实验这种极端严谨的场合，基于p值的统计分析依然会造成假象。2003年的五夸克态因p值被宣布发现，然而又因原始实验无法复现而被否定。&lt;a href=&#34;https://www.nature.com/articles/506150a&#34;&gt;Regina Nuzzo在Nature杂志上对p值滥用进行了批判&lt;/a&gt;，通过实例证明科学研究中普遍存在p值统计学显著结果不可重现的问题。&lt;/p&gt;
&lt;p&gt;1960年代，纯粹贝叶斯主义者所罗门诺夫超越了哥德尔不完备理论，超越了邱奇-图灵论题，提出了贝叶斯理论的计算形式化，即&lt;a href=&#34;https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference&#34;&gt;所罗门诺夫归纳推断理论&lt;/a&gt;，并证明了该理论具有完备性和不可计算性。所罗门诺夫给出了一个惊人的观察：完备性和可计算性，不可得兼。任何理论，只要是可计算的，就不能检出所有规律。而像所罗门诺夫归纳推断理论这样具有完备性的理论，则必然是不可计算的。&lt;/p&gt;
&lt;p&gt;所罗门诺夫理论的完备性意味着贝叶斯公式（这里是简化表示）P(θ|d) = P(d|θ)P(θ)/P(d)几乎可以被认为是真理公式。演绎推理则是贝叶斯推理在真假确定场景下的正确特例：当P(d) = 1时，概率语言中P(θ|d) = P(d|θ)P(θ)/P(d) 就自然退化成经典逻辑语言d =&amp;gt; θ，即d蕴含θ了。&lt;/p&gt;
&lt;h2 id=&#34;规则的尽头是贝叶斯&#34;&gt;规则的尽头是贝叶斯&lt;/h2&gt;
&lt;p&gt;下图是摘自&lt;a href=&#34;https://plato.stanford.edu/entries/computability&#34;&gt;Computability and Complexity&lt;/a&gt;，最早出自Descriptive Complexity(Immerman 1999)的可计算性与复杂度世界图，自上而下，逐渐从神秘过渡到问题。
&lt;img src=&#34;https://cmbbq.github.io/img/CaC.jpeg&#34; alt=&#34;cac&#34;&gt;&lt;/p&gt;
&lt;p&gt;当我们用if else条件判断，用计算机程序语言，无论如何努力，都难以编写出效果让人满意的程序时。我们往往是在面对神秘，而非问题。&lt;/p&gt;
&lt;p&gt;这种场景下，经典逻辑的手段已经穷尽，演绎明确不是真理之钥，但我们可以观想所罗门诺夫之妖。&lt;/p&gt;
&lt;p&gt;所罗门诺夫之妖所喻的纯粹贝叶斯主义固然具有不可计算性，却可以被各种手段近似模拟，典型的结构就是深度神经网络，用数十亿、数百亿、数千亿个参数/权重，用臃肿笨拙地方式去模拟贝叶斯推断。&lt;/p&gt;
&lt;p&gt;在决策和博弈型任务上，强化学习对比基于规则的方法，取得了惊人的效果。目前的强化学习基本上能求解任意人类游戏/博弈，DeepMind的星际争霸AI AlphaStar击败MaNa，Dota2的OpenAI Five击败OG，围棋的AlphaGo击败柯洁。这是此前基于规则的方法几乎不可能实现的。&lt;/p&gt;
&lt;p&gt;在理解和生成型任务上，大语言模型对比传统的NLP方法，达到了超出训练目标的额外效果，涌现出种种惊喜，比如单步逻辑推理能力，以及某些简单复合问题的逻辑推理能力。&lt;/p&gt;
&lt;p&gt;基于图灵机、λ演算的计算理论根基数十年岿然不动，一朝有变，则蕴含着更深刻的变革——我们正见证从图灵机的演绎推理到神经网络的统计推断的跳跃，从“可计算且不完备”到“不可计算且完备”的质变。&lt;/p&gt;
&lt;p&gt;前文&lt;a href=&#34;https://cmbbq.github.io/posts/reality-knowledge&#34;&gt;Knowledge is Embeddings of Reality&lt;/a&gt;中提到对音频的理解可以是不同深度，不同角度的，音乐信息检索可以用信号处理的基于规则的方法，也可以用resnet这样的深度神经网络，或whisper这样的大模型。理解愈深邃，愈不精确，应用也就愈发灵活，愈发贴近人类思维。这是理解的视角，若是从计算的视角讲，基于信号处理的系统是在做演绎推理，因此可计算性最强，但天然具有不完备性，无论我们给代码里增加多少个公理，增加多少执行路径，设计多么巧妙的手工特征，注入多么鬼斧神工的专家洞察，都总有覆盖不到的场景，甚至可以说只能覆盖到很小很小的一部分，因为音频数据来自于现实，原始数据具备极高的整体复杂度和细微结构精致度，超越了人类思考和编程的能力极限，人类专家设计的音频特征能考虑抗噪，具备简单的鲁棒性，就已经是sota了。而基于深度模型、乃至大模型的方法则是对贝叶斯推断的近似，放弃精确性、增加计算成本的同时，从演绎转变成了归纳，从收敛走向开放，哪怕仅仅是对贝叶斯的近似，也换取了一定程度上的完备性——不局限于简单的抗噪、抗变调，开始在旋律、情绪、风格，甚至歌词的语言逻辑层面上考虑音频的相似度。&lt;/p&gt;
&lt;h2 id=&#34;认知计算人性化与完备化的革命&#34;&gt;认知计算：人性化与完备化的革命&lt;/h2&gt;
&lt;p&gt;基于推理的算法，本质是为形式化系统（λ演算、图灵机）手工编写公理（先验数据，如魔数、掩码表、配置文件）或规则（传统的条件控制与算术计算）进行正确、高效但不完备的演绎推理。&lt;/p&gt;
&lt;p&gt;基于推断的算法，本质是使用形式化系统（计算神经网络依然是形式化系统，其公理变成了更庞大的模型权重，规则变成了矩阵乘加、归一化、激活函数等数学算子的组合）对&lt;a href=&#34;https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference&#34;&gt;所罗门诺夫归纳推断理论&lt;/a&gt;（形式化的纯粹贝叶斯主义）进行拙劣、错误，但有用的近似。&lt;/p&gt;
&lt;p&gt;统计推断，模型推理，英文中都是inference，中文用词却不一，不知是巧合还是集体无意识暗合真理：推断是归纳，推理则是演绎。纯粹贝叶斯主义的所罗门诺夫归纳推断近乎全知，不可计算，无穷尽，不停机，而我们用模型权重为公理，用矩阵乘加、批正态化、激活函数组合为规则写出来的模型推理程序本质上还是基于演绎推理，训练或者说优化、学习的过程亦然。因此在计算机科学的AI应用语境下我们把inference称为推理完全合理，正如在统计学语境下我们把inference称为推断，也是无比合理的。&lt;/p&gt;
&lt;p&gt;所谓认知计算，就是用演绎推理的凡俗躯壳容纳纯粹贝叶斯主义全知魂灵的神降，补齐符号逻辑、规则系统和信号处理手段在“人性化(align to humanity)”、“完备化(pin down reality)”上的缺憾。&lt;/p&gt;
&lt;p&gt;可以断言，尽管AI学界和业界浮躁且浮夸，受传统学科乃至计算系统传统领域的研究者诟病，但深度学习本身绝非niche应用，更非Metaverse、Web3那种VC追逐的玩具和噱头。所有计算领域——互联网服务、数据中心应用、智能终端、游戏AI，都将受益于认知计算进步，人类文明很有可能也会进入新的阶段。&lt;/p&gt;
&lt;p&gt;近未来人类文明在基础物理、材料、能源领域或难有寸进，但在认知科学上构筑出辉煌的认知计算体系几乎是确定性的，因为认知计算所需的硬件进步不构成瓶颈，至少不会像对撞机、深空旅行、可控核聚变那样受到时空尺度和材料极限的约束。毕竟考虑即使是小小一只蜜蜂不足2立方厘米的大脑，就能拥有超越人类目前基于规则和导航辅助的先进算法的复杂3D环境寻路避障能力和天生的贝叶斯推断能力，亿万年生物演化的自然结构已经存在，我们不必担心这种结构根本不存在：它就在那，我们所需做的仅仅是近似和模仿。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Knowledge is Embeddings of Reality</title>
      <link>https://cmbbq.github.io/posts/reality-knowledge/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/reality-knowledge/</guid>
      <description>现实有无限维度，现实的背后还有潜在现实，结构的边缘会涌现出新的深层结构。
人类对现实的感知、认知、理解，本质上是将高维现实投射到一个低维表示上，这种低维表示在统计学和机器学习中被称为embedding（嵌入），没错，知识就是将现实在人类有限的认知空间的一个嵌入，紧凑、浅薄且片面。
举个具体的例子，人眼观察到的彩虹呈现七色。
但这只是对无限广博的现实的最初观察，是最直观的认知。
人类通过对人体生理学的研究，发现七色感知是一个更深层的潜在现实的涌现结构：多种光感受器的叠加反应。
人类颜色感知系统对红绿蓝敏感的三种光感受器，叠加反应产生的三个峰和四个谷决定了人类能感知到7种颜色。 鸟类由于拥有对紫外线敏感的光感受器，就能感知到9种颜色，因此可以看到九色彩虹。
无论人类的七色彩虹还是鸟类的九色彩虹，都只是知识（或者说embedding），而不是现实。
人类通过数学工具和电磁学研究，进一步认识到光谱是连续的，包括无线电波、微波、热、红外线、可见光、紫外线等。
这接近现实了吗？不，这依然只是浅薄的embedding。。
人类通过量子物理学的研究，发现现有的电磁学依旧是一个更深层的潜在现实的涌现模型——电磁波是量子粒子流的一种表现形式：光子。
光子的认知，折射出更复杂的现实，因为一个光子同时拥有无限的位置和频率，直到被观察到才能确定其位置或能量，需用海森堡不确定性原理、薛定谔方程来描述，这依旧不是真理，不是终极，仍然是在现实之上所施加的结构(structure imposed upon reality)。
类似地，现实中的一首歌曲，看似简单平凡，实际上却拥有无穷维度，我们可以从任意一个角度去理解它，将它嵌入到任意一个向量空间。
假设用户手机的传感器捕捉了一段音频，将其上传到平台上，消重（时频图的mask特征）、指纹(时频图的landmark特征)、翻唱（浅层cnn）、哼唱（whisper大模型）、多模态大模型应用分别捕捉了这段音频的某种特征，产生了不同层次的知识（embedding），理解愈发深邃，愈发不精确，应用也就愈发灵活，愈发贴近人类思维。
后端的消重服务或指纹服务将表示音频的信号采样数据通过fft计算得到时频图，再基于时频图提取某种mask特征或landmark特征，用于直接刻画其能量分布特征，然后与音频特征库里存着的现有特征做对比就可以找到匹配的音频，这就是音频消重和指纹识别的过程：对信号做简单数学处理，生成的就是高度特化、具体、紧凑、确定、可解释的知识，表示这种知识的embedding或不具有抗噪性（mask特征），或具有少许抗噪性（landmark特征），但无论如何都只适用于原曲匹配，不知何为翻唱，何为remix，何为二创，何为串烧。
翻唱识别服务能通过一个学习了歌曲旋律的神经网络对音频信号进行处理，产出一定程度上表达旋律特征的embedding，将曲库里的翻唱embedding存入向量数据库，通过KNN/ANN召回的结果往往不限于原曲，还能容忍变调、音色上的改变。
哼唱识别服务可以基于Whisper这样的音频大模型实现，这种深层网络学习了人类的自然语言，因此对音频信号处理时，提取了表达歌词信息的embedding。基于这样的embedding，可以在跑调非常严重，且没有伴奏的情况下，依然能识别出用户在哼什么歌。
考虑未来的多模态大模型，更是可以直接将embedding存在transformer的ffn层，不必借助外部的向量数据库或倒排索引，直接在模型内部刻画embedding之间的复杂关系，让更多概念之间产生关联：识别出歌曲的意境、歌唱者的情绪、歌词内容的图片，甚至联系用户交互的上下文，衍生出更客制化、场景相关的输出。
人工智能领域用transformer简单搭建的大模型拥有出人意料的逻辑能力，昭示着人类意识的本质或许原本就极端肤浅：将知识内化、知识关联后，用天生的脑回路做贝叶斯推理，就产生了逻辑，诚然现有的结构远不如人脑高效，但有和无的界限已经被突破。
足够好的计算神经网络可以被视作神经科学的涌现模型。 一如往昔。 一如神经科学是细胞生物学的涌现。 一如细胞生物学是分子生物学的涌现。 一如分子生物学是物理化学的涌现。 一如物理化学是量子物理的涌现。</description>
      <content>&lt;p&gt;现实有无限维度，现实的背后还有潜在现实，结构的边缘会涌现出新的深层结构。&lt;/p&gt;
&lt;p&gt;人类对现实的感知、认知、理解，本质上是将高维现实投射到一个低维表示上，这种低维表示在统计学和机器学习中被称为embedding（嵌入），没错，知识就是将现实在人类有限的认知空间的一个嵌入，紧凑、浅薄且片面。&lt;/p&gt;
&lt;p&gt;举个具体的例子，人眼观察到的彩虹呈现七色。&lt;/p&gt;
&lt;p&gt;但这只是对无限广博的现实的最初观察，是最直观的认知。&lt;/p&gt;
&lt;p&gt;人类通过对人体生理学的研究，发现七色感知是一个更深层的潜在现实的涌现结构：多种光感受器的叠加反应。&lt;/p&gt;
&lt;p&gt;人类颜色感知系统对红绿蓝敏感的三种光感受器，叠加反应产生的三个峰和四个谷决定了人类能感知到7种颜色。
&lt;img src=&#34;https://cmbbq.github.io/img/color.png&#34; alt=&#34;color&#34;&gt;&lt;/p&gt;
&lt;p&gt;鸟类由于拥有对紫外线敏感的光感受器，就能感知到9种颜色，因此可以看到九色彩虹。&lt;/p&gt;
&lt;p&gt;无论人类的七色彩虹还是鸟类的九色彩虹，都只是知识（或者说embedding），而不是现实。&lt;/p&gt;
&lt;p&gt;人类通过数学工具和电磁学研究，进一步认识到光谱是连续的，包括无线电波、微波、热、红外线、可见光、紫外线等。&lt;/p&gt;
&lt;p&gt;这接近现实了吗？不，这依然只是浅薄的embedding。。&lt;/p&gt;
&lt;p&gt;人类通过量子物理学的研究，发现现有的电磁学依旧是一个更深层的潜在现实的涌现模型——电磁波是量子粒子流的一种表现形式：光子。&lt;/p&gt;
&lt;p&gt;光子的认知，折射出更复杂的现实，因为一个光子同时拥有无限的位置和频率，直到被观察到才能确定其位置或能量，需用海森堡不确定性原理、薛定谔方程来描述，这依旧不是真理，不是终极，仍然是在现实之上所施加的结构(structure imposed upon reality)。&lt;/p&gt;
&lt;p&gt;类似地，现实中的一首歌曲，看似简单平凡，实际上却拥有无穷维度，我们可以从任意一个角度去理解它，将它嵌入到任意一个向量空间。&lt;/p&gt;
&lt;p&gt;假设用户手机的传感器捕捉了一段音频，将其上传到平台上，消重（时频图的mask特征）、指纹(时频图的landmark特征)、翻唱（浅层cnn）、哼唱（whisper大模型）、多模态大模型应用分别捕捉了这段音频的某种特征，产生了不同层次的知识（embedding），理解愈发深邃，愈发不精确，应用也就愈发灵活，愈发贴近人类思维。&lt;/p&gt;
&lt;p&gt;后端的消重服务或指纹服务将表示音频的信号采样数据通过fft计算得到时频图，再基于时频图提取某种mask特征或landmark特征，用于直接刻画其能量分布特征，然后与音频特征库里存着的现有特征做对比就可以找到匹配的音频，这就是音频消重和指纹识别的过程：对信号做简单数学处理，生成的就是高度特化、具体、紧凑、确定、可解释的知识，表示这种知识的embedding或不具有抗噪性（mask特征），或具有少许抗噪性（landmark特征），但无论如何都只适用于原曲匹配，不知何为翻唱，何为remix，何为二创，何为串烧。&lt;/p&gt;
&lt;p&gt;翻唱识别服务能通过一个学习了歌曲旋律的神经网络对音频信号进行处理，产出一定程度上表达旋律特征的embedding，将曲库里的翻唱embedding存入向量数据库，通过KNN/ANN召回的结果往往不限于原曲，还能容忍变调、音色上的改变。&lt;/p&gt;
&lt;p&gt;哼唱识别服务可以基于Whisper这样的音频大模型实现，这种深层网络学习了人类的自然语言，因此对音频信号处理时，提取了表达歌词信息的embedding。基于这样的embedding，可以在跑调非常严重，且没有伴奏的情况下，依然能识别出用户在哼什么歌。&lt;/p&gt;
&lt;p&gt;考虑未来的多模态大模型，更是可以直接将embedding存在transformer的ffn层，不必借助外部的向量数据库或倒排索引，直接在模型内部刻画embedding之间的复杂关系，让更多概念之间产生关联：识别出歌曲的意境、歌唱者的情绪、歌词内容的图片，甚至联系用户交互的上下文，衍生出更客制化、场景相关的输出。&lt;/p&gt;
&lt;p&gt;人工智能领域用transformer简单搭建的大模型拥有出人意料的逻辑能力，昭示着人类意识的本质或许原本就极端肤浅：将知识内化、知识关联后，用天生的脑回路做贝叶斯推理，就产生了逻辑，诚然现有的结构远不如人脑高效，但有和无的界限已经被突破。&lt;/p&gt;
&lt;p&gt;足够好的计算神经网络可以被视作神经科学的涌现模型。
一如往昔。
一如神经科学是细胞生物学的涌现。
一如细胞生物学是分子生物学的涌现。
一如分子生物学是物理化学的涌现。
一如物理化学是量子物理的涌现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/em.png&#34; alt=&#34;em&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>On NCO</title>
      <link>https://cmbbq.github.io/posts/on-nco/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-nco/</guid>
      <description>凸优化收敛时间一般是polynomial的，线性规划和最小二乘就是凸优化的特例。
非凸优化non-convex optimization是一种至少np-hard的问题，不存在通用解法。想要确定问题是否有解，局部最优是否全局最优，或目标函数是否有界都会随着变量和约束数目指数爆炸blow up，局部优化手段对算法参数敏感，又高度依赖initial guess，这使局部非凸优化more-art-than-technology，相比而言线性规划是毫无art可言的。
深度神经网络作为通用函数拟合器，最重要的作用是拟合非凸函数，因为复杂问题一般不可以用凸函数拟合。ChatGPT这类生成式模型就是对target和input间互信息的非凸优化。怎么训好模型，目前依然是一个art。
随机梯度下降stochastic gradient descent(SGD)被证明可以收敛于凸函数、可微和利普希茨连续函数，但还不能确定在非凸函数上的效果，SGD收敛缓慢，还不一定达到局部最优，更不一定达到全局最优。如果选择一个足够靠近全局最优的点，或许可以用SGD收敛到全局最优，但这一方面耗时间，另一方面只适用于特殊场景。对于深度神经网络来说，一旦陷入错误的局部最优，就要用不同的初始化配置或加入额外的梯度更新噪音。如果遇到鞍点，则需找到海森矩阵或计算下降方向。如果陷入低梯度区域，则需batchnorm，或使用relu做激活函数。如果因高曲率而使得steps过大，则应使用adaptive step size或限制梯度step尺度。此外，如果超参有问题，还需要用各种超参优化的方法。总之，目前深度学习的NCO还是处于art的阶段。</description>
      <content>&lt;p&gt;凸优化收敛时间一般是polynomial的，线性规划和最小二乘就是凸优化的特例。&lt;/p&gt;
&lt;p&gt;非凸优化non-convex optimization是一种至少np-hard的问题，不存在通用解法。想要确定问题是否有解，局部最优是否全局最优，或目标函数是否有界都会随着变量和约束数目指数爆炸blow up，局部优化手段对算法参数敏感，又高度依赖initial guess，这使局部非凸优化more-art-than-technology，相比而言线性规划是毫无art可言的。&lt;/p&gt;
&lt;p&gt;深度神经网络作为通用函数拟合器，最重要的作用是拟合非凸函数，因为复杂问题一般不可以用凸函数拟合。ChatGPT这类生成式模型就是对target和input间互信息的非凸优化。怎么训好模型，目前依然是一个art。&lt;/p&gt;
&lt;p&gt;随机梯度下降stochastic gradient descent(SGD)被证明可以收敛于凸函数、可微和利普希茨连续函数，但还不能确定在非凸函数上的效果，SGD收敛缓慢，还不一定达到局部最优，更不一定达到全局最优。如果选择一个足够靠近全局最优的点，或许可以用SGD收敛到全局最优，但这一方面耗时间，另一方面只适用于特殊场景。对于深度神经网络来说，一旦陷入错误的局部最优，就要用不同的初始化配置或加入额外的梯度更新噪音。如果遇到鞍点，则需找到海森矩阵或计算下降方向。如果陷入低梯度区域，则需batchnorm，或使用relu做激活函数。如果因高曲率而使得steps过大，则应使用adaptive step size或限制梯度step尺度。此外，如果超参有问题，还需要用各种超参优化的方法。总之，目前深度学习的NCO还是处于art的阶段。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>A Decade of Tussle between CPU and GPU</title>
      <link>https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/</guid>
      <description>GPU和CPU方法的边界何在？ 做AI应用的架构工作，遇到新的计算密集任务的第一问往往是：这应该上GPU，还是NUMA物理机？
这个问题可以归约为On the Limits of GPU Acceleration(2010)中提出的问题：&amp;ldquo;大致相同能耗的前提下，能和不能用GPU有效加速计算的边界在哪？&amp;rdquo; Vuduc的这个研究分析了3种具有代表性复杂行为和访存不规则性的计算：稀疏线性系统的迭代求解器、稀疏矩阵的乔列斯基分解、快速多极子算法，得出的结论是——大致地说，良好优化的GPU实现在相同能耗下和良好优化的CPU实现在性能上是相仿的。可见当年用GPGPU加速还是一个普遍得不偿失的工作，毕竟要付出剧烈变更编程模型的代价。
GPU和CPU的价格-性能趋势 上述结论在2010年成立，如今13年过去了，GPU固然飞速发展（无论是硬件、软件，还是生态、应用、资金投入），但GPU发展速度是否已经甩开CPU了呢？
摩尔定律是两年翻倍，而黄氏定律则是宣称通过软硬件协同能达到1.08年翻倍。
甚至如果我们考虑成本因素，根据经验数据，GPU FLOP/s每刀的增长速度是和CPU相仿的。这和2023年工业界的经验是吻合的——CPU在大多数应用语境下依然是成本更低的选择。</description>
      <content>&lt;h2 id=&#34;gpu和cpu方法的边界何在&#34;&gt;GPU和CPU方法的边界何在？&lt;/h2&gt;
&lt;p&gt;做AI应用的架构工作，遇到新的计算密集任务的第一问往往是：这应该上GPU，还是NUMA物理机？&lt;/p&gt;
&lt;p&gt;这个问题可以归约为&lt;a href=&#34;https://www.usenix.org/legacy/event/hotpar10/tech/full_papers/main.pdf&#34;&gt;On the Limits of GPU Acceleration(2010)&lt;/a&gt;中提出的问题：&amp;ldquo;大致相同能耗的前提下，能和不能用GPU有效加速计算的边界在哪？&amp;rdquo; Vuduc的这个研究分析了3种具有代表性复杂行为和访存不规则性的计算：稀疏线性系统的迭代求解器、稀疏矩阵的乔列斯基分解、快速多极子算法，得出的结论是——大致地说，良好优化的GPU实现在相同能耗下和良好优化的CPU实现在性能上是相仿的。可见当年用GPGPU加速还是一个普遍得不偿失的工作，毕竟要付出剧烈变更编程模型的代价。&lt;/p&gt;
&lt;h2 id=&#34;gpu和cpu的价格-性能趋势&#34;&gt;GPU和CPU的价格-性能趋势&lt;/h2&gt;
&lt;p&gt;上述结论在2010年成立，如今13年过去了，GPU固然飞速发展（无论是硬件、软件，还是生态、应用、资金投入），但GPU发展速度是否已经甩开CPU了呢？&lt;/p&gt;
&lt;p&gt;摩尔定律是两年翻倍，而&lt;a href=&#34;https://en.wikipedia.org/wiki/Huang%27s_law&#34;&gt;黄氏定律&lt;/a&gt;则是宣称通过软硬件协同能达到1.08年翻倍。&lt;/p&gt;
&lt;p&gt;甚至如果我们考虑成本因素，根据&lt;a href=&#34;https://epochai.org/blog/trends-in-gpu-price-performance&#34;&gt;经验数据&lt;/a&gt;，GPU FLOP/s每刀的增长速度是和CPU相仿的。这和2023年工业界的经验是吻合的——CPU在大多数应用语境下依然是成本更低的选择。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/4.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar1&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/5.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar2&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/6.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar3&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Digital Representations of Sound</title>
      <link>https://cmbbq.github.io/posts/digital-representations-of-sound/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/digital-representations-of-sound/</guid>
      <description>声音的本质是振动。
对于人耳来说，空气分子的振动抵达鼓膜时就会引起鼓膜振动——鼓膜顾名思义就是一层薄膜结构，被空气分子撞击就如同击鼓——带动鼓膜后的听小骨振动，传入内耳，再引发卵圆窗振动，卵圆窗这层薄膜之后是充满液体的耳蜗管道，这些液体随着卵圆窗振动而流动，冲刷耳蜗上的毛细胞的纤毛，从而产生生物电信号，经过神经传入大脑，就形成了听觉。
对于电容式麦克风来说，当有声音时，两块金属极板就开始振动，其间的距离会产生变化，电压形成了对空气分子振幅的模拟，因此这种电压信号被称为模拟信号。只有离散的数据才能被计算机存储、计算，因此要形成数字音频，还需要声卡的模数转换电路将连续的模拟信号采样后转化为离散的数字信号。这是一个连续曲线转化成柱状图的过程，采样率决定了柱的粗细，bit depth则决定柱高度的精度。如果bit depth为N，则振幅的取值范围为2^N，音乐App里标注的16-bit, 24-bit, 32-bit就指的是bit depth，显然bit depth越高，振幅的表示越精确。比特率bit rate = sample rate * bit depth，即每秒用多少bit来表示音频数据。
PGC和UGC的差别主要是信噪比，毕竟录音的时候连手机都不能带，空调、电灯都可能影响波形电平稳定。UGC可以想象，直播或者念稿子，噪声的平均振幅往往还比背景音乐更大。音频从32bit到16bit是一个量化的过程，不过无论采样精度多高，都终归是对现实的一个量化，量化error引入量化噪声，所以任何数字音频都是有底噪的——白噪声在各频率上都是一样的能量，由于能量（响度）较低，被称为noise floor，表现为微弱的嘶嘶声。
音频时域数据的纵坐标为振幅，横坐标为时间，可绘制成波形图——其震动频率决定音高，平均振幅决定响度，具体波形和这种其他因素决定音色。
时域与频域是对信号波的两个观察面。时域是真实世界唯一存在的域，频域则是对时域的数学构造。任何时域信号都可以表示为不同频率的正弦波信号的叠加。
常见的音频格式包括wav、mdi、mp3、mp3pro、wma、realaudio、audible、aac、ogg vorbis、ape、flac。不同的音频编码有不同的目标，在压缩（降低传输所需信道带宽）和质量（对人耳来说）之间做tradeoff。
各种格式的数字音频都能转成采样率8000Hz的PCM格式音频，PCM(pulse code modulation)是最简单的时域编码方式，就是对信号的离散和量化（通常是对数量化）。1/8000s长度的帧即构成了PCM音频在时域上的最小单位，每个帧包含channel数目个采样点，如果channel数目为1，则帧大小就等于bit depth。我们可以把PCM音频切分成多个chunk，每个chunk有固定大小（比如包含1024个帧），chunk与chunk之间必须有重合（比如256个帧）。之所以要有些重合，是为了对抗Time Skew（假如不做重合，查询音频的chunk的起始帧和库中的音频各个chunk的起始帧有一定offset就查询不到了）。
接下来只需对各个chunk进行1维离散傅立叶变换（fftw_plan_dft_1d）就能得到各个chunk的频域信息（如下图，横坐标为频率，纵坐标为能量）。
再将这些帧的频谱按时间顺序拼起来（可以有一些重合）就形成一个y轴为频率，x轴为时间，z轴为能量的三维表面（也可以用平面彩图表示，z轴的高度换成色彩来表示能量），这就形成了频谱图（spectrogram）。
有了频谱图，就可以尝试从较长的数据中提炼出简短的信息作为这段音频的指纹了。在频谱图上可以逐帧找到各帧上显著的高能频率点（salient peaks，能量超过一定阈值，且比周围所有点都高）——这些点本身已经可以作为音频的指纹特征了，只是不够鲁棒，因为点与点之间是无关的，满屏都是噪点的情况下就会有误匹配。一个改进方案是这些点右侧划定一个目标区域，在区域内找出一些点，形成几个pair，这些pair叫做landmark特征，即[t1, f1, t2, f2]，其抗噪能力增强了很多。</description>
      <content>&lt;p&gt;声音的本质是振动。&lt;/p&gt;
&lt;p&gt;对于人耳来说，空气分子的振动抵达鼓膜时就会引起鼓膜振动——鼓膜顾名思义就是一层薄膜结构，被空气分子撞击就如同击鼓——带动鼓膜后的听小骨振动，传入内耳，再引发卵圆窗振动，卵圆窗这层薄膜之后是充满液体的耳蜗管道，这些液体随着卵圆窗振动而流动，冲刷耳蜗上的毛细胞的纤毛，从而产生生物电信号，经过神经传入大脑，就形成了听觉。&lt;/p&gt;
&lt;p&gt;对于电容式麦克风来说，当有声音时，两块金属极板就开始振动，其间的距离会产生变化，电压形成了对空气分子振幅的模拟，因此这种电压信号被称为模拟信号。只有离散的数据才能被计算机存储、计算，因此要形成数字音频，还需要声卡的模数转换电路将连续的模拟信号采样后转化为离散的数字信号。这是一个连续曲线转化成柱状图的过程，采样率决定了柱的粗细，bit depth则决定柱高度的精度。如果bit depth为N，则振幅的取值范围为2^N，音乐App里标注的16-bit, 24-bit, 32-bit就指的是bit depth，显然bit depth越高，振幅的表示越精确。比特率bit rate = sample rate * bit depth，即每秒用多少bit来表示音频数据。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/audio1.png&#34; alt=&#34;audio1&#34;&gt;&lt;/p&gt;
&lt;p&gt;PGC和UGC的差别主要是信噪比，毕竟录音的时候连手机都不能带，空调、电灯都可能影响波形电平稳定。UGC可以想象，直播或者念稿子，噪声的平均振幅往往还比背景音乐更大。音频从32bit到16bit是一个量化的过程，不过无论采样精度多高，都终归是对现实的一个量化，量化error引入量化噪声，所以任何数字音频都是有底噪的——白噪声在各频率上都是一样的能量，由于能量（响度）较低，被称为noise floor，表现为微弱的嘶嘶声。&lt;/p&gt;
&lt;p&gt;音频时域数据的纵坐标为振幅，横坐标为时间，可绘制成波形图——其震动频率决定音高，平均振幅决定响度，具体波形和这种其他因素决定音色。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/wave.jpeg&#34; alt=&#34;wave&#34;&gt;&lt;/p&gt;
&lt;p&gt;时域与频域是对信号波的两个观察面。时域是真实世界唯一存在的域，频域则是对时域的数学构造。任何时域信号都可以表示为不同频率的正弦波信号的叠加。&lt;/p&gt;
&lt;p&gt;常见的音频格式包括wav、mdi、mp3、mp3pro、wma、realaudio、audible、aac、ogg vorbis、ape、flac。不同的音频编码有不同的目标，在压缩（降低传输所需信道带宽）和质量（对人耳来说）之间做tradeoff。&lt;/p&gt;
&lt;p&gt;各种格式的数字音频都能转成采样率8000Hz的PCM格式音频，PCM(pulse code modulation)是最简单的时域编码方式，就是对信号的离散和量化（通常是对数量化）。1/8000s长度的帧即构成了PCM音频在时域上的最小单位，每个帧包含channel数目个采样点，如果channel数目为1，则帧大小就等于bit depth。我们可以把PCM音频切分成多个chunk，每个chunk有固定大小（比如包含1024个帧），chunk与chunk之间必须有重合（比如256个帧）。之所以要有些重合，是为了对抗Time Skew（假如不做重合，查询音频的chunk的起始帧和库中的音频各个chunk的起始帧有一定offset就查询不到了）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/timeskew.png&#34; alt=&#34;ts&#34;&gt;&lt;/p&gt;
&lt;p&gt;接下来只需对各个chunk进行1维离散傅立叶变换（fftw_plan_dft_1d）就能得到各个chunk的频域信息（如下图，横坐标为频率，纵坐标为能量）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spec.png&#34; alt=&#34;sp&#34;&gt;&lt;/p&gt;
&lt;p&gt;再将这些帧的频谱按时间顺序拼起来（可以有一些重合）就形成一个y轴为频率，x轴为时间，z轴为能量的三维表面（也可以用平面彩图表示，z轴的高度换成色彩来表示能量），这就形成了频谱图（spectrogram）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spec2.png&#34; alt=&#34;sp&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/spec3.png&#34; alt=&#34;sp&#34;&gt;&lt;/p&gt;
&lt;p&gt;有了频谱图，就可以尝试从较长的数据中提炼出简短的信息作为这段音频的指纹了。在频谱图上可以逐帧找到各帧上显著的高能频率点（salient peaks，能量超过一定阈值，且比周围所有点都高）——这些点本身已经可以作为音频的指纹特征了，只是不够鲁棒，因为点与点之间是无关的，满屏都是噪点的情况下就会有误匹配。一个改进方案是这些点右侧划定一个目标区域，在区域内找出一些点，形成几个pair，这些pair叫做landmark特征，即[t1, f1, t2, f2]，其抗噪能力增强了很多。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spec4.png&#34; alt=&#34;sp&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
