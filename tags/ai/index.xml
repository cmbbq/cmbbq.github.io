<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ai on Cmbbq&#39;s Encyclopedia</title>
    <link>https://cmbbq.github.io/tags/ai/</link>
    <description>Recent content in ai on Cmbbq&#39;s Encyclopedia</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://cmbbq.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Personal GPT</title>
      <link>https://cmbbq.github.io/posts/run-llm-with-local-cpu/</link>
      <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/run-llm-with-local-cpu/</guid>
      <description>Minimalist Local LLM 新一轮AI热潮中，stable diffusion和LLM均呈现出了前所未有的社区繁荣。繁荣的同时带来了信息的爆炸和臃肿，极简主义显得尤为珍贵。
在预训练模型中，极简主义的代表是小而美的“大模型”Mistral7B。在推理框架中，极简主义的代表是对抗各种大公司重量级项目的llama.cpp。
Mistral7B是开源模型领域的新近进展，模型虽小，却展示了惊人的能力。根据Mistral AI自己的benchmark和一些社区的测试反馈，几乎在各方面击败Llama2 13B。
Mistral AI提供了在本地GPU、云端跑Mistral7B的官方支持。
不过这么小的模型，其实也可以在本地CPU上跑的——借助llama.cpp。
llama.cpp和whisper.cpp都是C++程序员Georgi Gerganov的个人项目，基于自己的ml primitive库ggml实现，用最少的代码量实现了尽可能多的推理优化，针对Apple芯片做得尤其好，也适用于Linux服务器，适用于有或无GPU，单或多GPU的硬件setup。ggml规定的模型文件格式ggul甚至也在社区中被广泛使用，成为事实上的标准之一。
Intel Xeon Platinum 8336C + Debian10 从TheBloke上下载特定量化版本的模型：https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF git clone https://github.com/ggerganov/llama.cpp.git apt-get install libopenblas-dev 安装blas库，这里选用openblas。 修改pkgconfig/openblas.pc或Makefile解决llama.cpp对openblas的依赖问题。最终要保证给编译期提供正确的LD Flags。 make LLAMA_OPENBLAS=1 完成编译。 ./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot; 64物理核打满，采样速率4000t/s，prompt处理速率80t/s，生成速率20t/s。
Apple M1 Pro + macOS Monterey 下载模型和llama.cpp repo。 直接make就默认启用metal gpu加速和Accelerated Framework。 ./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.</description>
      <content>&lt;h2 id=&#34;minimalist-local-llm&#34;&gt;Minimalist Local LLM&lt;/h2&gt;
&lt;p&gt;新一轮AI热潮中，stable diffusion和LLM均呈现出了前所未有的社区繁荣。繁荣的同时带来了信息的爆炸和臃肿，极简主义显得尤为珍贵。&lt;/p&gt;
&lt;p&gt;在预训练模型中，极简主义的代表是小而美的“大模型”Mistral7B。在推理框架中，极简主义的代表是对抗各种大公司重量级项目的llama.cpp。&lt;/p&gt;
&lt;p&gt;Mistral7B是开源模型领域的新近进展，模型虽小，却展示了惊人的能力。根据Mistral AI自己的benchmark和一些社区的测试反馈，几乎在各方面击败Llama2 13B。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/mistral.png&#34; alt=&#34;mistral&#34;&gt;&lt;/p&gt;
&lt;p&gt;Mistral AI提供了在本地GPU、云端跑Mistral7B的官方支持。&lt;/p&gt;
&lt;p&gt;不过这么小的模型，其实也可以在本地CPU上跑的——借助llama.cpp。&lt;/p&gt;
&lt;p&gt;llama.cpp和whisper.cpp都是C++程序员Georgi Gerganov的个人项目，基于自己的ml primitive库ggml实现，用最少的代码量实现了尽可能多的推理优化，针对Apple芯片做得尤其好，也适用于Linux服务器，适用于有或无GPU，单或多GPU的硬件setup。ggml规定的模型文件格式ggul甚至也在社区中被广泛使用，成为事实上的标准之一。&lt;/p&gt;
&lt;h2 id=&#34;intel-xeon-platinum-8336c--debian10&#34;&gt;Intel Xeon Platinum 8336C + Debian10&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;从TheBloke上下载特定量化版本的模型：https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git clone https://github.com/ggerganov/llama.cpp.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;apt-get install libopenblas-dev&lt;/code&gt; 安装blas库，这里选用openblas。&lt;/li&gt;
&lt;li&gt;修改pkgconfig/openblas.pc或Makefile解决llama.cpp对openblas的依赖问题。最终要保证给编译期提供正确的LD Flags。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make LLAMA_OPENBLAS=1&lt;/code&gt; 完成编译。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;64物理核打满，采样速率4000t/s，prompt处理速率80t/s，生成速率20t/s。&lt;/p&gt;
&lt;h2 id=&#34;apple-m1-pro--macos-monterey&#34;&gt;Apple M1 Pro + macOS Monterey&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;下载模型和llama.cpp repo。&lt;/li&gt;
&lt;li&gt;直接&lt;code&gt;make&lt;/code&gt;就默认启用metal gpu加速和Accelerated Framework。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;M1 GPU打到90%（剩下还有10%WindowServer在用），采样速率9000t/s，prompt处理速率60t/s，生成速率20t/s。&lt;/p&gt;
&lt;p&gt;一边看视频（chrome GPU占用约20%），一边跑mistral也能有19.69t/s。&lt;/p&gt;
&lt;h2 id=&#34;token-sampling&#34;&gt;Token Sampling&lt;/h2&gt;
&lt;p&gt;不同的sampling机制对文本生成有显著影响，本地LLM的可玩性来源很大程度上就是客制sampling。&lt;/p&gt;
&lt;p&gt;Transformer模型根据前n个token，预测下一个token。每个token都有其next tokens的score，而next tokens的取值范围就是vocabulary（transformer架构最后有一个linear layer，将输出映射到vocabulary上，每个词都有对应的），这些score经过softmax将score数组转化成probability数组，根据probability挑选下一个token的过程就称为token sampling。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Greedy采样： 如果每次都确定性地选择probability最高的那个token，单步决策，就是greedy sampler，优势是速度快，劣势是牺牲了模型的多样性，容易陷入重复循环和对训练数据的过拟合。llama.cpp里设置&amp;ndash;top_p 0.0 &amp;ndash;top_k 1就相当于开greedy采样。&lt;/li&gt;
&lt;li&gt;Top-k采样： 从概率分布的前k个token里面进行随机采样。&lt;/li&gt;
&lt;li&gt;Top-p采样： 引入超参p，把token probability降序排序后，选取前面一部分，使这部分概率和为p，而不是固定选前k个token。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在使用LLM时，采样机制不同，效果也会大不相同。想要更高的确定性，更朴实无华的预测，则可以尝试greedy、低k的topk、低p的top-p采样。想要多样性和新颖性，则可尝试高k的top-k和高p的top-p。&lt;/p&gt;
&lt;p&gt;此外，llama.cpp还实现了一些额外的采样机制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Min p filter：允许采样环节剔除低于阈值的低分候选。&lt;/li&gt;
&lt;li&gt;Tail free sampling：根据概率的二阶导数之和采样，即根据概率降速剔除尾部低概率tokens。&lt;/li&gt;
&lt;li&gt;Locally typical samplling：参数控制是否倾向局部语境内的典型的tokens。&lt;/li&gt;
&lt;li&gt;Mirostat sampling：&lt;a href=&#34;https://arxiv.org/abs/2007.14966&#34;&gt;Mirostat算法&lt;/a&gt;会调整top-k的k，避免陷入boredom trap（模式崩塌）和perplexity trap（不一致）。&lt;/li&gt;
&lt;li&gt;logit bias: 人为指定某个token的优先级，比如&amp;ndash;logit-bias 29905-inf就把&amp;quot;&amp;quot;token设为负无穷。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prompting&#34;&gt;Prompting&lt;/h2&gt;
&lt;p&gt;用一大段prompt，让llm假装自己是一个爱说emoji，语言风格浮夸的音乐推荐bot，效果相当不错。
&lt;img src=&#34;https://cmbbq.github.io/img/music_bot.png&#34; alt=&#34;bot&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Efficient ANNS at Scale</title>
      <link>https://cmbbq.github.io/posts/efficient-anns-at-scale/</link>
      <pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/efficient-anns-at-scale/</guid>
      <description>向量检索分为KNN和ANN，KNN就是用brute-force计算出query向量和所有索引向量的相似度，取前k个。ANN则是通过近似手段加速计算，以误差换效率。
如何做高效的向量检索？方法有二，一是提升编码率（不仅能降低内存占用，也能加速计算，因为便于SIMD且减少访存）的量化（IVF向量量化、PQ量化、ScaNN各向异性量化、4bit量化），二是缩小搜索空间的图搜索（HNSW）。
如何在十亿、百亿级特征库上做向量检索？主要挑战是索引无法放入内存，因而需要考虑内存-磁盘混合方案（SPANN）。
本文只介绍一小部分有效且正交的技术，这些技术组合在一起可形成当前的最佳实践。其他一些古典、经典技术可以参考本文引用的论文的背景综述。
相似度 首先回顾一下什么是向量之间的相似度:
对向量X和Y来说，x和y越接近，则x越大的地方y也大，x小的地方y也小，则内积越大。因此内积可用作相似度打分，不过不能抵抗尺度变化。 为了抵抗尺度变化，往往将内积正则化到[-1,1]，就变成了余弦相似度cosθ，θ为向量夹角，显然θ越小，向量越接近。 欧几里得距离正则化到[-1,1]后相当于sqrt(2-2cosθ)。可见欧式距离平方也是和余弦相似度成比的。 正则化后欧式距离、内积、cosθ三者同源，所以我们一般就用余弦相似度cosθ去度量向量/embedding之间的相似程度。除非又特殊的抵抗位置变化的需求，可采用皮尔森相关系数，皮尔森相关系数是协方差（两个变量的总体误差）的正则化，不过它同时也可以被视为将x和y中心化后的余弦相似度。
向量量化 量化器是D维向量 x ∈ R^D 到 q(x) ∈ C = {c_0, c_1, &amp;hellip; c_k-1} 的映射函数q。c_i即聚类中心centroid。 所谓密码本（codebook）就是一个lookup table，用centroid下标作为原始向量的低bit表示，密码本大小就是k。
向量量化本质是有损压缩，表征学习本质上也是有损压缩，一个白盒，一个黑盒，都是试图降低高维数据的编码率。
IVF：聚类、倒排、剪枝 倒排（特指IVF）是一种古老的量化技术，早期应用于Video Google。核心思想是基于k-means聚类做向量量化。k-means聚类起源于信号处理领域，目标是把n个向量分区到k个clusters，每个向量属于离centroid最近的cluster。
聚类后有些边缘数据点既属于A，也属于B，那就把它同时放到A和B的centroid对应的postinglist里面去，不过重复放置会导致postinglist膨胀，为了应对这种膨胀，可采取剪枝策略，具体可参考微软的SPTAG项目和SPANN论文。
Faiss的IVFFlat就是在聚类形成centroid为term的倒排后提供检索能力，nlist为聚类数，nprobe为临近centroid探测数，nprobe=nlist时退化为暴搜。IVFFLat在检索时将query邻近的nprobe个centroid对应的所有原始向量作为搜索空间进行暴搜。由于centroid的表示方式可以简化成clusterid，所以实际上IVF也是一种量化：用一个数组存放所有的centroid向量，数组下标即可表示centroid。
PQ：乘积量化 基于聚类的量化对低维向量比较有效，但随着向量维度升高，出现误差也变高，而且这种误差不是提升聚类数目能解决的。如果把聚类数目（codebook大小）提升到2^64，训练这个聚类的开销就会高得无法接受，需要数倍于2^64的样本，内存显然也放不下。**乘积量化（PQ）**就是降维手段解决IVF量化误差的技术，Jegou et al., 2011以及ScaNN均讨论了PQ。其核心思想是对索引数据量化，将d维空间切分为M个子空间，将高维向量的误差拟合为分段的低维向量误差之和。
乘积量化中，IVF倒排链存的不是原始向量，而是原始向量的一种编码：
求原始d维向量与高维粗聚类centroid的残差（残差的目的是中心化，让数据点分布变得更集中，使精度更好） 将残差向量切成M个分段，每个分段维度为d/M 每个分段做k*=2^n个细聚类，n即为低维细聚类clusterid的编码长度，4bit或8bit 用每个分段的邻近低维细聚类centroid去量化该分段上的原始残差分段 Faiss的IVFPQ就结合了IVF和PQ，IVF粗聚类是一级量化，PQ则是二级量化。一方面索引体积减小，另一方面计算上也只需要计算残差和分段细聚类centroid近邻的距离，还能SIMD加速，性能收益非常显著。不过IVFPQ的distance究竟只是一种拟合，是距离粗聚类centroid的距离加上M个分段里query残差和邻近细聚类centroid的距离总和，这种拟合可以一定程度上帮助找到近邻，但不适合用作距离，可以把IVFPQ当做粗排，结果再做一次brute-force或高精度ANN重算，得到更精确的距离。
ScaNN提出了各向异性量化，算是正本清源，纠正了此前就近选择centroid在数学上的错误，并且实践了4-bit量化取得非常好的效果（其实4bit量化的效果还比各向异性更大，但各向异性在理论上贡献更大）。传统的量化把数据点量化到离自己最近的聚类中心，但这么做未必是误差最低的，越平行的向量之间内积越大，越正交的向量之间内积越小，因此赋予平行向量更高的误差惩罚权重效果更好。这就意味着不一定量化到距离最近的centroid了，目标是整体编码率不变的前提下，让平行方向量化误差更小，正交方向误差高一些其实无所谓，从而提升整体的ANN效果。
假设M = 4，采用8bit量化，原始向量是256维f32向量，则PQ将256个float32压缩到64个int8，内存减少到1/16。乘积量化显然可以显著减少内存和存储开销，其实这也能加速计算，原因如下：
高效点乘（其实也是误差换效率）：O(dk+mn) 比 O(nd)快，d维度的query和n个量化后的向量点乘，引入m个大小为k的量化codebook，mn可以忽略不计，k远小于n。 memory bandwidth：现代处理器需要“高计算量per memory read”才能充分发挥硬件的计算性能。量化压缩数据点后内存带宽的使用也下降了，变得更计算密集。 向量量化中引入的低bit表示，尤其是4bit量化，和低bit浮点数量化一样，可以享受SIMD的性能红利。 最佳实践：根据应用场景将各种正交技术进行正确组合 HNSW、IVF、PQ、各向异性的score-aware quantization loss、brute-force事实上都是正交的技术，可以组合起来使用。
比如进行了PQ量化的数据点可以再构建一个HNSW，用图方法在超大规模数据集上做查询显然比ivfpq更高效。各向异性量化则是对此前PQ量化的修正。考虑到量化后的距离失真，还能用brute-force把ivf+pq+hnsw的粗排结果重算一下得到最精确的距离，由于已经经过一轮粗排，候选数据集大小从十亿级别缩小到万级别，brute-force的开销就完全可以接受了。</description>
      <content>&lt;p&gt;向量检索分为KNN和ANN，KNN就是用brute-force计算出query向量和所有索引向量的相似度，取前k个。ANN则是通过近似手段加速计算，以误差换效率。&lt;/p&gt;
&lt;p&gt;如何做高效的向量检索？方法有二，一是提升编码率（不仅能降低内存占用，也能加速计算，因为便于SIMD且减少访存）的量化（IVF向量量化、PQ量化、ScaNN各向异性量化、4bit量化），二是缩小搜索空间的图搜索（HNSW）。&lt;/p&gt;
&lt;p&gt;如何在十亿、百亿级特征库上做向量检索？主要挑战是索引无法放入内存，因而需要考虑内存-磁盘混合方案（SPANN）。&lt;/p&gt;
&lt;p&gt;本文只介绍一小部分有效且正交的技术，这些技术组合在一起可形成当前的最佳实践。其他一些古典、经典技术可以参考本文引用的论文的背景综述。&lt;/p&gt;
&lt;h1 id=&#34;相似度&#34;&gt;相似度&lt;/h1&gt;
&lt;p&gt;首先回顾一下什么是&lt;strong&gt;向量之间的相似度&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对向量X和Y来说，x和y越接近，则x越大的地方y也大，x小的地方y也小，则内积越大。因此内积可用作相似度打分，不过不能抵抗尺度变化。&lt;/li&gt;
&lt;li&gt;为了抵抗尺度变化，往往将内积正则化到[-1,1]，就变成了余弦相似度cosθ，θ为向量夹角，显然θ越小，向量越接近。&lt;/li&gt;
&lt;li&gt;欧几里得距离正则化到[-1,1]后相当于sqrt(2-2cosθ)。可见欧式距离平方也是和余弦相似度成比的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/sim_measure.png&#34; alt=&#34;sim_measure&#34;&gt;&lt;/p&gt;
&lt;p&gt;正则化后欧式距离、内积、cosθ三者同源，所以我们一般就用余弦相似度cosθ去度量向量/embedding之间的相似程度。除非又特殊的抵抗位置变化的需求，可采用皮尔森相关系数，皮尔森相关系数是协方差（两个变量的总体误差）的正则化，不过它同时也可以被视为将x和y中心化后的余弦相似度。&lt;/p&gt;
&lt;h1 id=&#34;向量量化&#34;&gt;向量量化&lt;/h1&gt;
&lt;p&gt;量化器是D维向量 x  ∈ R^D 到 q(x) ∈ C = {c_0, c_1, &amp;hellip; c_k-1} 的映射函数q。c_i即聚类中心centroid。
所谓密码本（codebook）就是一个lookup table，用centroid下标作为原始向量的低bit表示，密码本大小就是k。&lt;/p&gt;
&lt;p&gt;向量量化本质是有损压缩，表征学习本质上也是有损压缩，一个白盒，一个黑盒，都是试图降低高维数据的编码率。&lt;/p&gt;
&lt;h1 id=&#34;ivf聚类倒排剪枝&#34;&gt;IVF：聚类、倒排、剪枝&lt;/h1&gt;
&lt;p&gt;倒排（特指IVF）是一种古老的量化技术，早期应用于&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2003/Sivic03/sivic03.pdf&#34;&gt;Video Google&lt;/a&gt;。核心思想是基于k-means聚类做向量量化。k-means聚类起源于信号处理领域，目标是把n个向量分区到k个clusters，每个向量属于离centroid最近的cluster。&lt;/p&gt;
&lt;p&gt;聚类后有些边缘数据点既属于A，也属于B，那就把它同时放到A和B的centroid对应的postinglist里面去，不过重复放置会导致postinglist膨胀，为了应对这种膨胀，可采取剪枝策略，具体可参考微软的SPTAG项目和&lt;a href=&#34;https://arxiv.org/pdf/2111.08566.pdf&#34;&gt;SPANN论文&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Faiss的IVFFlat就是在聚类形成centroid为term的倒排后提供检索能力，nlist为聚类数，nprobe为临近centroid探测数，nprobe=nlist时退化为暴搜。IVFFLat在检索时将query邻近的nprobe个centroid对应的所有原始向量作为搜索空间进行暴搜。由于centroid的表示方式可以简化成clusterid，所以实际上IVF也是一种量化：用一个数组存放所有的centroid向量，数组下标即可表示centroid。&lt;/p&gt;
&lt;h1 id=&#34;pq乘积量化&#34;&gt;PQ：乘积量化&lt;/h1&gt;
&lt;p&gt;基于聚类的量化对低维向量比较有效，但随着向量维度升高，出现误差也变高，而且这种误差不是提升聚类数目能解决的。如果把聚类数目（codebook大小）提升到2^64，训练这个聚类的开销就会高得无法接受，需要数倍于2^64的样本，内存显然也放不下。**乘积量化（PQ）**就是降维手段解决IVF量化误差的技术，&lt;a href=&#34;https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf&#34;&gt;Jegou et al., 2011&lt;/a&gt;以及&lt;a href=&#34;https://arxiv.org/pdf/1908.10396.pdf&#34;&gt;ScaNN&lt;/a&gt;均讨论了PQ。其核心思想是对索引数据量化，将d维空间切分为M个子空间，将高维向量的误差拟合为分段的低维向量误差之和。&lt;/p&gt;
&lt;p&gt;乘积量化中，IVF倒排链存的不是原始向量，而是原始向量的一种编码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;求原始d维向量与高维粗聚类centroid的残差（残差的目的是中心化，让数据点分布变得更集中，使精度更好）&lt;/li&gt;
&lt;li&gt;将残差向量切成M个分段，每个分段维度为d/M&lt;/li&gt;
&lt;li&gt;每个分段做k*=2^n个细聚类，n即为低维细聚类clusterid的编码长度，4bit或8bit&lt;/li&gt;
&lt;li&gt;用每个分段的邻近低维细聚类centroid去量化该分段上的原始残差分段&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Faiss的IVFPQ就结合了IVF和PQ，IVF粗聚类是一级量化，PQ则是二级量化。一方面索引体积减小，另一方面计算上也只需要计算残差和分段细聚类centroid近邻的距离，还能SIMD加速，性能收益非常显著。不过IVFPQ的distance究竟只是一种拟合，是距离粗聚类centroid的距离加上M个分段里query残差和邻近细聚类centroid的距离总和，这种拟合可以一定程度上帮助找到近邻，但不适合用作距离，可以把IVFPQ当做粗排，结果再做一次brute-force或高精度ANN重算，得到更精确的距离。&lt;/p&gt;
&lt;p&gt;ScaNN提出了各向异性量化，算是正本清源，纠正了此前就近选择centroid在数学上的错误，并且实践了4-bit量化取得非常好的效果（其实4bit量化的效果还比各向异性更大，但各向异性在理论上贡献更大）。传统的量化把数据点量化到离自己最近的聚类中心，但这么做未必是误差最低的，越平行的向量之间内积越大，越正交的向量之间内积越小，因此赋予平行向量更高的误差惩罚权重效果更好。这就意味着不一定量化到距离最近的centroid了，目标是整体编码率不变的前提下，让平行方向量化误差更小，正交方向误差高一些其实无所谓，从而提升整体的ANN效果。&lt;/p&gt;
&lt;p&gt;假设M = 4，采用8bit量化，原始向量是256维f32向量，则PQ将256个float32压缩到64个int8，内存减少到1/16。乘积量化显然可以显著减少内存和存储开销，其实这也能加速计算，原因如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高效点乘（其实也是误差换效率）：O(dk+mn) 比 O(nd)快，d维度的query和n个量化后的向量点乘，引入m个大小为k的量化codebook，mn可以忽略不计，k远小于n。&lt;/li&gt;
&lt;li&gt;memory bandwidth：现代处理器需要“高计算量per memory read”才能充分发挥硬件的计算性能。量化压缩数据点后内存带宽的使用也下降了，变得更计算密集。&lt;/li&gt;
&lt;li&gt;向量量化中引入的低bit表示，尤其是4bit量化，和低bit浮点数量化一样，可以享受SIMD的性能红利。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;最佳实践根据应用场景将各种正交技术进行正确组合&#34;&gt;最佳实践：根据应用场景将各种正交技术进行正确组合&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2003/Sivic03/sivic03.pdf&#34;&gt;HNSW&lt;/a&gt;、IVF、PQ、各向异性的score-aware quantization loss、brute-force事实上都是正交的技术，可以组合起来使用。&lt;/p&gt;
&lt;p&gt;比如进行了PQ量化的数据点可以再构建一个HNSW，用图方法在超大规模数据集上做查询显然比ivfpq更高效。各向异性量化则是对此前PQ量化的修正。考虑到量化后的距离失真，还能用brute-force把ivf+pq+hnsw的粗排结果重算一下得到最精确的距离，由于已经经过一轮粗排，候选数据集大小从十亿级别缩小到万级别，brute-force的开销就完全可以接受了。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Evolution of Data Center Applications</title>
      <link>https://cmbbq.github.io/posts/evolution-of-data-center-applications/</link>
      <pubDate>Sat, 05 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/evolution-of-data-center-applications/</guid>
      <description>数据中心应用（数据库、消息队列、检索服务、参数服务器、直播服务、广告服务等）作为互联网服务的主体，位于终端应用和硬件基础设施之间，桥接了需求端和硬件端，同时还受到成本、营收、算法、法律等因素的制约。 近期硬件侧、算法侧均有变化或变革，因而有必要反思数据中心应用的演进方向，讨论数据中心应用的近未来发展趋势、潜在的创新点和可收割的低垂果实。
变革中的不变量：数据中心应用能耗 全球数据中心能耗十年来几乎没有增长，而同期的服务需求、计算、存储、数据传输总量则经历了数量级增长。作为从业者，我们对互联网的爆发式增长应该不陌生，因此全球数据中心能耗从10到18年仅涨了6%的事实略微反直觉——从能耗的角度看，这几乎是一个停滞的产业。
CMOS工艺和软硬件技术的整体进步可以解释服务器能量效率上的提升，但能耗停滞的根本原因还在于互联网企业盈利的本质是对有支付能力的10亿国外人口和10亿国内人口征服务税，个别成功的初创企业可以迅速扩张，但互联网行业作为整体，在数据中心能耗上的投入增长受制于全球平民收入增长。
当初创企业的规模扩张到互联网规模后，机器资源的增长速率应该会从爆炸式上升陡然转入停滞，进入自然汰换阶段。 从这个角度可以得出结论，数据中心应用在规模扩张结束后，也应从能耗粗放型转为精细集约型，通过软硬件协同充分释放硬件潜力，通过算法创新减少总计算量。
算法迭代：从演绎推理到归纳推断 算法侧的趋势是“transformers getting even more attention”。
计算机理论源于符号逻辑，1930年代图灵、哥德尔、邱奇各自独立提出图灵机、广义递归函数、lambda演算，以及三者相应的可计算性。图灵机的停机问题和哥德尔不完备定律驳斥了1920年代希尔伯特计划的可行性，证明了形式化系统是不完备的，通过有限个公理和规则永远无法推导出全部的真理。
正如人的无知可以分为问题（可计算）和神秘（不可计算），人的理性也可以分为演绎推理和归纳推断，前者求解问题，后者分析神秘。
计算机程序基于图灵机模型，天然就是适合进行演绎推理的形式化系统。至今我们依然基于规则系统(人工编程进行条件控制）、信号处理（比如音频特征提取）、状态机等形式化方法实现大多数数据中心应用：比如各类数据库、检索系统、参数服务器、游戏AI。
AI时代来临之后，通过更多的权重（作为公理）和由大量算子（作为规则）组成深度神经网络（从实现角度来说，仍然是形式化系统）去模拟或者说近似“归纳推断”。（纯粹贝叶斯主义的形式化表达是所罗门诺夫归纳推断法，这个方法本身是永不停机，越算越停不下来的，所以只能近似，不能用实现）这种近似无论多么粗糙、低效，归根到底还是提升了计算机语言和集成电路的表达能力，完成了推理到推断的跳跃。
数据中心应用多了归纳推断能力之后，就产生了对复杂现实（现实信号，如图片、语料库、音频，整体上具有高维度的同时，局部往往还存在具有高数据精致度的结构，这里精致度可以用香农信息熵或柯氏复杂性形式化定义）理解能力(pin down reality)，因此涌现出了一些基于知识的AI系统(knowledge-based AI)，比如推荐系统、广告投送、翻唱识别、聊天机器人。共同点是将现实映射到表征低维子空间上，这个子空间里的向量或者说embedding可以从某个角度有效刻画现实。Transformer做得更进一步，等价于将现实映射到多个相干性低的低维子空间上（见https://arxiv.org/abs/2306.01129），从而从多角度多层次理解现实，通过表征空间编码率与各子空间编码率之和的差值度量特征的紧凑性和判别力。
在游戏和博弈领域，强化学习也取得了规则系统无法企及的成就，在星际2、Dota2（固定阵容5v5，solo）、围棋等领域击败职业选手。
此外，归纳推断相比演绎推理还有一个重要特质，那就是更符合人类大脑（或推广到任意地球生物大脑，蜜蜂的3d寻路避障能力，蚊子的血管定位能力都是基于天然的贝叶斯归纳推断。也只有受过良好训练的人类才能进行缓慢的演绎推理）的思维习惯，因此人类社会的传统，人类语言的不确定性、人类的偏好都难以用形式化方法描述，却可以被概率语言描述，被归纳推断方法预测。
算法进步的下一个课题则是研究同时具备演绎推理和归纳推断能力的模型。现有的Transformer的确展现出简单多步算术的拟合能力，但拟合出来的是一个步数相关的超复杂非线性系统，而不是简单的算术规则，所以往往正确率到八十左右就再也上不去了。对于形式化系统来说，推理步数不影响公理和规则总数，只稍稍影响计算量。但对于LLM这种归纳推断模型来说，一旦推理步数变多，计算变得更复杂，LLM的符号逻辑能力就会迅速下降，在训练数据中从未出现过相同执行路径的动态规划问题里面可以直接降到0。理论上来说，无限精度的Transformer是图灵完备的（见https://arxiv.org/abs/1901.03429），而只要不是无限精度，就不是图灵完备，实际应用中模型权重和计算的精度即使以常规计算机应用标准看也是非常低的，而模型架构又没有针对多步逻辑推理进行设计，再加上深度学习本身就是在拟合一个近似函数，三重因素叠加导致现有LLM的演绎推理能力不尽如人意。
从算法迭代角度讲，数据中心应用的发展趋势是在保证“可计算性”(字面意思，非形式化定义)的前提下，追求“人性化”(align to humanity)、“完备化”（pin down reality）这两个演绎推理手段无法达成的理想属性。智能创作、个性化推荐、聪明的Agent（助手、游戏NPC、机器人、自动驾驶），复杂系统预测（天气预报、交易系统），大模型训练（模型并行、高性能网络、C2C互连、光学I/O、异构计算）都是有前景的方向。
而那些已经能用规则系统高效解决的问题则不必强行应用归纳推断模型，那样只会增加成本和错误率，比如音乐指纹识别、关系型数据库、图片旋转变形、无损压缩。甚至某些看似“神秘”的领域，如OOD text classification，基于表征学习白盒化研究成果和信息论，也能给出直接的数学解，比如近期一个研究（“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors）就用14行代码超过了百亿参数的bert，（后来被发现实验代码有误，实际上效果没那么好）这个论文里提出用gzip等无损压缩工具去近似文本数据的柯氏复杂性，用信息距离（原理类似马毅提出的rate reduction）做KNN，效果非常好，在音频领域也可以做类似的尝试。
数据中心硬件基础设施 先简单介绍一下常见的数据中心硬件基础设施：
比较老的是Cascade Lake 14nm的24核per die的机器，一个芯片24核，两个就是48个物理核，也就是我们平时说的96核机器。也有其他套餐，最多是每个die上28核，总共112个逻辑核。 比较新的Ice Lake是10nm工艺一般是32核的，对应128个逻辑核，最贵的套餐可以到40核。仍然是基于Monolithic die，巨大的晶粒上用mesh总线把40个核放到一起。 最新的7nm工艺Sapphire Rapids，以及对标它的AMD Genoa今年已经量产。 关于网卡，现在用的比较多的是Mellanox 25G CX4/CX5卡，也有相当多的100G dual-port CX6卡，都是支持RDMA的，不用RDMA也能提供相当好的高速以太网性能。如果是做虚拟化的，比如AWS，阿里云，还可以把这些网卡的smartNiC能力利用起来，offload虚拟化开销。 相对之前的Lakes，Sapphire Rapids变化非常大，此前的monolithic die路线确实走到了极限，Sapphire Rapids也进入了multi-die时代：芯片内部分为4个die，这也可以视作向chiplet方向演化。
Sapphire Rapids最多支持8个socket，每个芯片可以支持60核。IO技术上支持CXL 1.1、PCIe 5.0、UPI 2.0，HBM2e(optional)。
AMD的第四代EPYC的主力型号Genoa今年Q1也开始量产，基本上和Sapphire Rapids对标，采用5nm工艺。虽然说AVX512被很多人诟病，实用表现非常一般，但Zen4也支持了，毕竟XeongenEPYC目标客户就是音视频处理和AI workload越来越多的数据中心应用。Genoa是每个die/chiplet 8核的，12个CPU cluster die中间围绕一个IO die的设计，相当于一个芯片96核，是典型的chiplet范式设计。</description>
      <content>&lt;p&gt;数据中心应用（数据库、消息队列、检索服务、参数服务器、直播服务、广告服务等）作为互联网服务的主体，位于终端应用和硬件基础设施之间，桥接了需求端和硬件端，同时还受到成本、营收、算法、法律等因素的制约。
近期硬件侧、算法侧均有变化或变革，因而有必要反思数据中心应用的演进方向，讨论数据中心应用的近未来发展趋势、潜在的创新点和可收割的低垂果实。&lt;/p&gt;
&lt;h2 id=&#34;变革中的不变量数据中心应用能耗&#34;&gt;变革中的不变量：数据中心应用能耗&lt;/h2&gt;
&lt;p&gt;全球数据中心能耗十年来几乎没有增长，而同期的服务需求、计算、存储、数据传输总量则经历了数量级增长。作为从业者，我们对互联网的爆发式增长应该不陌生，因此全球数据中心能耗从10到18年仅涨了6%的事实略微反直觉——从能耗的角度看，这几乎是一个停滞的产业。&lt;/p&gt;
&lt;p&gt;CMOS工艺和软硬件技术的整体进步可以解释服务器能量效率上的提升，但能耗停滞的根本原因还在于互联网企业盈利的本质是对有支付能力的10亿国外人口和10亿国内人口征服务税，个别成功的初创企业可以迅速扩张，但互联网行业作为整体，在数据中心能耗上的投入增长受制于全球平民收入增长。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/DatacenterPower.jpeg&#34; alt=&#34;DatacenterPower&#34;&gt;&lt;/p&gt;
&lt;p&gt;当初创企业的规模扩张到互联网规模后，机器资源的增长速率应该会从爆炸式上升陡然转入停滞，进入自然汰换阶段。
从这个角度可以得出结论，数据中心应用在规模扩张结束后，也应从能耗粗放型转为精细集约型，通过软硬件协同充分释放硬件潜力，通过算法创新减少总计算量。&lt;/p&gt;
&lt;h2 id=&#34;算法迭代从演绎推理到归纳推断&#34;&gt;算法迭代：从演绎推理到归纳推断&lt;/h2&gt;
&lt;p&gt;算法侧的趋势是“transformers getting even more attention”。&lt;/p&gt;
&lt;p&gt;计算机理论源于符号逻辑，1930年代图灵、哥德尔、邱奇各自独立提出图灵机、广义递归函数、lambda演算，以及三者相应的可计算性。图灵机的停机问题和哥德尔不完备定律驳斥了1920年代希尔伯特计划的可行性，证明了形式化系统是不完备的，通过有限个公理和规则永远无法推导出全部的真理。&lt;/p&gt;
&lt;p&gt;正如人的无知可以分为问题（可计算）和神秘（不可计算），人的理性也可以分为演绎推理和归纳推断，前者求解问题，后者分析神秘。&lt;/p&gt;
&lt;p&gt;计算机程序基于图灵机模型，天然就是适合进行演绎推理的形式化系统。至今我们依然基于规则系统(人工编程进行条件控制）、信号处理（比如音频特征提取）、状态机等形式化方法实现大多数数据中心应用：比如各类数据库、检索系统、参数服务器、游戏AI。&lt;/p&gt;
&lt;p&gt;AI时代来临之后，通过更多的权重（作为公理）和由大量算子（作为规则）组成深度神经网络（从实现角度来说，仍然是形式化系统）去模拟或者说近似“归纳推断”。（纯粹贝叶斯主义的形式化表达是所罗门诺夫归纳推断法，这个方法本身是永不停机，越算越停不下来的，所以只能近似，不能用实现）这种近似无论多么粗糙、低效，归根到底还是提升了计算机语言和集成电路的表达能力，完成了推理到推断的跳跃。&lt;/p&gt;
&lt;p&gt;数据中心应用多了归纳推断能力之后，就产生了对复杂现实（现实信号，如图片、语料库、音频，整体上具有高维度的同时，局部往往还存在具有高数据精致度的结构，这里精致度可以用香农信息熵或柯氏复杂性形式化定义）理解能力(pin down reality)，因此涌现出了一些基于知识的AI系统(knowledge-based AI)，比如推荐系统、广告投送、翻唱识别、聊天机器人。共同点是将现实映射到表征低维子空间上，这个子空间里的向量或者说embedding可以从某个角度有效刻画现实。Transformer做得更进一步，等价于将现实映射到多个相干性低的低维子空间上（见https://arxiv.org/abs/2306.01129），从而从多角度多层次理解现实，通过表征空间编码率与各子空间编码率之和的差值度量特征的紧凑性和判别力。&lt;/p&gt;
&lt;p&gt;在游戏和博弈领域，强化学习也取得了规则系统无法企及的成就，在星际2、Dota2（固定阵容5v5，solo）、围棋等领域击败职业选手。&lt;/p&gt;
&lt;p&gt;此外，归纳推断相比演绎推理还有一个重要特质，那就是更符合人类大脑（或推广到任意地球生物大脑，蜜蜂的3d寻路避障能力，蚊子的血管定位能力都是基于天然的贝叶斯归纳推断。也只有受过良好训练的人类才能进行缓慢的演绎推理）的思维习惯，因此人类社会的传统，人类语言的不确定性、人类的偏好都难以用形式化方法描述，却可以被概率语言描述，被归纳推断方法预测。&lt;/p&gt;
&lt;p&gt;算法进步的下一个课题则是研究同时具备演绎推理和归纳推断能力的模型。现有的Transformer的确展现出简单多步算术的拟合能力，但拟合出来的是一个步数相关的超复杂非线性系统，而不是简单的算术规则，所以往往正确率到八十左右就再也上不去了。对于形式化系统来说，推理步数不影响公理和规则总数，只稍稍影响计算量。但对于LLM这种归纳推断模型来说，一旦推理步数变多，计算变得更复杂，LLM的符号逻辑能力就会迅速下降，在训练数据中从未出现过相同执行路径的动态规划问题里面可以直接降到0。理论上来说，无限精度的Transformer是图灵完备的（见https://arxiv.org/abs/1901.03429），而只要不是无限精度，就不是图灵完备，实际应用中模型权重和计算的精度即使以常规计算机应用标准看也是非常低的，而模型架构又没有针对多步逻辑推理进行设计，再加上深度学习本身就是在拟合一个近似函数，三重因素叠加导致现有LLM的演绎推理能力不尽如人意。&lt;/p&gt;
&lt;p&gt;从算法迭代角度讲，数据中心应用的发展趋势是在保证“可计算性”(字面意思，非形式化定义)的前提下，追求“人性化”(align to humanity)、“完备化”（pin down reality）这两个演绎推理手段无法达成的理想属性。智能创作、个性化推荐、聪明的Agent（助手、游戏NPC、机器人、自动驾驶），复杂系统预测（天气预报、交易系统），大模型训练（模型并行、高性能网络、C2C互连、光学I/O、异构计算）都是有前景的方向。&lt;/p&gt;
&lt;p&gt;而那些已经能用规则系统高效解决的问题则不必强行应用归纳推断模型，那样只会增加成本和错误率，比如音乐指纹识别、关系型数据库、图片旋转变形、无损压缩。甚至某些看似“神秘”的领域，如OOD text classification，基于表征学习白盒化研究成果和信息论，也能给出直接的数学解，比如近期一个研究（“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors）就用14行代码超过了百亿参数的bert，（后来被发现实验代码有误，实际上效果没那么好）这个论文里提出用gzip等无损压缩工具去近似文本数据的柯氏复杂性，用信息距离（原理类似马毅提出的rate reduction）做KNN，效果非常好，在音频领域也可以做类似的尝试。&lt;/p&gt;
&lt;h2 id=&#34;数据中心硬件基础设施&#34;&gt;数据中心硬件基础设施&lt;/h2&gt;
&lt;p&gt;先简单介绍一下常见的数据中心硬件基础设施：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;比较老的是Cascade Lake 14nm的24核per die的机器，一个芯片24核，两个就是48个物理核，也就是我们平时说的96核机器。也有其他套餐，最多是每个die上28核，总共112个逻辑核。&lt;/li&gt;
&lt;li&gt;比较新的Ice Lake是10nm工艺一般是32核的，对应128个逻辑核，最贵的套餐可以到40核。仍然是基于Monolithic die，巨大的晶粒上用mesh总线把40个核放到一起。&lt;/li&gt;
&lt;li&gt;最新的7nm工艺Sapphire Rapids，以及对标它的AMD Genoa今年已经量产。&lt;/li&gt;
&lt;li&gt;关于网卡，现在用的比较多的是Mellanox 25G  CX4/CX5卡，也有相当多的100G dual-port CX6卡，都是支持RDMA的，不用RDMA也能提供相当好的高速以太网性能。如果是做虚拟化的，比如AWS，阿里云，还可以把这些网卡的smartNiC能力利用起来，offload虚拟化开销。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相对之前的Lakes，Sapphire Rapids变化非常大，此前的monolithic die路线确实走到了极限，Sapphire Rapids也进入了multi-die时代：芯片内部分为4个die，这也可以视作向chiplet方向演化。&lt;/p&gt;
&lt;p&gt;Sapphire Rapids最多支持8个socket，每个芯片可以支持60核。IO技术上支持CXL 1.1、PCIe 5.0、UPI 2.0，HBM2e(optional)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/4th_xeon.jpeg&#34; alt=&#34;Xeon&#34;&gt;&lt;/p&gt;
&lt;p&gt;AMD的第四代EPYC的主力型号Genoa今年Q1也开始量产，基本上和Sapphire Rapids对标，采用5nm工艺。虽然说AVX512被很多人诟病，实用表现非常一般，但Zen4也支持了，毕竟XeongenEPYC目标客户就是音视频处理和AI workload越来越多的数据中心应用。Genoa是每个die/chiplet 8核的，12个CPU cluster die中间围绕一个IO die的设计，相当于一个芯片96核，是典型的chiplet范式设计。&lt;/p&gt;
&lt;h2 id=&#34;芯片设计的迭代chipletization&#34;&gt;芯片设计的迭代：Chipletization&lt;/h2&gt;
&lt;p&gt;近期芯片设计领域一个显著变革是Chiplets+SiP(System in Package)范式取代die size较大的SoC+PCB合封。
Chiplets同时受到业界和学术界的关注，被IBM research称为&amp;quot;what’s next in computing&amp;quot;，后续章节中对计算、IO、内存等技术的讨论也都涉及chiplets和co-packaging，因此我们首先讨论芯片层次的迭代。&lt;/p&gt;
&lt;p&gt;所谓chiplet partitioning就是将电路切分成模块化的子系统，每个子系统都是一个独立晶粒（die），即chiplet，多个chiplet用2.5D/3D技术封装成一个芯片(package)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/chiplet.png&#34; alt=&#34;Chiplet&#34;&gt;&lt;/p&gt;
&lt;p&gt;Chiplet-reuse范式相比传统的IP-reuse（IP在芯片语境下指的是具有独立功能和成熟设计的电路模块）的优势如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先进CMOS制程（7nm以下）由于技术原因不太可能在大晶粒上获得高yield，die size越小成本越低。&lt;/li&gt;
&lt;li&gt;先进CMOS制程下，不太可能同时缩小电源管理、快速IO SerDes等模拟IP，先进CMOS一般只用于处理器和加速器。&lt;/li&gt;
&lt;li&gt;允许模块化设计，让设计者可以专注于单个模块的极致优化，并选择最合适的技术：比如CPU和GPU用先进制程，模拟模块用成熟制程，高带宽内存HBM用DRAM，AI加速器可以用非易失性内存。&lt;/li&gt;
&lt;li&gt;允许芯片/package层次的异构集成：让通用CPU、优化后的GPU、嵌入式的FPGA、专用的机器学习电路、光学IO模组、高带宽内存等模块以合适的方式，用先进的使用硅通孔（TSV）、微凸块（micro-bumps）、甚至die-to-wafer混合键合技术的3D封装方案，像乐高积木一样搭出完整的系统。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;过去我们设想的各类DSA、AI芯片、FPGA百花齐放的异构计算时代并没有如期到来，而是被NV的GPU软硬件结合且计算互连一体化的方案碾轧了，几乎只剩下TPU在继续向v5迭代。&lt;/p&gt;
&lt;p&gt;但未来的服务器芯片本身就存在多样化异构共封装集成的可能性和倾向性。CPO(co-packaged optics)、HBM(high-bandwidth memory)这些神奇物种因Chipletization的契机而得以进驻其中。&lt;/p&gt;
&lt;p&gt;更多的功能也就意味着更高的可编程性。有些功能甚至可以带来革命性变革，比如光学IO带来的超高通信带宽，HBM带来的超高访存带宽，高性能offpackge互连技术（Nvlink-C2C）、多个Chiplet之间的Mesh互连(NvSwitch)，现在被Nvidia用来搭建H100，被谷歌用来搭建TPUv4，未来则可能颠覆host-centric的数据中心应用设计范式，迎来硬件资源解聚（disaggregation）的新计算体系：适应资源解聚的操作系统(LegoOS就是基于早期IB network的一个尝试)、系统语言ABI、新的高级语言、新的网络IO、存储和计算形态都有可能从中孵化而生。&lt;/p&gt;
&lt;h2 id=&#34;计算和内存层次的迭代可扩展的众核numa架构&#34;&gt;计算和内存层次的迭代：可扩展的众核NUMA架构&lt;/h2&gt;
&lt;p&gt;在商用服务器领域，Chiplet范式中的一部分设想已经实现了，比如AMD很早就开始应用chiplet，也部分解决了chiplet间IO问题，实现了有可扩展性的众核NUMA架构。SPR之前的Xeon物理机也是NUMA，虽然只有2个NUMA节点（目前Intel的NUMA node太大了，所以不太好称之为Chiplet）。&lt;/p&gt;
&lt;p&gt;μArch对计算/访存密集型数据中心应用的性能工程有直接影响。下图是一个6chiplet封装的96核概念机。显然当我们把集成电路的黑盒拆开，就可以看到更细粒度的组件以及它们组成的网络（Network-on-Chip）结构。这个概念机集成了各种先进设计，不仅有many-core，还支持完整的cache coherency。相比过去的多核架构，众核架构的内存层次也相应变得更深，cache miss的代价变得更高。以至于Rust的标准库用B树去实现map（而C++中众所周知是红黑树），这就是处理器和内存频率差距逐渐拉大的结果，（夸张地说）现在的内存已经慢得像是当年的磁盘了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/IntAct.png&#34; alt=&#34;IntAct&#34;&gt;&lt;/p&gt;
&lt;p&gt;针对NUMA架构，系统层的Linux内核和KVM的NUMA-aware scheduler，应用层的网络框架Seastar、数据库ScyllaDB、内存数据库DragonFly等都已经注意到感知硬件拓扑能极大提升整体性能（ScyllaDB、DragonFly分别数倍领先于对标的Cassandra、Redis），提出了share-nothing高性能架构：避免锁和不必要的共享内存、避免不必要的远端内存访问、避免不必要的跨晶粒通信，设计缓存友好的数据结构，更好地利用晶粒内本地的L1 cache——考虑到目前我们用的Cascade Lake机器并非完全cache coherent，未来即使做到完全cache coherent，shared cache的coherency机制也几乎一定有开销，总之在复杂拓扑深内存层次时代，需警惕cache miss。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/NUMA.png&#34; alt=&#34;NUMA&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;重新遇到io瓶颈先进互连再次成为hpc核心&#34;&gt;重新遇到I/O瓶颈：先进互连再次成为HPC核心&lt;/h2&gt;
&lt;p&gt;数据中心应用的IO占了全球IO traffic的76%，和计算一样，IO也是耗电的，而且和计算一样，数据中心IO耗电也十年没有变过，被硬件进步offset掉了。和计算一样，互连是分层级的，近期die-to-die(on-package)链路层面有UCIe标准的发布，off-package层面有基于PCIe6.0的CXL3.0，900GB/s的NvLinkC2C，inter-node层面有Infiniband NDR。这些是基于电的互连，相比而言光学互连更有前景，但也更困难，而且还在早期研发阶段。&lt;/p&gt;
&lt;p&gt;过去的大数据和前大模型AI时代对IO的需求较低，标准以太网足以支撑大部分数据中心应用，包括parameter servers。大模型训练产生了新的计算、IO形态，内存放不下模型，不得不做模型并行后，IO就重新成了瓶颈：H100的8个GPU每个都需要7.2Tbps的off-package带宽，相比之下，连ToR交换机都只需要10+Tbps。AI专用GPU在大模型训练场景下的带宽需求已经非常接近交换机（交换机和GPU一样，都是巨型ASIC，也都是co-packaged optics适用的领域）。在交换机领域，谷歌已经研发出了实用且收益显著的纯光学链路交换机。在GPU互连上，NV也提出过光学互连的GPU的概念系统，甚至还设计了相应的带外置激光源的GPU机架和顺便解决冷却问题的稀疏布线。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/optics.png&#34; alt=&#34;optics&#34;&gt;&lt;/p&gt;
&lt;p&gt;先进IO技术与HPC(高性能计算)的发展密可不分，尽管HPC或者说超算在大众的想象中一直和超强悍的处理器、加速器直接相关，但实际上恰恰相反，传统的HPC workload（建模、模拟类的科学计算）的计算用的往往是普通的商用节点，反而是互连必须用高性能的HPC interconnect技术。传统超算的异构性体现在IO技术，而非FPGA、专用ASIC的应用。&lt;/p&gt;
&lt;p&gt;后来到了大数据分析和AI时代，标准以太网足以支持适应当时模型参数量的AI训练负载。主流的互联网大数据应用可以完全基于商用IO技术和商用计算节点实现。而少数AI DC的异构性主要体现在加速器技术（GPU、TPU、专用AI芯片）而非IO。&lt;/p&gt;
&lt;p&gt;如今出现了大模型训练主导的异构负载，大模型参数量激增导致内存不足，不得不进行模型并行后，die-to-die带宽、off-package带宽、Inter-node带宽重新成为瓶颈。先进(异构)互连技术重新成为HPC的核心话题。&lt;/p&gt;
&lt;p&gt;Datacenter AI重回异构IO+异构计算架构，本质是超算化，因此刚好也能适应建模+模拟类的传统HPC负载，其实给了互联网行业一个新的机会，那就是卷赢大模型训练的同时，还可以顺便进军超算行业，为高校、科研机构提供廉价、可靠、易用、随时oncall的科学计算能力，舆论上一定程度上扭转互联网公司对社会缺乏贡献的负面形象，为继续征收互联网服务税寻求合法性支撑。&lt;/p&gt;
&lt;h2 id=&#34;io技术的迭代光学io愈发接近计算端点&#34;&gt;I/O技术的迭代：光学IO愈发接近计算端点&lt;/h2&gt;
&lt;p&gt;先进铜缆互连是现在，共封装光学互连则是未来。&lt;/p&gt;
&lt;p&gt;前文提到的Co-packaging是先进互连的关键技术，一方面将多个晶粒共封装本身就可以缩短IO链路，降低IO能耗，另一方面允许集成共封装光学模组技术CPO(co-packaged optics)。&lt;/p&gt;
&lt;p&gt;数据中心IO的一个演化趋势是&amp;quot;bring fiber closer to endpoints&amp;quot;。光链路相比电链路，一个明显优势是传输距离更远（受制于频率相关的衰减）。另一个优势是随着带宽增加，电信号不断变短，噪音不断变大，IB network已经逼近铜缆极限，继续发展下去只能从铜缆走向光纤。此外，高频下电互连和连接器既要接收又要发射，会经历显著的串扰，这也限制了电互连的封装密度。光纤作为信号传输介质几乎是理想的，唯一低效的地方就是两端的电光转换部分。&lt;/p&gt;
&lt;p&gt;现在in-racks连接主流方案是铜缆，inter-rack交换则基于以太网链路。超大数据中心里，缆线长就达到几公里，因此越来越多使用光缆——甚至短距离链路现在也越来越多地用光缆。数据中心里，fiber越来越接近endpoints，越来越接近cpus、gpus，最新的趋势是直接将光学组件集成到硅片上。CPO把光电链路结合在一起，无需intervening receive and re-transmit的过程，把光电转换(optoelectonic conversion)步骤省略了。第一代CPO是pluggable optics，第二代是On-Board Optics/Near Package Optics，第三代是2.5D CPO，第四代是3D CPO，第五代则是Integrated Laser。&lt;/p&gt;
&lt;p&gt;Google的TPUv4超算最大的创新就是4k节点上可重配置的纯光学链路的光学交换机(OCS)，节省了光电转换的能耗， Infiniband将铜缆高性能互连发展到极致，后续的roadmap也是从铜缆到光电共封装。Nvidia虽然一直在推电链路方案（Nvlink），但也和Ayar Labs签署了研发合作关系，开始支持带外激光器和硅光子互连技术的研究，毕竟NvLink本质还是NUMA，可以扩展到8GPU，16GPU，但不可能把数据中心规模的一万个GPU连起来。HP也于去年与Ayar Labs合作，试图将硅光子学引入它们的先进HPC IO产品Slignshot互连。Intel也在研究激光器嵌入芯片内部的集成方案。&lt;/p&gt;
&lt;p&gt;下图列出了interposer、PCB、CPO、电缆、有源光缆的耗电、成本、密度、传输距离指标。CPO的优势是显而易见的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/CPO.png&#34; alt=&#34;CPO&#34;&gt;&lt;/p&gt;
&lt;p&gt;在当前的技术水平下，CPO仅被视为一个电光(E/O)桥的角色，以解决SiP的互连带宽密度瓶颈问题。对分布式训练等应用场景来说，电光桥，或者说bring fiber closer to endpoints已经可以大幅降低能耗，提升性能了。但CPO的潜力远不限于此，如果给CPO chiplet稍微加一些功能，就可以像协处理器、smartNiC那样offload一些CPU work，比如做一些简单的数据预处理、后处理，又如CPO无需经过CPU直接就能访问HBM，从而提供DMA能力，这对解聚架构非常有帮助，无需物理上做pooling，又比铜缆IB网络更快。&lt;/p&gt;
&lt;p&gt;这意味着光学IO不仅可以解决大模型训练带来的带宽问题，还给数据中心应用从host-centric向解聚（disaggregated）架构转型提供了可能。&lt;/p&gt;
&lt;p&gt;何谓解聚范式？与传统的服务器中心范式相反，解聚范式是指将作为整体的服务器掰开，拆成CPU、DRAM、磁盘、加速器等独立的硬件资源进行资源抽象和管理的数据中心应用架构设计范式。硬件解聚并非新概念。18年的USENIX OSDI最佳论文LegoOS，一句&amp;quot;We believe that datacenters should break monolithic servers&amp;quot;，充满了信念感。当年的Infiniband还没有进化到NDR版本，光学I/O也还远离数据中心内部端点，但已经足以支撑这样的宏大叙事。&lt;/p&gt;
&lt;p&gt;有了高性能网络，解聚架构就能有效提升数据中心应用的资源利用率，减轻物理机上CPU、加速器、内存、磁盘等资源在host-centric范式下不可避免的over-provisioning问题。&lt;/p&gt;
&lt;h2 id=&#34;评估-gh200-grace-hopper-superchip&#34;&gt;评估 GH200 Grace Hopper Superchip&lt;/h2&gt;
&lt;p&gt;NVIDIA宣称Grace Hopper Superchip是世界上第一个真正支持HPC和AI负载的异构加速平台。
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper.png&#34; alt=&#34;GraceHopper&#34;&gt;
如下图所示，这个superchip是一个把Grace Arm Neoverse CPU+LPDDR5x内存和H100 Tensor Core GPU+HBM，NVLink-C2C集成合封成PCB的集成方案。
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper2.png&#34; alt=&#34;GraceHopper&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper3.png&#34; alt=&#34;GraceHopper&#34;&gt;&lt;/p&gt;
&lt;p&gt;这并不是一个创新方案，一方面悖逆Chipletization的潮流（除了HBM算是chiplet外，H100、Grace、NvSwitch都是巨型SoC/ASIC，况且即使是HBM也是PCB合封，而不是SiP），另一方面也没有进行任何CPU-GPU超融合（物理上融合至单个SoC上，逻辑上统一页表管理、内存、缓存、并发模型等）的探索或尝试（GPU设计之初就存在太多和CPU无法兼容的设计，比如缓存模型、内存模型和并发模型，如今CUDA根基已成很难回头），只是简单粗暴的将CPU、高带宽内存、H100以PCB合封的方式集成，用NVLink-C2C提供内存一致性和更高off-package的带宽（并未尝试任何先进IO技术）。软件上也没能在CUDA基础上提供更强的可编程性，仅仅提供coherent memory access，编程模型仍然是完全异构的（这也是因为CUDA自诞生之初就是个图形加速库，也没法考虑未来会出现对这种superchip的同构编程模型的需求）。&lt;/p&gt;
&lt;p&gt;但这是一个低风险高执行力的集成方案，正如扎克伯格所说，&amp;ldquo;Move fast and break nothing&amp;rdquo;，把原本优秀的组件原封不动地合封起来，不做侵入式修改，只要动作足够快，就能迅速占领市场，构建生态，并支撑溢价。市场上有更完美的memory coherence方案（比如AMD MI300X），更好的CPU-GPU超融合方案，也有比不得不为图形负载妥协的GPU效率更高的AI芯片，但就是没有CUDA异构编程体系，以及Grace Hooper这样把计算、内存和IO瓶颈都解决得差不多的完整解决方案。&lt;/p&gt;
&lt;p&gt;总之，NV的方案作为生态(GPU + CUDA)与生物(ChatGPT根据A100量体裁衣的训练方案)互相作用下的best-of-breed，远远没达到理想最优，甚至也不在正确的技术路线上，AMD的所谓APU以及国内的AI DSA（如Biren）仍有弯道超车的希望。&lt;/p&gt;
&lt;p&gt;讨论计算系统的新机会&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（应用）端到（硬件）端的全栈优化，或者说软硬件协同。
&lt;ul&gt;
&lt;li&gt;TVM: deep learning compiler stack for cpu, gpu and specialized accelerators&lt;/li&gt;
&lt;li&gt;GPU + CUDA&lt;/li&gt;
&lt;li&gt;GH20 Grace Hopper + 新的CUDA NUMA内存API+异构编程API&lt;/li&gt;
&lt;li&gt;司内的LavaRecord全链路优化项目，向下(LavaUOS)对接新存储硬件，试图在nvme ssd上建立高效的用户态IO软件栈。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;应用机器学习方法对参数空间较大的系统做auto-tuning。
&lt;ul&gt;
&lt;li&gt;存储引擎如rocksdb调参&lt;/li&gt;
&lt;li&gt;深度学习模型在异构硬件上的auto TVM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;先进互连技术支持下的资源解聚架构设计。
&lt;ul&gt;
&lt;li&gt;LegoOS&lt;/li&gt;
&lt;li&gt;PolarDB-X的存算分离和memory pooling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;计算节点上的Share-nothing架构，以及data-oriented设计。
&lt;ul&gt;
&lt;li&gt;应用框架层面已有Libtorque、DragonFly、Seastar、Scylladb等先例，主要是IO密集应用——不过只要是内存占用大的CPU应用，大多可以视为IO密集的，因为cache miss上来之后访存占比往往会远超计算。&lt;/li&gt;
&lt;li&gt;虚拟化方向，交大IPADS实验室的CPS: A Cooperative Para-virtualized Scheduling Framework for Manycore Machines，提出协作式半虚拟化调度机制，大幅提升众核虚拟机可扩展性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;基于深度模型白盒化研究和已有的数学工具，用direct math solution取代黑盒模型的近似。
&lt;ul&gt;
&lt;li&gt;例如“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors用压缩+信息距离+KNN的简洁解决方案。&lt;/li&gt;
&lt;li&gt;用异类不相干性、同类可压缩性（稀疏性）衡量embedding效果，不必借助某种端到端应用的指标间接衡量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文献和进一步阅读&#34;&gt;参考文献和进一步阅读&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Learning One-hidden-layer Neural Networks with Landscape Design：即使是最简单的深度学习非凸优化场景，用数学工具（数学最优化方法）进行解释也极为困难。&lt;/li&gt;
&lt;li&gt;Functionality and performance of NVLink with IBM POWER9 processors：几年前IBM Power9（美国能源部的Summit和Sierra超算系统）就在用NVLink，而且hardware cache coherence设计（以及hardware atomic ops，addr translation）已经非常完善，比Grace Hooper方案更完善。&lt;/li&gt;
&lt;li&gt;Faith and Fate: Limits of Transformers on Compositionality：大语言模型涌现出演绎逻辑能力，但在多步复合问题上表现不佳，在训练样本中从未出现过计算图中相同计算路径的动态规划问题上准确率更是迅速跌落。与其他emprical study相比，这个研究更严肃，也更全面，考虑了计算图中训练时未见的splits带来的影响。我们有理由确信，大语言模型涌现的演绎推理能力会受制于transformer的天然局限。&lt;/li&gt;
&lt;li&gt;Teaching Arithmetic to Small Transformers：基于transformer的小语言模型足以学习简单算术能力， 提供包含正确的计算步骤的训练数据（chain-of-thought style data）是提升算术学习能力的关键，简单粗暴地用题目和结果进行训练，单纯靠增加模型大小无法提升准确率。&lt;/li&gt;
&lt;li&gt;A Survey of Large Language Models：提供了对大语言模型的up-to-date review。&lt;/li&gt;
&lt;li&gt;Variantional Inference: A Review For Statisticians：提供了解释VI、理解VI的统计学家视角，讨论了VI应用于指数级模型族的特例，并给出一个贝叶斯高斯混合模型的例子，并推导出一种使用随机优化来扩展至海量数据的VI变体。&lt;/li&gt;
&lt;li&gt;Training language models to follow instructions with human feedback：OpenAI的经验介绍，重点是RLHF。&lt;/li&gt;
&lt;li&gt;GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE：来自semianalysis的爆料，颇具可信度。&lt;/li&gt;
&lt;li&gt;Efficiently Scale LLM Training Across a Large GPU Cluster with Alpa and Ray：LLM训练。&lt;/li&gt;
&lt;li&gt;Scaling Language Model Training to a Trillion Parameters Using Megatron：Megatron（repo： &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;https://github.com/NVIDIA/Megatron-LM&lt;/a&gt; ，paper： &lt;a href=&#34;https://arxiv.org/pdf/1909.08053.pdf&#34;&gt;https://arxiv.org/pdf/1909.08053.pdf&lt;/a&gt; ）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=eqWPyaRcILQ&#34;&gt;https://www.youtube.com/watch?v=eqWPyaRcILQ&lt;/a&gt; 微软Azure硬件系统和基础设施团队的Ram Huggahalli关于Co-Packaged Optics的talk。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Xt-GY8Pkt6g&#34;&gt;https://www.youtube.com/watch?v=Xt-GY8Pkt6g&lt;/a&gt; 研究光通信和先进互连技术的Tony Chan Carusone关于Co-Packaged Optics以及Evolution of IO的talk。&lt;/li&gt;
&lt;li&gt;Next-generation Co-Packaged Optics for Future Disaggregated AI Systems：对共封装光学模组以及未来的解聚AI系统的洞察。&lt;/li&gt;
&lt;li&gt;TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings : Google的TPUv4，重点是纯光链路交换机&lt;/li&gt;
&lt;li&gt;LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation ：乐高OS，基于早期Infiniband高速网络做硬件资源解聚的尝试&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.hpcwire.com/2020/11/16/nvidia-mellanox-debuts-ndr-400-gigabit-infiniband-at-sc20/&#34;&gt;https://www.hpcwire.com/2020/11/16/nvidia-mellanox-debuts-ndr-400-gigabit-infiniband-at-sc20/&lt;/a&gt; Mellanox(Nvidia)的Infiniband NDR版本，以及roadmap。&lt;/li&gt;
&lt;li&gt;Rack-scale disaggregated cloud data centers: The dReDBox project vision: 数据中心应用解聚架构的早期尝试。&lt;/li&gt;
&lt;li&gt;White-Box Transformers via Sparse Rate Reduction：马毅团队对transformer的白盒化解释，此前马毅已经给出了更通用的rate reduction原则： Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bgavran/Category_Theory_Machine_Learning&#34;&gt;https://github.com/bgavran/Category_Theory_Machine_Learning&lt;/a&gt; 深度学习的范畴论解释。深度学习可解释性和逆向工作还可以参考Christopher Olah的blog: colah.github.io，Olah有许多深刻的洞察，比如https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ 流形假设的可视化和深度学习分类的解释。&lt;/li&gt;
&lt;li&gt;IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management：先进IC设计领域的论文，给出了一个集成了chiplet范式、3d封装、完全cache coherence等先进概念的96核众核原型系统。&lt;/li&gt;
&lt;li&gt;“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors：无损压缩近似柯氏复杂性，然后计算信息距离（类似rate reduction，刻画了总体与分类之间的信息差，分类的编码长度低而总体的编码长度高，表明这种分类具有异类强区分性和同类可压缩性），依据信息距离做简单的KNN即可完成分类。这个研究的代码有错，并不能击败BERT，见https://kenschutte.com/gzip-knn-paper/。&lt;/li&gt;
&lt;li&gt;M. Li and P.M.B. Vitányi, An Introduction to Kolmogorov Complexity and Its Applications 柯尔莫哥洛夫复杂性的介绍和应用&lt;/li&gt;
&lt;li&gt;A Mathematical Theory of Communication 1948年香农信息论的论文原著&lt;/li&gt;
&lt;li&gt;hwloc doc：hwloc的文档，hwloc是NUMA-discovery + cpu/memory-binding library。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://man7.org/linux/man-pages/man2/mbind.2.html&#34;&gt;https://man7.org/linux/man-pages/man2/mbind.2.html&lt;/a&gt;：libnuma的NUMA memory policy函数。&lt;/li&gt;
&lt;li&gt;On the Turing Completeness of Modern Neural Network Architectures 证明了无限精度transformer是图灵完备的，即任意图灵机都可被无限精度transformer模拟，但只要是固定精度就不是图灵完备的。&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Our Rationality can be Divided into Induction &amp; Deduction</title>
      <link>https://cmbbq.github.io/posts/induction-deduction/</link>
      <pubDate>Wed, 28 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/induction-deduction/</guid>
      <description>诺姆·乔姆斯基说：人的无知，可分为神秘和问题(mysteries &amp;amp; problems)。
诚哉斯言，为了对抗无知，人的理性，也可分为归纳和演绎(induction &amp;amp; deduction)。
前者推断神秘，后者解决问题。
前者完备而不可计算，后者可计算而不完备。
何为问题？ 何为问题？本文中指代可求解问题——有良好定义的输入、输出，存在确定性算法能逐步计算，最终得到正确结果，比如八皇后问题，哈密顿路径问题。
何为神秘？ 何为神秘？本文中指代不可求解问题——或需要无限计算资源，或具有不可判定性质（不可判定本质上就是不可计算），比如图灵提出的不可判定的停机问题。哥德尔构造的『真但不可证』的哥德尔语句。又如纯粹贝叶斯主义的置信度计算和预测必须同时考虑无限个预测性理论，现实计算机不仅处理不了计算量的无限性，还处理不了各个算法分支的不可终止性——并不是所有计算都会终止，有的甚至越算越复杂，永不止歇。
何为演绎？ 何为演绎？本文中指代演绎推理。欧几里得几何、初等算术、图灵机、λ演算等形式化系统（或者说演绎装置），就是由一组公理和一系列推理规则所组成，允许从公理推导出新理论的系统。
数理逻辑和演绎推理具有天然的美感。我们小时候学欧式几何时，应该都赞叹过这种形式主义美感：从简洁的五条公理，竟然能推导出复杂、庞大、深邃而又确切无疑正确的体系。演绎似乎是真理之钥。
自然而然地，历史上有数学家希望将所有数学理论建立在一组有限而完备的公理基础上，提供这些公理是一致的证明。1920年代，形式主义派的领导者希尔伯特希望建立一个形式化的证明体系，用严格遵循运算规则的符号逻辑语言表述所有数学陈述，给当时正面临多种悖论危机的数学一个安全的基础。这就是希尔伯特计划，一个人类理性对抗神秘的伟大尝试。希尔伯特说：『我们必须知道，我们也终将知道！』这是形式主义者的战斗宣言，他们坚信：数学具有完备性(在形式化数学里，所有数学陈述可证)、一致性（在形式化数学里，不存在矛盾）、可判定性（总有算法能判定某个数学陈述的真假）、保守性（证明不依赖于理想对象，如不可数集合）。只要这4点假设为真，无论现实多么宏大广博，真理如何高不可攀，人类只要持有演绎推理之钥，就能证明一切真理。
然而希尔伯特计划还未开始，就被哥德尔、图灵、邱奇终结。1930年代，哥德尔提出“广义递归函数”，图灵构造“图灵机”，邱奇提出“λ演算”，三人各自独立地形式化了可计算性：λ可计算性等价于图灵机可计算性，图灵机可计算性又等价于广义递归性。哥德尔不完备定理，证明了(1)一个蕴含初等算术的形式系统，若一致，则不完备。(2)该系统的一致性不能在系统内部证明。图灵提出停机问题，证明了我们无法判定图灵机是否会停止。停机问题的不可判定性蕴含了数学的不可判定性。一旦停机问题可判定，图灵机就能用来证明任意数学问题，因为理论上我们可以编写一个图灵机程序从公理开始构造出所有定理，每个定理生成后检查一下是不是目标问题——比如孪生素数猜想——如果是，图灵机程序终止，如果不是，则不停机。
何为归纳？ 何为归纳？本文中指代统计推断，泛指是使用数据分析来推断概率分布属性的过程。
统计学有贝叶斯学派和频率学派之争。贝叶斯学派认为待估计的模型参数是一个具有模型主观性的随机变量，而用来估计模型参数的数据反过来是确定的常数，讨论观测数据的概率分布才是没有意义的，概率被解释为模型在现有知识基础上对一件事情发生的相信程度。频率学派认为事件本身具有某种客观的随机性，概率是一个确定的值，讨论概率的分布没有意义，概率只是重复实验次数趋近于无穷后频率的极限值。贝叶斯理论最早起源于1763年，此后被频率学派以居高临下的姿态视作已经足够好的正统方法的补充，或者干脆是当成某种有趣但错误的尝试。直到上个世纪末才重新被学界重视。部分原因是计算机取代计算器，成为新的计算工具，克服了贝叶斯方法在计算上的困难。在通用电子计算机出现之前，频率学派对某些特定问题做出特殊假设的做法显然更经济，往往也相当有效，只不过这些人为假设难以证伪也难以证明。
归根到底，频率学派的归纳推断只能勉强算是贝叶斯推断的近似，还有一部分近似是错误的近似。CERN通过大型强子对撞机发现希格斯玻色子，实际上就是依据：假设不存在希格斯玻色子，观测数据的p值&amp;lt;0.00003%，这个阈值看起来足够低，因此CERN的研究者以及整个人类社会就接受了希格斯玻色子的存在。这个0.0003%的阈值是哪里来的呢？和人文社科研究中那些p值&amp;lt;5%一样都来源于直觉和人为假设。所谓的科学，不过是建立在如此薄弱的基础之上。即使在物理学实验这种极端严谨的场合，基于p值的统计分析依然会造成假象。2003年的五夸克态因p值被宣布发现，然而又因原始实验无法复现而被否定。Regina Nuzzo在Nature杂志上对p值滥用进行了批判，通过实例证明科学研究中普遍存在p值统计学显著结果不可重现的问题。
1960年代，纯粹贝叶斯主义者所罗门诺夫超越了哥德尔不完备理论，超越了邱奇-图灵论题，提出了贝叶斯理论的计算形式化，即所罗门诺夫归纳推断理论，并证明了该理论具有完备性和不可计算性。所罗门诺夫给出了一个惊人的观察：完备性和可计算性，不可得兼。任何理论，只要是可计算的，就不能检出所有规律。而像所罗门诺夫归纳推断理论这样具有完备性的理论，则必然是不可计算的。
所罗门诺夫理论的完备性意味着贝叶斯公式（这里是简化表示）P(θ|d) = P(d|θ)P(θ)/P(d)几乎可以被认为是真理公式。演绎推理则是贝叶斯推理在真假确定场景下的正确特例：当P(d) = 1时，概率语言中P(θ|d) = P(d|θ)P(θ)/P(d) 就自然退化成经典逻辑语言d =&amp;gt; θ，即d蕴含θ了。
规则的尽头是贝叶斯 下图是摘自Computability and Complexity，最早出自Descriptive Complexity(Immerman 1999)的可计算性与复杂度世界图，自上而下，逐渐从神秘过渡到问题。 当我们用if else条件判断，用计算机程序语言，无论如何努力，都难以编写出效果让人满意的程序时。我们往往是在面对神秘，而非问题。
这种场景下，经典逻辑的手段已经穷尽，演绎明确不是真理之钥，但我们可以观想所罗门诺夫之妖。
所罗门诺夫之妖所喻的纯粹贝叶斯主义固然具有不可计算性，却可以被各种手段近似模拟，典型的结构就是深度神经网络，用数十亿、数百亿、数千亿个参数/权重，用臃肿笨拙地方式去模拟贝叶斯推断。
在决策和博弈型任务上，强化学习对比基于规则的方法，取得了惊人的效果。目前的强化学习基本上能求解任意人类游戏/博弈，DeepMind的星际争霸AI AlphaStar击败MaNa，Dota2的OpenAI Five击败OG，围棋的AlphaGo击败柯洁。这是此前基于规则的方法几乎不可能实现的。
在理解和生成型任务上，大语言模型对比传统的NLP方法，达到了超出训练目标的额外效果，涌现出种种惊喜，比如单步逻辑推理能力，以及某些简单复合问题的逻辑推理能力。
基于图灵机、λ演算的计算理论根基数十年岿然不动，一朝有变，则蕴含着更深刻的变革——我们正见证从图灵机的演绎推理到神经网络的统计推断的跳跃，从“可计算且不完备”到“不可计算且完备”的质变。
前文Knowledge is Embeddings of Reality中提到对音频的理解可以是不同深度，不同角度的，音乐信息检索可以用信号处理的基于规则的方法，也可以用resnet这样的深度神经网络，或whisper这样的大模型。理解愈深邃，愈不精确，应用也就愈发灵活，愈发贴近人类思维。这是理解的视角，若是从计算的视角讲，基于信号处理的系统是在做演绎推理，因此可计算性最强，但天然具有不完备性，无论我们给代码里增加多少个公理，增加多少执行路径，设计多么巧妙的手工特征，注入多么鬼斧神工的专家洞察，都总有覆盖不到的场景，甚至可以说只能覆盖到很小很小的一部分，因为音频数据来自于现实，原始数据具备极高的整体复杂度和细微结构精致度，超越了人类思考和编程的能力极限，人类专家设计的音频特征能考虑抗噪，具备简单的鲁棒性，就已经是sota了。而基于深度模型、乃至大模型的方法则是对贝叶斯推断的近似，放弃精确性、增加计算成本的同时，从演绎转变成了归纳，从收敛走向开放，哪怕仅仅是对贝叶斯的近似，也换取了一定程度上的完备性——不局限于简单的抗噪、抗变调，开始在旋律、情绪、风格，甚至歌词的语言逻辑层面上考虑音频的相似度。
认知计算：人性化与完备化的革命 基于推理的算法，本质是为形式化系统（λ演算、图灵机）手工编写公理（先验数据，如魔数、掩码表、配置文件）或规则（传统的条件控制与算术计算）进行正确、高效但不完备的演绎推理。
基于推断的算法，本质是使用形式化系统（计算神经网络依然是形式化系统，其公理变成了更庞大的模型权重，规则变成了矩阵乘加、归一化、激活函数等数学算子的组合）对所罗门诺夫归纳推断理论（形式化的纯粹贝叶斯主义）进行拙劣、错误，但有用的近似。
统计推断，模型推理，英文中都是inference，中文用词却不一，不知是巧合还是集体无意识暗合真理：推断是归纳，推理则是演绎。纯粹贝叶斯主义的所罗门诺夫归纳推断近乎全知，不可计算，无穷尽，不停机，而我们用模型权重为公理，用矩阵乘加、批正态化、激活函数组合为规则写出来的模型推理程序本质上还是基于演绎推理，训练或者说优化、学习的过程亦然。因此在计算机科学的AI应用语境下我们把inference称为推理完全合理，正如在统计学语境下我们把inference称为推断，也是无比合理的。
所谓认知计算，就是用演绎推理的凡俗躯壳容纳纯粹贝叶斯主义全知魂灵的神降，补齐符号逻辑、规则系统和信号处理手段在“人性化(align to humanity)”、“完备化(pin down reality)”上的缺憾。
可以断言，尽管AI学界和业界浮躁且浮夸，受传统学科乃至计算系统传统领域的研究者诟病，但深度学习本身绝非niche应用，更非Metaverse、Web3那种VC追逐的玩具和噱头。所有计算领域——互联网服务、数据中心应用、智能终端、游戏AI，都将受益于认知计算进步，人类文明很有可能也会进入新的阶段。
近未来人类文明在基础物理、材料、能源领域或难有寸进，但在认知科学上构筑出辉煌的认知计算体系几乎是确定性的，因为认知计算所需的硬件进步不构成瓶颈，至少不会像对撞机、深空旅行、可控核聚变那样受到时空尺度和材料极限的约束。毕竟考虑即使是小小一只蜜蜂不足2立方厘米的大脑，就能拥有超越人类目前基于规则和导航辅助的先进算法的复杂3D环境寻路避障能力和天生的贝叶斯推断能力，亿万年生物演化的自然结构已经存在，我们不必担心这种结构根本不存在：它就在那，我们所需做的仅仅是近似和模仿。</description>
      <content>&lt;p&gt;诺姆·乔姆斯基说：人的无知，可分为神秘和问题(mysteries &amp;amp; problems)。&lt;/p&gt;
&lt;p&gt;诚哉斯言，为了对抗无知，人的理性，也可分为归纳和演绎(induction &amp;amp; deduction)。&lt;/p&gt;
&lt;p&gt;前者推断神秘，后者解决问题。&lt;/p&gt;
&lt;p&gt;前者&lt;a href=&#34;(https://en.wikipedia.org/wiki/Complete_theory)&#34;&gt;完备&lt;/a&gt;而不可计算，后者&lt;a href=&#34;https://en.wikipedia.org/wiki/Computability_theory&#34;&gt;可计算&lt;/a&gt;而不完备。&lt;/p&gt;
&lt;h2 id=&#34;何为问题&#34;&gt;何为问题？&lt;/h2&gt;
&lt;p&gt;何为问题？本文中指代可求解问题——有良好定义的输入、输出，存在确定性算法能逐步计算，最终得到正确结果，比如&lt;a href=&#34;https://en.wikipedia.org/wiki/Eight_queens_puzzle&#34;&gt;八皇后问题&lt;/a&gt;，&lt;a href=&#34;https://en.wikipedia.org/wiki/Hamiltonian_path_problem&#34;&gt;哈密顿路径问题&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;何为神秘&#34;&gt;何为神秘？&lt;/h2&gt;
&lt;p&gt;何为神秘？本文中指代不可求解问题——或需要无限计算资源，或具有不可判定性质（不可判定本质上就是不可计算），比如图灵提出的&lt;a href=&#34;https://en.wikipedia.org/wiki/Undecidable_problem&#34;&gt;不可判定&lt;/a&gt;的&lt;a href=&#34;https://en.wikipedia.org/wiki/Halting_problem&#34;&gt;停机问题&lt;/a&gt;。哥德尔构造的『真但不可证』的&lt;a href=&#34;https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems&#34;&gt;哥德尔语句&lt;/a&gt;。又如纯粹贝叶斯主义的置信度计算和预测必须同时考虑无限个预测性理论，现实计算机不仅处理不了计算量的无限性，还处理不了各个算法分支的不可终止性——并不是所有计算都会终止，有的甚至越算越复杂，永不止歇。&lt;/p&gt;
&lt;h2 id=&#34;何为演绎&#34;&gt;何为演绎？&lt;/h2&gt;
&lt;p&gt;何为演绎？本文中指代&lt;a href=&#34;https://en.wikipedia.org/wiki/Deductive_reasoning&#34;&gt;演绎推理&lt;/a&gt;。欧几里得几何、初等算术、图灵机、&lt;a href=&#34;https://en.wikipedia.org/wiki/Lambda_calculus&#34;&gt;λ演算&lt;/a&gt;等形式化系统（或者说演绎装置），就是由一组公理和一系列推理规则所组成，允许从公理推导出新理论的系统。&lt;/p&gt;
&lt;p&gt;数理逻辑和演绎推理具有天然的美感。我们小时候学欧式几何时，应该都赞叹过这种形式主义美感：从简洁的五条公理，竟然能推导出复杂、庞大、深邃而又确切无疑正确的体系。演绎似乎是真理之钥。&lt;/p&gt;
&lt;p&gt;自然而然地，历史上有数学家希望将所有数学理论建立在一组有限而完备的公理基础上，提供这些公理是一致的证明。1920年代，形式主义派的领导者希尔伯特希望建立一个形式化的证明体系，用严格遵循运算规则的符号逻辑语言表述所有数学陈述，给当时正面临多种&lt;a href=&#34;https://en.wikipedia.org/wiki/Foundations_of_mathematics#Foundational_crisis&#34;&gt;悖论危机&lt;/a&gt;的数学一个安全的基础。这就是&lt;a href=&#34;https://en.wikipedia.org/wiki/Hilbert%27s_program&#34;&gt;希尔伯特计划&lt;/a&gt;，一个人类理性对抗神秘的伟大尝试。希尔伯特说：『我们必须知道，我们也终将知道！』这是形式主义者的战斗宣言，他们坚信：数学具有完备性(在形式化数学里，所有数学陈述可证)、一致性（在形式化数学里，不存在矛盾）、可判定性（总有算法能判定某个数学陈述的真假）、保守性（证明不依赖于理想对象，如不可数集合）。只要这4点假设为真，无论现实多么宏大广博，真理如何高不可攀，人类只要持有演绎推理之钥，就能证明一切真理。&lt;/p&gt;
&lt;p&gt;然而希尔伯特计划还未开始，就被哥德尔、图灵、邱奇终结。1930年代，哥德尔提出“广义递归函数”，图灵构造“图灵机”，邱奇提出“λ演算”，三人各自独立地形式化了可计算性：λ可计算性等价于图灵机可计算性，图灵机可计算性又等价于广义递归性。&lt;a href=&#34;https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems#First_incompleteness_theorem&#34;&gt;哥德尔不完备定理&lt;/a&gt;，证明了(1)一个蕴含初等算术的形式系统，若一致，则不完备。(2)该系统的一致性不能在系统内部证明。图灵提出停机问题，证明了我们无法判定图灵机是否会停止。停机问题的不可判定性蕴含了数学的不可判定性。一旦停机问题可判定，图灵机就能用来证明任意数学问题，因为理论上我们可以编写一个图灵机程序从公理开始构造出所有定理，每个定理生成后检查一下是不是目标问题——比如孪生素数猜想——如果是，图灵机程序终止，如果不是，则不停机。&lt;/p&gt;
&lt;h2 id=&#34;何为归纳&#34;&gt;何为归纳？&lt;/h2&gt;
&lt;p&gt;何为归纳？本文中指代&lt;a href=&#34;https://en.wikipedia.org/wiki/Statistical_inference&#34;&gt;统计推断&lt;/a&gt;，泛指是使用数据分析来推断概率分布属性的过程。&lt;/p&gt;
&lt;p&gt;统计学有贝叶斯学派和频率学派之争。贝叶斯学派认为待估计的模型参数是一个具有模型主观性的随机变量，而用来估计模型参数的数据反过来是确定的常数，讨论观测数据的概率分布才是没有意义的，概率被解释为模型在现有知识基础上对一件事情发生的相信程度。频率学派认为事件本身具有某种客观的随机性，概率是一个确定的值，讨论概率的分布没有意义，概率只是重复实验次数趋近于无穷后频率的极限值。贝叶斯理论最早起源于1763年，此后被频率学派以居高临下的姿态视作已经足够好的正统方法的补充，或者干脆是当成某种有趣但错误的尝试。直到上个世纪末才重新被学界重视。部分原因是计算机取代计算器，成为新的计算工具，克服了贝叶斯方法在计算上的困难。在通用电子计算机出现之前，频率学派对某些特定问题做出特殊假设的做法显然更经济，往往也相当有效，只不过这些人为假设难以证伪也难以证明。&lt;/p&gt;
&lt;p&gt;归根到底，频率学派的归纳推断只能勉强算是贝叶斯推断的近似，还有一部分近似是错误的近似。CERN通过大型强子对撞机发现希格斯玻色子，实际上就是依据：假设不存在希格斯玻色子，观测数据的p值&amp;lt;0.00003%，这个阈值看起来足够低，因此CERN的研究者以及整个人类社会就接受了希格斯玻色子的存在。这个0.0003%的阈值是哪里来的呢？和人文社科研究中那些p值&amp;lt;5%一样都来源于直觉和人为假设。所谓的科学，不过是建立在如此薄弱的基础之上。即使在物理学实验这种极端严谨的场合，基于p值的统计分析依然会造成假象。2003年的五夸克态因p值被宣布发现，然而又因原始实验无法复现而被否定。&lt;a href=&#34;https://www.nature.com/articles/506150a&#34;&gt;Regina Nuzzo在Nature杂志上对p值滥用进行了批判&lt;/a&gt;，通过实例证明科学研究中普遍存在p值统计学显著结果不可重现的问题。&lt;/p&gt;
&lt;p&gt;1960年代，纯粹贝叶斯主义者所罗门诺夫超越了哥德尔不完备理论，超越了邱奇-图灵论题，提出了贝叶斯理论的计算形式化，即&lt;a href=&#34;https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference&#34;&gt;所罗门诺夫归纳推断理论&lt;/a&gt;，并证明了该理论具有完备性和不可计算性。所罗门诺夫给出了一个惊人的观察：完备性和可计算性，不可得兼。任何理论，只要是可计算的，就不能检出所有规律。而像所罗门诺夫归纳推断理论这样具有完备性的理论，则必然是不可计算的。&lt;/p&gt;
&lt;p&gt;所罗门诺夫理论的完备性意味着贝叶斯公式（这里是简化表示）P(θ|d) = P(d|θ)P(θ)/P(d)几乎可以被认为是真理公式。演绎推理则是贝叶斯推理在真假确定场景下的正确特例：当P(d) = 1时，概率语言中P(θ|d) = P(d|θ)P(θ)/P(d) 就自然退化成经典逻辑语言d =&amp;gt; θ，即d蕴含θ了。&lt;/p&gt;
&lt;h2 id=&#34;规则的尽头是贝叶斯&#34;&gt;规则的尽头是贝叶斯&lt;/h2&gt;
&lt;p&gt;下图是摘自&lt;a href=&#34;https://plato.stanford.edu/entries/computability&#34;&gt;Computability and Complexity&lt;/a&gt;，最早出自Descriptive Complexity(Immerman 1999)的可计算性与复杂度世界图，自上而下，逐渐从神秘过渡到问题。
&lt;img src=&#34;https://cmbbq.github.io/img/CaC.jpeg&#34; alt=&#34;cac&#34;&gt;&lt;/p&gt;
&lt;p&gt;当我们用if else条件判断，用计算机程序语言，无论如何努力，都难以编写出效果让人满意的程序时。我们往往是在面对神秘，而非问题。&lt;/p&gt;
&lt;p&gt;这种场景下，经典逻辑的手段已经穷尽，演绎明确不是真理之钥，但我们可以观想所罗门诺夫之妖。&lt;/p&gt;
&lt;p&gt;所罗门诺夫之妖所喻的纯粹贝叶斯主义固然具有不可计算性，却可以被各种手段近似模拟，典型的结构就是深度神经网络，用数十亿、数百亿、数千亿个参数/权重，用臃肿笨拙地方式去模拟贝叶斯推断。&lt;/p&gt;
&lt;p&gt;在决策和博弈型任务上，强化学习对比基于规则的方法，取得了惊人的效果。目前的强化学习基本上能求解任意人类游戏/博弈，DeepMind的星际争霸AI AlphaStar击败MaNa，Dota2的OpenAI Five击败OG，围棋的AlphaGo击败柯洁。这是此前基于规则的方法几乎不可能实现的。&lt;/p&gt;
&lt;p&gt;在理解和生成型任务上，大语言模型对比传统的NLP方法，达到了超出训练目标的额外效果，涌现出种种惊喜，比如单步逻辑推理能力，以及某些简单复合问题的逻辑推理能力。&lt;/p&gt;
&lt;p&gt;基于图灵机、λ演算的计算理论根基数十年岿然不动，一朝有变，则蕴含着更深刻的变革——我们正见证从图灵机的演绎推理到神经网络的统计推断的跳跃，从“可计算且不完备”到“不可计算且完备”的质变。&lt;/p&gt;
&lt;p&gt;前文&lt;a href=&#34;https://cmbbq.github.io/posts/reality-knowledge&#34;&gt;Knowledge is Embeddings of Reality&lt;/a&gt;中提到对音频的理解可以是不同深度，不同角度的，音乐信息检索可以用信号处理的基于规则的方法，也可以用resnet这样的深度神经网络，或whisper这样的大模型。理解愈深邃，愈不精确，应用也就愈发灵活，愈发贴近人类思维。这是理解的视角，若是从计算的视角讲，基于信号处理的系统是在做演绎推理，因此可计算性最强，但天然具有不完备性，无论我们给代码里增加多少个公理，增加多少执行路径，设计多么巧妙的手工特征，注入多么鬼斧神工的专家洞察，都总有覆盖不到的场景，甚至可以说只能覆盖到很小很小的一部分，因为音频数据来自于现实，原始数据具备极高的整体复杂度和细微结构精致度，超越了人类思考和编程的能力极限，人类专家设计的音频特征能考虑抗噪，具备简单的鲁棒性，就已经是sota了。而基于深度模型、乃至大模型的方法则是对贝叶斯推断的近似，放弃精确性、增加计算成本的同时，从演绎转变成了归纳，从收敛走向开放，哪怕仅仅是对贝叶斯的近似，也换取了一定程度上的完备性——不局限于简单的抗噪、抗变调，开始在旋律、情绪、风格，甚至歌词的语言逻辑层面上考虑音频的相似度。&lt;/p&gt;
&lt;h2 id=&#34;认知计算人性化与完备化的革命&#34;&gt;认知计算：人性化与完备化的革命&lt;/h2&gt;
&lt;p&gt;基于推理的算法，本质是为形式化系统（λ演算、图灵机）手工编写公理（先验数据，如魔数、掩码表、配置文件）或规则（传统的条件控制与算术计算）进行正确、高效但不完备的演绎推理。&lt;/p&gt;
&lt;p&gt;基于推断的算法，本质是使用形式化系统（计算神经网络依然是形式化系统，其公理变成了更庞大的模型权重，规则变成了矩阵乘加、归一化、激活函数等数学算子的组合）对&lt;a href=&#34;https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference&#34;&gt;所罗门诺夫归纳推断理论&lt;/a&gt;（形式化的纯粹贝叶斯主义）进行拙劣、错误，但有用的近似。&lt;/p&gt;
&lt;p&gt;统计推断，模型推理，英文中都是inference，中文用词却不一，不知是巧合还是集体无意识暗合真理：推断是归纳，推理则是演绎。纯粹贝叶斯主义的所罗门诺夫归纳推断近乎全知，不可计算，无穷尽，不停机，而我们用模型权重为公理，用矩阵乘加、批正态化、激活函数组合为规则写出来的模型推理程序本质上还是基于演绎推理，训练或者说优化、学习的过程亦然。因此在计算机科学的AI应用语境下我们把inference称为推理完全合理，正如在统计学语境下我们把inference称为推断，也是无比合理的。&lt;/p&gt;
&lt;p&gt;所谓认知计算，就是用演绎推理的凡俗躯壳容纳纯粹贝叶斯主义全知魂灵的神降，补齐符号逻辑、规则系统和信号处理手段在“人性化(align to humanity)”、“完备化(pin down reality)”上的缺憾。&lt;/p&gt;
&lt;p&gt;可以断言，尽管AI学界和业界浮躁且浮夸，受传统学科乃至计算系统传统领域的研究者诟病，但深度学习本身绝非niche应用，更非Metaverse、Web3那种VC追逐的玩具和噱头。所有计算领域——互联网服务、数据中心应用、智能终端、游戏AI，都将受益于认知计算进步，人类文明很有可能也会进入新的阶段。&lt;/p&gt;
&lt;p&gt;近未来人类文明在基础物理、材料、能源领域或难有寸进，但在认知科学上构筑出辉煌的认知计算体系几乎是确定性的，因为认知计算所需的硬件进步不构成瓶颈，至少不会像对撞机、深空旅行、可控核聚变那样受到时空尺度和材料极限的约束。毕竟考虑即使是小小一只蜜蜂不足2立方厘米的大脑，就能拥有超越人类目前基于规则和导航辅助的先进算法的复杂3D环境寻路避障能力和天生的贝叶斯推断能力，亿万年生物演化的自然结构已经存在，我们不必担心这种结构根本不存在：它就在那，我们所需做的仅仅是近似和模仿。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Knowledge is Embeddings of Reality</title>
      <link>https://cmbbq.github.io/posts/reality-knowledge/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/reality-knowledge/</guid>
      <description>现实有无限维度，现实的背后还有潜在现实，结构的边缘会涌现出新的深层结构。
人类对现实的感知、认知、理解，本质上是将高维现实投射到一个低维表示上，这种低维表示在统计学和机器学习中被称为embedding（嵌入），没错，知识就是将现实在人类有限的认知空间的一个嵌入，紧凑、浅薄且片面。
举个具体的例子，人眼观察到的彩虹呈现七色。
但这只是对无限广博的现实的最初观察，是最直观的认知。
人类通过对人体生理学的研究，发现七色感知是一个更深层的潜在现实的涌现结构：多种光感受器的叠加反应。
人类颜色感知系统对红绿蓝敏感的三种光感受器，叠加反应产生的三个峰和四个谷决定了人类能感知到7种颜色。 鸟类由于拥有对紫外线敏感的光感受器，就能感知到9种颜色，因此可以看到九色彩虹。
无论人类的七色彩虹还是鸟类的九色彩虹，都只是知识（或者说embedding），而不是现实。
人类通过数学工具和电磁学研究，进一步认识到光谱是连续的，包括无线电波、微波、热、红外线、可见光、紫外线等。
这接近现实了吗？不，这依然只是浅薄的embedding。。
人类通过量子物理学的研究，发现现有的电磁学依旧是一个更深层的潜在现实的涌现模型——电磁波是量子粒子流的一种表现形式：光子。
光子的认知，折射出更复杂的现实，因为一个光子同时拥有无限的位置和频率，直到被观察到才能确定其位置或能量，需用海森堡不确定性原理、薛定谔方程来描述，这依旧不是真理，不是终极，仍然是在现实之上所施加的结构(structure imposed upon reality)。
类似地，现实中的一首歌曲，看似简单平凡，实际上却拥有无穷维度，我们可以从任意一个角度去理解它，将它嵌入到任意一个向量空间。
假设用户手机的传感器捕捉了一段音频，将其上传到平台上，消重（时频图的mask特征）、指纹(时频图的landmark特征)、翻唱（浅层cnn）、哼唱（whisper大模型）、多模态大模型应用分别捕捉了这段音频的某种特征，产生了不同层次的知识（embedding），理解愈发深邃，愈发不精确，应用也就愈发灵活，愈发贴近人类思维。
后端的消重服务或指纹服务将表示音频的信号采样数据通过fft计算得到时频图，再基于时频图提取某种mask特征或landmark特征，用于直接刻画其能量分布特征，然后与音频特征库里存着的现有特征做对比就可以找到匹配的音频，这就是音频消重和指纹识别的过程：对信号做简单数学处理，生成的就是高度特化、具体、紧凑、确定、可解释的知识，表示这种知识的embedding或不具有抗噪性（mask特征），或具有少许抗噪性（landmark特征），但无论如何都只适用于原曲匹配，不知何为翻唱，何为remix，何为二创，何为串烧。
翻唱识别服务能通过一个学习了歌曲旋律的神经网络对音频信号进行处理，产出一定程度上表达旋律特征的embedding，将曲库里的翻唱embedding存入向量数据库，通过KNN/ANN召回的结果往往不限于原曲，还能容忍变调、音色上的改变。
哼唱识别服务可以基于Whisper这样的音频大模型实现，这种深层网络学习了人类的自然语言，因此对音频信号处理时，提取了表达歌词信息的embedding。基于这样的embedding，可以在跑调非常严重，且没有伴奏的情况下，依然能识别出用户在哼什么歌。
考虑未来的多模态大模型，更是可以直接将embedding存在transformer的ffn层，不必借助外部的向量数据库或倒排索引，直接在模型内部刻画embedding之间的复杂关系，让更多概念之间产生关联：识别出歌曲的意境、歌唱者的情绪、歌词内容的图片，甚至联系用户交互的上下文，衍生出更客制化、场景相关的输出。
人工智能领域用transformer简单搭建的大模型拥有出人意料的逻辑能力，昭示着人类意识的本质或许原本就极端肤浅：将知识内化、知识关联后，用天生的脑回路做贝叶斯推理，就产生了逻辑，诚然现有的结构远不如人脑高效，但有和无的界限已经被突破。
足够好的计算神经网络可以被视作神经科学的涌现模型。 一如往昔。 一如神经科学是细胞生物学的涌现。 一如细胞生物学是分子生物学的涌现。 一如分子生物学是物理化学的涌现。 一如物理化学是量子物理的涌现。</description>
      <content>&lt;p&gt;现实有无限维度，现实的背后还有潜在现实，结构的边缘会涌现出新的深层结构。&lt;/p&gt;
&lt;p&gt;人类对现实的感知、认知、理解，本质上是将高维现实投射到一个低维表示上，这种低维表示在统计学和机器学习中被称为embedding（嵌入），没错，知识就是将现实在人类有限的认知空间的一个嵌入，紧凑、浅薄且片面。&lt;/p&gt;
&lt;p&gt;举个具体的例子，人眼观察到的彩虹呈现七色。&lt;/p&gt;
&lt;p&gt;但这只是对无限广博的现实的最初观察，是最直观的认知。&lt;/p&gt;
&lt;p&gt;人类通过对人体生理学的研究，发现七色感知是一个更深层的潜在现实的涌现结构：多种光感受器的叠加反应。&lt;/p&gt;
&lt;p&gt;人类颜色感知系统对红绿蓝敏感的三种光感受器，叠加反应产生的三个峰和四个谷决定了人类能感知到7种颜色。
&lt;img src=&#34;https://cmbbq.github.io/img/color.png&#34; alt=&#34;color&#34;&gt;&lt;/p&gt;
&lt;p&gt;鸟类由于拥有对紫外线敏感的光感受器，就能感知到9种颜色，因此可以看到九色彩虹。&lt;/p&gt;
&lt;p&gt;无论人类的七色彩虹还是鸟类的九色彩虹，都只是知识（或者说embedding），而不是现实。&lt;/p&gt;
&lt;p&gt;人类通过数学工具和电磁学研究，进一步认识到光谱是连续的，包括无线电波、微波、热、红外线、可见光、紫外线等。&lt;/p&gt;
&lt;p&gt;这接近现实了吗？不，这依然只是浅薄的embedding。。&lt;/p&gt;
&lt;p&gt;人类通过量子物理学的研究，发现现有的电磁学依旧是一个更深层的潜在现实的涌现模型——电磁波是量子粒子流的一种表现形式：光子。&lt;/p&gt;
&lt;p&gt;光子的认知，折射出更复杂的现实，因为一个光子同时拥有无限的位置和频率，直到被观察到才能确定其位置或能量，需用海森堡不确定性原理、薛定谔方程来描述，这依旧不是真理，不是终极，仍然是在现实之上所施加的结构(structure imposed upon reality)。&lt;/p&gt;
&lt;p&gt;类似地，现实中的一首歌曲，看似简单平凡，实际上却拥有无穷维度，我们可以从任意一个角度去理解它，将它嵌入到任意一个向量空间。&lt;/p&gt;
&lt;p&gt;假设用户手机的传感器捕捉了一段音频，将其上传到平台上，消重（时频图的mask特征）、指纹(时频图的landmark特征)、翻唱（浅层cnn）、哼唱（whisper大模型）、多模态大模型应用分别捕捉了这段音频的某种特征，产生了不同层次的知识（embedding），理解愈发深邃，愈发不精确，应用也就愈发灵活，愈发贴近人类思维。&lt;/p&gt;
&lt;p&gt;后端的消重服务或指纹服务将表示音频的信号采样数据通过fft计算得到时频图，再基于时频图提取某种mask特征或landmark特征，用于直接刻画其能量分布特征，然后与音频特征库里存着的现有特征做对比就可以找到匹配的音频，这就是音频消重和指纹识别的过程：对信号做简单数学处理，生成的就是高度特化、具体、紧凑、确定、可解释的知识，表示这种知识的embedding或不具有抗噪性（mask特征），或具有少许抗噪性（landmark特征），但无论如何都只适用于原曲匹配，不知何为翻唱，何为remix，何为二创，何为串烧。&lt;/p&gt;
&lt;p&gt;翻唱识别服务能通过一个学习了歌曲旋律的神经网络对音频信号进行处理，产出一定程度上表达旋律特征的embedding，将曲库里的翻唱embedding存入向量数据库，通过KNN/ANN召回的结果往往不限于原曲，还能容忍变调、音色上的改变。&lt;/p&gt;
&lt;p&gt;哼唱识别服务可以基于Whisper这样的音频大模型实现，这种深层网络学习了人类的自然语言，因此对音频信号处理时，提取了表达歌词信息的embedding。基于这样的embedding，可以在跑调非常严重，且没有伴奏的情况下，依然能识别出用户在哼什么歌。&lt;/p&gt;
&lt;p&gt;考虑未来的多模态大模型，更是可以直接将embedding存在transformer的ffn层，不必借助外部的向量数据库或倒排索引，直接在模型内部刻画embedding之间的复杂关系，让更多概念之间产生关联：识别出歌曲的意境、歌唱者的情绪、歌词内容的图片，甚至联系用户交互的上下文，衍生出更客制化、场景相关的输出。&lt;/p&gt;
&lt;p&gt;人工智能领域用transformer简单搭建的大模型拥有出人意料的逻辑能力，昭示着人类意识的本质或许原本就极端肤浅：将知识内化、知识关联后，用天生的脑回路做贝叶斯推理，就产生了逻辑，诚然现有的结构远不如人脑高效，但有和无的界限已经被突破。&lt;/p&gt;
&lt;p&gt;足够好的计算神经网络可以被视作神经科学的涌现模型。
一如往昔。
一如神经科学是细胞生物学的涌现。
一如细胞生物学是分子生物学的涌现。
一如分子生物学是物理化学的涌现。
一如物理化学是量子物理的涌现。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/em.png&#34; alt=&#34;em&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>On NCO</title>
      <link>https://cmbbq.github.io/posts/on-nco/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-nco/</guid>
      <description>凸优化收敛时间一般是polynomial的，线性规划和最小二乘就是凸优化的特例。
非凸优化non-convex optimization是一种至少np-hard的问题，不存在通用解法。想要确定问题是否有解，局部最优是否全局最优，或目标函数是否有界都会随着变量和约束数目指数爆炸blow up，局部优化手段对算法参数敏感，又高度依赖initial guess，这使局部非凸优化more-art-than-technology，相比而言线性规划是毫无art可言的。
深度神经网络作为通用函数拟合器，最重要的作用是拟合非凸函数，因为复杂问题一般不可以用凸函数拟合。ChatGPT这类生成式模型就是对target和input间互信息的非凸优化。怎么训好模型，目前依然是一个art。
随机梯度下降stochastic gradient descent(SGD)被证明可以收敛于凸函数、可微和利普希茨连续函数，但还不能确定在非凸函数上的效果，SGD收敛缓慢，还不一定达到局部最优，更不一定达到全局最优。如果选择一个足够靠近全局最优的点，或许可以用SGD收敛到全局最优，但这一方面耗时间，另一方面只适用于特殊场景。对于深度神经网络来说，一旦陷入错误的局部最优，就要用不同的初始化配置或加入额外的梯度更新噪音。如果遇到鞍点，则需找到海森矩阵或计算下降方向。如果陷入低梯度区域，则需batchnorm，或使用relu做激活函数。如果因高曲率而使得steps过大，则应使用adaptive step size或限制梯度step尺度。此外，如果超参有问题，还需要用各种超参优化的方法。总之，目前深度学习的NCO还是处于art的阶段。</description>
      <content>&lt;p&gt;凸优化收敛时间一般是polynomial的，线性规划和最小二乘就是凸优化的特例。&lt;/p&gt;
&lt;p&gt;非凸优化non-convex optimization是一种至少np-hard的问题，不存在通用解法。想要确定问题是否有解，局部最优是否全局最优，或目标函数是否有界都会随着变量和约束数目指数爆炸blow up，局部优化手段对算法参数敏感，又高度依赖initial guess，这使局部非凸优化more-art-than-technology，相比而言线性规划是毫无art可言的。&lt;/p&gt;
&lt;p&gt;深度神经网络作为通用函数拟合器，最重要的作用是拟合非凸函数，因为复杂问题一般不可以用凸函数拟合。ChatGPT这类生成式模型就是对target和input间互信息的非凸优化。怎么训好模型，目前依然是一个art。&lt;/p&gt;
&lt;p&gt;随机梯度下降stochastic gradient descent(SGD)被证明可以收敛于凸函数、可微和利普希茨连续函数，但还不能确定在非凸函数上的效果，SGD收敛缓慢，还不一定达到局部最优，更不一定达到全局最优。如果选择一个足够靠近全局最优的点，或许可以用SGD收敛到全局最优，但这一方面耗时间，另一方面只适用于特殊场景。对于深度神经网络来说，一旦陷入错误的局部最优，就要用不同的初始化配置或加入额外的梯度更新噪音。如果遇到鞍点，则需找到海森矩阵或计算下降方向。如果陷入低梯度区域，则需batchnorm，或使用relu做激活函数。如果因高曲率而使得steps过大，则应使用adaptive step size或限制梯度step尺度。此外，如果超参有问题，还需要用各种超参优化的方法。总之，目前深度学习的NCO还是处于art的阶段。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>A Decade of Tussle between CPU and GPU</title>
      <link>https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/</guid>
      <description>GPU和CPU方法的边界何在？ 做AI应用的架构工作，遇到新的计算密集任务的第一问往往是：这应该上GPU，还是NUMA物理机？
这个问题可以归约为On the Limits of GPU Acceleration(2010)中提出的问题：&amp;ldquo;大致相同能耗的前提下，能和不能用GPU有效加速计算的边界在哪？&amp;rdquo; Vuduc的这个研究分析了3种具有代表性复杂行为和访存不规则性的计算：稀疏线性系统的迭代求解器、稀疏矩阵的乔列斯基分解、快速多极子算法，得出的结论是——大致地说，良好优化的GPU实现在相同能耗下和良好优化的CPU实现在性能上是相仿的。可见当年用GPGPU加速还是一个普遍得不偿失的工作，毕竟要付出剧烈变更编程模型的代价。
GPU和CPU的价格-性能趋势 上述结论在2010年成立，如今13年过去了，GPU固然飞速发展（无论是硬件、软件，还是生态、应用、资金投入），但GPU发展速度是否已经甩开CPU了呢？
摩尔定律是两年翻倍，而黄氏定律则是宣称通过软硬件协同能达到1.08年翻倍。
甚至如果我们考虑成本因素，根据经验数据，GPU FLOP/s每刀的增长速度是和CPU相仿的。这和2023年工业界的经验是吻合的——CPU在大多数应用语境下依然是成本更低的选择。</description>
      <content>&lt;h2 id=&#34;gpu和cpu方法的边界何在&#34;&gt;GPU和CPU方法的边界何在？&lt;/h2&gt;
&lt;p&gt;做AI应用的架构工作，遇到新的计算密集任务的第一问往往是：这应该上GPU，还是NUMA物理机？&lt;/p&gt;
&lt;p&gt;这个问题可以归约为&lt;a href=&#34;https://www.usenix.org/legacy/event/hotpar10/tech/full_papers/main.pdf&#34;&gt;On the Limits of GPU Acceleration(2010)&lt;/a&gt;中提出的问题：&amp;ldquo;大致相同能耗的前提下，能和不能用GPU有效加速计算的边界在哪？&amp;rdquo; Vuduc的这个研究分析了3种具有代表性复杂行为和访存不规则性的计算：稀疏线性系统的迭代求解器、稀疏矩阵的乔列斯基分解、快速多极子算法，得出的结论是——大致地说，良好优化的GPU实现在相同能耗下和良好优化的CPU实现在性能上是相仿的。可见当年用GPGPU加速还是一个普遍得不偿失的工作，毕竟要付出剧烈变更编程模型的代价。&lt;/p&gt;
&lt;h2 id=&#34;gpu和cpu的价格-性能趋势&#34;&gt;GPU和CPU的价格-性能趋势&lt;/h2&gt;
&lt;p&gt;上述结论在2010年成立，如今13年过去了，GPU固然飞速发展（无论是硬件、软件，还是生态、应用、资金投入），但GPU发展速度是否已经甩开CPU了呢？&lt;/p&gt;
&lt;p&gt;摩尔定律是两年翻倍，而&lt;a href=&#34;https://en.wikipedia.org/wiki/Huang%27s_law&#34;&gt;黄氏定律&lt;/a&gt;则是宣称通过软硬件协同能达到1.08年翻倍。&lt;/p&gt;
&lt;p&gt;甚至如果我们考虑成本因素，根据&lt;a href=&#34;https://epochai.org/blog/trends-in-gpu-price-performance&#34;&gt;经验数据&lt;/a&gt;，GPU FLOP/s每刀的增长速度是和CPU相仿的。这和2023年工业界的经验是吻合的——CPU在大多数应用语境下依然是成本更低的选择。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/4.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar1&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/5.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar2&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/6.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar3&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Digital Representation of Sound</title>
      <link>https://cmbbq.github.io/posts/digital-representation-of-sound/</link>
      <pubDate>Tue, 02 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/digital-representation-of-sound/</guid>
      <description>声音的本质是振动。
对于人耳来说，空气分子的振动抵达鼓膜时就会引起鼓膜振动——鼓膜顾名思义就是一层薄膜结构，被空气分子撞击就如同击鼓——带动鼓膜后的听小骨振动，传入内耳，再引发卵圆窗振动，卵圆窗这层薄膜之后是充满液体的耳蜗管道，这些液体随着卵圆窗振动而流动，冲刷耳蜗上的毛细胞的纤毛，从而产生生物电信号，经过神经传入大脑，就形成了听觉。
对于电容式麦克风来说，当有声音时，两块金属极板就开始振动，其间的距离会产生变化，电压形成了对空气分子振幅的模拟，因此这种电压信号被称为模拟信号。只有离散的数据才能被计算机存储、计算，因此要形成数字音频，还需要声卡的模数转换电路将连续的模拟信号采样后转化为离散的数字信号。这是一个连续曲线转化成柱状图的过程，采样率决定了柱的粗细，bit depth则决定柱高度的精度。如果bit depth为N，则振幅的取值范围为2^N，音乐App里标注的16-bit, 24-bit, 32-bit就指的是bit depth，显然bit depth越高，振幅的表示越精确。比特率bit rate = sample rate * bit depth，即每秒用多少bit来表示音频数据。
PGC和UGC的差别主要是信噪比，毕竟录音的时候连手机都不能带，空调、电灯都可能影响波形电平稳定。UGC可以想象，直播或者念稿子，噪声的平均振幅往往还比背景音乐更大。音频从32bit到16bit是一个量化的过程，不过无论采样精度多高，都终归是对现实的一个量化，量化error引入量化噪声，所以任何数字音频都是有底噪的——白噪声在各频率上都是一样的能量，由于能量（响度）较低，被称为noise floor，表现为微弱的嘶嘶声。
音频时域数据的纵坐标为振幅，横坐标为时间，可绘制成波形图——其震动频率决定音高，平均振幅决定响度，具体波形和这种其他因素决定音色。
时域与频域是对信号波的两个观察面。时域是真实世界唯一存在的域，频域则是对时域的数学构造。任何时域信号都可以表示为不同频率的正弦波信号的叠加。
常见的音频格式包括wav、mdi、mp3、mp3pro、wma、realaudio、audible、aac、ogg vorbis、ape、flac。不同的音频编码有不同的目标，在压缩（降低传输所需信道带宽）和质量（对人耳来说）之间做tradeoff。
各种格式的数字音频都能转成采样率8000Hz的PCM格式音频，PCM(pulse code modulation)是最简单的时域编码方式，就是对信号的离散和量化（通常是对数量化）。1/8000s长度的帧即构成了PCM音频在时域上的最小单位，每个帧包含channel数目个采样点，如果channel数目为1，则帧大小就等于bit depth。我们可以把PCM音频切分成多个chunk，每个chunk有固定大小（比如包含1024个帧），chunk与chunk之间必须有重合（比如256个帧）。之所以要有些重合，是为了对抗Time Skew（假如不做重合，查询音频的chunk的起始帧和库中的音频各个chunk的起始帧有一定offset就查询不到了）。
接下来只需对各个chunk进行1维离散傅立叶变换（fftw_plan_dft_1d）就能得到各个chunk的频域信息（如下图，横坐标为频率，纵坐标为能量）。
再将这些帧的频谱按时间顺序拼起来（可以有一些重合）就形成一个y轴为频率，x轴为时间，z轴为能量的三维表面（也可以用平面彩图表示，z轴的高度换成色彩来表示能量），这就形成了频谱图（spectrogram）。
有了频谱图，就可以尝试从较长的数据中提炼出简短的信息作为这段音频的指纹了。在频谱图上可以逐帧找到各帧上显著的高能频率点（salient peaks，能量超过一定阈值，且比周围所有点都高）——这些点本身已经可以作为音频的指纹特征了，只是不够鲁棒，因为点与点之间是无关的，满屏都是噪点的情况下就会有误匹配。一个改进方案是这些点右侧划定一个目标区域，在区域内找出一些点，形成几个pair，这些pair叫做landmark特征，即[t1, f1, t2, f2]，其抗噪能力增强了很多。</description>
      <content>&lt;p&gt;声音的本质是振动。&lt;/p&gt;
&lt;p&gt;对于人耳来说，空气分子的振动抵达鼓膜时就会引起鼓膜振动——鼓膜顾名思义就是一层薄膜结构，被空气分子撞击就如同击鼓——带动鼓膜后的听小骨振动，传入内耳，再引发卵圆窗振动，卵圆窗这层薄膜之后是充满液体的耳蜗管道，这些液体随着卵圆窗振动而流动，冲刷耳蜗上的毛细胞的纤毛，从而产生生物电信号，经过神经传入大脑，就形成了听觉。&lt;/p&gt;
&lt;p&gt;对于电容式麦克风来说，当有声音时，两块金属极板就开始振动，其间的距离会产生变化，电压形成了对空气分子振幅的模拟，因此这种电压信号被称为模拟信号。只有离散的数据才能被计算机存储、计算，因此要形成数字音频，还需要声卡的模数转换电路将连续的模拟信号采样后转化为离散的数字信号。这是一个连续曲线转化成柱状图的过程，采样率决定了柱的粗细，bit depth则决定柱高度的精度。如果bit depth为N，则振幅的取值范围为2^N，音乐App里标注的16-bit, 24-bit, 32-bit就指的是bit depth，显然bit depth越高，振幅的表示越精确。比特率bit rate = sample rate * bit depth，即每秒用多少bit来表示音频数据。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/audio1.png&#34; alt=&#34;audio1&#34;&gt;&lt;/p&gt;
&lt;p&gt;PGC和UGC的差别主要是信噪比，毕竟录音的时候连手机都不能带，空调、电灯都可能影响波形电平稳定。UGC可以想象，直播或者念稿子，噪声的平均振幅往往还比背景音乐更大。音频从32bit到16bit是一个量化的过程，不过无论采样精度多高，都终归是对现实的一个量化，量化error引入量化噪声，所以任何数字音频都是有底噪的——白噪声在各频率上都是一样的能量，由于能量（响度）较低，被称为noise floor，表现为微弱的嘶嘶声。&lt;/p&gt;
&lt;p&gt;音频时域数据的纵坐标为振幅，横坐标为时间，可绘制成波形图——其震动频率决定音高，平均振幅决定响度，具体波形和这种其他因素决定音色。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/wave.jpeg&#34; alt=&#34;wave&#34;&gt;&lt;/p&gt;
&lt;p&gt;时域与频域是对信号波的两个观察面。时域是真实世界唯一存在的域，频域则是对时域的数学构造。任何时域信号都可以表示为不同频率的正弦波信号的叠加。&lt;/p&gt;
&lt;p&gt;常见的音频格式包括wav、mdi、mp3、mp3pro、wma、realaudio、audible、aac、ogg vorbis、ape、flac。不同的音频编码有不同的目标，在压缩（降低传输所需信道带宽）和质量（对人耳来说）之间做tradeoff。&lt;/p&gt;
&lt;p&gt;各种格式的数字音频都能转成采样率8000Hz的PCM格式音频，PCM(pulse code modulation)是最简单的时域编码方式，就是对信号的离散和量化（通常是对数量化）。1/8000s长度的帧即构成了PCM音频在时域上的最小单位，每个帧包含channel数目个采样点，如果channel数目为1，则帧大小就等于bit depth。我们可以把PCM音频切分成多个chunk，每个chunk有固定大小（比如包含1024个帧），chunk与chunk之间必须有重合（比如256个帧）。之所以要有些重合，是为了对抗Time Skew（假如不做重合，查询音频的chunk的起始帧和库中的音频各个chunk的起始帧有一定offset就查询不到了）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/timeskew.png&#34; alt=&#34;ts&#34;&gt;&lt;/p&gt;
&lt;p&gt;接下来只需对各个chunk进行1维离散傅立叶变换（fftw_plan_dft_1d）就能得到各个chunk的频域信息（如下图，横坐标为频率，纵坐标为能量）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spec.png&#34; alt=&#34;sp&#34;&gt;&lt;/p&gt;
&lt;p&gt;再将这些帧的频谱按时间顺序拼起来（可以有一些重合）就形成一个y轴为频率，x轴为时间，z轴为能量的三维表面（也可以用平面彩图表示，z轴的高度换成色彩来表示能量），这就形成了频谱图（spectrogram）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spec2.png&#34; alt=&#34;sp&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/spec3.png&#34; alt=&#34;sp&#34;&gt;&lt;/p&gt;
&lt;p&gt;有了频谱图，就可以尝试从较长的数据中提炼出简短的信息作为这段音频的指纹了。在频谱图上可以逐帧找到各帧上显著的高能频率点（salient peaks，能量超过一定阈值，且比周围所有点都高）——这些点本身已经可以作为音频的指纹特征了，只是不够鲁棒，因为点与点之间是无关的，满屏都是噪点的情况下就会有误匹配。一个改进方案是这些点右侧划定一个目标区域，在区域内找出一些点，形成几个pair，这些pair叫做landmark特征，即[t1, f1, t2, f2]，其抗噪能力增强了很多。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spec4.png&#34; alt=&#34;sp&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Order Emerges from Self-Assembly of Dissipative Structures</title>
      <link>https://cmbbq.github.io/posts/on-order/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-order/</guid>
      <description></description>
      <content></content>
    </item>
    
  </channel>
</rss>
