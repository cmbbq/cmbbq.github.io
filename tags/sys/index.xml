<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sys on Cmbbq&#39;s Encyclopedia</title>
    <link>https://cmbbq.github.io/tags/sys/</link>
    <description>Recent content in sys on Cmbbq&#39;s Encyclopedia</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://cmbbq.github.io/tags/sys/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Serving</title>
      <link>https://cmbbq.github.io/posts/llm-serving/</link>
      <pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/llm-serving/</guid>
      <description>LLM推理有别于小模型推理，更加memory-bound，输入输出也有可以利用的独特模式，自然涌现出系统层面可优化的点。我将这些系统层面的优化机会粗略分为batching优化、sampling优化、模型压缩3类。
其中batching优化负责输入端将GPU等加速器的计算单元喂的更饱和、更好地利用SRAM locality1，sampling优化2负责输出端更快速拿到LLM的结果，模型压缩负责让大模型不要那么大。
LLM Serving的计算形态 LLM serving相比此前主流的深度模型serving，有一些独有的特性和约束：
一次LLM调用中很多时间耗费在加载模型参数上（HBM-&amp;gt;SRAM）。 batching可以让一次参数加载负责多个seq的推理，显著减少了总体的模型参数加载开销。 Seq2Seq：变长输入、变长输出。 因此batching机制必须适应batch size，seq len皆为变量。 自回归（通过若干次迭代才能处理一个请求），且为每个对话维护一个跨迭代的KV。 纯粹无状态的方法需每次重新计算所有KV，开销会大得无法接受，因此必须基于KV caching做incremental decoding。 RNN也是自回归，transformer相比RNN，还有一个不同之处是每次迭代KV cache都会变大。 非自回归模型往往以request为粒度做batching，对自回归场景并不适用，后者明显更适合以iteration为粒度。 GPT不同于原版transformer，是decoder-only的。 此前encoder-only和encoder-decoder架构的变长batching padding策略不适用于LLM。 GPT多了一个前置的计算密集的prefill阶段（也叫infill），用于处理prompt。 prefill阶段和增量迭代阶段的序列不好batching（计算要完全一样才好batching）。 这个阶段消化prompt，考虑到很多LLM应用都有非常巨大、内容也差不多的prompt，prefill阶段很多计算都是重复的。 GPT在生成token概率分布后，还有一个后置的sampling阶段：基于概率密度从vocab中选择token。 token选择完成后，当前迭代选中的token还需要再作为下次迭代的forward pass输入参数。 若seq[A~A+5]在此前的迭代中已经进行了采样，在新一轮迭代中除了需要对seq[A+6]采样之外，仍然要对seq[A~A+5]再算一遍。 已处理tokens的decoding结果可以cache下来。 很多LLM应用会要求结构化输出，输出的格式比较固定。 Transformer的attention算子的input张量形状和已处理tokens长度有关。 不同序列，序列长度不同，attn计算的输入形状不统一，就不好做batching。 好在attn计算做不做batching影响不大，因为attn计算并不涉及任何模型权重，也就不存在一次参数加载重用于多序列推理的加速效应。 LLM在GPU上跑的一个关键瓶颈是将数据（请求+参数）加载进HBM的开销。LLM serving吞吐明显会受限于batch size——即一次性能放入多少数据而不至于爆显存。 在简单的静态batching实现中，seq_len * nr_seqs + 模型大小决定了显存占用。seq_len假设定的比较高，又用不完，那也会让nr_seqs缩小很多。 显存如此紧张，自然使量化、剪枝等模型压缩技术在LLM serving中变得尤为重要，比如awq, gptq, gguf, smooth quant。 Continuous Batching 上文提及LLM做batching好处虽多，困难更多，并不存在特别简单、显而易见的GPT特供batching机制。
Orca3首先为GPT模型的batching提出了一个完整的解决方案，在迭代层面进行调度，并通过一个必要的selective batching机制剔除不能做batching的操作（若不做剔除，两个请求整体能做batching的几率可以忽略不计），这些操作虽然不能做batching，但对整体性能提升的影响很小。
Paged Attention Orca方案并未考虑KV cache的HBM占用，默认预分配max_seq_len。
但monolithic KV cache导致HBM碎片化，为每个seq预分配巨大内存进而导致并发不足也是真实瓶颈所在，vLLM中就对此进行了优化，提出了Paged Attention4，一种近似页表的分块kv cache技术：
在prefill阶段允许kv cache在非连续内存中以页的方式组织起来，因此不必为max_seq_len提前分配内存，运行时再分配就好。 大多数seq显然不会触及max_seq_len，paged attention因此节省了大量内存，也就允许batch size大大提高。 vLLM的实现中并未采纳Orca的selective batching，主要是因为它的paged attention算子是自己写的cuda，可以与非attn算子一起兼容batching。vLLM将prefill和decoding分开做batching，整体上就不需要实现selective batching这么麻烦的机制了。 但这种做法也阻止了prefill和decoding step的融合。如果某个prompt过长，prefill开销太大，确实会出现block后续所有decoding batch的情形。 详见Paged Attention。</description>
      <content>&lt;p&gt;LLM推理有别于小模型推理，更加memory-bound，输入输出也有可以利用的独特模式，自然涌现出系统层面可优化的点。我将这些系统层面的优化机会粗略分为batching优化、sampling优化、模型压缩3类。&lt;/p&gt;
&lt;p&gt;其中batching优化负责输入端将GPU等加速器的计算单元喂的更饱和、更好地利用SRAM locality&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，sampling优化&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;负责输出端更快速拿到LLM的结果，模型压缩负责让大模型不要那么大。&lt;/p&gt;
&lt;h2 id=&#34;llm-serving的计算形态&#34;&gt;LLM Serving的计算形态&lt;/h2&gt;
&lt;p&gt;LLM serving相比此前主流的深度模型serving，有一些独有的特性和约束：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一次LLM调用中很多时间耗费在加载模型参数上（HBM-&amp;gt;SRAM）。
&lt;ul&gt;
&lt;li&gt;batching可以让一次参数加载负责多个seq的推理，显著减少了总体的模型参数加载开销。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Seq2Seq：变长输入、变长输出。
&lt;ul&gt;
&lt;li&gt;因此batching机制必须适应batch size，seq len皆为变量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自回归（通过若干次迭代才能处理一个请求），且为每个对话维护一个跨迭代的KV。
&lt;ul&gt;
&lt;li&gt;纯粹无状态的方法需每次重新计算所有KV，开销会大得无法接受，因此必须基于KV caching做incremental decoding。&lt;/li&gt;
&lt;li&gt;RNN也是自回归，transformer相比RNN，还有一个不同之处是每次迭代KV cache都会变大。&lt;/li&gt;
&lt;li&gt;非自回归模型往往以request为粒度做batching，对自回归场景并不适用，后者明显更适合以iteration为粒度。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GPT不同于原版transformer，是decoder-only的。
&lt;ul&gt;
&lt;li&gt;此前encoder-only和encoder-decoder架构的变长batching padding策略不适用于LLM。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GPT多了一个前置的计算密集的prefill阶段（也叫infill），用于处理prompt。
&lt;ul&gt;
&lt;li&gt;prefill阶段和增量迭代阶段的序列不好batching（计算要完全一样才好batching）。&lt;/li&gt;
&lt;li&gt;这个阶段消化prompt，考虑到很多LLM应用都有非常巨大、内容也差不多的prompt，prefill阶段很多计算都是重复的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GPT在生成token概率分布后，还有一个后置的sampling阶段：基于概率密度从vocab中选择token。
&lt;ul&gt;
&lt;li&gt;token选择完成后，当前迭代选中的token还需要再作为下次迭代的forward pass输入参数。&lt;/li&gt;
&lt;li&gt;若seq[A~A+5]在此前的迭代中已经进行了采样，在新一轮迭代中除了需要对seq[A+6]采样之外，仍然要对seq[A~A+5]再算一遍。&lt;/li&gt;
&lt;li&gt;已处理tokens的decoding结果可以cache下来。&lt;/li&gt;
&lt;li&gt;很多LLM应用会要求结构化输出，输出的格式比较固定。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Transformer的attention算子的input张量形状和已处理tokens长度有关。
&lt;ul&gt;
&lt;li&gt;不同序列，序列长度不同，attn计算的输入形状不统一，就不好做batching。&lt;/li&gt;
&lt;li&gt;好在attn计算做不做batching影响不大，因为attn计算并不涉及任何模型权重，也就不存在一次参数加载重用于多序列推理的加速效应。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;LLM在GPU上跑的一个关键瓶颈是将数据（请求+参数）加载进HBM的开销。LLM serving吞吐明显会受限于batch size——即一次性能放入多少数据而不至于爆显存。
&lt;ul&gt;
&lt;li&gt;在简单的静态batching实现中，seq_len * nr_seqs + 模型大小决定了显存占用。seq_len假设定的比较高，又用不完，那也会让nr_seqs缩小很多。&lt;/li&gt;
&lt;li&gt;显存如此紧张，自然使量化、剪枝等模型压缩技术在LLM serving中变得尤为重要，比如awq, gptq, gguf, smooth quant。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;continuous-batching&#34;&gt;Continuous Batching&lt;/h2&gt;
&lt;p&gt;上文提及LLM做batching好处虽多，困难更多，并不存在特别简单、显而易见的GPT特供batching机制。&lt;/p&gt;
&lt;p&gt;Orca&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;首先为GPT模型的batching提出了一个完整的解决方案，在迭代层面进行调度，并通过一个必要的selective batching机制剔除不能做batching的操作（若不做剔除，两个请求整体能做batching的几率可以忽略不计），这些操作虽然不能做batching，但对整体性能提升的影响很小。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/orca.png&#34; alt=&#34;orca&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;paged-attention&#34;&gt;Paged Attention&lt;/h2&gt;
&lt;p&gt;Orca方案并未考虑KV cache的HBM占用，默认预分配max_seq_len。&lt;/p&gt;
&lt;p&gt;但monolithic KV cache导致HBM碎片化，为每个seq预分配巨大内存进而导致并发不足也是真实瓶颈所在，vLLM中就对此进行了优化，提出了Paged Attention&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;，一种近似页表的分块kv cache技术：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在prefill阶段允许kv cache在非连续内存中以页的方式组织起来，因此不必为max_seq_len提前分配内存，运行时再分配就好。&lt;/li&gt;
&lt;li&gt;大多数seq显然不会触及max_seq_len，paged attention因此节省了大量内存，也就允许batch size大大提高。&lt;/li&gt;
&lt;li&gt;vLLM的实现中并未采纳Orca的selective batching，主要是因为它的paged attention算子是自己写的cuda，可以与非attn算子一起兼容batching。vLLM将prefill和decoding分开做batching，整体上就不需要实现selective batching这么麻烦的机制了。
&lt;ul&gt;
&lt;li&gt;但这种做法也阻止了prefill和decoding step的融合。如果某个prompt过长，prefill开销太大，确实会出现block后续所有decoding batch的情形。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/block_table.png&#34; alt=&#34;block_table&#34;&gt;&lt;/p&gt;
&lt;p&gt;详见&lt;a href=&#34;https://cmbbq.github.io/posts/paged-attention&#34;&gt;Paged Attention&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;dynamic-splitfuse&#34;&gt;Dynamic SplitFuse&lt;/h2&gt;
&lt;p&gt;DeepSpeed-FastGen&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;中提出了SplitFuse，也是Continuous Batching的一个演化版本。思路是切分长prompt请求成若干个小的step，这些小的step开销较低，可填充调度缝隙，同时还保证prefill(prompt generation)和decode(token generation)的steps开销一致，就可以确保不存在大小不同的workload，取得一定的吞吐提升，但最主要的优势是能稳住tail-latency，在线服务场景下下限更高。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/split_fuse.png&#34; alt=&#34;split_fuse&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;quantization-and-pruning&#34;&gt;Quantization and Pruning&lt;/h2&gt;
&lt;p&gt;由于Nvidia架构上天然偏向图形负载，显存天然就给的不足，各种量化、剪枝技术除了降低计算量外，在LLM serving方向上还起到了关键性的降低显存占用的效果。&lt;/p&gt;
&lt;p&gt;详见&lt;a href=&#34;https://cmbbq.github.io/posts/quantization-and-pruning&#34;&gt;Quantization and Pruning&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;radix-attention&#34;&gt;Radix Attention&lt;/h2&gt;
&lt;p&gt;SGLang采用了Radix attention&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;技术，将common prefix的KV以radix tree的形式保留下来，使kv cache的生命周期不局限于一次请求，而是真正构成跨多次请求的LRU cache，适应prompt巨大且大多有相同前缀的实际应用场景。&lt;/p&gt;
&lt;h2 id=&#34;flash-attention&#34;&gt;Flash Attention&lt;/h2&gt;
&lt;p&gt;Continuous batching提升了非attn操作的SRAM locality，针对kv计算，Flash Attention&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;则令attn计算内层循环fit in SRAM。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/fast_attention.png&#34; alt=&#34;fast_att&#34;&gt;&lt;/p&gt;
&lt;p&gt;详见&lt;a href=&#34;https://cmbbq.github.io/posts/flash-attention&#34;&gt;Flash Attention&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;speculative-decoding&#34;&gt;Speculative Decoding&lt;/h2&gt;
&lt;p&gt;Speculative decoding&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;的思路是选用tokenizer相同，大小不同的两个模型。假设大模型的latency大体上是小模型的N倍。小模型输出N个token的时间内，大模型把这N个token拿过来append到seq上形成input，再输出1个token，总计生成N+1个token。基于greedy decoding对这N+1个token进行采样，采样结果如果和小模型的结果match，就直接用了，不match就停下，在停下的地方把原本的小模型token改成大模型采样结果对应的token。整个过程中，大模型实际上只需要一个forward pass，运气好就能一下子输出N+1个token，运气差就输出1个token。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/speculative_decoding.png&#34; alt=&#34;speculative_decoding&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;structured-decoding&#34;&gt;Structured Decoding&lt;/h2&gt;
&lt;p&gt;SGLang基于一个压缩有限状态机实现了structured decoding&lt;sup id=&#34;fnref1:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;，用于对特定结构化输出（比如支持regex的JSON模板）进行加速，一次性decode多个token。假设这个结构化输出的JSON模板中总是有一个key是&amp;quot;top5 candidate&amp;quot;，那就可以把&amp;quot;top5 candidate&amp;quot;这个multi-token词组当成一个token一轮迭代处理掉。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/structured_decoding.png&#34; alt=&#34;structured_decoding&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;GPU/NPU/TPU等加速器需将模型参数从off-chip memory加载到on-chip SRAM才能进行底层硬件算子的计算，对较大的模型，这种加载开销往往才是瓶颈所在。因此batching不仅仅能提升加速器计算单元的利用率，还能通过一份模型参数在多个请求中重用，更好地利用SRAM locality。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;sampling指的基于density做token-selection的过程，decoding指的是整个decoder-only transformer的inference过程。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Gyeong-In Yu and Joo Seong Jeong. Orca: A Distributed Serving System for Transformer-Based Generative Models. OSDI 22. &lt;a href=&#34;https://www.usenix.org/system/files/osdi22-yu.pdf&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;W. Kwon, et al. Efficient Memory Management for Large Language Model Serving with PagedAttention. &lt;a href=&#34;https://arxiv.org/pdf/2309.06180&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;C. Holmes, et al. DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference. &lt;a href=&#34;https://arxiv.org/pdf/2401.08671&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;L. Zheng, et al. SGLang: Efficient Execution of Structured Language Model Programs. &lt;a href=&#34;https://arxiv.org/pdf/2312.07104&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;T. Dao, et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. &lt;a href=&#34;https://arxiv.org/pdf/2205.14135&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;Y. Leviathan, et al. Fast Inference from Transformers via Speculative Decoding. &lt;a href=&#34;https://arxiv.org/pdf/2211.17192&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Idiomatic Practices in C&#43;&#43; Systems Engineering</title>
      <link>https://cmbbq.github.io/posts/engineering-practices/</link>
      <pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/engineering-practices/</guid>
      <description>用最小技术集构建当前问题的直接解。 如果已有技术修修补补后勉强解决问题，但又存在根本缺陷，则考虑研发新系统作为直接解。 不为未来的不确定需求破坏当前方案的简洁性。 管理复杂度，提升可理解性和可维护性。 核心代码fit in人脑短期记忆的解决方案是技术资产，反之则是技术债。 若核心代码复杂度高到无法fit in单人短期记忆，则将其合理切分成多个模块，交由多人维护。 避免引入过多的第三方库，规避C++ dependency hell。 避免解释型注释，让代码自解释。 如果某一行代码做了什么需要注释，那么说明它还可以进行更好的重构。 这种注释就像todo标志，表明有空就应该把它重构一下，提升代码质量的同时顺便去除注释。 剔除这类注释，可以避免日后迭代中，注释逐渐和代码对不上，对读者产生灾难性的误导。 尽量阐释为何这么做(WHY)，而不注解做了什么(WHAT)。 比较复杂的场景，可以用大段文字整体介绍设计思路。 这段文字可以放在代码最前面醒目位置，方便随着版本更新也进行相应更新。不必写额外的文档，或Readme，因为外置的文档和代码之间的映射也是脆弱、难以长期维护的。 使每个编译单元内自带文档、单测和构建指令 编译单元(.cc和它包含的头文件)承载了某个功能的实现，但对这个功能的测试、文档描述、构建脚本却往往放在其他文件里，有时会在很奇怪的位置，甚至混杂在其他复杂文件中，对于新接手项目的人来说了解这个编译单元的全貌就变得很麻烦。 所以不妨直接把单测代码写在每个.cc文件最下面，把单测的构建信息以注解形式写在.cc文件最上方，把文档以大段注释的方式写在.cc/.h的醒目位置。 如无特殊需求，构建工具不妨就用emake。 非性能攸关场景，尽可能拆分出更多函数，让每个函数只做一件事。 如果一个复杂函数能够拆解成多个函数，则将其拆解。 如果多个函数共享一些变量，则重构成类。 尽可能避免使用exception，而是使用result monad。 避免异常引入不必要的性能开销。 唯有禁用异常，才能convey fallibility through APIs。 std::expected&amp;lt;T,E&amp;gt;或best::result&amp;lt;T,E&amp;gt;都是不错的选择。 慎重对待需要就地处理的局部错误。 错误是分层的，处理错误的context也是分层的，有些错误只能在它的上一层级得到妥善处理，一旦向上层传播，就会丢失正确处理它的context，因此有必要从接口设计层面慎重对待这类错误。 如果整个项目的std::expected&amp;lt;T,E&amp;gt;中的E都是一个全局错误类型，编程时可以非常轻松地进行error propagation。但这也意味着有些不该抛给上层的错误也容易被调用者抛上去。 应赋予需要就地处理的局部错误一个独占的类型1，并用嵌套的expected表示返回值类型，如expectd&amp;lt;expected&amp;lt;T, SpecialError&amp;gt;, Error&amp;gt;，迫使调用者对SpecialError做单独处理。 用FTADLE实现多态。 我们当然不会用笨重的继承+动态绑定（subtype），也少用丑陋的模板+concept（ducktype）。 相比而言，FTADLE是更加简洁、灵活、美观、bug-free且易维护的定制化方案，巧妙利用了C++的一个冷门语言特性（ADL），实现了一种优雅的archetype多态。2 Error Handling&amp;#160;&amp;#x21a9;&amp;#xfe0e;
Paradigms of Generic Programming: Archetype, Ducktype, Subtype&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
      <content>&lt;h2 id=&#34;用最小技术集构建当前问题的直接解&#34;&gt;用最小技术集构建当前问题的直接解。&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果已有技术修修补补后勉强解决问题，但又存在根本缺陷，则考虑研发新系统作为直接解。&lt;/li&gt;
&lt;li&gt;不为未来的不确定需求破坏当前方案的简洁性。&lt;/li&gt;
&lt;li&gt;管理复杂度，提升可理解性和可维护性。
&lt;ul&gt;
&lt;li&gt;核心代码fit in人脑短期记忆的解决方案是技术资产，反之则是技术债。&lt;/li&gt;
&lt;li&gt;若核心代码复杂度高到无法fit in单人短期记忆，则将其合理切分成多个模块，交由多人维护。&lt;/li&gt;
&lt;li&gt;避免引入过多的第三方库，规避C++ dependency hell。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;避免解释型注释让代码自解释&#34;&gt;避免解释型注释，让代码自解释。&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果某一行代码做了什么需要注释，那么说明它还可以进行更好的重构。
&lt;ul&gt;
&lt;li&gt;这种注释就像todo标志，表明有空就应该把它重构一下，提升代码质量的同时顺便去除注释。&lt;/li&gt;
&lt;li&gt;剔除这类注释，可以避免日后迭代中，注释逐渐和代码对不上，对读者产生灾难性的误导。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;尽量阐释为何这么做(WHY)，而不注解做了什么(WHAT)。
&lt;ul&gt;
&lt;li&gt;比较复杂的场景，可以用大段文字整体介绍设计思路。&lt;/li&gt;
&lt;li&gt;这段文字可以放在代码最前面醒目位置，方便随着版本更新也进行相应更新。不必写额外的文档，或Readme，因为外置的文档和代码之间的映射也是脆弱、难以长期维护的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;使每个编译单元内自带文档单测和构建指令&#34;&gt;使每个编译单元内自带文档、单测和构建指令&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;编译单元(.cc和它包含的头文件)承载了某个功能的实现，但对这个功能的测试、文档描述、构建脚本却往往放在其他文件里，有时会在很奇怪的位置，甚至混杂在其他复杂文件中，对于新接手项目的人来说了解这个编译单元的全貌就变得很麻烦。&lt;/li&gt;
&lt;li&gt;所以不妨直接把单测代码写在每个.cc文件最下面，把单测的构建信息以注解形式写在.cc文件最上方，把文档以大段注释的方式写在.cc/.h的醒目位置。&lt;/li&gt;
&lt;li&gt;如无特殊需求，构建工具不妨就用&lt;a href=&#34;https://github.com/skywind3000/emake&#34;&gt;emake&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;非性能攸关场景尽可能拆分出更多函数让每个函数只做一件事&#34;&gt;非性能攸关场景，尽可能拆分出更多函数，让每个函数只做一件事。&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果一个复杂函数能够拆解成多个函数，则将其拆解。&lt;/li&gt;
&lt;li&gt;如果多个函数共享一些变量，则重构成类。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;尽可能避免使用exception而是使用result-monad&#34;&gt;尽可能避免使用exception，而是使用result monad。&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;避免异常引入不必要的性能开销。&lt;/li&gt;
&lt;li&gt;唯有禁用异常，才能convey fallibility through APIs。&lt;/li&gt;
&lt;li&gt;std::expected&amp;lt;T,E&amp;gt;或best::result&amp;lt;T,E&amp;gt;都是不错的选择。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;慎重对待需要就地处理的局部错误&#34;&gt;慎重对待需要就地处理的局部错误。&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;错误是分层的，处理错误的context也是分层的，有些错误只能在它的上一层级得到妥善处理，一旦向上层传播，就会丢失正确处理它的context，因此有必要从接口设计层面慎重对待这类错误。&lt;/li&gt;
&lt;li&gt;如果整个项目的std::expected&amp;lt;T,E&amp;gt;中的E都是一个全局错误类型，编程时可以非常轻松地进行error propagation。但这也意味着有些不该抛给上层的错误也容易被调用者抛上去。&lt;/li&gt;
&lt;li&gt;应赋予需要就地处理的局部错误一个独占的类型&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，并用嵌套的expected表示返回值类型，如&lt;code&gt;expectd&amp;lt;expected&amp;lt;T, SpecialError&amp;gt;, Error&amp;gt;&lt;/code&gt;，迫使调用者对SpecialError做单独处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;用ftadle实现多态&#34;&gt;用FTADLE实现多态。&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;我们当然不会用笨重的继承+动态绑定（subtype），也少用丑陋的模板+concept（ducktype）。&lt;/li&gt;
&lt;li&gt;相比而言，FTADLE是更加简洁、灵活、美观、bug-free且易维护的定制化方案，巧妙利用了C++的一个冷门语言特性（&lt;a href=&#34;https://en.cppreference.com/w/cpp/language/adl&#34;&gt;ADL&lt;/a&gt;），实现了一种优雅的archetype多态。&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://cmbbq.github.io/posts/error-handling&#34;&gt;Error Handling&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://cmbbq.github.io/posts/paradigms-of-generic-programming/&#34;&gt;Paradigms of Generic Programming: Archetype, Ducktype, Subtype&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Quantization and Pruning</title>
      <link>https://cmbbq.github.io/posts/quantization-and-pruning/</link>
      <pubDate>Mon, 02 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/quantization-and-pruning/</guid>
      <description>模型压缩技术中，最实用的是量化，其次还可以尝试剪枝。量化降低精度，剪枝裁剪参数。
Quantization 可以将k-bit量化问题视作将取值范围$(x_{min},x_{max})$的float数值$x$经过量化函数$g(x)$映射到取值范围$(0,2^{k-1})$的int型数值$Q$，并尽可能减少整体模型精度损失（well，如果没有端到端的模型accuracy/perplexity指标，也可以把目标设置为最小化output MSE/RMSE）的问题。$Q=round(g(x))$，
Uniform vs Non-uniform 根据Q的分布是否为均匀分布，可讲量化器分为uniform quantizers和non-uniform quantizers1。
Non-uniform quantization往往用几个离散的等级模拟其他分布（$x$真实的分布可能是lognorm分布、norm分布），以期在更稠密的取值范围内提升精度，缺点是计算量更大一些。
Affine vs Scale 在uniform quantization中，变换函数又有两个选择：affine和scale。前者用一个仿射函数（$g(x)=kx+b$），后者只用（$g(x)=kx$），0在映射后仍为0，$Q$和$x$在0两侧对称，因此又叫对称量化，实际上是仿射量化的一个特例，由于去掉了offset，计算更简化了，也容易向量化。
PTQ vs QAT 根据是否涉及backprop，量化可以分为PTQ(post-training Quantization)和QAT(Quantization Aware Training)这两类。QAT因训练成本高昂，难以扩展到大模型。因此大模型量化更多地使用PTQ。
Dynamic vs Static 静态精度量化把权重、激活函数、梯度统一转成低精度表示，比如W8A8量化。推理阶段W8A8完全是int8算术计算，不需要执行任何量化、反量化函数。
静态量化的参数（scale factor $k$、zero point $b$）是固定的。那么如何决定合适的$k$或$b$呢？静态量化往往需要通过在校准集上收集激活分布，寻找最小化MSE的最优解。但这种校准过程也会引入过拟合校准集的问题。在输入数据的分布非常明确，可以被校准集正确刻画的场景下，可以使用静态量化。
动态精度量化，又称weight-only量化，只把权重量化到低精度，激活仍保留高精度（因此模型变成了混合精度模型）。动态量化中，量化参数是推理时即时演算的，因此无需专门的校准阶段。推理时激活精度会动态调整精度（上限是模型中存储的激活精度，下限是权重精度，需施加量化函数到激活，或反量化函数到权重），因此保留了部分浮点数计算。
通常静态量化适合CNN，动态量化适合RNN、transformers。
量化器的计算量在混合精度模型（比如int4权重量化+fp16激活函数量化的gptq，或torchao QAT的8da4w，或llama.cpp的q4_k）中会影响推理性能。以llama.cpp的q4_k为例，中间张量（比如两个4-bits的sum，再用4-bits有几率产生overflow）有些需要用8-bits而非4-bits量化，可以把模型张量用计算量更高误差更少的量化器（比如non-uniform量化器，计算量虽大但不影响运行时开销），中间张量用计算量小的uniform量化器2。
LLM Quantization GPTQ3：一种适用于大模型的oneshot量化方法，把所有权重都批量放入矩阵中，逐层量化，每次都最小化输出MSE。GPTQ采用int4/fp16混合精度量化，4-bit用于权重量化，激活函数仍用fp16。GPTQ利用二阶信息做误差补偿，但可能在重建过程中过拟合校准集，导致模型损失泛用性。
AWQ4：一种低比特weight-only的量化方法，只对权重进行量化，将激活函数和梯度保留为全精度。AWQ认为只有0.1~1%的权重是salient的，应跳过这些salient权重。激活后的分布比权重本身更加salient，因此AWQ根据激活分布寻找需要被跳过的权重。
GGUF：在llama.cpp的k-quant体系中，qN_0表示N-bits scale量化，qN_1表示N-bits affine量化，qN_k代表特殊的block-wise量化，把原模型权重分块，每个块有自己的根据最大值简单算出的scaling factor（这样显然并不最优，后来又做了改进版本，见2），salient权重被量化到高精度，其他的则量化到低精度，是混合精度的。以q2_k quant为例，salient权重被量化到4-bit，而其他权重则是2bit。q4_0则分别把所有权重都量化到4-bit。
SmoothQuant5：与per-channel激活量化不同，SmoothQuant把magnitudes做了一个smooth操作，避免过于剧烈的inter-channel variation。SmoothQuant把原本非常uniform的权重分别变得稍有起伏，但实际上仍然易于计算。
k-bit Inference Scaling Laws 根据6中的三万五千次k-bit推理实验，模型总体积不变的情况下，4-bit精度几乎永远是最优解。
Pruning 对应PTQ，也存在PTP(Post-Training Pruning)，本文主要讨论PTP，即无需高昂重训练成本（可以通过LoRA恢复）的剪枝。
结构化稀疏 结构化稀疏在特定维度（chanel、conv kernel）上对卷积、矩阵乘做剪枝操作，改变其shape，生成更小的模型。
LLM-Pruner7、Torch-Pruning8是针对LLM的结构化稀疏方法。Isomorphic Pruning9则是近期针对ViT和现代CNN的SOTA方法。
非结构化稀疏 非结构化稀疏以每一个参数为单元进行稀疏，不改变参数矩阵shape，只是令其中部分值为零。要求底层推理实现能有效地利用矩阵稀疏性进行加速。
SparseGPT10在不显著牺牲perplexity的前提下，对175B级别的模型使用（one-shot，无需retraining），达到60%非结构化稀疏。
SparseGPT将剪枝问题规约到超大规模稀疏回归实例上，用一种新的近似稀疏回归求解器高效求解，能在单GPU上几个小时内跑完100B级模型的稀疏。
半结构化稀疏 N:M Pruning11是一种半结构化稀疏方法。SparseGPT这样的非结构化稀疏技术可以通过修改、适配成2:4 sparsity在A100上得到加速。
Raghuraman Krishnamoorthi.</description>
      <content>&lt;p&gt;模型压缩技术中，最实用的是量化，其次还可以尝试剪枝。量化降低精度，剪枝裁剪参数。&lt;/p&gt;
&lt;h1 id=&#34;quantization&#34;&gt;Quantization&lt;/h1&gt;
&lt;p&gt;可以将k-bit量化问题视作将取值范围$(x_{min},x_{max})$的float数值$x$经过量化函数$g(x)$映射到取值范围$(0,2^{k-1})$的int型数值$Q$，并尽可能减少整体模型精度损失（well，如果没有端到端的模型accuracy/perplexity指标，也可以把目标设置为最小化output MSE/RMSE）的问题。$Q=round(g(x))$，&lt;/p&gt;
&lt;h2 id=&#34;uniform-vs-non-uniform&#34;&gt;Uniform vs Non-uniform&lt;/h2&gt;
&lt;p&gt;根据Q的分布是否为均匀分布，可讲量化器分为uniform quantizers和non-uniform quantizers&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;Non-uniform quantization往往用几个离散的等级模拟其他分布（$x$真实的分布可能是lognorm分布、norm分布），以期在更稠密的取值范围内提升精度，缺点是计算量更大一些。&lt;/p&gt;
&lt;h2 id=&#34;affine-vs-scale&#34;&gt;Affine vs Scale&lt;/h2&gt;
&lt;p&gt;在uniform quantization中，变换函数又有两个选择：affine和scale。前者用一个仿射函数（$g(x)=kx+b$），后者只用（$g(x)=kx$），0在映射后仍为0，$Q$和$x$在0两侧对称，因此又叫对称量化，实际上是仿射量化的一个特例，由于去掉了offset，计算更简化了，也容易向量化。&lt;/p&gt;
&lt;h2 id=&#34;ptq-vs-qat&#34;&gt;PTQ vs QAT&lt;/h2&gt;
&lt;p&gt;根据是否涉及backprop，量化可以分为PTQ(post-training Quantization)和QAT(Quantization Aware Training)这两类。QAT因训练成本高昂，难以扩展到大模型。因此大模型量化更多地使用PTQ。&lt;/p&gt;
&lt;h2 id=&#34;dynamic-vs-static&#34;&gt;Dynamic vs Static&lt;/h2&gt;
&lt;p&gt;静态精度量化把权重、激活函数、梯度统一转成低精度表示，比如W8A8量化。推理阶段W8A8完全是int8算术计算，不需要执行任何量化、反量化函数。&lt;/p&gt;
&lt;p&gt;静态量化的参数（scale factor $k$、zero point $b$）是固定的。那么如何决定合适的$k$或$b$呢？静态量化往往需要通过在校准集上收集激活分布，寻找最小化MSE的最优解。但这种校准过程也会引入过拟合校准集的问题。在输入数据的分布非常明确，可以被校准集正确刻画的场景下，可以使用静态量化。&lt;/p&gt;
&lt;p&gt;动态精度量化，又称weight-only量化，只把权重量化到低精度，激活仍保留高精度（因此模型变成了混合精度模型）。动态量化中，量化参数是推理时即时演算的，因此无需专门的校准阶段。推理时激活精度会动态调整精度（上限是模型中存储的激活精度，下限是权重精度，需施加量化函数到激活，或反量化函数到权重），因此保留了部分浮点数计算。&lt;/p&gt;
&lt;p&gt;通常静态量化适合CNN，动态量化适合RNN、transformers。&lt;/p&gt;
&lt;p&gt;量化器的计算量在混合精度模型（比如int4权重量化+fp16激活函数量化的gptq，或torchao QAT的8da4w，或llama.cpp的q4_k）中会影响推理性能。以llama.cpp的q4_k为例，中间张量（比如两个4-bits的sum，再用4-bits有几率产生overflow）有些需要用8-bits而非4-bits量化，可以把模型张量用计算量更高误差更少的量化器（比如non-uniform量化器，计算量虽大但不影响运行时开销），中间张量用计算量小的uniform量化器&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h2 id=&#34;llm-quantization&#34;&gt;LLM Quantization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GPTQ&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;：一种适用于大模型的oneshot量化方法，把所有权重都批量放入矩阵中，逐层量化，每次都最小化输出MSE。GPTQ采用int4/fp16混合精度量化，4-bit用于权重量化，激活函数仍用fp16。GPTQ利用二阶信息做误差补偿，但可能在重建过程中过拟合校准集，导致模型损失泛用性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AWQ&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;：一种低比特weight-only的量化方法，只对权重进行量化，将激活函数和梯度保留为全精度。AWQ认为只有0.1~1%的权重是salient的，应跳过这些salient权重。激活后的分布比权重本身更加salient，因此AWQ根据激活分布寻找需要被跳过的权重。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GGUF：在llama.cpp的k-quant体系中，qN_0表示N-bits scale量化，qN_1表示N-bits affine量化，qN_k代表特殊的block-wise量化，把原模型权重分块，每个块有自己的根据最大值简单算出的scaling factor（这样显然并不最优，后来又做了改进版本，见&lt;sup id=&#34;fnref1:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;），salient权重被量化到高精度，其他的则量化到低精度，是混合精度的。以q2_k quant为例，salient权重被量化到4-bit，而其他权重则是2bit。q4_0则分别把所有权重都量化到4-bit。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SmoothQuant&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;：与per-channel激活量化不同，SmoothQuant把magnitudes做了一个smooth操作，避免过于剧烈的inter-channel variation。SmoothQuant把原本非常uniform的权重分别变得稍有起伏，但实际上仍然易于计算。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/smooth_quant.png&#34; alt=&#34;smooth&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;k-bit-inference-scaling-laws&#34;&gt;k-bit Inference Scaling Laws&lt;/h2&gt;
&lt;p&gt;根据&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;中的三万五千次k-bit推理实验，模型总体积不变的情况下，4-bit精度几乎永远是最优解。&lt;/p&gt;
&lt;h1 id=&#34;pruning&#34;&gt;Pruning&lt;/h1&gt;
&lt;p&gt;对应PTQ，也存在PTP(Post-Training Pruning)，本文主要讨论PTP，即无需高昂重训练成本（可以通过LoRA恢复）的剪枝。&lt;/p&gt;
&lt;h2 id=&#34;结构化稀疏&#34;&gt;结构化稀疏&lt;/h2&gt;
&lt;p&gt;结构化稀疏在特定维度（chanel、conv kernel）上对卷积、矩阵乘做剪枝操作，改变其shape，生成更小的模型。&lt;/p&gt;
&lt;p&gt;LLM-Pruner&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;、Torch-Pruning&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;是针对LLM的结构化稀疏方法。Isomorphic Pruning&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;则是近期针对ViT和现代CNN的SOTA方法。&lt;/p&gt;
&lt;h2 id=&#34;非结构化稀疏&#34;&gt;非结构化稀疏&lt;/h2&gt;
&lt;p&gt;非结构化稀疏以每一个参数为单元进行稀疏，不改变参数矩阵shape，只是令其中部分值为零。要求底层推理实现能有效地利用矩阵稀疏性进行加速。&lt;/p&gt;
&lt;p&gt;SparseGPT&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;在不显著牺牲perplexity的前提下，对175B级别的模型使用（one-shot，无需retraining），达到60%非结构化稀疏。&lt;/p&gt;
&lt;p&gt;SparseGPT将剪枝问题规约到超大规模稀疏回归实例上，用一种新的近似稀疏回归求解器高效求解，能在单GPU上几个小时内跑完100B级模型的稀疏。&lt;/p&gt;
&lt;h2 id=&#34;半结构化稀疏&#34;&gt;半结构化稀疏&lt;/h2&gt;
&lt;p&gt;N:M Pruning&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;是一种半结构化稀疏方法。SparseGPT这样的非结构化稀疏技术可以通过修改、适配成2:4 sparsity在A100上得到加速。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Raghuraman Krishnamoorthi. Quantizing Deep Convolutional Networks for Efficient Inference: A whitepaper. &lt;a href=&#34;https://arxiv.org/pdf/1806.08342&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/397&#34;&gt;llama.cpp issue: Investigate alternative approach for Q4 quantization&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;E. Frantar, et al. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. &lt;a href=&#34;https://arxiv.org/pdf/2210.17323&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Ji Lin, et al. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. &lt;a href=&#34;https://arxiv.org/pdf/2306.00978&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;G. Xiao, et al. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. &lt;a href=&#34;https://arxiv.org/pdf/2211.10438&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;T. Dettmers, L. Zettlemoyer. The case for 4-bit precision: k-bit Inference Scaling Laws. &lt;a href=&#34;https://arxiv.org/pdf/2212.09720&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.11627&#34;&gt;LLM-Pruner: On the Structural Pruning of Large Language Models&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;G. Fang, et al. DepGraph: Towards Any Structural Pruning. &lt;a href=&#34;https://arxiv.org/pdf/2301.12900&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;G. Fang, et al. Isomorphic Pruning for Vision Models. &lt;a href=&#34;https://arxiv.org/pdf/2407.04616&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;E. Frantar, D. Alistarh. SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot. &lt;a href=&#34;https://arxiv.org/pdf/2301.00774&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;A. Zhou, et al. Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch. &lt;a href=&#34;https://arxiv.org/pdf/2102.04010&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Optimizing AI Inference</title>
      <link>https://cmbbq.github.io/posts/optimizing-ai-inference/</link>
      <pubDate>Wed, 28 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/optimizing-ai-inference/</guid>
      <description>优化的几个抽象层次 应用侧的AI Engineering大致上可以分为MLOps和推理优化。推理优化又可以分为几个抽象层次：
最高层是模型优化（详见1）：量化、知识蒸馏、参数剪枝、通道剪枝。 中间层是图优化和通用优化：operator fusion/reconstruction、loop interchanges、data layout rewrites。 底层是硬件算子：最优化的底层算子必然是最特化的（tensor-specific + hardware-specific + framework-agnostic），因此需将中层IR绑定到硬件算子（即后端算子选择），这些硬件算子往往需要根据设备信息，充分利用vectorization、parallelism、locality，做显式cache管理，通过合理的指令重排隐藏memory latency, 利用SIMD/AMX/DSP/GPGPU架构做memory tiling和minibatch block gemm。 最底层还有LLVM层面的low-level codegen优化，负责最终生成优化的机器码。 第一层的模型优化减少了模型本身的总计算量，与底层优化正交。第二、第三层的推理优化归根到底都是硬件使能，只不过二层对几乎所有硬件都有效，三层则根据设备信息细化。从实现方法上讲，二、三层优化又可分为基于AI编译器的优化和手工优化。
基于编译的优化 编译，或者说DSLs + Optimizing Compilers，是解决领域问题优化的一个通用解法，为不同硬件提供可移植的优化。比如十年前就有Halide为图像和张量的并行计算提供了一个DSL+编译器，将算法规格本身和优化细节解耦。甚至更底层的gcc/llvm本身也是将算法和优化解耦的例子，在machine code codegen层面做的优化。
ML Compilers的优势和劣势 如今的ML compilers是这种基于编译的思路的延续2，其优势在于：
可移植性：硬件一方面在不断迭代更新，另一方面也不断有新硬件、新架构涌现。端边侧的设备要繁杂的多。Datacenter场景还好，但也面临N卡禁运，国产替代的问题，国产GPGPU还没有形成明确的一两家独大的格局3，因此适配新硬件是近未来必需解决的事情。 将算法与优化清晰解耦：工程上可以提效，也有助于降低因复杂度爆炸而注入缺陷、使得项目逐渐失控、无法维护的风险。 其劣势在于：
不完备：在处理大多数场景、常规问题时性能不错，但总会遇到一些预料之外的edge cases，fallback to the slow path，性能骤然劣化，很难通过tweak DSL code生成更优的代码。 不灵活：考虑到编译器codegen产物往往不那么human-readble，工程上针对edge cases、ad-hoc需求做灵活的手工优化就比较困难。 难以真正击败专家手动优化：正如至今不存在能在性能上击败C/C++的函数式语言编译器，ML compiler也只是给出足够好的解，通常不及专家充分优化的C/C++实现。 ML Compilers的工作流程 ML编译器的工作流是高层抽象到底层抽象的lowering过程。ML编译器后端包含各种passes，所谓pass就是lowering规则。最终会根据设备信息，形成一种硬件特化、张量特化的硬件算子描述，这种描述在TVM里叫做schedule，在Triton里叫做plan。再进一步CodeGen阶段，是从ML编译器自己的语言翻译到language compiler后端，比如LLVM IR，然后交由LLVM编译成可执行的machine code。
MLIR中dialects可对passes进行分层或分类，一个典型的dialects分层(见4)自上而下如是：
OpGraph -&amp;gt; TSOWB(e.g. late hlo) -&amp;gt; CGASel -&amp;gt; HHO(e.g. Linalg) -&amp;gt; MHA(e.g. stripe/affine) -&amp;gt; HLTSIR(e.g. vector dialects) -&amp;gt; TSIR(e.</description>
      <content>&lt;h2 id=&#34;优化的几个抽象层次&#34;&gt;优化的几个抽象层次&lt;/h2&gt;
&lt;p&gt;应用侧的AI Engineering大致上可以分为MLOps和推理优化。推理优化又可以分为几个抽象层次：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最高层是模型优化（详见&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;）：量化、知识蒸馏、参数剪枝、通道剪枝。&lt;/li&gt;
&lt;li&gt;中间层是图优化和通用优化：operator fusion/reconstruction、loop interchanges、data layout rewrites。&lt;/li&gt;
&lt;li&gt;底层是硬件算子：最优化的底层算子必然是最特化的（tensor-specific + hardware-specific + framework-agnostic），因此需将中层IR绑定到硬件算子（即后端算子选择），这些硬件算子往往需要根据设备信息，充分利用vectorization、parallelism、locality，做显式cache管理，通过合理的指令重排隐藏memory latency, 利用SIMD/AMX/DSP/GPGPU架构做memory tiling和minibatch block gemm。&lt;/li&gt;
&lt;li&gt;最底层还有LLVM层面的low-level codegen优化，负责最终生成优化的机器码。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第一层的模型优化减少了模型本身的总计算量，与底层优化正交。第二、第三层的推理优化归根到底都是硬件使能，只不过二层对几乎所有硬件都有效，三层则根据设备信息细化。从实现方法上讲，二、三层优化又可分为基于AI编译器的优化和手工优化。&lt;/p&gt;
&lt;h2 id=&#34;基于编译的优化&#34;&gt;基于编译的优化&lt;/h2&gt;
&lt;p&gt;编译，或者说DSLs + Optimizing Compilers，是解决领域问题优化的一个通用解法，为不同硬件提供可移植的优化。比如十年前就有Halide为图像和张量的并行计算提供了一个DSL+编译器，将算法规格本身和优化细节解耦。甚至更底层的gcc/llvm本身也是将算法和优化解耦的例子，在machine code codegen层面做的优化。&lt;/p&gt;
&lt;h3 id=&#34;ml-compilers的优势和劣势&#34;&gt;ML Compilers的优势和劣势&lt;/h3&gt;
&lt;p&gt;如今的ML compilers是这种基于编译的思路的延续&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;，其优势在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可移植性：硬件一方面在不断迭代更新，另一方面也不断有新硬件、新架构涌现。端边侧的设备要繁杂的多。Datacenter场景还好，但也面临N卡禁运，国产替代的问题，国产GPGPU还没有形成明确的一两家独大的格局&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;，因此适配新硬件是近未来必需解决的事情。&lt;/li&gt;
&lt;li&gt;将算法与优化清晰解耦：工程上可以提效，也有助于降低因复杂度爆炸而注入缺陷、使得项目逐渐失控、无法维护的风险。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其劣势在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不完备：在处理大多数场景、常规问题时性能不错，但总会遇到一些预料之外的edge cases，fallback to the slow path，性能骤然劣化，很难通过tweak DSL code生成更优的代码。&lt;/li&gt;
&lt;li&gt;不灵活：考虑到编译器codegen产物往往不那么human-readble，工程上针对edge cases、ad-hoc需求做灵活的手工优化就比较困难。&lt;/li&gt;
&lt;li&gt;难以真正击败专家手动优化：正如至今不存在能在性能上击败C/C++的函数式语言编译器，ML compiler也只是给出足够好的解，通常不及专家充分优化的C/C++实现。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ml-compilers的工作流程&#34;&gt;ML Compilers的工作流程&lt;/h3&gt;
&lt;p&gt;ML编译器的工作流是高层抽象到底层抽象的&lt;code&gt;lowering&lt;/code&gt;过程。ML编译器后端包含各种&lt;code&gt;passes&lt;/code&gt;，所谓pass就是lowering规则。最终会根据设备信息，形成一种硬件特化、张量特化的硬件算子描述，这种描述在TVM里叫做&lt;code&gt;schedule&lt;/code&gt;，在Triton里叫做&lt;code&gt;plan&lt;/code&gt;。再进一步CodeGen阶段，是从ML编译器自己的语言翻译到language compiler后端，比如LLVM IR，然后交由LLVM编译成可执行的machine code。&lt;/p&gt;
&lt;p&gt;MLIR中&lt;code&gt;dialects&lt;/code&gt;可对passes进行分层或分类，一个典型的dialects分层(见&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;)自上而下如是：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;OpGraph -&amp;gt; TSOWB(e.g. late hlo) -&amp;gt; CGASel -&amp;gt; HHO(e.g. Linalg) -&amp;gt; MHA(e.g. stripe/affine) -&amp;gt; HLTSIR(e.g. vector dialects) -&amp;gt; TSIR(e.g. llvm)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Triton的大致流程如下：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;面向用户的Python/C++的kernel代码
--&amp;gt; [ML Compiler前端，有时候可能只是某种动转静工具，forward一次，然后转写]
设备无关的 High-level IR
--&amp;gt; [ML Compiler后端Passes，图优化+算子选择+内存优化]
硬件特化的 Low-level IR [Schedule/Plan]
--&amp;gt; [ML Compiler后端Passes，把自己内部的Schedule/Plan翻译到LLVM IR]
LLVM IR 
--&amp;gt; [LLVM&amp;#39;s NVPTX back-end，进入Language Compilation层面]
PTX
--&amp;gt; [CUDA ptxas assembler]
CUBIN
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Intel MLIR graph compiler的lowering pipeline如下：&lt;/p&gt;
&lt;p&gt;Computation Graphs -&amp;gt; linalg &lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; -&amp;gt; layout propagation &lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; -&amp;gt; tiling &lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; -&amp;gt; fusion &lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt; -&amp;gt; micro kernel &lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt; -&amp;gt; vector &lt;sup id=&#34;fnref1:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; -&amp;gt; bufferization &lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt; -&amp;gt; memory planning -&amp;gt; LLVM IR -&amp;gt; 交由LLVM处理。&lt;/p&gt;
&lt;p&gt;上述lowering pipeline又可分为tensor-land和memref-land两个大的区块，bufferization之前都是tensor-land。在tensor-land，所有tensor操作都默认不是in-place的，哪怕是明显可以in-place的relu。在memref-land才会考虑内存访问的进一步优化。&lt;/p&gt;
&lt;h2 id=&#34;手工优化&#34;&gt;手工优化&lt;/h2&gt;
&lt;p&gt;很多时候，目标模型的架构是确定的，目标机器的架构也就固定几种，ML compilers的可移植性优势——自动算子绑定/算子选择的优势——就近乎不存在了。&lt;/p&gt;
&lt;p&gt;典型的例子是llama.cpp，和支撑它的ggml，llama.cpp+ggml通过一个人类可读的最小化C/C++项目，实现了各种精度的量化、自动微分、AVX/AVX2优化、Metal优化、flashatt算子、Multi-GPU pipeline并行等各个抽象层次上最直接有效的优化机制，最终也达成相当好的效果，尤其是在本地设备上。&lt;/p&gt;
&lt;p&gt;这种手动优化的系统优势在于系统是透明的，程序员可以读懂整个系统，精准定位到涉及某个问题的代码行，从而具备ML编译方案中不具备的灵活性，适合应对ad-hoc需求，适用于模型架构和硬件设备稳定的场景。&lt;/p&gt;
&lt;p&gt;手动优化的劣势是一旦出现新架构、新设备，就需要重写代码。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://cmbbq.github.io/posts/quantization_and_pruning&#34;&gt;Quantization and Pruning&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;TVM可以视为Halide在ML领域的&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;国产AI芯片处于混战阶段：华为Atlas系列、壁仞BR100、瑞芯微rk NPU、百度昆仑芯XPU、比特大陆（bm-se/sc）、寒武纪MLU、海光DCU、燧原GCU等。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://mlir.llvm.org/docs/Rationale/RationaleLinalgDialect/&#34;&gt;Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;&lt;code&gt;Linalg&lt;/code&gt; is a DSL(a high-level MLIR dialect) for expressing linear algebra operations in MLIR, designed to solve the High-level Hierarchical Optimization (HHO box) in MLIR and to interoperate nicely within a Mixture Of Expert Compilers environment (i.e. the CGSel box).&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;把layout调整好，比如$M\times N$调成$32\times 32$分块。&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;算子分块，或者说矩阵乘法分块层，属于scf dialect，即structured control flow，之前叫LoopOps。&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;算子融合，比如elementwise+reduce的op fusion。&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;micro kernel一般是手写的，比如分块后的最小粒度的matmul，一般是64*64的，直接交给编译器是做不好的，要对不同硬件要用不同指令，不同顺序，不同寄存器。&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;Bufferization in MLIR is the process of converting ops with tensor semantics to ops with memref semantics. 这一阶段会尽可能尝试将一些tensor计算的内存占用in-place化，终极目标是用更少的内存，减少copy次数。&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Paged Attention</title>
      <link>https://cmbbq.github.io/posts/paged-attention/</link>
      <pubDate>Tue, 30 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/paged-attention/</guid>
      <description>在LLM serving场景，通过恰当的batching积攒足够多的请求，可提升LLM吞吐。但每个请求对应的KV Cache非常巨大——在原始实现中，KV Cache需要为max_tokens预留内存，但每个请求实际上携带的tokens数目普遍远小于max_tokens，造成大量内存浪费和反复的动态内存分配。
PagedAttention1的目标就是消除这种内存浪费、在请求内部和请求之间灵活共享一些KV cache。vLLM通过PagedAttention达到此前sota的FasterTransformer和Orca的2~4倍吞吐。
此前，朴素的KV Cache实现如下图所示，简短的prompt和并不长的当前iteration只占据了7+4个slots，剩余2038个slots不得不预留在内存里以支撑最大序列长达2048的承诺（采样结束才能知道实际tokens数目），在这之后才能是下一个请求的slots，中间预留的部分就完全浪费掉了。
PagedAttention把连续的kv存在不连续的内存空间上，借用类似页表的机制（引入一个block_table）规避内存碎片化问题。具体来讲，PagedAttention把KV cache分区成若干个K blocks和V blocks，每个K/V block容纳固定数量tokens所对应的K/V向量，因此attention计算也被转化为blockwise计算。这和FlashAttention有点像，不过应用的尺度不同，前者是为了大规模serving时克服碎片化、按需取用，后者是为了单次self-attention计算能全部fit in SRAM。
这个物理kv blocks显然是支持多个请求复用的。如下图所示，为每个请求维护一个小的block table即可。
W. Kwon, et al. Efficient Memory Management for Large Language Model Serving with PagedAttention. [pdf]&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description>
      <content>&lt;p&gt;在LLM serving场景，通过恰当的batching积攒足够多的请求，可提升LLM吞吐。但每个请求对应的KV Cache非常巨大——在原始实现中，KV Cache需要为&lt;code&gt;max_tokens&lt;/code&gt;预留内存，但每个请求实际上携带的tokens数目普遍远小于&lt;code&gt;max_tokens&lt;/code&gt;，造成大量内存浪费和反复的动态内存分配。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PagedAttention&lt;/code&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;的目标就是消除这种内存浪费、在请求内部和请求之间灵活共享一些KV cache。vLLM通过&lt;code&gt;PagedAttention&lt;/code&gt;达到此前sota的&lt;code&gt;FasterTransformer&lt;/code&gt;和&lt;code&gt;Orca&lt;/code&gt;的2~4倍吞吐。&lt;/p&gt;
&lt;p&gt;此前，朴素的KV Cache实现如下图所示，简短的prompt和并不长的当前iteration只占据了7+4个slots，剩余2038个slots不得不预留在内存里以支撑最大序列长达2048的承诺（采样结束才能知道实际tokens数目），在这之后才能是下一个请求的slots，中间预留的部分就完全浪费掉了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/naive_kv_cache.png&#34; alt=&#34;naive_kv_cache&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PagedAttention&lt;/code&gt;把连续的kv存在不连续的内存空间上，借用类似页表的机制（引入一个block_table）规避内存碎片化问题。具体来讲，&lt;code&gt;PagedAttention&lt;/code&gt;把KV cache分区成若干个K blocks和V blocks，每个K/V block容纳固定数量tokens所对应的K/V向量，因此attention计算也被转化为blockwise计算。这和&lt;code&gt;FlashAttention&lt;/code&gt;有点像，不过应用的尺度不同，前者是为了大规模serving时克服碎片化、按需取用，后者是为了单次self-attention计算能全部fit in SRAM。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/block_table.png&#34; alt=&#34;block_table&#34;&gt;&lt;/p&gt;
&lt;p&gt;这个物理kv blocks显然是支持多个请求复用的。如下图所示，为每个请求维护一个小的block table即可。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/vllm_two_requests.png&#34; alt=&#34;vllm_two_requests&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;W. Kwon, et al. Efficient Memory Management for Large Language Model Serving with PagedAttention. &lt;a href=&#34;https://arxiv.org/pdf/2309.06180&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Flash Attention</title>
      <link>https://cmbbq.github.io/posts/flash-attention/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/flash-attention/</guid>
      <description>由于self-attention的时、空复杂度都是序列长的平方，长序列LLM和高分辨率ViT都是非常吃内存的。
此前针对self-attention的优化大多是近似计算，其核心是优化FLOP，把理论上的时间复杂度降低到O(N)，但这并不能有效加速self-attention，因为该操作（以及transformer中多数操作）的实际瓶颈在访存——更准确地说是HBM和SRAM之间的IO。
FlashAttention的原理就是基于tiling，确保内层循环计算fit in SRAM，减少了HBM和SRAM之间的IO频次，因而真切有效地提升了transformer性能，解锁了更长的context，迅速在各种高性能框架中得到应用。
传统的Self-Attention实现 Attention Layer具有将局部信息和张量中较远位置的信息结合起来的能力——通过为所得张量的每个组件与输入张量的每个组件计算注意力得分，不受局部性约束地，在整个张量范围内对特征进行平均。
给定$N^Q\times D^{QK}$维queries张量$Q$，$N^{KV}\times D^{QK}$维keys张量$K$，$N^{KV}\times D^V$维values张量$V$，通过Attention操作$att(K,Q,V)$计算得到$N^Q\times D^V$维的张量Y：
$$Y = att(K,Q,V) = \underbrace{softargmax(\frac{QK^T}{\frac{1}{\sqrt{D^{QK}}}})}_A V$$
整个过程分两步，第一步先计算每个query index $q$和每个key index $k$的注意力得分，即queries和keys点乘后的softargmax结果：$A_{q,k} = \frac{exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_k ) }{\sum_l exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_l)}$，其中$\frac{1}{\sqrt{D^{QK}}}$是一个缩放参数，用于保证取值范围在$D^{QK}$变大时大体不变。
得到注意力得分$A_{q,k}$后，再进行第二步计算：$Y_q = \sum_k A_{q,k}V_k$。注意力得分即queries和keys之间的匹配程度，匹配程度越高，该权重就越高。如果某个query和某个key匹配度达到极限，注意力得分接近1，则直接拿到这个key对应的value。如果和好几个key都有中等水准的匹配，则按注意力得分做加权平均。
FlashAttention的IO优化 FlashAttention注意到事实上并不需要完整的输入K、V、Q，可以分批读入、分批计算、分批把结果写回O，即所谓tiling：
在外层循环，遍历K、V矩阵。每次只需加载一个block的 $K^T$ 和 $V$ 到片上SRAM中。 在内层循环，遍历Q的各个block，加载到SRAM中，进行 $att(K,Q,V)$ 计算，把局部结果写回HBM上 $N\times d$ 的结果矩阵1。 对softmax归一化因子做相应调整，即可保证最后加起来得到的最终结果和标准实现等价，具体代数见2附录。 设置K、V的block size为 $\lceil \frac{M}{4d} \rceil$，Q、O的block size为$min(\lceil \frac{M}{4d} \rceil, d)$3。 此外，对于训练负载来说，FlashAttention还在反向传播中复用前馈过程暂存的softmax归一化因子$\frac{1}{\sqrt{D^{QK}}}$，这也比从HBM读$N\times N$的巨大attention中间矩阵要快得多。这可被视作selective gradient checkpointing。
FlashAttention2: 改进并行和工作切分 相比GEMM，FlashAttention只达到了25~40%理论FLOPs/s，可优化空间巨大。FlashAttention24在原版基础上做了并行化和工作切分优化。
减少非矩阵乘算子，因为GPU矩阵乘是高度优化的，其他算子与之差距非常大。 避免循环中算O时每次都rescale，而是在算最终结果时施加softmax归一化因子。 对反向传播时保持的状态进行了精简。 在不同线程块[^8]上进行并行计算，从而充分利用GPU资源。 原版一个线程块处理一个head/一个batch，每个线程块跑在一个SM上。不过在长序列场景，head数目和batch size可能都偏小，导致二者相乘后都未必能打满A100的128个SM。 显而易见可以并行的部分是外层循环，可以随便调度到不同线程块上，相互之间完全没有通信需求。 反向传播时并行化外层循环也只有dQ更新时需要简单的通信/同步，这是一个顺序不重要的加法，atomic add足以解决。 既然有了线程块，就要考虑在每个线程块内，不同warps之间如何进行工作的切分。 如上图所示，FlashAttention的切分方式导致内层循环中各个warp都需要把结果写到共享内存且做一个同步加，存在一定的通信开销，而FlashAttention2的切分方式可以保证warp之间完全没有通信需求。</description>
      <content>&lt;p&gt;由于&lt;code&gt;self-attention&lt;/code&gt;的时、空复杂度都是序列长的平方，长序列LLM和高分辨率ViT都是非常吃内存的。&lt;/p&gt;
&lt;p&gt;此前针对&lt;code&gt;self-attention&lt;/code&gt;的优化大多是近似计算，其核心是优化FLOP，把理论上的时间复杂度降低到O(N)，但这并不能有效加速&lt;code&gt;self-attention&lt;/code&gt;，因为该操作（以及transformer中多数操作）的实际瓶颈在访存——更准确地说是HBM和SRAM之间的IO。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;FlashAttention&lt;/code&gt;的原理就是基于&lt;code&gt;tiling&lt;/code&gt;，确保内层循环计算fit in SRAM，减少了HBM和SRAM之间的IO频次，因而真切有效地提升了transformer性能，解锁了更长的context，迅速在各种高性能框架中得到应用。&lt;/p&gt;
&lt;h2 id=&#34;传统的self-attention实现&#34;&gt;传统的Self-Attention实现&lt;/h2&gt;
&lt;p&gt;Attention Layer具有将局部信息和张量中较远位置的信息结合起来的能力——通过为所得张量的每个组件与输入张量的每个组件计算注意力得分，不受局部性约束地，在整个张量范围内对特征进行平均。&lt;/p&gt;
&lt;p&gt;给定$N^Q\times D^{QK}$维queries张量$Q$，$N^{KV}\times D^{QK}$维keys张量$K$，$N^{KV}\times D^V$维values张量$V$，通过&lt;code&gt;Attention操作&lt;/code&gt;$att(K,Q,V)$计算得到$N^Q\times D^V$维的张量Y：&lt;/p&gt;
&lt;p&gt;$$Y = att(K,Q,V) = \underbrace{softargmax(\frac{QK^T}{\frac{1}{\sqrt{D^{QK}}}})}_A V$$&lt;/p&gt;
&lt;p&gt;整个过程分两步，第一步先计算每个query index $q$和每个key index $k$的注意力得分，即queries和keys点乘后的&lt;code&gt;softargmax&lt;/code&gt;结果：$A_{q,k} = \frac{exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_k ) }{\sum_l exp(\frac{1}{\sqrt{D^{QK}}} Q_q \cdot K_l)}$，其中$\frac{1}{\sqrt{D^{QK}}}$是一个缩放参数，用于保证取值范围在$D^{QK}$变大时大体不变。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/attention.png&#34; alt=&#34;att&#34;&gt;&lt;/p&gt;
&lt;p&gt;得到注意力得分$A_{q,k}$后，再进行第二步计算：$Y_q = \sum_k A_{q,k}V_k$。注意力得分即queries和keys之间的匹配程度，匹配程度越高，该权重就越高。如果某个query和某个key匹配度达到极限，注意力得分接近1，则直接拿到这个key对应的value。如果和好几个key都有中等水准的匹配，则按注意力得分做加权平均。&lt;/p&gt;
&lt;h2 id=&#34;flashattention的io优化&#34;&gt;FlashAttention的IO优化&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;FlashAttention&lt;/code&gt;注意到事实上并不需要完整的输入K、V、Q，可以分批读入、分批计算、分批把结果写回O，即所谓&lt;code&gt;tiling&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在外层循环，遍历K、V矩阵。每次只需加载一个block的 $K^T$ 和 $V$ 到片上SRAM中。&lt;/li&gt;
&lt;li&gt;在内层循环，遍历Q的各个block，加载到SRAM中，进行 $att(K,Q,V)$ 计算，把局部结果写回HBM上 $N\times d$ 的结果矩阵&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。&lt;/li&gt;
&lt;li&gt;对softmax归一化因子做相应调整，即可保证最后加起来得到的最终结果和标准实现等价，具体代数见&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;附录。&lt;/li&gt;
&lt;li&gt;设置K、V的block size为 $\lceil \frac{M}{4d} \rceil$，Q、O的block size为$min(\lceil \frac{M}{4d} \rceil, d)$&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/fast_attention.png&#34; alt=&#34;fast_att&#34;&gt;&lt;/p&gt;
&lt;p&gt;此外，对于训练负载来说，&lt;code&gt;FlashAttention&lt;/code&gt;还在反向传播中复用前馈过程暂存的softmax归一化因子$\frac{1}{\sqrt{D^{QK}}}$，这也比从HBM读$N\times N$的巨大attention中间矩阵要快得多。这可被视作selective gradient checkpointing。&lt;/p&gt;
&lt;h2 id=&#34;flashattention2-改进并行和工作切分&#34;&gt;FlashAttention2: 改进并行和工作切分&lt;/h2&gt;
&lt;p&gt;相比GEMM，&lt;code&gt;FlashAttention&lt;/code&gt;只达到了25~40%理论FLOPs/s，可优化空间巨大。&lt;code&gt;FlashAttention2&lt;/code&gt;&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;在原版基础上做了并行化和工作切分优化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;减少非矩阵乘算子，因为GPU矩阵乘是高度优化的，其他算子与之差距非常大。
&lt;ul&gt;
&lt;li&gt;避免循环中算O时每次都rescale，而是在算最终结果时施加softmax归一化因子。&lt;/li&gt;
&lt;li&gt;对反向传播时保持的状态进行了精简。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;在不同线程块[^8]上进行并行计算，从而充分利用GPU资源。
&lt;ul&gt;
&lt;li&gt;原版一个线程块处理一个head/一个batch，每个线程块跑在一个SM上。不过在长序列场景，head数目和batch size可能都偏小，导致二者相乘后都未必能打满A100的128个SM。&lt;/li&gt;
&lt;li&gt;显而易见可以并行的部分是外层循环，可以随便调度到不同线程块上，相互之间完全没有通信需求。&lt;/li&gt;
&lt;li&gt;反向传播时并行化外层循环也只有dQ更新时需要简单的通信/同步，这是一个顺序不重要的加法，atomic add足以解决。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;既然有了线程块，就要考虑在每个线程块内，不同warps之间如何进行工作的切分。
&lt;img src=&#34;https://cmbbq.github.io/img/work_part.png&#34; alt=&#34;work_part&#34;&gt;&lt;/p&gt;
&lt;p&gt;如上图所示，&lt;code&gt;FlashAttention&lt;/code&gt;的切分方式导致内层循环中各个warp都需要把结果写到共享内存且做一个同步加，存在一定的通信开销，而&lt;code&gt;FlashAttention2&lt;/code&gt;的切分方式可以保证warp之间完全没有通信需求。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;其中，d为head dimension。N为序列长度。$N \gg d$。GPT2中 $N=1024，d=64$。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;T. Dao, et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. &lt;a href=&#34;https://arxiv.org/pdf/2205.14135&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;其中，M为SRAM大小。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. &lt;a href=&#34;https://arxiv.org/pdf/2307.08691&#34;&gt;[pdf]&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Linkers &amp; Loaders</title>
      <link>https://cmbbq.github.io/posts/linkers-and-loaders/</link>
      <pubDate>Mon, 04 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/linkers-and-loaders/</guid>
      <description>链接器或加载器的基本工作是绑定——把抽象的名字绑定到更具体的名字上，比如把getline函数绑定到“.text的第612字节处”。
地址绑定的历史 打孔卡带(punched card/paper tape)计算机时代，程序员把符号程序手动汇编成机器码输入机器。代码中如果使用了名字（符号地址），也需程序员手动翻译成地址。因此代码里的任意一条指令的增减都可能影响到机器码中的所有地址。
这是名字与地址过早绑定的恶果。汇编器允许程序员用符号名字写程序，解决了这一问题。
打孔卡带时代也已经有了子程序和库的概念，当时把一些子程序分类存放在卡带里，主程序使用子程序时就需要加载并重排子程序卡带。这个过程实际上就是手动的library search和重定位。
在操作系统出现前，每个程序都默认把整个机器内存空间独享，自然可以用固定的内存地址，毕竟机器的所有地址都是可用的。操作系统出现后，程序必需与操作系统、甚至其他程序共享内存空间，实际地址在操作系统加载程序成后才能知道，这又将地址绑定从链接时延后至加载时，因此重定位加载器从链接器中独立出来。链接器负责部分地址绑定，在每个程序内部做相对地址重分配；加载器负责重定位，进行最终的地址分配。
早期内存非常紧张，程序体量很快超过了内存上限，因此链接器提供了一种overlay机制，允许不同部分的程序共享同一块内存。直到90年代出现虚拟内存之后这个机制才消失。硬件重定位和虚拟内存的出现使链接器和加载器变得更简单。
计算机执行一个程序的多个副本时，这个程序的大部分内容实际上是可以共享的，因此引入了分段机制，将只读代码段和可写代码段分离，一个机器上只需要加载一份只读代码段。因此链接器需额外为每个代码段分配地址。
计算机即使在执行多个不同程序，这些程序往往也共享大量代码，因此出现了共享库。静态共享库不够灵活，库里的任何代码改动都要求重新链接。因此出现了动态共享库，令符号和分段并不绑定实际地址，而是推迟到程序运行时再进行绑定——甚至还能进一步延迟，在首次调用时才绑定。
链接 vs 加载 链接器负责符号解析，加载器负责程序加载，二者都可以做重定位，也存在三合一的linking loaders。
符号解析：所谓符号，就是程序调用子程序的媒依。链接器将诸如sqrt这样的函数名解析为库中的位置，并给调用方的代码打个补丁，让调用指令指向这个位置。 程序加载：加载即把程序拷贝到内存，也顺带做些设置内存保护位、安排虚拟内存映射等事。 重定位：编译器、汇编器为每个编译单元（文件）生成的程序地址从零开始。往往链接器把多个子程序拼成一个完整程序时会做一次重定位。这个完整程序的地址仍然从零开始，因此加载器将程序加载进内存后又会做一次重定位。 2-pass链接 链接和编译、汇编一样，也是个2-pass过程。链接器以对象文件、静态库、动态库、命令行参数为输入，输出可执行文件，如开启debug，还伴随生成debugger符号文件或load map。其中对象文件、静态库、动态库都是分段的（code/data），且至少有一个符号表，导出/导入一些符号。
链接器在1st pass中扫描所有输入文件，获取各分段大小，收集所有符号定义和引用，从而创建一个统合分段表和一个统合符号表，进而为每个符号分配位置，确定输出地址空间的分段大小和位置。
随后链接器在2nd pass中读取此前生成的对象文件，把所有符号引用替换为数值地址，把所有代码、数据中的内存地址调整成重定位后的分段地址，最后再给更新后的对象文件添加header、重定位段、符号表。
如果程序用到了动态链接，则符号表包含runtime linker解析动态符号所需的信息。通常链接器还会生成一些胶水代码，为调用动态链接库提供调用例程。
无论程序是否使用动态链接，符号表中总会提供一些供重链接和debug用的信息——很多对象格式都是可以重链接的，即允许生成的对象文件作为后续链接的输入。
对象文件 编译器和汇编器为源码生成的二进制码文件即对象文件，包含header、object code、重定向列表（一些链接时需重定向的位置）、全局符号表、debug信息等内容。
对象文件作为原材料，最终可用于三类最终产物：linkables、executables、loadables。
Linkables包含丰富的符号信息、重定位信息，object code也组织成细小的逻辑段，方便链接器后期加工，做符号解析和重定位。 Executables包含页对齐的object code（允许映射到虚拟内存），不需要提供动态链接需求之外的任何符号信息，也只需要提供很少或不提供重定位信息。其object code被组织成较大粒度的段或反映硬件执行环境的特定分段，往往分成read-only和read-write pages。 Loadables可能只需包含object code，也可能需提供完整的符号和重定位信息，这取决于系统runtime的实现。 典型的对象文件格式Unix a.out包含header、text section、data section、other sections。 其header（以BSD为例）包含text segment size、inited data size、uninited data size（BSS段）、symbol table size、entry point（起始地址）、text 重定位 size、data 重定位 size。
加载a.out时，操作系统先读header，获取各分段大小，再查找是否已存在共享代码段，若没有再新建一个，将text分段映射到内存空间，创建足够大的私有数据分段，把bss分段初始化为零，创建并映射栈分段（往往独立于数据段，因为堆和栈增长速度往往不同），把程序运行的初始参数入栈，最后设置寄存器并跳转到程序的起始地址。
为了减少不必要的paging，让对象文件能直接映射到4K的页，后续UNIX上出了一些pageable格式把header扩展到4K，把text分段的边界向上取整到下一个4K。这样做的缺点是不够紧凑，浪费磁盘空间。后来又出现了一些compact pageable格式，把header直接视作text分段的一部分（QMAGIC和ELF）。
a.out不支持重定位，也不支持C++的initializer/finalizer代码的特殊处理，被支持cross-compilation、动态链接等机制的ELF（Executable and Linking Format）取代。
ELF采用了DWARF作为其debugging格式，提供三种文件类型：relocatable、executable、shared object。</description>
      <content>&lt;p&gt;链接器或加载器的基本工作是绑定——把抽象的名字绑定到更具体的名字上，比如把&lt;code&gt;getline&lt;/code&gt;函数绑定到“.text的第612字节处”。&lt;/p&gt;
&lt;h2 id=&#34;地址绑定的历史&#34;&gt;地址绑定的历史&lt;/h2&gt;
&lt;p&gt;打孔卡带(punched card/paper tape)计算机时代，程序员把符号程序手动汇编成机器码输入机器。代码中如果使用了名字（符号地址），也需程序员手动翻译成地址。因此代码里的任意一条指令的增减都可能影响到机器码中的所有地址。&lt;/p&gt;
&lt;p&gt;这是名字与地址过早绑定的恶果。汇编器允许程序员用符号名字写程序，解决了这一问题。&lt;/p&gt;
&lt;p&gt;打孔卡带时代也已经有了子程序和库的概念，当时把一些子程序分类存放在卡带里，主程序使用子程序时就需要加载并重排子程序卡带。这个过程实际上就是手动的&lt;code&gt;library search&lt;/code&gt;和&lt;code&gt;重定位&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;在操作系统出现前，每个程序都默认把整个机器内存空间独享，自然可以用固定的内存地址，毕竟机器的所有地址都是可用的。操作系统出现后，程序必需与操作系统、甚至其他程序共享内存空间，实际地址在操作系统加载程序成后才能知道，这又将地址绑定从链接时延后至加载时，因此重定位加载器从链接器中独立出来。链接器负责部分地址绑定，在每个程序内部做相对地址重分配；加载器负责重定位，进行最终的地址分配。&lt;/p&gt;
&lt;p&gt;早期内存非常紧张，程序体量很快超过了内存上限，因此链接器提供了一种overlay机制，允许不同部分的程序共享同一块内存。直到90年代出现虚拟内存之后这个机制才消失。硬件重定位和虚拟内存的出现使链接器和加载器变得更简单。&lt;/p&gt;
&lt;p&gt;计算机执行一个程序的多个副本时，这个程序的大部分内容实际上是可以共享的，因此引入了分段机制，将只读代码段和可写代码段分离，一个机器上只需要加载一份只读代码段。因此链接器需额外为每个代码段分配地址。&lt;/p&gt;
&lt;p&gt;计算机即使在执行多个不同程序，这些程序往往也共享大量代码，因此出现了共享库。静态共享库不够灵活，库里的任何代码改动都要求重新链接。因此出现了动态共享库，令符号和分段并不绑定实际地址，而是推迟到程序运行时再进行绑定——甚至还能进一步延迟，在首次调用时才绑定。&lt;/p&gt;
&lt;h2 id=&#34;链接-vs-加载&#34;&gt;链接 vs 加载&lt;/h2&gt;
&lt;p&gt;链接器负责符号解析，加载器负责程序加载，二者都可以做重定位，也存在三合一的&lt;code&gt;linking loaders&lt;/code&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;符号解析：所谓符号，就是程序调用子程序的媒依。链接器将诸如sqrt这样的函数名解析为库中的位置，并给调用方的代码打个补丁，让调用指令指向这个位置。&lt;/li&gt;
&lt;li&gt;程序加载：加载即把程序拷贝到内存，也顺带做些设置内存保护位、安排虚拟内存映射等事。&lt;/li&gt;
&lt;li&gt;重定位：编译器、汇编器为每个编译单元（文件）生成的程序地址从零开始。往往链接器把多个子程序拼成一个完整程序时会做一次重定位。这个完整程序的地址仍然从零开始，因此加载器将程序加载进内存后又会做一次重定位。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2-pass链接&#34;&gt;2-pass链接&lt;/h2&gt;
&lt;p&gt;链接和编译、汇编一样，也是个2-pass过程。链接器以对象文件、静态库、动态库、命令行参数为输入，输出可执行文件，如开启debug，还伴随生成debugger符号文件或load map。其中对象文件、静态库、动态库都是分段的（code/data），且至少有一个符号表，导出/导入一些符号。&lt;/p&gt;
&lt;p&gt;链接器在1st pass中扫描所有输入文件，获取各分段大小，收集所有符号定义和引用，从而创建一个统合分段表和一个统合符号表，进而为每个符号分配位置，确定输出地址空间的分段大小和位置。&lt;/p&gt;
&lt;p&gt;随后链接器在2nd pass中读取此前生成的对象文件，把所有符号引用替换为数值地址，把所有代码、数据中的内存地址调整成重定位后的分段地址，最后再给更新后的对象文件添加header、重定位段、符号表。&lt;/p&gt;
&lt;p&gt;如果程序用到了动态链接，则符号表包含&lt;code&gt;runtime linker&lt;/code&gt;解析动态符号所需的信息。通常链接器还会生成一些胶水代码，为调用动态链接库提供调用例程。&lt;/p&gt;
&lt;p&gt;无论程序是否使用动态链接，符号表中总会提供一些供重链接和debug用的信息——很多对象格式都是可以重链接的，即允许生成的对象文件作为后续链接的输入。&lt;/p&gt;
&lt;h2 id=&#34;对象文件&#34;&gt;对象文件&lt;/h2&gt;
&lt;p&gt;编译器和汇编器为源码生成的二进制码文件即对象文件，包含header、object code、重定向列表（一些链接时需重定向的位置）、全局符号表、debug信息等内容。&lt;/p&gt;
&lt;p&gt;对象文件作为原材料，最终可用于三类最终产物：linkables、executables、loadables。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linkables包含丰富的符号信息、重定位信息，object code也组织成细小的逻辑段，方便链接器后期加工，做符号解析和重定位。&lt;/li&gt;
&lt;li&gt;Executables包含页对齐的object code（允许映射到虚拟内存），不需要提供动态链接需求之外的任何符号信息，也只需要提供很少或不提供重定位信息。其object code被组织成较大粒度的段或反映硬件执行环境的特定分段，往往分成read-only和read-write pages。&lt;/li&gt;
&lt;li&gt;Loadables可能只需包含object code，也可能需提供完整的符号和重定位信息，这取决于系统runtime的实现。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;典型的对象文件格式Unix a.out包含header、text section、data section、other sections。
其header（以BSD为例）包含text segment size、inited data size、uninited data size（BSS段）、symbol table size、entry point（起始地址）、text 重定位 size、data 重定位 size。&lt;/p&gt;
&lt;p&gt;加载a.out时，操作系统先读header，获取各分段大小，再查找是否已存在共享代码段，若没有再新建一个，将text分段映射到内存空间，创建足够大的私有数据分段，把bss分段初始化为零，创建并映射栈分段（往往独立于数据段，因为堆和栈增长速度往往不同），把程序运行的初始参数入栈，最后设置寄存器并跳转到程序的起始地址。&lt;/p&gt;
&lt;p&gt;为了减少不必要的paging，让对象文件能直接映射到4K的页，后续UNIX上出了一些pageable格式把header扩展到4K，把text分段的边界向上取整到下一个4K。这样做的缺点是不够紧凑，浪费磁盘空间。后来又出现了一些compact pageable格式，把header直接视作text分段的一部分（QMAGIC和ELF）。&lt;/p&gt;
&lt;p&gt;a.out不支持重定位，也不支持C++的initializer/finalizer代码的特殊处理，被支持cross-compilation、动态链接等机制的ELF（Executable and Linking Format）取代。&lt;/p&gt;
&lt;p&gt;ELF采用了DWARF作为其debugging格式，提供三种文件类型：relocatable、executable、shared object。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Relocatables可被编译器、汇编器创建，但需要进一步被链接后才能运行。&lt;/li&gt;
&lt;li&gt;Executables做完了重定向和静态符号解析，可直接映射至内存。&lt;/li&gt;
&lt;li&gt;Shared objects就是动态链接库，包含链接时所需的符号信息和运行时可执行的代码。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ELF被设计为具备双重属性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加载视角下是即将放入内存的loadable segments：在加载器看来，ELF是由program header描述的一组分段，无需关心分区。可执行分段只有廖廖几个。典型的BFD-ld或Gold链接的Linux ELF一般将其分为2个loadable分段：RE（只读可执行，包含.text，.rodata等）、RW（读写，包含.data，.bss等）。这样可以减少内核mmap次数到2次，但把只读数据放进只读可执行分段总归牺牲了安全性。比较新的Linux出于安全考虑分成3个分段R、RE、RW。将ELF header和.rodata放进R。更新一些的BFD-ld虽然把ELF header和.rodata分在R分段，但忘记合并这两个R了，导致出现4个loadable分段。&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;链接视角下是磁盘上的linkable sections：分区机制允许链接器对ELF进一步加工。单个分段由若干分区（section）组成。比如一个loadable read-only分段可以包含可执行代码、只读数据、动态链接符号这三个分区。Relocatables有分区表。Executables有ELF header表。Shared objects则兼具二者。典型的ELF可重定位程序包含十余个分区，例如.text、.data、.rodata、.bss、.rel.text（代码分区的重定位信息）、.rel.data、.rel.rodata、.init（C++全局变量构造函数）、.fini（C++全局变量析构函数）、.symtab（符号表）、.dynsym（动态库符号表）、.strtab、.dynstr、.interp（解释器路径）。可执行ELF和重定位ELF在格式上基本一致，只不过数据被重新安排，使文件能直接映射到内存，即pageable。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://149520725.v2.pressablecdn.com//wp-content/uploads/2018/01/Image5.png&#34; alt=&#34;sections_segments&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;静态库和动态库&#34;&gt;静态库和动态库&lt;/h2&gt;
&lt;p&gt;静态库本质上是一组对象文件，再稍微多加一点点信息（甚至有些系统直接把对象文件拼接起来就算是合法的静态链接库了）。&lt;/p&gt;
&lt;p&gt;链接器在处理完常规输入文件后，如果发现有的导入符号未定义，则遍历库，寻找该符号，将包含这些符号的文件链接起来。&lt;/p&gt;
&lt;p&gt;动态库让链接过程变得稍微复杂一点，将上述工作部分从链接转移到了加载时。链接器会在链接时找到能够解析未定义符号的那些动态库，但暂不链接任何代码，而是在输出文件里备注一下在哪个动态库可以招到特定符号，从而令加载器在加载程序时绑定这些动态库。&lt;/p&gt;
&lt;h2 id=&#34;链接需遵循abi&#34;&gt;链接需遵循ABI&lt;/h2&gt;
&lt;p&gt;每个操作系统都会给出一个ABI让程序使用，包括一些系统调用、封装系统调用的技术、内存地址规则、寄存器规则、调用约定。链接器必需是ABI compliant的，必需遵循ABI要求，提供特定静态数据的地址表，以符合调用约定的方式进行标准的函数调用。&lt;/p&gt;
&lt;p&gt;以Intel x86为例，提供6个32位通用寄存器EAX、EBX、ECX、EDX、ESI、EDI，两个寻址寄存器EBP、ESP，6个16位寄存器CS、DS、ES、FS、GS、SS。其中ESP是硬件栈指针，EBP通常是当前帧指针。&lt;/p&gt;
&lt;p&gt;x86架构里存在硬件栈，有硬件返回命令——硬件电路将返回地址push到栈上，并跳转至该地址。其他架构大多保存在寄存器里，因此x86上软件无需把寄存器里的返回地址放到主存的某个地方保存起来。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://stackoverflow.com/questions/57761007/why-an-elf-executable-could-have-4-load-segments&#34;&gt;Why an ELF executable could have 4 LOAD segments?&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Tech Talk: Wall is Coming</title>
      <link>https://cmbbq.github.io/posts/tech-talk-wall-is-coming/</link>
      <pubDate>Thu, 22 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/tech-talk-wall-is-coming/</guid>
      <description>内存墙和登纳德定律失效 “内存墙（the Memory Wall）”和“登纳德定律失效（the break down of Dennard Scaling）”是计算生态演化中的两个核心矛盾。
为了对抗Dennard Scaling的失效，计算硬件的架构从单核向多核、众核突围。 为了掩盖内存墙问题，内存层级（memory hierarchy）变得越来越深，off-chip互连带宽不得不迅速提高。 起初单核到多核是巨变，逼迫软件架构进行痛苦重构，并发编程问题木秀于林，因此被近20年的工业界实践+学术界研究集火秒了——如今我们有多线程编程范式、异步回调范式、goroutine式的有栈协程、C++/Rust的async/await无栈协程、lockfree/waitfree数据结构等诸多工具。 相较而言，这几十年来内存墙问题由于其隐蔽性、不紧急性、棘手性，不仅没得到妥善解决，反而根深蒂固，愈发遮掩不住，暴露在软件工程师面前，因此这次分享的重点是内存墙。
不过在进入正题之前，还需先介绍一下数据中心硬件和微处理器架构演化的些许背景。
21世纪数据中心硬件的演化简史 00s，Commodity Computing时代 远古时期我们并不会说“数据中心硬件”，因为还不存在现代意义上的互联网产业，也就没有现代意义上的数据中心。自然也就没有“for 大数据/云/Edge/AI”的营销话术，更多的是强调用廉价、不可靠、大规模的commodity hardware搭建分布式有状态系统，Google在这方面做了开创性的尝试。
相对IBM mainframe、super computer而言，当年x86的廉价是一目了然的。Google草创之初用的是奔腾2。 银行业、DAPRA成就了IBM power系列，如今Power9/10机器虽然被Xeon/EPYC完爆，仍然可以靠银行软件的祖宗之法不可变和政府订单苟延残喘，保有一份niche市场。
10s之前是x86微处理器完全不具备多核可扩展性的年代。
FSB(front-side bus)是罪魁祸首。下图架构中，内存总线和PCIe总线需共享一个FSB才能与CPU相连，导致FSB成为瓶颈，CPU数量不具备横向扩展性。 当年的PCIe还是1.0（03年初代PCIe），带宽和lane数都相当有限，即使强上多核，网络IO、磁盘IO也跟不上。 零零年代恰逢摩尔定律逐渐在单核语境失效，专注提升芯片性能在2004年后不再可行，硬件厂商不得不在架构上向多核方向突围。 一零年代初，多核时代 2012年的Intel Xeon E5-2600 V1 32nm Sandy Bridge是里程碑式的服务器产品，移除FSB（这个实际上在09年的Nehalem机器上就已经做了）、引入QPI/DMI取代FSB、PCIe2.0，使微架构获取多核可扩展性。
E5 family算是耳熟能详，虽然普遍要到寿命极限，但至今应该仍有很多公司在用。 当年买电脑时，所谓二代i3/i5/i7就是SandyBridge，和一代i5/i7的Nehalem有代差。
Sandy Bridge之后是Haswell，变化不大。Haswell之后是Broadwell，环状拓扑Broadwell Ring即得名于此。 再后面就是Skylake，开始冠上Scalable之名了，从多核走向众核，从近代走到现代。
一零年代末~二零年代初，众核时代 17年Intel推出了1st gen Xeon Scalable，Skylake，采用了Mesh Architecture。见Things are getting meshy 同时期AMD也推出了ENYC 7001，算是打破了Xeon的垄断局面。在US-TTP机房我们就有不少AMD机器。
Skylake的升级版Ice Lake并未顺利孵化，因为18年出了Meltdown/Spectre的大新闻（speculative execution的安全漏洞），于是在Skylake上修了漏洞，19年推出Cascade Lake作为2nd-Gen Xeon。原本的顺位继承者Ice Lake在21年姗姗来迟，变成了第三代。
Cooper Lake和Ice Lake同代，都被称为第三代，但实际上架构和Ice Lake不同，是基于Skylake改的，专为多socket(4~8s)设计，相比general-purpose的Ice Lake， Cooper Lake稍稍超出commodity hardware范畴，估计是想卖给特定的专用计算领域，用来替代老旧的UNIX系统，比如Oracle Solaris，IBM AIX。互联网场景下我们还是倾向于横向扩容而不是纵向扩容。</description>
      <content>&lt;h1 id=&#34;内存墙和登纳德定律失效&#34;&gt;内存墙和登纳德定律失效&lt;/h1&gt;
&lt;p&gt;“内存墙（the Memory Wall）”和“登纳德定律失效（the break down of Dennard Scaling）”是计算生态演化中的两个核心矛盾。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为了对抗Dennard Scaling的失效，计算硬件的架构从单核向多核、众核突围。&lt;/li&gt;
&lt;li&gt;为了掩盖内存墙问题，内存层级（memory hierarchy）变得越来越深，off-chip互连带宽不得不迅速提高。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;起初单核到多核是巨变，逼迫软件架构进行痛苦重构，并发编程问题木秀于林，因此被近20年的工业界实践+学术界研究集火秒了——如今我们有多线程编程范式、异步回调范式、goroutine式的有栈协程、C++/Rust的async/await无栈协程、lockfree/waitfree数据结构等诸多工具。
相较而言，这几十年来内存墙问题由于其隐蔽性、不紧急性、棘手性，不仅没得到妥善解决，反而根深蒂固，愈发遮掩不住，暴露在软件工程师面前，因此这次分享的重点是内存墙。&lt;/p&gt;
&lt;p&gt;不过在进入正题之前，还需先介绍一下数据中心硬件和微处理器架构演化的些许背景。&lt;/p&gt;
&lt;h1 id=&#34;21世纪数据中心硬件的演化简史&#34;&gt;21世纪数据中心硬件的演化简史&lt;/h1&gt;
&lt;h2 id=&#34;00scommodity-computing时代&#34;&gt;00s，Commodity Computing时代&lt;/h2&gt;
&lt;p&gt;远古时期我们并不会说“数据中心硬件”，因为还不存在现代意义上的互联网产业，也就没有现代意义上的数据中心。自然也就没有“for 大数据/云/Edge/AI”的营销话术，更多的是强调用廉价、不可靠、大规模的commodity hardware搭建分布式有状态系统，Google在这方面做了开创性的尝试。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;相对IBM mainframe、super computer而言，当年x86的廉价是一目了然的。Google草创之初用的是奔腾2。
银行业、DAPRA成就了IBM power系列，如今Power9/10机器虽然被Xeon/EPYC完爆，仍然可以靠银行软件的祖宗之法不可变和政府订单苟延残喘，保有一份niche市场。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/commodity_computing.png&#34; alt=&#34;commodity_computing&#34;&gt;&lt;/p&gt;
&lt;p&gt;10s之前是x86微处理器完全不具备多核可扩展性的年代。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;FSB&lt;/code&gt;(front-side bus)是罪魁祸首。下图架构中，内存总线和PCIe总线需共享一个&lt;code&gt;FSB&lt;/code&gt;才能与CPU相连，导致&lt;code&gt;FSB&lt;/code&gt;成为瓶颈，CPU数量不具备横向扩展性。&lt;/li&gt;
&lt;li&gt;当年的&lt;code&gt;PCIe&lt;/code&gt;还是1.0（03年初代&lt;code&gt;PCIe&lt;/code&gt;），带宽和lane数都相当有限，即使强上多核，网络IO、磁盘IO也跟不上。
&lt;img src=&#34;https://cmbbq.github.io/img/fsb.png&#34; alt=&#34;fsb&#34;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;零零年代恰逢摩尔定律逐渐在单核语境失效，专注提升芯片性能在2004年后不再可行，硬件厂商不得不在架构上向多核方向突围。
&lt;img src=&#34;https://cmbbq.github.io/img/clock.png&#34; alt=&#34;clock&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;一零年代初多核时代&#34;&gt;一零年代初，多核时代&lt;/h2&gt;
&lt;p&gt;2012年的Intel Xeon E5-2600 V1 32nm Sandy Bridge是里程碑式的服务器产品，移除&lt;code&gt;FSB&lt;/code&gt;（这个实际上在09年的Nehalem机器上就已经做了）、引入&lt;code&gt;QPI&lt;/code&gt;/&lt;code&gt;DMI&lt;/code&gt;取代&lt;code&gt;FSB&lt;/code&gt;、PCIe2.0，使微架构获取多核可扩展性。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;E5 family算是耳熟能详，虽然普遍要到寿命极限，但至今应该仍有很多公司在用。
当年买电脑时，所谓二代i3/i5/i7就是&lt;code&gt;SandyBridge&lt;/code&gt;，和一代i5/i7的&lt;code&gt;Nehalem&lt;/code&gt;有代差。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;Sandy Bridge&lt;/code&gt;之后是&lt;code&gt;Haswell&lt;/code&gt;，变化不大。&lt;code&gt;Haswell&lt;/code&gt;之后是&lt;code&gt;Broadwell&lt;/code&gt;，环状拓扑&lt;code&gt;Broadwell Ring&lt;/code&gt;即得名于此。
&lt;img src=&#34;https://cmbbq.github.io/img/broadwell_ring.png&#34; alt=&#34;clock&#34;&gt;
再后面就是Skylake，开始冠上Scalable之名了，从多核走向众核，从近代走到现代。&lt;/p&gt;
&lt;h2 id=&#34;一零年代末二零年代初众核时代&#34;&gt;一零年代末~二零年代初，众核时代&lt;/h2&gt;
&lt;p&gt;17年Intel推出了1st gen Xeon Scalable，&lt;code&gt;Skylake&lt;/code&gt;，采用了Mesh Architecture。见Things are getting meshy
同时期AMD也推出了ENYC 7001，算是打破了Xeon的垄断局面。在US-TTP机房我们就有不少AMD机器。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Skylake&lt;/code&gt;的升级版&lt;code&gt;Ice Lake&lt;/code&gt;并未顺利孵化，因为18年出了Meltdown/Spectre的大新闻（speculative execution的安全漏洞），于是在&lt;code&gt;Skylake&lt;/code&gt;上修了漏洞，19年推出&lt;code&gt;Cascade Lake&lt;/code&gt;作为2nd-Gen Xeon。原本的顺位继承者&lt;code&gt;Ice Lake&lt;/code&gt;在21年姗姗来迟，变成了第三代。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Cooper Lake&lt;/code&gt;和&lt;code&gt;Ice Lake&lt;/code&gt;同代，都被称为第三代，但实际上架构和&lt;code&gt;Ice Lake&lt;/code&gt;不同，是基于&lt;code&gt;Skylake&lt;/code&gt;改的，专为多socket(4~8s)设计，相比general-purpose的&lt;code&gt;Ice Lake&lt;/code&gt;， &lt;code&gt;Cooper Lake&lt;/code&gt;稍稍超出commodity hardware范畴，估计是想卖给特定的专用计算领域，用来替代老旧的UNIX系统，比如&lt;code&gt;Oracle Solaris&lt;/code&gt;，&lt;code&gt;IBM AIX&lt;/code&gt;。互联网场景下我们还是倾向于横向扩容而不是纵向扩容。&lt;/p&gt;
&lt;p&gt;目前数据中心应用的主力机型是&lt;code&gt;Ice Lake&lt;/code&gt;、&lt;code&gt;Cascade Lake&lt;/code&gt;机器，23~24年起计算/访存密集的场景则会逐步用到第四代Xeon：&lt;code&gt;Sapphire Rapids&lt;/code&gt;机器。
19年20年我们还零零散散有一些1st gen Xeon Scalable Gold机器，后来很快就汰换掉了。
&lt;img src=&#34;https://cmbbq.github.io/img/broadwell_ring.png&#34; alt=&#34;clock&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Ice Lake&lt;/code&gt;和&lt;code&gt;Cascade Lake&lt;/code&gt;都是monolithic mesh设计。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这里monolithic是相对于chiplet/tile-based而言的单个huge die承载many-core的范式；&lt;/li&gt;
&lt;li&gt;这里mesh是相对于此前E5时代Broadwell Ring环状拓扑而言的网状拓扑。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;二者差异主要在制程（10nm vs 14nm）、最大核心数、PCIe路数、内存通道数（单socket支持的DIMM&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;数 16 vs 12）。&lt;/p&gt;
&lt;p&gt;24年起开始交付的4th-Gen &lt;code&gt;Sapphire Rapids&lt;/code&gt;的芯片架构从mono-die转型为更类似AMD的multi-die(Intel自称是tile-based），微架构从sunny cove更新到golden cove（tpause指令可用于优化spinlock），配置相当华丽，支持先进互连协议（&lt;code&gt;PCIe5&lt;/code&gt;/&lt;code&gt;CXL&lt;/code&gt;），支持DDR5，新指令集&lt;code&gt;AMX&lt;/code&gt;，顶配还有3D堆叠的on-package HBM&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;h2 id=&#34;一些总结&#34;&gt;一些总结&lt;/h2&gt;
&lt;h3 id=&#34;硬件生态里生命和环境也是相互塑造的&#34;&gt;硬件生态里，生命和环境也是相互塑造的&lt;/h3&gt;
&lt;p&gt;PC用户成就了繁荣、易获得、标准化的x86 商用硬件市场。商用硬件集群刚好又适应互联网workloads（没有大量浮点数计算，integer server为主，但数据量极为庞大），才有了分布式系统支撑的现代数据中心，再然后才有硬件厂商为数据中心定制优化的服务器硬件，比如Scalable Xeon。&lt;/p&gt;
&lt;p&gt;PC玩家成就了Nvidia GPU，GPU恰好适应AI workloads，于是有了各种MLSys和GPGPU应用。然后才有Nvidia在加速计算方向上的投入。有了A100之后，ChatGPT训练就是在A100+IB network基础上量体裁衣做的大规模模型并行。随后又因为ChatGPT的轰动效应，反哺了高端GPU产业。&lt;/p&gt;
&lt;p&gt;生态位很难被人为设计、凭空创造，比如Intel Optane PMem，占据了非常合理的生态位，很多系统方向的研究都是靠PMem发的论文，因为它太合理了，弥补了磁盘和内存之间的空缺。但还是因为需求侧跟不上，在22年被砍掉了。究其根本，PMem在AI训推场景下没啥用，在主流的搜广推应用上不能带来显著成本优势，在交易场景和分析型场景要么比不上磁盘+内存，要么不值得投入人力去大改架构。&lt;/p&gt;
&lt;h3 id=&#34;现代硬件特征&#34;&gt;现代硬件特征&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;核心数众多&lt;/li&gt;
&lt;li&gt;设法缓解Memory Wall问题——近20年来DRAM cycle time每年缩减的速率与摩尔定律对比呈相对停滞，cache miss的后果愈发严重。
&lt;ul&gt;
&lt;li&gt;Memory hierarchy变深：cache变多，最新的顶配SPR机器还增加了on-package HBM，本地内存之外还有远端NUMA node，内存下面还可以有PMem、SSD，本地节点之外还有局域网/云端节点。总之，是通过增加层级隐藏Memory Wall问题。&lt;/li&gt;
&lt;li&gt;Off-package/chip-to-chip互连带宽提升：跟上核心数增多带来的IO需求，缓解批量读写场景下的Memory Wall问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;黑盒化和白盒化同时发生&#34;&gt;黑盒化和白盒化同时发生&lt;/h3&gt;
&lt;p&gt;现代硬件对工程能力提出了更苛刻的要求——通过off-CPU analysis、data-oriented cache-friendly设计、手动内存管理、甚至手动prefetching才能真正释放其性能潜力。&lt;/p&gt;
&lt;p&gt;但这和现代软件工程愈发简化的发展方向背道而驰——通过runtime、虚拟机、动态语言、微服务范式的职责划分、基于hypervisor的虚拟化和容器化等手段解放程序员心智。&lt;/p&gt;
&lt;p&gt;这种背道而驰使现代软件实践走向两条岔路，一个是以白盒化为手段的基础设施建设：更好的hypervisor、更好的mlsys、高性能检索、高性能存储、高性能网络，另一个是以黑盒化为目标的上层应用：利用虚拟化、容器化、微服务化、动态语言、runtime语言的便捷性提升productivity。&lt;/p&gt;
&lt;h1 id=&#34;现代硬件上的性能工程实践&#34;&gt;现代硬件上的性能工程实践&lt;/h1&gt;
&lt;h2 id=&#34;对优化空间的理解&#34;&gt;对优化空间的理解&lt;/h2&gt;
&lt;p&gt;性能工程即有系统方法论指导的软件优化实践。
可以从两个视角分解“优化任务”：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优化 = 减少算法总工作量 + 硬件使能&lt;/li&gt;
&lt;li&gt;优化 = 减少运行时间 = 减少CPU时间 + 减少阻塞时间
“算法改良”和“硬件使能”接近正交，“on-CPU time”和“off-CPU time”又大体互补，因此可以为优化空间$W$构造正交基{硬件使能$x$，算法优化$y$，算术密度$z$}，则$W = {[x,y,z] \in R^3 | 0 \le z \le 1 }$。&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;damn&#34;&gt;&lt;svg width=&#34;480&#34; height=&#34;420&#34;&gt;&lt;/svg&gt;&lt;/div&gt;
&lt;h2 id=&#34;推导出的一些heuristics&#34;&gt;推导出的一些Heuristics&lt;/h2&gt;
&lt;p&gt;基于对优化空间完整图景的理解，能推导出一些性能工程的Heuristics：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应首先确定程序的算术密度
&lt;ul&gt;
&lt;li&gt;为何区分on-CPU vs off-CPU至关重要？因为你的100%忙碌的CPU可能并不忙碌。CPU profiling仅描述完整图景的一部分，甚至一小部分。常用的CPU利用率这一指标具有欺骗性和迷惑性，实则既包括on-CPU计算也包括off-CPU阻塞。如果存在严重访存瓶颈，100%的CPU的superscalar pipeline里全是stall的空泡，CPU的各类算术逻辑单元、SIMD/AMX等专用计算硬件都在空等。&lt;/li&gt;
&lt;li&gt;当然off-CPU过高也可能是磁盘/网络IO密集导致的，但这类IO-bound应用往往不是成本大头，还不足以兴师动众地做优化，除非是专门做存储或专门搞网络的infra部门才需要关心。此外还有可能是代码写得有问题，锁粒度太大，过度持有锁，同样造成off-CPU 比例过高。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如今的内存应被视为外设
&lt;ul&gt;
&lt;li&gt;曾经为慢速外设准备的数据结构，如今适合内存场景：
&lt;ul&gt;
&lt;li&gt;C++的有序map是用红黑树实现的，Rust选择了B+树，以便利用其更好的locality，因为如今的内存已经和几十年前的磁盘一样慢得不可容忍了。&lt;/li&gt;
&lt;li&gt;DashTable这种原本用在PMem（非易适性内存，内存带宽远低于DRAM）上的数据结构，如今被DragonFly拿来用在内存数据库上，性能远超Redis/Memcached。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;因此需将内存层级白盒化，充分发挥硬件潜力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;避免和编译器优化撞车
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;不需要用移位操作或其他汇编指令优化乘除法。乘以8必然会被自动优化成左移动3。除法也会被优化成乘加。下面的例子是非常古老的编译器对 volatile int y = x / 71的处理。编译器优化乘法还会用LEA指令，LEA原本旨在加速小结构体数组的成员地址计算，但实际上也能用来加速乘法，比如乘以5可以写成lea eax, [eax*4 + eax]，利用现成的电路就比较快，算是硬件使能领域里编译器帮你做好的工作。&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-assembly&#34; data-lang=&#34;assembly&#34;&gt;// volatile int y = x / 71;8b 0c 24        mov ecx, DWORD PTR _x$[esp+8] ; load x into ecx
mov eax, -423447479 ; magic happens starting here...
imul ecx            ; edx:eax = x * 0xe6c2b44903 d1           add edx, ecx        ; edx = x + edx
sar edx, 6          ; edx &amp;gt;&amp;gt;= 6 (with sign fill)

mov eax, edx        ; eax = edx
shr eax, 31         ; eax &amp;gt;&amp;gt;= 31 (no sign fill)
add eax, edx        ; eax += edx

mov DWORD PTR _y$[esp+8], eax
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可以放心去做手动SIMD。手动SIMD和LLVM自动向量化优化位于不同抽象层次，基本上也不必对LLVM的自动向量化抱有期望。短期内见不到程序语言和库层面对SIMD进行良好抽象的希望。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;手动SIMD需要程序员根据目标机器微架构型号选择合适的指令（不同微架构的各种simd指令的latency各不相同）；&lt;/li&gt;
&lt;li&gt;选择恰当的simd size(并非越大越好，而且不同size对应不同的shuffle)；&lt;/li&gt;
&lt;li&gt;还要处理最后不满一个batch的数据导致的各种各样的corner cases；&lt;/li&gt;
&lt;li&gt;对数据地址进行对齐或对不对齐数据进行容忍。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用最新的编译器，用O3，很多细节就不必手动优化，如Copy Ellison，Tail-recursion Elimination，甚至Mutually recursion Elimination，Inlining，多数Loop优化——Loop unrolling(+步长)/fission(逆fusion)/tiling(cache blocking)/unswitching(内层去分支化)/自动向量化/interchange。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;涉及到逻辑和具体应用场景，没有标准优化策略的优化仍需自己做，比如Loop fusion，调整递归粒度，调整编码策略（紧凑程度和编解码开销的tradeoff）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;访存优化的一些tricks&#34;&gt;访存优化的一些Tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如何度量算术密度或访存密度?
&lt;ul&gt;
&lt;li&gt;perf 或 perf_event_open: cache_miss/instructions, 或ipc&lt;/li&gt;
&lt;li&gt;Intel PCM&lt;/li&gt;
&lt;li&gt;eBPF tools，比如https://github.com/iovisor/bcc&lt;/li&gt;
&lt;li&gt;静态分析：load/store指令占比可粗略翻译算术/访存密度，但受缓存命中率影响较大。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如何根据内存配置设置合适的object padding？
&lt;ul&gt;
&lt;li&gt;内存配置中有两个影响应用层的重要概念：内存通道（memory channels）、内存列（memory ranks）。x86架构下内存通道和内存列在内存地址上interleaving，即均匀分布且递增。因此RAM可视为$n_chan \times n_rank$个block组成。其DIMM架构如下图。
[图片]&lt;/li&gt;
&lt;li&gt;具体的object padding方式可以参考下面这段伪码，其中64B是cache line size，也恰好是一个block的size。大体思路是先保证内存池里的地址都是64B的整数倍，再保证下一个对象的block id和n_chan*n_rank互质。&lt;/li&gt;
&lt;li&gt;这个padding不让对象的起始地址反复命中同一个channel或同一个rank，令下一个对象起始地址落入不同的channel/rank，充分利用不同内存通道、不同内存列，避免通道、列之间的负载不均，提升访存带宽。
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;static&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;object_align&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; obj_size)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; nchan &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_nchannel&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; nrank &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_nrank&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; new_obj_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (obj_size &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;63&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (&lt;span style=&#34;color:#a6e22e&#34;&gt;get_gcd&lt;/span&gt;(new_obj_size, nrank &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; nchan) &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                new_obj_size&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; new_obj_size &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cache优化：Cache line对齐避免false sharing；利用cache blocking。&lt;/li&gt;
&lt;li&gt;规避锁瓶颈
&lt;ul&gt;
&lt;li&gt;基于静态锁分析或ebpf off-CPU分析，找到过分粗粒度的锁和过度超期持有的锁。&lt;/li&gt;
&lt;li&gt;寻找更好的并发数据结构：无锁实现良莠不齐；Many-core scalability最好的永远是array。&lt;/li&gt;
&lt;li&gt;Kernel bypassing：规避某些带锁的内核实现，如用户态网络协议栈替代内核栈。&lt;/li&gt;
&lt;li&gt;Share-nothing：最极端的做法可以效仿seastar，每个核心只在自己的专用内存上执行单线程代码，尽量避免CPU-to-CPU traffic，将锁彻底从代码里消除。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In-register storage：尽量保证简单函数用到的参数、存放中间产物的容器足够小，可完全用寄存器存下，编译器自动就会把所有load/store优化掉。&lt;/li&gt;
&lt;li&gt;考虑使用预分配内存和栈上静态结构：不得不访存时，生命周期较长的对象可考虑使用预分配内存，临时容器可以考虑根据线上数据分布设计成按某种规则切分的静态结构，并放在栈上（栈内存分配只是栈指针移动，而malloc复杂的多，如果说默认版本的malloc还有全局锁，不够scalable）。&lt;/li&gt;
&lt;li&gt;考虑使用编译期evaluation和全局静态内存区。&lt;/li&gt;
&lt;li&gt;利用1GB huge pages存数据，相比4KB默认页，hugepage所需的page table entry总数大大减少，可显著减小page table size和tlb size、降低tlb miss和page table walk开销，提升内存分配的连续性和内存访问的局部性，这些都有助于提升内存带宽。&lt;/li&gt;
&lt;li&gt;利用4MB large pages存代码，.text segment也可以用更大的页（不过最大只支持4MB），ITLB和DTLB一样，miss也会造成stall。ITLB问题的诊断可借助https://github.com/intel/iodlr，解决方法把.text移动到已有的large pages里&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;或静态链接并使用libhugetlbfs库。目前尚未看到该优化在真实项目中落地，但看起来相当promising，可参考&lt;a href=&#34;https://www.intel.com/content/dam/develop/external/us/en/documents/runtimeperformanceoptimizationblueprint-largecodepages-q1update.pdf&#34;&gt;Runtime Performance Optimization Blueprint: Large Code Pages&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;尊重NUMA拓扑，避免远端内存访问，即UPI traffic。
&lt;img src=&#34;https://cmbbq.github.io/img/NUMA.png&#34; alt=&#34;numa&#34;&gt;&lt;/li&gt;
&lt;li&gt;利用新架构特性和新的指令集扩展，如基于AMX实现GEMM的精度和性能优化广泛用在各种训推框架，AVX512_IFMA指令扩展做大数乘法已被用在新版本的OpenSSL中，以及基于QAT（QuickAssist）对AES、RSA、ECC等密码学应用做硬件加速。&lt;/li&gt;
&lt;li&gt;利用prefetching让cache变得更聪明。
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;所谓hardware prefetching就是从内存预取数据到cache（通常是LLC）。Hardware prefetcher有简单的stride pattern识别逻辑，比如a,a+2,a+4,a+6这种loop是可以被识别的。没必要刻意触发hardware prefetching，正常的代码都可以触发。但需避免误触发hardware prefetching——比如一个横跨多个cache line的大结构体其实只需要访问它的前几个field，但hardware prefetcher误以为还要接着读，就会造成cache pollution。稍微调整下这几个fields的访问顺序，破坏掉constant stride pattern即可。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;找到合适的timing使用software prefetching，预取数据会带来巨大吞吐性能提升，也能用来隐藏latency，但究竟何时预取，预取哪些数据只能靠反复尝试。而一旦timing选错了反而造成cache污染，导致性能下降。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;尽量让prefetch指令分散（最好和load也分散），夹杂在计算指令中间。如果连续prefetch，那和一堆load一样，也会造成空泡。&lt;/li&gt;
&lt;li&gt;选择合适的PSD(prefetch scheduling distance)，即提前几个iteration预取。对计算量大的loop，可以提前1个iteration，对于计算量小的，可能要提前多个iteration。下面这个例子PSD=3。&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-assembly&#34; data-lang=&#34;assembly&#34;&gt;top_loop:
prefetchnta [edx + esi + 128*3]
prefetchnta [edx*4 + esi + 128*3]
movaps xmm1, [edx + esi] 
movaps xmm2, [edx*4 + esi]
movaps xmm3, [edx + esi + 16] 
movaps xmm4, [edx*4 + esi + 16]
add esi, 128 
cmp esi, ecx 
jl top_loop
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;双层循环时，需注意弥补内外循环切换时的空泡，为外层循环也做prefetch。&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;; j&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;) { 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    prefetch a[i][j&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;// 最后一次iteration，不需要prefetch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    computation a[i][j] &lt;span style=&#34;color:#75715e&#34;&gt;// 第一次a[i+1][j]未预取，会miss
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;} 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// 优化后
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (j &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; j &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt;; j&lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    prefetch a[i][j&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    computation a[i][j]  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prefetch a[i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]  &lt;span style=&#34;color:#75715e&#34;&gt;// 提前准备好 a[i][0]，否则会在a[i][j+8]阻塞
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;computation a[i][j] &lt;span style=&#34;color:#75715e&#34;&gt;// 最后一个iteration单独处理，因为它不需要prefetch。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;利用先进互连技术，如&lt;code&gt;CXL&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/spr-cxl.png&#34; alt=&#34;cxl&#34;&gt;&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;
svg {
    box-shadow: 0 0 10px #999;
    border-radius: 5px;
}
&lt;/style&gt;
&lt;script type=&#34;module&#34;&gt;
import {
  drag,
  color,
  select,
  range,
  randomUniform,
  randomNormal,
  scaleOrdinal,
  selectAll,
  schemePastel1,
} from &#34;https://cdn.skypack.dev/d3@7.8.5&#34;;
import {
    gridPlanes3D,
    points3D,
    lineStrips3D,
} from &#34;https://cdn.skypack.dev/d3-3d@1.0.0&#34;;
document.addEventListener(&#34;DOMContentLoaded&#34;, () =&gt; {
    console.log(&#34;dom loaded, starts to draw svg ...&#34;);
    const width = 480;
    const height = 420;
    const origin = { x: width/2, y: height/2 };
    const offset = origin.x - origin.y;
    const j = 10;
    const scale = 20;
    const key = (d) =&gt; d.id;
    const startAngle = Math.PI/2;
    // const startAngle = 0;
    const colorScale = scaleOrdinal(schemePastel1);
    let scatter = [];
    let yLine = [];
    let xLine = [];
    let zLine = [];
    let xGrid = [];
    let beta = 0;
    let alpha = 0;
    let mx, my, mouseX = 0, mouseY = 0;
    const svg = select(&#34;svg&#34;)
        .call(
          drag()
            .on(&#34;drag&#34;, dragged)
            .on(&#34;start&#34;, dragStart)
            .on(&#34;end&#34;, dragEnd)
        )
        .append(&#34;g&#34;);
    const grid3d = gridPlanes3D()
        .rows(20)
        .origin(origin)
        .rotateY(startAngle)
        .rotateX(-startAngle)
        .scale(scale);
  const points3d = points3D()
    .origin(origin)
    .rotateY(startAngle)
    .rotateX(-startAngle)
    .scale(scale);
  const yScale3d = lineStrips3D()
      .origin(origin)
      .rotateY(startAngle)
      .rotateX(-startAngle)
      .scale(scale);
  const xScale3d = lineStrips3D()
      .origin(origin)
      .rotateY(startAngle)
      .rotateX(-startAngle)
      .scale(scale);
  const zScale3d = lineStrips3D()
      .origin(origin)
      .rotateY(startAngle)
      .rotateX(-startAngle)
      .scale(scale);
  function processData(data, tt, recolor) {
    /* ----------- GRID ----------- */
    const xGrid = svg.selectAll(&#34;path.grid&#34;).data(data[0], key);
    xGrid
      .enter()
      .append(&#34;path&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d grid&#34;)
      .merge(xGrid)
      .attr(&#34;stroke&#34;, &#34;black&#34;)
      .attr(&#34;stroke-width&#34;, 0.3)
      .attr(&#34;fill&#34;, (d) =&gt; (d.ccw ? &#34;#eee&#34; : &#34;#aaa&#34;))
      .attr(&#34;fill-opacity&#34;, 0.7)
      .attr(&#34;d&#34;, grid3d.draw);
    xGrid.exit().remove();
    /* ----------- POINTS ----------- */
    const points = svg.selectAll(&#34;circle&#34;).data(data[1], key);
    function GetColor(x, y){
      // console.log(&#34;x: %d, y: %d&#34;, x, y);
      // return (x &gt; 0) ? 5 : -5 + (y &gt; 0) ? 3 : -3;
      if (x &gt;= 0 &amp;&amp; y &gt;= 0) return schemePastel1[0];
      if (x &lt; 0 &amp;&amp; y &gt;= 0) return schemePastel1[1];
      if (x &lt; 0 &amp;&amp; y &lt; 0) return schemePastel1[2];
      if (x &gt;= 0 &amp;&amp; y &lt; 0) return schemePastel1[3];
    }
    if(recolor){
      points
      .enter()
      .append(&#34;circle&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d&#34;)
      .attr(&#34;opacity&#34;, 0)
      .attr(&#34;cx&#34;, posPointX)
      .attr(&#34;cy&#34;, posPointY)
      .merge(points)
      .transition()
      .duration(tt)
      .attr(&#34;r&#34;, 3)
      .attr(&#34;stroke&#34;, (d) =&gt; color(colorScale(d.id)).darker(3))
      .attr(&#34;fill&#34;, (d) =&gt; GetColor(d.projected.x - origin.x, d.projected.y - origin.y))
      .attr(&#34;opacity&#34;, 1)
      .attr(&#34;cx&#34;, posPointX)
      .attr(&#34;cy&#34;, posPointY);
    }else{
      points
      .enter()
      .append(&#34;circle&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d&#34;)
      .attr(&#34;opacity&#34;, 0)
      .attr(&#34;cx&#34;, posPointX)
      .attr(&#34;cy&#34;, posPointY)
      .merge(points)
      .transition()
      .duration(tt)
      .attr(&#34;r&#34;, 3)
      .attr(&#34;stroke&#34;, (d) =&gt; color(colorScale(d.id)).darker(3))
      .attr(&#34;opacity&#34;, 1)
      .attr(&#34;cx&#34;, posPointX)
      .attr(&#34;cy&#34;, posPointY);
    }
    points.exit().remove();
    /* ----------- x-Scale ----------- */
    const xScale = svg.selectAll(&#34;path.xScale&#34;).data(data[3]);
    xScale
      .enter()
      .append(&#34;path&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d xScale&#34;)
      .merge(xScale)
      .attr(&#34;stroke&#34;, &#34;black&#34;)
      .attr(&#34;stroke-width&#34;, 1.5)
      .attr(&#34;d&#34;, xScale3d.draw);
    xScale.exit().remove();
    /* ----------- y-Scale ----------- */
    const yScale = svg.selectAll(&#34;path.yScale&#34;).data(data[2]);
    yScale
      .enter()
      .append(&#34;path&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d yScale&#34;)
      .merge(yScale)
      .attr(&#34;stroke&#34;, &#34;black&#34;)
      .attr(&#34;stroke-width&#34;, 1.5)
      .attr(&#34;d&#34;, yScale3d.draw);
    yScale.exit().remove();
    /* ----------- z-Scale ----------- */
    const zScale = svg.selectAll(&#34;path.zScale&#34;).data(data[4]);
    zScale
      .enter()
      .append(&#34;path&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d zScale&#34;)
      .merge(zScale)
      .attr(&#34;stroke&#34;, &#34;black&#34;)
      .attr(&#34;stroke-width&#34;, 1.5)
      .attr(&#34;d&#34;, zScale3d.draw);
    zScale.exit().remove();
    /* ----------- y-Scale Text ----------- */
    const yText = svg.selectAll(&#34;text.yText&#34;).data(data[2][0]);
    function GetYText(y){
      if (y==-11){
        return &#34;[Arithmetic Intensity]&#34;;
      }else{
        return  (-y*10 + 100)/2+&#34;%&#34;;
      }
    }
    function GetYWeight(y){
      if (y==-11){
        return 700;
      }else{
        return  350;
      }
    }
    yText
      .enter()
      .append(&#34;text&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d yText&#34;)
      .attr(&#34;font-family&#34;, &#34;system-ui, sans-serif&#34;)
      .merge(yText)
      .each(function (d) {
        d.centroid = { x: d.rotated.x, y: d.rotated.y, z: d.rotated.z };
      })
      .attr(&#34;x&#34;, (d) =&gt; d.projected.x)
      .attr(&#34;y&#34;, (d) =&gt; d.projected.y)
      .style(&#34;font-weight&#34;, (d) =&gt; GetYWeight(d.y))
      .text((d) =&gt; GetYText(d.y))
      .attr(&#34;fill&#34;, &#34;#78E2A0&#34;);
    yText.exit().remove();
    /* ----------- x-Scale Text ----------- */
    const xText = svg.selectAll(&#34;text.xText&#34;).data(data[3][0]);
    xText
      .enter()
      .append(&#34;text&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d xText&#34;)
      .attr(&#34;font-family&#34;, &#34;system-ui, sans-serif&#34;)
      .merge(xText)
      .each(function (d) {
        d.centroid = { x: d.rotated.x, y: d.rotated.y, z: d.rotated.z };
      })
      .attr(&#34;x&#34;, (d) =&gt; d.projected.x)
      .attr(&#34;y&#34;, (d) =&gt; d.projected.y)
      .attr(&#34;z&#34;, (d) =&gt; d.projected.z)
      .text((d) =&gt;  d.x == 10 ? &#34;[Hardware Enablement]&#34; : &#34;&#34;)
      .style(&#34;font-weight&#34;, 700)
      .attr(&#34;fill&#34;, &#34;#78E2A0&#34;);
    xText.exit().remove();
    /* ----------- x-Scale Text ----------- */
    const zText = svg.selectAll(&#34;text.zText&#34;).data(data[4][0]);
    zText
      .enter()
      .append(&#34;text&#34;)
      .attr(&#34;class&#34;, &#34;d3-3d zText&#34;)
      .attr(&#34;font-family&#34;, &#34;system-ui, sans-serif&#34;)
      .merge(zText)
      .each(function (d) {
        d.centroid = { x: d.rotated.x, y: d.rotated.y, z: d.rotated.z };
      })
      .attr(&#34;x&#34;, (d) =&gt; d.projected.x)
      .attr(&#34;y&#34;, (d) =&gt; d.projected.y)
      .attr(&#34;z&#34;, (d) =&gt; d.projected.z)
      .text((d) =&gt;  d.z == 10 ? &#34;[Work Reduction]&#34; : &#34;&#34;)
      .style(&#34;font-weight&#34;, 700)
      .attr(&#34;fill&#34;, &#34;#78E2A0&#34;);
    zText.exit().remove(); 
    selectAll(&#34;.d3-3d&#34;).sort(points3d.sort);
  }
  function posPointX(d) {
    return d.projected.x;
  }
  function posPointY(d) {
    return d.projected.y;
  }
  function init() {
    xGrid = [];
    scatter = [];
    yLine = [];
    xLine = [];
    zLine = [];
    let cnt = 0; 
    for (let z = -j; z &lt; j; z++) {
      for (let x = -j; x &lt; j; x++) {
        xGrid.push({ x: x, y: 0, z: z}); // grid position
        scatter.push({
          x: x,
          y: randomNormal(0, 0.8)()*3,
          // y: randomUniform(9, -9)(),
          z: z,
          id: &#34;point-&#34; + cnt++,
        });
      }
    }
    range(-10, 12, 1).forEach((d) =&gt; {
      yLine.push({ x: 0, y: -d, z: 0 });
      xLine.push({ x: -d, y: 0, z: 0 });
      zLine.push({ x: 0, y: 0, z: -d });
    });
    const data = [
      grid3d(xGrid),
      points3d(scatter),
      yScale3d([yLine]),
      xScale3d([xLine]),
      zScale3d([zLine]),
    ];
    processData(data, 1000, true);
  }
  function dragStart(event) {
    mx = event.x;
    my = event.y;
  }
  function dragged(event) {
    beta = (event.x - mx + mouseX) * (Math.PI / offset);
    alpha = (event.y - my + mouseY) * (Math.PI / offset) * -1;
    const data = [
      grid3d.rotateY(beta + startAngle).rotateX(alpha - startAngle)(xGrid),
      points3d.rotateY(beta + startAngle).rotateX(alpha - startAngle)(scatter),
      yScale3d.rotateY(beta + startAngle).rotateX(alpha - startAngle)([yLine]),
      xScale3d.rotateY(beta + startAngle).rotateX(alpha - startAngle)([xLine]),
      zScale3d.rotateY(beta + startAngle).rotateX(alpha - startAngle)([zLine]),
    ];
    processData(data, 0, false);
  }
  function dragEnd(event) {
    mouseX = event.x - mx + mouseX;
    mouseY = event.y - my + mouseY;
  }
  selectAll(&#34;button&#34;).on(&#34;click&#34;, init);
  init();
});
&lt;/script&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;DIMM(dual in-line memory module)，即ram stick，内存条，DDR(Double Data Rate)技术的物理具现。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/products/sku/232592/intel-xeon-cpu-max-9480-processor-112-5m-cache-1-90-ghz/specifications.html&#34;&gt;https://www.intel.com/content/www/us/en/products/sku/232592/intel-xeon-cpu-max-9480-processor-112-5m-cache-1-90-ghz/specifications.html&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/intel/iodlr/blob/master/large_page-c/large_page.c&#34;&gt;https://github.com/intel/iodlr/blob/master/large_page-c/large_page.c&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Work Reduction vs Hardware Enablement</title>
      <link>https://cmbbq.github.io/posts/work-reduction-vs-hardware-enablement/</link>
      <pubDate>Thu, 08 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/work-reduction-vs-hardware-enablement/</guid>
      <description>Optimization can be divided into work reduction and hardware enablement.
Work, in terms of computer programming, is basically a gross measure of how much stuff the program needs to do.
The idea of work optimization is to reduce the amount of stuff the program needs to do. Commonly employed techniques include: approximation, tail-recursion elimination, coarsening/refining recursion, inlining, loop fusion, loop unrolling, hoisting, short-circuiting, common-subexpression elimination, compile-time initialization, compile-time evaluation, exploiting sparsity, caching, pre-computation, and bit hacks.</description>
      <content>&lt;blockquote&gt;
&lt;p&gt;Optimization can be divided into work reduction and hardware enablement.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Work, in terms of computer programming, is basically a gross measure of how much stuff the program needs to do.&lt;/p&gt;
&lt;p&gt;The idea of work optimization is to reduce the amount of stuff the program needs to do. Commonly employed techniques include: &lt;code&gt;approximation&lt;/code&gt;, &lt;code&gt;tail-recursion elimination&lt;/code&gt;, &lt;code&gt;coarsening/refining recursion&lt;/code&gt;, &lt;code&gt;inlining&lt;/code&gt;, &lt;code&gt;loop fusion&lt;/code&gt;, &lt;code&gt;loop unrolling&lt;/code&gt;, &lt;code&gt;hoisting&lt;/code&gt;, &lt;code&gt;short-circuiting&lt;/code&gt;, &lt;code&gt;common-subexpression elimination&lt;/code&gt;, &lt;code&gt;compile-time initialization&lt;/code&gt;, &lt;code&gt;compile-time evaluation&lt;/code&gt;, &lt;code&gt;exploiting sparsity&lt;/code&gt;, &lt;code&gt;caching&lt;/code&gt;, &lt;code&gt;pre-computation&lt;/code&gt;, and &lt;code&gt;bit hacks&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;While reducing work undoubtedly serves as an essential heuristic for reducing overall running time, it&amp;rsquo;s not the sole determinant of the running time; it doesn&amp;rsquo;t capture the whole picture of computer programming, leaving the intricate nature of computer hardware unaddressed.&lt;/p&gt;
&lt;p&gt;To thoroughly investigate architectural improvements that unlock hardware potential, we must delve into numerous aspects in hardware and micro-architecture: &lt;code&gt;the ISA&lt;/code&gt;, &lt;code&gt;pipeline stages&lt;/code&gt;, &lt;code&gt;superscalar processing&lt;/code&gt;, &lt;code&gt;out-of-order execution&lt;/code&gt;, &lt;code&gt;paging&lt;/code&gt;, &lt;code&gt;caching&lt;/code&gt;, &lt;code&gt;vectorization&lt;/code&gt;, &lt;code&gt;speculation&lt;/code&gt;, &lt;code&gt;hardware prefetching&lt;/code&gt;, &lt;code&gt;branch prediction&lt;/code&gt;, etc.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Historically computer architecture leverages either locality or parallelism to enhance performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To exploit locality, the memory hierarchy(&lt;code&gt;registers&lt;/code&gt;-&amp;gt;&lt;code&gt;L1/L2/L3 caches&lt;/code&gt;-&amp;gt;&lt;code&gt;local DRAM&lt;/code&gt;-&amp;gt;&lt;code&gt;remote DRAM&lt;/code&gt;-&amp;gt;&lt;code&gt;PMem&lt;/code&gt;-&amp;gt;&lt;code&gt;SSD&lt;/code&gt;) was made deeper for hiding performance issue; hardware prefetchers and branch predictors were used to predict immediate accesses and move data or instructions closer to processors. As programmers, we are working on one layer obove the computer architecture layer, what we can do is to write NUMA-aware, cache-aligned, and perhaps vectorized code with regular data access patterns and proper software pre-fetching.&lt;/p&gt;
&lt;p&gt;To exploit parallelism, superscalar out-of-order pipelines with micro-ops, vector hardware, multi-core were introduced. Correspondingly, we need to keep all these things busy by using techniques like &lt;code&gt;bit tricks&lt;/code&gt;, &lt;code&gt;ILP&lt;/code&gt;(Instruction-level parallelism), &lt;code&gt;AVX&lt;/code&gt;/&lt;code&gt;SSE&lt;/code&gt;, &lt;code&gt;AMX&lt;/code&gt;, multithread/multi-process programming and offloading to accelerators like &lt;code&gt;DPU&lt;/code&gt;s or &lt;code&gt;GPU&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s explore &lt;code&gt;ILP&lt;/code&gt; further, as it is more connected with the μ-arch designs inside a processor, such as out-of-order execution, data bypassing, register renaming, and so on. To exploit CPU μ-arch programmatically, we can (1)use separate functional units, (2)write likely/unlikely hints for branch prediction, and (3)break dependency in the data-flow graph beforehand to reduce data hazards.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Wall is Coming</title>
      <link>https://cmbbq.github.io/posts/wall-is-coming/</link>
      <pubDate>Wed, 07 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/wall-is-coming/</guid>
      <description>The memory wall is coming. Given that DRAM access has become the de-facto bottleneck for many of today&amp;rsquo;s datacenter applications, it seems logical to optimize these applications by shamelessly copying prior wisdom on dealing with slow peripherals like PMems or SSDs.
PMems offer only $\frac{1}{14}$~$\frac{1}{3}$ of DRAMs&amp;rsquo; bandwidth. SSDs are typically 1~2 orders of magnitude slower than DRAMs. To overcome their slowness, many data structures were crafted specifically for them.</description>
      <content>&lt;p&gt;The memory wall is coming. Given that DRAM access has become the de-facto bottleneck for many of today&amp;rsquo;s datacenter applications, it seems logical to optimize these applications by shamelessly copying prior wisdom on dealing with slow peripherals like &lt;code&gt;PMem&lt;/code&gt;s or &lt;code&gt;SSD&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PMem&lt;/code&gt;s offer only $\frac{1}{14}$~$\frac{1}{3}$ of DRAMs&amp;rsquo; bandwidth. &lt;code&gt;SSD&lt;/code&gt;s are typically 1~2 orders of magnitude slower than DRAMs. To overcome their slowness, many data structures were crafted specifically for them.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Dashtable&lt;/code&gt;[Lu et al., 2020]&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, for example, was targeting scalable hasing on the once fashionable &lt;code&gt;PMem&lt;/code&gt;s. Hasing on &lt;code&gt;PMem&lt;/code&gt;s sounds somewhat niche. But in reality, if any being could survive &lt;code&gt;PMem&lt;/code&gt;&amp;rsquo;s hellish bandwidth, you can definitely count on it for arbitrary memory-bound application. In 2022, Intel killed off its &lt;code&gt;PMem&lt;/code&gt; business&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. &lt;code&gt;PMem&lt;/code&gt; died. Yet the &lt;code&gt;Dash&lt;/code&gt; methodology still thrives. Today we use it in regular &lt;code&gt;DRAM&lt;/code&gt; setups. Check this out, the &lt;a href=&#34;https://github.com/dragonflydb/dragonfly&#34;&gt;DragonFly&lt;/a&gt; project. It&amp;rsquo;s based on &lt;code&gt;Dash&lt;/code&gt; and claims to be the modern replacement for Redis and Memcached, delivering 25x more throughput.&lt;/p&gt;
&lt;p&gt;The core idea behind &lt;code&gt;Dashtable&lt;/code&gt; is trading space for time, or more specifically, each bucket paying a little bit extra space in metadata to buy (1)faster bucket probing with fingerprints of keys and (2)lightweight optimistic concurrency control with version locks, (3)stashing overflow records, which helps to alleviate segment splits caused by unbalanced bucket loads.&lt;/p&gt;
&lt;p&gt;Another noticeable development is that modern-day system language &lt;code&gt;Rust&lt;/code&gt;, has opted for &lt;code&gt;B-tree&lt;/code&gt;s over &lt;code&gt;Red-black tree&lt;/code&gt;s for its ordered maps. The 50+ years old &lt;code&gt;B-tree&lt;/code&gt;, once targeting disk storage, might find its place in today&amp;rsquo;s memory-bound applications. That is interesting because the &lt;code&gt;Red-black tree&lt;/code&gt;s was deemed memory-efficient and is still widely in use as the default implementation of &lt;code&gt;C++&lt;/code&gt;&amp;rsquo;s &lt;code&gt;std::map&lt;/code&gt;. Yet &lt;code&gt;B-tree&lt;/code&gt; somehow beats &lt;code&gt;Red-black tree&lt;/code&gt; on modern servers.&lt;/p&gt;
&lt;p&gt;So what happened to modern hardware? Well, it mostly involves the memory wall problem, which refers to the increasing gap between processor and memory speed. For decades, the memory wall problem has never been resolved, but only mitigated by introducing deeper and deeper memory hierarchies. In today&amp;rsquo;s architectures, L3 become much slower than L1/L2, and &lt;code&gt;DRAM&lt;/code&gt;s should be labeled as dangerously slow as disks were in the 1980s perspective.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;B. Lu, X. Hao, T. Wang, and E. Lo. Dash: Scalable Hashing on Persistent Memory. CoRR abs/2003.07302, 2020. &lt;a href=&#34;https://arxiv.org/pdf/2003.07302.pdf&#34;&gt;[pdf]&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.datacenterdynamics.com/en/news/intel-kills-off-optane-memory-writes-off-559-million-inventory/&#34;&gt;Intel kills off Optane Memory, writes off $559 million inventory&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Dash: Scalable Hashing</title>
      <link>https://cmbbq.github.io/posts/dash/</link>
      <pubDate>Fri, 26 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/dash/</guid>
      <description>The main focus of the Dash paper was on the once fashionable persistent memory, but in reality, any memory bandwidth-limited scenario can benefit from it. With Intel killing off its pmem business, the significance of the Dash approach has shifted to regular DRAM applications.
Dynamic Hashing Dashtable, the proposed scalable hashtable, evolves from extendible hashing.
Extendible hashing is a hash system that uses the first $N$ bits of hashed values to look up buckets in a trie-structured directory.</description>
      <content>&lt;p&gt;The main focus of the Dash paper was on the once fashionable &lt;code&gt;persistent memory&lt;/code&gt;, but in reality, any &lt;code&gt;memory bandwidth&lt;/code&gt;-limited scenario can benefit from it. With Intel killing off its &lt;code&gt;pmem&lt;/code&gt; business, the significance of the &lt;code&gt;Dash&lt;/code&gt; approach has shifted to regular &lt;code&gt;DRAM&lt;/code&gt; applications.&lt;/p&gt;
&lt;h1 id=&#34;dynamic-hashing&#34;&gt;Dynamic Hashing&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;Dashtable&lt;/code&gt;, the proposed scalable hashtable, evolves from &lt;code&gt;extendible hashing&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Extendible hashing&lt;/code&gt; is a hash system that uses the first $N$ bits of hashed values to look up buckets in a trie-structured &lt;code&gt;directory&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;directory&lt;/code&gt; with &lt;code&gt;global depth&lt;/code&gt; $N$ can hold $2^N$ buckets. It means $N$ is the key size that maps the &lt;code&gt;directory&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Each bucket also has a &lt;code&gt;local depth&lt;/code&gt; $M(M \le N)$, which is the key size that previously mapped the &lt;code&gt;directory&lt;/code&gt;. Any bucket having a &lt;code&gt;local depth&lt;/code&gt; $M = N$ is pointed-to by exactly one &lt;code&gt;directory&lt;/code&gt; entry. Any bucket having a &lt;code&gt;local depth&lt;/code&gt; $M \lt N$ is pointed-to by more than one &lt;code&gt;directory&lt;/code&gt; entries.&lt;/p&gt;
&lt;p&gt;The minimal $N$ needed to ensure every item has a unique bucket index is 1 for 2 items. The minimal $N = 2$  for 4 items. Everytime a new  item added into a bucket, if the number of items in the bucket exceeds a certain threshold, a rehashing operation happens by splitting the bucket into 2 parts. Hence rehashing in this scheme doesn&amp;rsquo;t have to stop the world and do a full-table scan &amp;amp; copy, but instead is done incremental.&lt;/p&gt;
&lt;p&gt;Similar to &lt;code&gt;extendible hashing&lt;/code&gt;, &lt;code&gt;linear hashing&lt;/code&gt; also uses a &lt;code&gt;directory&lt;/code&gt; to orgranize and address buckets. The distinction lies in split control. In &lt;code&gt;linear hashing&lt;/code&gt;, a split typically occurs only if the load factor exceeds a threshold and the bucket to be split is chosen in a &amp;ldquo;linear&amp;rdquo; manner.&lt;/p&gt;
&lt;h1 id=&#34;dash-for-extendible-hashing&#34;&gt;Dash for Extendible Hashing&lt;/h1&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/dash_eh.png&#34; alt=&#34;dash_eh&#34;&gt;&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;Dash-EH&lt;/code&gt;, each &lt;code&gt;directory&lt;/code&gt; entry points to a &lt;code&gt;segment&lt;/code&gt; which consists of a fixed number of normal buckets and stash buckets. A &lt;code&gt;segment&lt;/code&gt; can be viewed as a sub-hashtable of constant size. The so-called stash buckets shares the same layout as the normal buckets, responsible for storing overflow records.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/dash_eh_bucket.png&#34; alt=&#34;dash_eh&#34;&gt;&lt;/p&gt;
&lt;p&gt;The core idea of &lt;code&gt;Dash-EH&lt;/code&gt; is to pay a little bit extra space in metadata to buy faster probing with fingerprints and lightweight concurrency control with version locks.&lt;/p&gt;
&lt;p&gt;Inside a &lt;code&gt;Dash-EH&lt;/code&gt; bucket, as shown in the figure above, the first 32 bytes are metadata, including version lock, counter, alloc bitmap, membership bitmap for load balancing, and 18 one-byte fingerprints for bucket probing(14 for slots in the bucket, 4 for overflow records originally hashed to this bucket). It is followed by $16(Bytes) \times 14 (records) = 224 Bytes$ payload, which stores 14 16-byte records.&lt;/p&gt;
&lt;h2 id=&#34;fingerprinting&#34;&gt;Fingerprinting&lt;/h2&gt;
&lt;p&gt;Bucket probing, which refers to searching for a slot in a bucket, is a basic operation of hashtables, needed by &lt;code&gt;search&lt;/code&gt;, &lt;code&gt;insert&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; operations to locate a particular key. Traditionally probing requires a linear scan, which is naturally slow on &lt;code&gt;PMem&lt;/code&gt; and could be completely unnecessary when a searched key doesn&amp;rsquo;t exist. &lt;code&gt;Dash-EH&lt;/code&gt; employs fingerprinting to reduce unnecessary scans. Fingerprints are the least significant byte of hashes of keys. To probe for a key, the probing thread first checks if any fingerprint in the bucket&amp;rsquo;s metadata matches the key&amp;rsquo;s, so it can skip buckets without any fingerprint match.&lt;/p&gt;
&lt;p&gt;Fingerprinting primarily benefits negative(key-not-found) &lt;code&gt;search&lt;/code&gt;es. The &lt;code&gt;Dash&lt;/code&gt; paper also claims fingerprinting enables using larger buckets spanning more than 2 cachelines. But I have to take it with a grain of salt. The paper itself uses a 256B setup. So does the DragonFly implementation&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;. In theory, larger buckets can indeed tolerate more collisions and improve the load factor; however, this may come at the cost of compromising locality to a certain degree - you don&amp;rsquo;t want to load multiple times for a single bucket access in a hashtable.&lt;/p&gt;
&lt;h2 id=&#34;bucket-load-balancing&#34;&gt;Bucket Load Balancing&lt;/h2&gt;
&lt;p&gt;Segmentation reduces cache misses on &lt;code&gt;directory&lt;/code&gt; by reducing its size. In the &lt;code&gt;extendible hashing&lt;/code&gt; scheme, if any bucket in a segment is full, the entire segment needs to be split, even though other buckets might have much free space.&lt;/p&gt;
&lt;p&gt;To prevent frequent segment splits, the &lt;code&gt;Dash-EH&lt;/code&gt; algorithm design incorporates bucket load balancing. For an &lt;code&gt;insert&lt;/code&gt; operation, &lt;code&gt;Dash-EH&lt;/code&gt; probes both bucket $B_b$ and $B_{b+1}$, and then inserts into the bucket that is less full. If both $B_b$ and $B_{b+1}$ are full, &lt;code&gt;Dash-EH&lt;/code&gt; tries to displace a &amp;ldquo;native record&amp;rdquo; from $B_{b+1}$ to $B_{b+2}$, or move a &amp;ldquo;rebalanced record&amp;rdquo; from $B_b$ back to $B_{b-1}$ where it originally belongs.&lt;/p&gt;
&lt;p&gt;The per-bucket membership bitmap is used to decide whether a record is rebalanced or native. If a bit is set in the membership bitmap, then the corresponding key was not directly hashing into this bucket(native) but placed here due to re-balancing(rebalanced).&lt;/p&gt;
&lt;p&gt;If both &lt;code&gt;insert&lt;/code&gt; and &lt;code&gt;displacement&lt;/code&gt; failed, &lt;code&gt;Dash-EH&lt;/code&gt; turns to the last resort - stashing. Each segment has a fixed number of stash buckets to hold these overflow records. Probing stash buckets introduces significant overhead to negative &lt;code&gt;search&lt;/code&gt;es and &lt;code&gt;insert&lt;/code&gt;s(needs uniqueness check). To address this issue, each normal bucket reserves certain metadata fields. 4 overflow fingerprints are reserved for overflow records stored in stsh buckets. A overflow bit indicates if there exists an overflow at all. So if there isn&amp;rsquo;t an overflow in a bucket, the &lt;code&gt;search&lt;/code&gt;/&lt;code&gt;insert&lt;/code&gt; operation doesn&amp;rsquo;t have to probe stash buckets. Anyway, it is still advisable to maintain a small number of stash buckets. The paper claims &amp;ldquo;using 2–4 stash buckets per segment can improve load factor to over 90% without imposing significant overhead&amp;rdquo;. In Dragonfly&amp;rsquo;s Dashtable, each segment has 56 regular buckets, and 4 stash buckets.&lt;/p&gt;
&lt;h2 id=&#34;lightweight-concurrency-control&#34;&gt;Lightweight Concurrency Control&lt;/h2&gt;
&lt;p&gt;The lightweight concurrency control in &lt;code&gt;Dash-EH&lt;/code&gt; naturally scales well on today&amp;rsquo;s &lt;code&gt;many-core&lt;/code&gt; architectures, out-performing traiditional bucket-level shared locks.&lt;/p&gt;
&lt;p&gt;Write operations follow traditional bucket-level locking to lock the affected buckets, using CAS over a lock bit. If a write is done, the writer thread resets the lock bit and increments the per-bucket version number by one.&lt;/p&gt;
&lt;p&gt;On the other hand, read operations are designed to be lock-free. Before a read, the reader thread first fetches a snapshot of the lock word, waits until the lock is released, then proceeds to read without holding any lock. After reading, it will check the lock word again to verify the version number stays unchanged. If the version is changed, it retries the entire operation.&lt;/p&gt;
&lt;h1 id=&#34;dash-for-linear-hashing&#34;&gt;Dash for Linear Hashing&lt;/h1&gt;
&lt;p&gt;The Dash paper also presents &lt;code&gt;Dash-LH&lt;/code&gt;, a Dash-enabled linear hashing approach built upon building blocks used in &lt;code&gt;Dash-EH&lt;/code&gt;, such as balanced &lt;code&gt;insert&lt;/code&gt;/&lt;code&gt;displacement&lt;/code&gt;, fingerprinting and optimistic concurrency; they are pretty much orthogonal after all. The main difference is that &lt;code&gt;Dash-LH&lt;/code&gt; split the segment pointed to by a pointer in a linear manner.&lt;/p&gt;
&lt;p&gt;Traditional &lt;code&gt;linear hashing&lt;/code&gt; links overflow records with a linklist. In &lt;code&gt;Dash-LH&lt;/code&gt;, it&amp;rsquo;s done more cache-friendly with stash buckets. It still needs to chain these stash buckets though. Still it&amp;rsquo;s much better than chaining individual records.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/dragonflydb/dragonfly/blob/main/docs/dashtable.md&#34;&gt;Dashtable in Dragonfly&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>DPDK is All You Need</title>
      <link>https://cmbbq.github.io/posts/dpdk/</link>
      <pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/dpdk/</guid>
      <description>对于访存密集的数据中心应用来说，DPDK提供了非常好的性能工程范式。本文只是浅尝辄止，汇集其中一部分值得借鉴的思想。
EAL：用户空间库 EAL(Envionmemt Abstraction Layer)是DPDK面向用户的用户空间库，提供了各种有用的工具，比如：运行时对CPU特性进行检测，更适合现代硬件的内存管理，绑核和指定任务在某个核上运行。
rte_eal_init()是初始化EAL的函数，有多平台的实现：Linux、FreeBSD、Windows。以它为例，可大致了解DPDK EAL做了哪些工作。
rte_eal_init()是一个冗长的初始化过程，包含下列步骤：检查CPU类型是否是DPDK支持的、设置日志等级、检测各个socket上的各个cpu、enable每个逻辑核、初始化插件（加载共享库，比如一些PMD drivers）、初始化tracing机制、解析各个设备的配置选项、初始化全局配置（主核id、逻辑核数、numa nodes数、iova模式1、内存拓扑配置）、初始化中断处理机制、初始化多进程通用channel、扫描所有总线上的设备、初始化malloc heap、注册多进程action callbacks（热插拔支持）、初始化巨页信息2、初始化内存和memzone、初始化HPET/TSC计时器3、检查本地socket上的内存、创建主线程和子线程的通信信道、创建工作线程、绑核、在工作线程上启动dummy function、初始化服务、嗅探所有总线上的设备和驱动、启动服务、开启telemetry（提供ethdev stats、ethdev port list、eal parameters等状态查询）。
切割需要权限的工作 DPDK程序跑在用户态的一个前提是有内核驱动帮忙处理一些硬件设备注册、中断映射的事情。Linux上有几个可用的内核驱动，比如vfio-pci、igb_uio、uio_pci_generic。它们是泛用的PCI内核驱动模块，对所有PCI设备均适用。igb_uio基于Linux UIO提供所有类型的中断支持，比较古老，也比较简单，不支持IOMMU，因此IOVA mode只能用PA mode。uio_pci_generic和igb_uio类似，不过不支持MSI和MSI-X中断。vfio-pci支持基于IOMMU做IOVA映射，兼容VA mode和PA mode，如果能用vfio，就用vfio，目前uio基本处于半废弃状态。
大多数设备需先从Linux内核驱动上解绑，然后再绑定到DPDK的内核驱动上。需用户在运行DPDK程序前，用usertools目录下的dpdk-devbind.py脚本做好设备和内核模块的解绑和绑定——这种需要root权限的准备工作也从用户态库中剥离了。
利用巨页，毕竟4KB已是古老时代的残响 DPDK基于mmap在hugetlbfs中进行巨页物理内存申请。使用更大的内存页相比4KB默认页(在x86上，DPDK目前支持2MB或1GB的巨页4)，所需的page table entry总数大大减少，可显著减小page table size和tlb size、降低tlb miss和page table walk开销，提升内存分配的连续性和内存访问的局部性，这些都有助于提升内存带宽。
尊重NUMA Node拓扑 DPDK的每个操作都是NUMA-aware的，提供的API默认是NUMA node亲和的，这让用户很难写出远端内存访问的代码。
尊重内存的硬件拓扑 DPDK的内存分配做得非常精细，利用了内存硬件拓扑等内存配置。
内存配置中有两个影响应用层的重要概念：内存通道（memory channels）、内存列（memory ranks）。
内存通道是CPU和内存之间的通信通道，理论上来讲内存带宽和通道数成正比，单个通道位宽64bit，2个就是128bit。内存通道数往往等于单socket支持的DIMM5数，毕竟足够多的内存还需足够多的通道才能保证和CPU的互连。
CPU和内存之间是64bit接口，但单个内存颗粒（DRAM chips）的位宽可能是4bit、16bit，需要多个内存颗粒并联形成一个64bit的内存列，连接到同一组chip select上，从而保证各内存颗粒可被同时访问。内存模组必须至少形成一个内存列，才能和CPU通信。内存标签上的2R×8就是指列数为2，颗粒位宽8bit，一共16个颗粒。
Depending on memory configuration on x86 arch, objects addresses are spread between channels and ranks in RAM
x86架构下内存通道和内存列在内存地址上interleaving，即均匀分布且递增，因此RAM可视作由$n_{chan}\times n_{rank}$个block组成的，其DIMM架构如下图。 如图所示，内存池最好不要让对象的起始地址反复命中同一个channel或同一个rank，而是要充分利用不同内存通道、不同内存列，避免通道、列之间的负载不均，提升访存带宽。
DPDK的mempool给对象大小加恰当的padding，令内存池中的下一个对象的起始地址分布在不同内存通道和列中。具体实现可参考下面的代码，其中64B6是x86的cache line size，也恰恰是一个block size，或memory bus width、channel width。无论如何，内存池里的地址都要首先保证是64B的整数倍，能整齐地放入cache，做到cache-friendly——事实上也是block friendly、memory bus width friendly，然后才是保证下一个对象的block id和$n_{chan}\times n_{rank}$互质。</description>
      <content>&lt;p&gt;对于访存密集的数据中心应用来说，DPDK提供了非常好的性能工程范式。本文只是浅尝辄止，汇集其中一部分值得借鉴的思想。&lt;/p&gt;
&lt;h2 id=&#34;eal用户空间库&#34;&gt;EAL：用户空间库&lt;/h2&gt;
&lt;p&gt;EAL(Envionmemt Abstraction Layer)是DPDK面向用户的用户空间库，提供了各种有用的工具，比如：运行时对CPU特性进行检测，更适合现代硬件的内存管理，绑核和指定任务在某个核上运行。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rte_eal_init()&lt;/code&gt;是初始化EAL的函数，有多平台的实现：Linux、FreeBSD、Windows。以它为例，可大致了解DPDK EAL做了哪些工作。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rte_eal_init()&lt;/code&gt;是一个冗长的初始化过程，包含下列步骤：检查CPU类型是否是DPDK支持的、设置日志等级、检测各个socket上的各个cpu、enable每个逻辑核、初始化插件（加载共享库，比如一些PMD drivers）、初始化&lt;a href=&#34;https://doc.dpdk.org/guides/prog_guide/trace_lib.html&#34;&gt;tracing机制&lt;/a&gt;、解析各个设备的配置选项、初始化全局配置（主核id、逻辑核数、numa nodes数、iova模式&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;、内存拓扑配置）、初始化中断处理机制、初始化多进程通用channel、扫描所有总线上的设备、初始化malloc heap、注册多进程action callbacks（热插拔支持）、初始化巨页信息&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;、初始化内存和memzone、初始化HPET/TSC计时器&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;、检查本地socket上的内存、创建主线程和子线程的通信信道、创建工作线程、绑核、在工作线程上启动dummy function、初始化服务、嗅探所有总线上的设备和驱动、启动服务、开启telemetry（提供ethdev stats、ethdev port list、eal parameters等状态查询）。&lt;/p&gt;
&lt;h2 id=&#34;切割需要权限的工作&#34;&gt;切割需要权限的工作&lt;/h2&gt;
&lt;p&gt;DPDK程序跑在用户态的一个前提是有内核驱动帮忙处理一些硬件设备注册、中断映射的事情。Linux上有几个可用的内核驱动，比如&lt;code&gt;vfio-pci&lt;/code&gt;、&lt;code&gt;igb_uio&lt;/code&gt;、&lt;code&gt;uio_pci_generic&lt;/code&gt;。它们是泛用的PCI内核驱动模块，对所有PCI设备均适用。&lt;code&gt;igb_uio&lt;/code&gt;基于Linux UIO提供所有类型的中断支持，比较古老，也比较简单，不支持IOMMU，因此IOVA mode只能用PA mode。&lt;code&gt;uio_pci_generic&lt;/code&gt;和&lt;code&gt;igb_uio&lt;/code&gt;类似，不过不支持MSI和MSI-X中断。&lt;code&gt;vfio-pci&lt;/code&gt;支持基于IOMMU做IOVA映射，兼容VA mode和PA mode，如果能用&lt;code&gt;vfio&lt;/code&gt;，就用&lt;code&gt;vfio&lt;/code&gt;，目前&lt;code&gt;uio&lt;/code&gt;基本处于半废弃状态。&lt;/p&gt;
&lt;p&gt;大多数设备需先从Linux内核驱动上解绑，然后再绑定到DPDK的内核驱动上。需用户在运行DPDK程序前，用&lt;code&gt;usertools&lt;/code&gt;目录下的&lt;code&gt;dpdk-devbind.py&lt;/code&gt;脚本做好设备和内核模块的解绑和绑定——这种需要root权限的准备工作也从用户态库中剥离了。&lt;/p&gt;
&lt;h2 id=&#34;利用巨页毕竟4kb已是古老时代的残响&#34;&gt;利用巨页，毕竟4KB已是古老时代的残响&lt;/h2&gt;
&lt;p&gt;DPDK基于mmap在hugetlbfs中进行巨页物理内存申请。使用更大的内存页相比4KB默认页(在x86上，DPDK目前支持2MB或1GB的巨页&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;)，所需的page table entry总数大大减少，可显著减小page table size和tlb size、降低tlb miss和page table walk开销，提升内存分配的连续性和内存访问的局部性，这些都有助于提升内存带宽。&lt;/p&gt;
&lt;h2 id=&#34;尊重numa-node拓扑&#34;&gt;尊重NUMA Node拓扑&lt;/h2&gt;
&lt;p&gt;DPDK的每个操作都是NUMA-aware的，提供的API默认是NUMA node亲和的，这让用户很难写出远端内存访问的代码。&lt;/p&gt;
&lt;h2 id=&#34;尊重内存的硬件拓扑&#34;&gt;尊重内存的硬件拓扑&lt;/h2&gt;
&lt;p&gt;DPDK的内存分配做得非常精细，利用了内存硬件拓扑等内存配置。&lt;/p&gt;
&lt;p&gt;内存配置中有两个影响应用层的重要概念：内存通道（memory channels）、内存列（memory ranks）。&lt;/p&gt;
&lt;p&gt;内存通道是CPU和内存之间的通信通道，理论上来讲内存带宽和通道数成正比，单个通道位宽64bit，2个就是128bit。内存通道数往往等于单socket支持的DIMM&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;数，毕竟足够多的内存还需足够多的通道才能保证和CPU的互连。&lt;/p&gt;
&lt;p&gt;CPU和内存之间是64bit接口，但单个内存颗粒（DRAM chips）的位宽可能是4bit、16bit，需要多个内存颗粒并联形成一个64bit的内存列，连接到同一组chip select上，从而保证各内存颗粒可被同时访问。内存模组必须至少形成一个内存列，才能和CPU通信。内存标签上的2R×8就是指列数为2，颗粒位宽8bit，一共16个颗粒。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Depending on memory configuration on x86 arch, objects addresses are spread between channels and ranks in RAM&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;x86架构下内存通道和内存列在内存地址上interleaving，即均匀分布且递增，因此RAM可视作由$n_{chan}\times n_{rank}$个block组成的，其DIMM架构如下图。
&lt;img src=&#34;https://cmbbq.github.io/img/2chan4rank.svg&#34; alt=&#34;2chan4rank&#34;&gt;&lt;/p&gt;
&lt;p&gt;如图所示，内存池最好不要让对象的起始地址反复命中同一个channel或同一个rank，而是要充分利用不同内存通道、不同内存列，避免通道、列之间的负载不均，提升访存带宽。&lt;/p&gt;
&lt;p&gt;DPDK的&lt;code&gt;mempool&lt;/code&gt;给对象大小加恰当的padding，令内存池中的下一个对象的起始地址分布在不同内存通道和列中。具体实现可参考下面的代码，其中64B&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;是x86的&lt;code&gt;cache line size&lt;/code&gt;，也恰恰是一个&lt;code&gt;block size&lt;/code&gt;，或&lt;code&gt;memory bus width&lt;/code&gt;、&lt;code&gt;channel width&lt;/code&gt;。无论如何，内存池里的地址都要首先保证是64B的整数倍，能整齐地放入cache，做到cache-friendly——事实上也是block friendly、memory bus width friendly，然后才是保证下一个对象的&lt;code&gt;block id&lt;/code&gt;和$n_{chan}\times n_{rank}$互质。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;static&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;arch_mem_object_align&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; obj_size)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; nchan &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rte_memory_get_nchannel&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; nrank &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;rte_memory_get_nrank&lt;/span&gt;();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; new_obj_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (obj_size &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;63&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; (&lt;span style=&#34;color:#a6e22e&#34;&gt;get_gcd&lt;/span&gt;(new_obj_size, nrank &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; nchan) &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		new_obj_size&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; new_obj_size &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;iova和va模式的连续性&#34;&gt;IOVA和VA模式的连续性&lt;/h2&gt;
&lt;p&gt;硬件不知VA，用户空间不知PA，DPDK的作用之一是bridge物理地址（PA）和虚拟地址（VA），因此给出一种IOVA(IO Virtual Address)是自然而然的设计。&lt;/p&gt;
&lt;p&gt;DPDK的IOVA模式有两种：PA mode和VA mode。如果用了PA mode，则分配给DPDK的所有IOVA地址都是物理地址，或者说，也是IO虚拟地址，只不过这个IO虚拟地址的内存布局与物理地址完全相同。PA mode的缺点是需要root权限以读取页表，且可能会继承物理内存的碎片性。因此DPDK引入了新的VA mode，一方面无需root权限，另一方面基于IOMMU&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;做物理内存的重映射，保证IO虚拟地址的连续性，并使其编码布局和一般虚拟地址在格式上做到匹配，这样就允许大片连续IOVA内存的申请。无论是硬件视角下，还是用户空间视角下，VA mode下IOVA内存区都是连续的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/iova.png&#34; alt=&#34;iova&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;固定物理地址刚好宜用dma&#34;&gt;固定物理地址刚好宜用DMA&lt;/h2&gt;
&lt;p&gt;尊重NUMA拓扑、尊重内存布局、IOVA的VA模式、使用巨页这些特性叠加起来，天然就决定了DPDK设计中，用户态进程用到的所有虚拟地址的underlying物理地址都是固定不变的——也就是说，这些地址是可以用于DMA的。DPDK用户态程序可不必涉足IO事务，让硬件自主代劳，通过固定不变的物理地址上的DMA事务完成。&lt;/p&gt;
&lt;h2 id=&#34;多进程范式&#34;&gt;多进程范式&lt;/h2&gt;
&lt;p&gt;DPDK还特别为多进程做了支持，允许一个主进程管理所有DPDK资源，多个子进程共享资源访问权。DPDK的额外努力在于它保证了子进程视野中的地址和peer进程、主进程视野中的地址是完全一样的，也就是说连指针都能跨进程传递——这听起来就相当危险，但性能肯定比各种安全的通信协作机制强。此外，DPDK还支持跨进程的全局锁，使多进程编程更接近多线程编程。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/memory-in-dpdk-part-2-deep-dive-into-iova.html&#34;&gt;Memory in DPDK Part 2: Deep Dive into IOVA&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;EAL用&lt;code&gt;mmap&lt;/code&gt;分配巨页物理内存，并将这些物理内存再通过内存池API暴露给服务层。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;EAL通过&lt;code&gt;mmap&lt;/code&gt;从用户空间访问HPET内核时间计数，暴露高精度计时器接口给服务层。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://www.intel.com/content/www/us/en/developer/articles/technical/memory-in-dpdk-part-1-general-concepts.html&#34;&gt;Memory in DPDK Part 1: General Concepts&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;DIMM(dual in-line memory module)，即ram stick，内存条，DDR(Double Data Rate)技术的物理具现。&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;无论是i686还是x86_64，cache size都是64B，不过有些场景下合理的cache padding size是128，因为prefetcher一次取两个cacheline。&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;IOMMU是连接在DMA-capable IO总线和主存之间，将设备的物理地址映射到虚拟地址空间的专用硬件。物理机通常都支持IOMMU，以Intel为例，IOMMU技术即Vt-d：Intel® Virtualization Technology for Directed I/O。&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>eBPF Tracing for Memory-Stalled Applications</title>
      <link>https://cmbbq.github.io/posts/ebpf/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/ebpf/</guid>
      <description>现代workloads的瓶颈 如今使用CPU而不将计算offload到GPU等加速器的数据中心应用，大多是访存密集应用，例如倒排检索、向量检索、模型推理、分析型数据库。
纯粹instruction-bound的workload反而罕见。即使是最典型的计算密集场景，充满了矩阵乘加的大模型训推，实际瓶颈也出现在内存IO、off-package互连上。
现代硬件的发展趋势是越来越深的内存hierarchy，相应地，现代workloads（训推、检索、数据分析）的瓶颈也逐渐向内存总线、Cache、MMU、CPU-to-CPU互连、inter-node互连转移。
如今内存慢得就像过去的磁盘。Rust的hashmap用B树而非红黑树实现。ScyllaDB和DragonFly通过利用numa机器局部性以及更好的cache优化分别击败Cassandra和Redis。至少，对不自研加速器硬件的互联网公司的研发团队——即数据中心应用的开发者来说，矩阵乘加offloading到GPU/ASIC/AMX即可，访存优化为代表的互连优化才是性能工程的主战场。
CPU Profiling和CPU利用率的局限性 对于访存密集应用来说，很多人被CPU利用率高误导，以为计算是瓶颈，于是做CPU profiling1和计算优化，往往效果不佳。假设只有10%线程时间在CPU上跑，而70%在等内存读写，那无论CPU profiling做得多好，也没办法找到真正的瓶颈。perf是典型的CPU profiling工具，perf record -F 1000是按照1000Hz采样，对各个函数上的时钟周期可以给出比较准确的估计，但这种采样在阻塞阶段不生效。 类似地，CPU火焰图也只是给出不阻塞时的采样数据，CPU火焰图的最上沿的函数就是所有on—CPU函数，其耗时之和是CPU time之和，不包括off-CPU time。
CPU利用率在目前仍然被广泛使用，但它已经过时。CPU利用率的真实含义是“非空闲率”，即CPU没有跑idle thread的比例，也就包括了内存IO阻塞、网络IO阻塞、spinlock盲等。这个概念过于古老，它出现时尚不存在memory wall，cpu并不显著快于主存，这显然已经不适用于现代硬件语境，容易给人计算单元是系统瓶颈的错觉。
eBPF off-CPU分析：in-kernel简报 怎么度量off-CPU time？最简单的办法是应用层tracing，记录重点代码各个函数和代码块的耗时。大多数情况下，这其实就是最好的方案，精度也不错。不过相比火焰图还不够帅，也不够全面，毕竟在哪加时间戳依赖主观判断，有时候真正的瓶颈会出现在意想不到的地方。
Linux 4.8+2可使用eBPF做off-CPU分析。比如eBPF工具bcc/cpudist，bcc/offcputime。offcputime生成的call stacks可以直接用flamegraph.pl绘制off-CPU火焰图。bcc包含了大量工具，其具体使用方式可参照其官方tutorial。
eBPF tracing与传统的off-CPU tracer(比如perf)相比，最显著的优势是不必把所有内核事件往用户空间dump（调度事件是非常频繁的，perf往往生成巨量数据，注入的额外开销太大，不仅仅是CPU开销，还有磁盘IO开销），而是在内核就按照某种可编程的规则做了总结，把精简的信息输出出来。
此外，内核支持eBPF后，各种off-CPU分析需求都可以统一用eBPF实现，不再需要针对不同场景使用或制作不同的工具了——此前用perf做事件追踪，用storage tracing做存储IO追踪，用内核统计数据观测调度时延，不成体系，且性能良莠不一。
eBPF追踪off-CPU时长的思路是在context switch事件结束时记录一次stack（off—CPU期间stack是不变的，一次足矣），为当前context的off-CPU时长增加线程睡眠时间。其伪代码如下：
on context switch finish: sleeptime[prev_thread_id] = timestamp if !sleeptime[thread_id] return delta = timestamp - sleeptime[thread_id] totaltime[pid, execname, user stack, kernel stack] += delta sleeptime[thread_id] = 0 on tracer exit: for each key in totaltime: print key print totaltime[key] 所以，什么是eBPF？ 简单说，eBPF是一个允许跑自定义代码做一些tracing和系统监控的in-kernel runtime，是BPF的升级版。</description>
      <content>&lt;h2 id=&#34;现代workloads的瓶颈&#34;&gt;现代workloads的瓶颈&lt;/h2&gt;
&lt;p&gt;如今使用CPU而不将计算offload到GPU等加速器的数据中心应用，大多是访存密集应用，例如倒排检索、向量检索、模型推理、分析型数据库。&lt;/p&gt;
&lt;p&gt;纯粹instruction-bound的workload反而罕见。即使是最典型的计算密集场景，充满了矩阵乘加的大模型训推，实际瓶颈也出现在内存IO、off-package互连上。&lt;/p&gt;
&lt;p&gt;现代硬件的发展趋势是越来越深的内存hierarchy，相应地，现代workloads（训推、检索、数据分析）的瓶颈也逐渐向内存总线、Cache、MMU、CPU-to-CPU互连、inter-node互连转移。&lt;/p&gt;
&lt;p&gt;如今内存慢得就像过去的磁盘。Rust的hashmap用B树而非红黑树实现。ScyllaDB和DragonFly通过利用numa机器局部性以及更好的cache优化分别击败Cassandra和Redis。至少，对不自研加速器硬件的互联网公司的研发团队——即数据中心应用的开发者来说，矩阵乘加offloading到GPU/ASIC/AMX即可，访存优化为代表的互连优化才是性能工程的主战场。&lt;/p&gt;
&lt;h2 id=&#34;cpu-profiling和cpu利用率的局限性&#34;&gt;CPU Profiling和CPU利用率的局限性&lt;/h2&gt;
&lt;p&gt;对于访存密集应用来说，很多人被CPU利用率高误导，以为计算是瓶颈，于是做CPU profiling&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;和计算优化，往往效果不佳。假设只有10%线程时间在CPU上跑，而70%在等内存读写，那无论CPU profiling做得多好，也没办法找到真正的瓶颈。&lt;code&gt;perf&lt;/code&gt;是典型的CPU profiling工具，&lt;code&gt;perf record -F 1000&lt;/code&gt;是按照1000Hz采样，对各个函数上的时钟周期可以给出比较准确的估计，但这种采样在阻塞阶段不生效。 类似地，CPU火焰图也只是给出不阻塞时的采样数据，CPU火焰图的最上沿的函数就是所有on—CPU函数，其耗时之和是CPU time之和，不包括off-CPU time。&lt;/p&gt;
&lt;p&gt;CPU利用率在目前仍然被广泛使用，但它已经过时。CPU利用率的真实含义是“非空闲率”，即CPU没有跑idle thread的比例，也就包括了内存IO阻塞、网络IO阻塞、spinlock盲等。这个概念过于古老，它出现时尚不存在memory wall，cpu并不显著快于主存，这显然已经不适用于现代硬件语境，容易给人计算单元是系统瓶颈的错觉。&lt;/p&gt;
&lt;h2 id=&#34;ebpf-off-cpu分析in-kernel简报&#34;&gt;eBPF off-CPU分析：in-kernel简报&lt;/h2&gt;
&lt;p&gt;怎么度量off-CPU time？最简单的办法是应用层tracing，记录重点代码各个函数和代码块的耗时。大多数情况下，这其实就是最好的方案，精度也不错。不过相比火焰图还不够帅，也不够全面，毕竟在哪加时间戳依赖主观判断，有时候真正的瓶颈会出现在意想不到的地方。&lt;/p&gt;
&lt;p&gt;Linux 4.8+&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;可使用eBPF做off-CPU分析。比如eBPF工具&lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/cpudist.py&#34;&gt;bcc/cpudist&lt;/a&gt;，&lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/tools/offcputime_example.txt&#34;&gt;bcc/offcputime&lt;/a&gt;。offcputime生成的call stacks可以直接用&lt;a href=&#34;https://github.com/brendangregg/FlameGraph&#34;&gt;flamegraph.pl&lt;/a&gt;绘制off-CPU火焰图。bcc包含了大量工具，其具体使用方式可参照其&lt;a href=&#34;https://github.com/iovisor/bcc/blob/master/docs/tutorial.md&#34;&gt;官方tutorial&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;eBPF tracing与传统的off-CPU tracer(比如&lt;code&gt;perf&lt;/code&gt;)相比，最显著的优势是不必把所有内核事件往用户空间dump（调度事件是非常频繁的，&lt;code&gt;perf&lt;/code&gt;往往生成巨量数据，注入的额外开销太大，不仅仅是CPU开销，还有磁盘IO开销），而是在内核就按照某种可编程的规则做了总结，把精简的信息输出出来。&lt;/p&gt;
&lt;p&gt;此外，内核支持eBPF后，各种off-CPU分析需求都可以统一用eBPF实现，不再需要针对不同场景使用或制作不同的工具了——此前用&lt;code&gt;perf&lt;/code&gt;做事件追踪，用storage tracing做存储IO追踪，用内核统计数据观测调度时延，不成体系，且性能良莠不一。&lt;/p&gt;
&lt;p&gt;eBPF追踪off-CPU时长的思路是在context switch事件结束时记录一次stack（off—CPU期间stack是不变的，一次足矣），为当前context的off-CPU时长增加线程睡眠时间。其伪代码如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;on context switch finish:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	sleeptime[prev_thread_id] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; timestamp
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;sleeptime[thread_id]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	delta &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; timestamp &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; sleeptime[thread_id]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	totaltime[pid, execname, user stack, kernel stack] &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; delta
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	sleeptime[thread_id] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;on tracer exit:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; each key &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; totaltime:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		print key
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;		print totaltime[key]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;所以什么是ebpf&#34;&gt;所以，什么是eBPF？&lt;/h2&gt;
&lt;p&gt;简单说，eBPF是一个允许跑自定义代码做一些tracing和系统监控的in-kernel runtime，是BPF的升级版。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/ebpf.png&#34; alt=&#34;ebpf&#34;&gt;&lt;/p&gt;
&lt;p&gt;最初的BPF，即Berkeley Packet Filter，是一个用于报文过滤的几乎被遗忘的古老内核特性。eBPF在BPF基础上做了扩展，允许事件源从报文扩展到多种多样的事件源，eBPF VM有更大的存储空间，更多寄存器和64位word size——BPF事实上提供了一个in-kernel的沙盒环境，或者说虚拟机，安全且受限地执行用户定义的程序。因此eBPF机制的出现实际上在内核态程序、用户态程序之外创造了新的软件品类。&lt;/p&gt;
&lt;p&gt;eBPF程序不是预编译或解释的，而是JIT CO-RE&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;的，程序出错既不abort，也不panic，而是返回error message。内核态直接访问资源，用户态通过系统调用或fault访问资源，eBPF则是通过一些受限的helper访问资源——目前来说主要作用还是tracing，做一些可观测性上的工作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/ebpf2.png&#34; alt=&#34;ebpf2&#34;&gt;&lt;/p&gt;
&lt;p&gt;eBPF把JIT编译器和安全验证器直接放到了内核里，用户态的bpf程序先经过parser变成AST，再做一些构造和语法分析，然后生成IR，最终生成优化后的bytecode。BPF bytecode作为输入进入内核的JIT和verifier再编译成机器码给CPU执行。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/ebpf3.png&#34; alt=&#34;ebpf3&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ebpf的其他应用&#34;&gt;eBPF的其他应用&lt;/h2&gt;
&lt;p&gt;eBPF除了用在可观测性上，还可以应用于网络，在L3/L4/L7做traffic control, monitoring或load balancing，比如libbpf &lt;code&gt;tc&lt;/code&gt;/&lt;code&gt;qdiscs&lt;/code&gt; library, &lt;code&gt;XDP&lt;/code&gt;(裸金属高性能可编程网络)/&lt;code&gt;Cilium&lt;/code&gt;(高性能云原生网络)/&lt;code&gt;Katran&lt;/code&gt;(传输层负载均衡)。&lt;/p&gt;
&lt;p&gt;此外，eBPF还可以应用于安全领域。毕竟eBPF可以观测系统中的各种事件，比如监控某些敏感文件（/etc/passwd这种）是否被篡改。基于这种观测能力在加上一些安全相关的先验知识，就可以做一些安全工具。K8s的&lt;code&gt;seccomp&lt;/code&gt;工具就是基于eBPF实现的。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;区别于off-CPU分析，这里的CPU profiling指狭义的on-CPU分析，不考虑阻塞中的thread time。&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;不过Linux 5.x才有完整的CO-RE和BTF支持，其中BTF(BPF Type Format)是为eBPF设计的内核数据结构描述机制。&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;CO-RE: Compile Once Run Everywhere，也就是说BPF bytecode是可以relocate的。&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</content>
    </item>
    
    <item>
      <title>Local LLM</title>
      <link>https://cmbbq.github.io/posts/local-llm/</link>
      <pubDate>Thu, 02 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/local-llm/</guid>
      <description>Minimalist Local LLM 新一轮AI热潮中，stable diffusion和LLM均呈现出了前所未有的社区繁荣。繁荣的同时带来了信息的爆炸和臃肿，极简主义显得尤为珍贵。
在预训练模型中，极简主义的代表是小而美的“大模型”Mistral7B。在推理框架中，极简主义的代表是对抗各种大公司重量级项目的llama.cpp。
Mistral7B是开源模型领域的新近进展，模型虽小，却展示了惊人的能力。根据Mistral AI自己的benchmark和一些社区的测试反馈，几乎在各方面击败llama2 13B。
Mistral AI提供了在本地GPU、云端跑Mistral7B的官方支持。
不过这么小的模型，其实也可以在本地CPU上跑的——借助llama.cpp。
llama.cpp和whisper.cpp大体上是Georgi Gerganov的个人项目，基于自己写的ml primitive库ggml实现，用最少的代码量实现了尽可能多的推理优化，针对Apple芯片做得尤其好，也适用于Linux服务器，适用于有或无GPU，单或多GPU的硬件setup。ggml规定的模型文件格式ggul甚至也在社区中被广泛使用，成为事实上的标准之一。
Intel Xeon Platinum 8336C + Debian10 从TheBloke上下载特定量化版本的模型：https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF git clone https://github.com/ggerganov/llama.cpp.git apt-get install libopenblas-dev 安装blas库，这里选用openblas。 修改pkgconfig/openblas.pc或Makefile解决llama.cpp对openblas的依赖问题。最终要保证给编译期提供正确的LD Flags。 make LLAMA_OPENBLAS=1 完成编译。 ./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot; 64物理核打满，采样速率4000t/s，prompt处理速率80t/s，生成速率20t/s。
Apple M1 Pro + macOS Monterey 下载模型和llama.cpp repo。 直接make就默认启用metal gpu加速和Accelerated Framework。 ./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.</description>
      <content>&lt;h2 id=&#34;minimalist-local-llm&#34;&gt;Minimalist Local LLM&lt;/h2&gt;
&lt;p&gt;新一轮AI热潮中，stable diffusion和LLM均呈现出了前所未有的社区繁荣。繁荣的同时带来了信息的爆炸和臃肿，极简主义显得尤为珍贵。&lt;/p&gt;
&lt;p&gt;在预训练模型中，极简主义的代表是小而美的“大模型”Mistral7B。在推理框架中，极简主义的代表是对抗各种大公司重量级项目的llama.cpp。&lt;/p&gt;
&lt;p&gt;Mistral7B是开源模型领域的新近进展，模型虽小，却展示了惊人的能力。根据Mistral AI自己的benchmark和一些社区的测试反馈，几乎在各方面击败llama2 13B。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/mistral.png&#34; alt=&#34;mistral&#34;&gt;&lt;/p&gt;
&lt;p&gt;Mistral AI提供了在本地GPU、云端跑Mistral7B的官方支持。&lt;/p&gt;
&lt;p&gt;不过这么小的模型，其实也可以在本地CPU上跑的——借助llama.cpp。&lt;/p&gt;
&lt;p&gt;llama.cpp和whisper.cpp大体上是Georgi Gerganov的个人项目，基于自己写的ml primitive库ggml实现，用最少的代码量实现了尽可能多的推理优化，针对Apple芯片做得尤其好，也适用于Linux服务器，适用于有或无GPU，单或多GPU的硬件setup。ggml规定的模型文件格式ggul甚至也在社区中被广泛使用，成为事实上的标准之一。&lt;/p&gt;
&lt;h2 id=&#34;intel-xeon-platinum-8336c--debian10&#34;&gt;Intel Xeon Platinum 8336C + Debian10&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;从TheBloke上下载特定量化版本的模型：https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git clone https://github.com/ggerganov/llama.cpp.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;apt-get install libopenblas-dev&lt;/code&gt; 安装blas库，这里选用openblas。&lt;/li&gt;
&lt;li&gt;修改pkgconfig/openblas.pc或Makefile解决llama.cpp对openblas的依赖问题。最终要保证给编译期提供正确的LD Flags。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;make LLAMA_OPENBLAS=1&lt;/code&gt; 完成编译。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;64物理核打满，采样速率4000t/s，prompt处理速率80t/s，生成速率20t/s。&lt;/p&gt;
&lt;h2 id=&#34;apple-m1-pro--macos-monterey&#34;&gt;Apple M1 Pro + macOS Monterey&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;下载模型和llama.cpp repo。&lt;/li&gt;
&lt;li&gt;直接&lt;code&gt;make&lt;/code&gt;就默认启用metal gpu加速和Accelerated Framework。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./main -m /path/to/mistral-7b-instruct-v0.1.Q5_K_M.gguf --color -c 2048 --temp 0.8 --repeat_penalty 1.1 -n -1 -i -p &amp;quot;xx&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;M1 GPU打到90%（剩下还有10%WindowServer在用），采样速率9000t/s，prompt处理速率60t/s，生成速率20t/s。&lt;/p&gt;
&lt;p&gt;一边看视频（chrome GPU占用约20%），一边跑mistral也能有19.69t/s。&lt;/p&gt;
&lt;h2 id=&#34;token-sampling&#34;&gt;Token Sampling&lt;/h2&gt;
&lt;p&gt;不同的sampling机制对文本生成有显著影响，本地LLM的可玩性来源很大程度上就是客制sampling。&lt;/p&gt;
&lt;p&gt;Transformer模型根据前n个token，预测下一个token。每个token都有其next tokens的score，而next tokens的取值范围就是vocabulary（transformer架构最后有一个linear layer，将输出映射到vocabulary上，所有tokens都有对应的scores/logits），这些score经过softmax将score数组转化成probability数组，根据probability挑选下一个token的过程就称为token sampling。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Greedy采样： 如果每次都确定性地选择probability最高的那个token，单步决策，就是greedy sampler，优势是速度快，劣势是牺牲了模型的多样性，容易陷入重复循环和对训练数据的过拟合。llama.cpp里设置&amp;ndash;top_p 0就相当于开greedy采样。&lt;/li&gt;
&lt;li&gt;Top-k采样： 从概率分布的前k个token里面进行随机采样。&lt;/li&gt;
&lt;li&gt;Top-p采样： 引入超参p，把token probability降序排序后，选取前面一部分，使这部分概率和为p，而不是固定选前k个token。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在使用LLM时，采样机制不同，效果也会大不相同。想要更高的确定性，更朴实无华的预测，则可以尝试greedy、低k的topk、低p的top-p采样。想要多样性和新颖性，则可尝试高k的top-k和高p的top-p。&lt;/p&gt;
&lt;p&gt;除了top-k，top-p外，llama.cpp还实现了一些额外的采样机制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Min p filter：允许采样环节剔除低于阈值的低分候选。&lt;/li&gt;
&lt;li&gt;Tail free sampling：根据概率的二阶导数之和采样，即根据概率降速剔除尾部低概率tokens。&lt;/li&gt;
&lt;li&gt;Locally typical samplling：参数控制是否倾向局部语境内的典型的tokens。&lt;/li&gt;
&lt;li&gt;Mirostat sampling：&lt;a href=&#34;https://arxiv.org/abs/2007.14966&#34;&gt;Mirostat算法&lt;/a&gt;会调整top-k的k，避免陷入boredom trap（模式崩塌）和perplexity trap（不一致）。&lt;/li&gt;
&lt;li&gt;logit bias: 人为指定某个token的优先级，比如&amp;ndash;logit-bias 29905-inf就把&amp;rsquo;\&amp;rsquo; token设为负无穷。&lt;/li&gt;
&lt;li&gt;temperature: 在对score向量（logits）做softmax前，把logits/temperature，则temp越高，softmax后概率分布的高低悬殊就会越接近，也就更有利于低概率tokens崭露头角。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prompting&#34;&gt;Prompting&lt;/h2&gt;
&lt;p&gt;让llm假装自己是一个javascript console，然后进行交互式的指令应答，甚至还能进行简单的算术计算，理解函数调用的返回值类型。当然，不能对正确性抱有期待。
&lt;img src=&#34;https://cmbbq.github.io/img/js_console.png&#34; alt=&#34;console&#34;&gt;&lt;/p&gt;
&lt;p&gt;用一大段prompt，让llm假装自己是一个爱说emoji，语言风格浮夸的音乐推荐bot，效果相当不错。
&lt;img src=&#34;https://cmbbq.github.io/img/music_bot.png&#34; alt=&#34;bot&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Efficient ANNS at Scale</title>
      <link>https://cmbbq.github.io/posts/efficient-anns-at-scale/</link>
      <pubDate>Tue, 12 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/efficient-anns-at-scale/</guid>
      <description>向量检索分为KNN和ANN，KNN就是用brute-force计算出query向量和所有索引向量的相似度，取前k个。ANN则是通过近似手段加速计算，以误差换效率。
如何做高效的向量检索？方法有二，一是提升编码率（不仅能降低内存占用，也能加速计算，因为便于SIMD且减少访存）的量化（IVF向量量化、PQ量化、ScaNN各向异性量化、4bit量化），二是缩小搜索空间的图搜索（HNSW）。
如何在十亿、百亿级特征库上做向量检索？主要挑战是索引无法放入内存，因而需要考虑内存-磁盘混合方案（SPANN）。
本文只介绍一小部分有效且正交的技术，这些技术组合在一起可形成当前的最佳实践。其他一些古典、经典技术可以参考本文引用的论文的背景综述。
相似度 首先回顾一下什么是向量之间的相似度:
对向量X和Y来说，x和y越接近，则x越大的地方y也大，x小的地方y也小，则内积越大。因此内积可用作相似度打分，不过不能抵抗尺度变化。 为了抵抗尺度变化，往往将内积正则化到[-1,1]，就变成了余弦相似度cosθ，θ为向量夹角，显然θ越小，向量越接近。 欧几里得距离正则化到[-1,1]后相当于sqrt(2-2cosθ)。可见欧式距离平方也是和余弦相似度成比的。 正则化后欧式距离、内积、cosθ三者同源，所以我们一般就用余弦相似度cosθ去度量向量/embedding之间的相似程度。除非又特殊的抵抗位置变化的需求，可采用皮尔森相关系数，皮尔森相关系数是协方差（两个变量的总体误差）的正则化，不过它同时也可以被视为将x和y中心化后的余弦相似度。
向量量化 量化器是D维向量 x ∈ R^D 到 q(x) ∈ C = {c_0, c_1, &amp;hellip; c_k-1} 的映射函数q。c_i即聚类中心centroid。 所谓密码本（codebook）就是一个lookup table，用centroid下标作为原始向量的低bit表示，密码本大小就是k。
向量量化本质是有损压缩，表征学习本质上也是有损压缩，一个白盒，一个黑盒，都是试图降低高维数据的编码率。
IVF：聚类、倒排、剪枝 倒排（特指IVF）是一种古老的量化技术，早期应用于Video Google。核心思想是基于k-means聚类做向量量化。k-means聚类起源于信号处理领域，目标是把n个向量分区到k个clusters，每个向量属于离centroid最近的cluster。
聚类后有些边缘数据点既属于A，也属于B，那就把它同时放到A和B的centroid对应的postinglist里面去，不过重复放置会导致postinglist膨胀，为了应对这种膨胀，可采取剪枝策略，具体可参考微软的SPTAG项目和SPANN论文。
Faiss的IVFFlat就是在聚类形成centroid为term的倒排后提供检索能力，nlist为聚类数，nprobe为临近centroid探测数，nprobe=nlist时退化为暴搜。IVFFLat在检索时将query邻近的nprobe个centroid对应的所有原始向量作为搜索空间进行暴搜。由于centroid的表示方式可以简化成clusterid，所以实际上IVF也是一种量化：用一个数组存放所有的centroid向量，数组下标即可表示centroid。
PQ：乘积量化 基于聚类的量化对低维向量比较有效，但随着向量维度升高，出现误差也变高，而且这种误差不是提升聚类数目能解决的。如果把聚类数目（codebook大小）提升到2^64，训练这个聚类的开销就会高得无法接受，需要数倍于2^64的样本，内存显然也放不下。**乘积量化（PQ）**就是降维手段解决IVF量化误差的技术，Jegou et al., 2011以及ScaNN均讨论了PQ。其核心思想是对索引数据量化，将d维空间切分为M个子空间，将高维向量的误差拟合为分段的低维向量误差之和。
乘积量化中，IVF倒排链存的不是原始向量，而是原始向量的一种编码：
求原始d维向量与高维粗聚类centroid的残差（残差的目的是中心化，让数据点分布变得更集中，使精度更好） 将残差向量切成M个分段，每个分段维度为d/M 每个分段做$k=2^n$个细聚类，n即为低维细聚类clusterid的编码长度，4bit或8bit 用每个分段的邻近低维细聚类centroid去量化该分段上的原始残差分段 Faiss的IVFPQ就结合了IVF和PQ，IVF粗聚类是一级量化，PQ则是二级量化。一方面索引体积减小，另一方面计算上也只需要计算残差和分段细聚类centroid近邻的距离，还能SIMD加速，性能收益非常显著。不过IVFPQ的distance究竟只是一种拟合，是距离粗聚类centroid的距离加上M个分段里query残差和邻近细聚类centroid的距离总和，这种拟合可以一定程度上帮助找到近邻，但不适合用作距离，可以把IVFPQ当做粗排，结果再做一次brute-force或高精度ANN重算，得到更精确的距离。
ScaNN提出了各向异性量化，算是正本清源，纠正了此前就近选择centroid在数学上的错误，并且实践了4-bit量化取得非常好的效果（其实4bit量化的效果还比各向异性更大，但各向异性在理论上贡献更大）。传统的量化把数据点量化到离自己最近的聚类中心，但这么做未必是误差最低的，越平行的向量之间内积越大，越正交的向量之间内积越小，因此赋予平行向量更高的误差惩罚权重效果更好。这就意味着不一定量化到距离最近的centroid了，目标是整体编码率不变的前提下，让平行方向量化误差更小，正交方向误差高一些其实无所谓，从而提升整体的ANN效果。
假设d/M = 4，采用8bit量化，原始向量是256维f32向量，则PQ将256个float32压缩到64个int8，内存减少到1/16。乘积量化显然可以显著减少内存和存储开销，其实这也能加速计算，原因如下：
高效点乘（其实也是误差换效率）：O(dk+mn) 比 O(nd)快，d维度的query和n个量化后的向量点乘，引入m个大小为k的量化codebook，mn可以忽略不计，k远小于n。 memory bandwidth：现代处理器需要“高计算量per memory read”才能充分发挥硬件的计算性能。量化压缩数据点后内存带宽的使用也下降了，变得更计算密集。 向量量化中引入的低bit表示，尤其是4bit量化，和低bit浮点数量化一样，可以享受SIMD、AMX的性能红利。 最佳实践：根据应用场景将各种正交技术进行正确组合 HNSW、IVF、PQ、各向异性的score-aware quantization loss、brute-force事实上都是正交的技术，可以组合起来使用。
比如进行了PQ量化的数据点可以再构建一个HNSW，用图方法在超大规模数据集上做查询显然比ivfpq更高效。各向异性量化则是对此前PQ量化的修正。考虑到量化后的距离失真，还能用brute-force把ivf+pq+hnsw的粗排结果重算一下得到最精确的距离，由于已经经过一轮粗排，候选数据集大小从十亿级别缩小到万级别，brute-force的开销就完全可以接受了。</description>
      <content>&lt;p&gt;向量检索分为KNN和ANN，KNN就是用brute-force计算出query向量和所有索引向量的相似度，取前k个。ANN则是通过近似手段加速计算，以误差换效率。&lt;/p&gt;
&lt;p&gt;如何做高效的向量检索？方法有二，一是提升编码率（不仅能降低内存占用，也能加速计算，因为便于SIMD且减少访存）的量化（IVF向量量化、PQ量化、ScaNN各向异性量化、4bit量化），二是缩小搜索空间的图搜索（HNSW）。&lt;/p&gt;
&lt;p&gt;如何在十亿、百亿级特征库上做向量检索？主要挑战是索引无法放入内存，因而需要考虑内存-磁盘混合方案（SPANN）。&lt;/p&gt;
&lt;p&gt;本文只介绍一小部分有效且正交的技术，这些技术组合在一起可形成当前的最佳实践。其他一些古典、经典技术可以参考本文引用的论文的背景综述。&lt;/p&gt;
&lt;h1 id=&#34;相似度&#34;&gt;相似度&lt;/h1&gt;
&lt;p&gt;首先回顾一下什么是&lt;strong&gt;向量之间的相似度&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对向量X和Y来说，x和y越接近，则x越大的地方y也大，x小的地方y也小，则内积越大。因此内积可用作相似度打分，不过不能抵抗尺度变化。&lt;/li&gt;
&lt;li&gt;为了抵抗尺度变化，往往将内积正则化到[-1,1]，就变成了余弦相似度cosθ，θ为向量夹角，显然θ越小，向量越接近。&lt;/li&gt;
&lt;li&gt;欧几里得距离正则化到[-1,1]后相当于sqrt(2-2cosθ)。可见欧式距离平方也是和余弦相似度成比的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/sim_measure.png&#34; alt=&#34;sim_measure&#34;&gt;&lt;/p&gt;
&lt;p&gt;正则化后欧式距离、内积、cosθ三者同源，所以我们一般就用余弦相似度cosθ去度量向量/embedding之间的相似程度。除非又特殊的抵抗位置变化的需求，可采用皮尔森相关系数，皮尔森相关系数是协方差（两个变量的总体误差）的正则化，不过它同时也可以被视为将x和y中心化后的余弦相似度。&lt;/p&gt;
&lt;h1 id=&#34;向量量化&#34;&gt;向量量化&lt;/h1&gt;
&lt;p&gt;量化器是D维向量 x  ∈ R^D 到 q(x) ∈ C = {c_0, c_1, &amp;hellip; c_k-1} 的映射函数q。c_i即聚类中心centroid。
所谓密码本（codebook）就是一个lookup table，用centroid下标作为原始向量的低bit表示，密码本大小就是k。&lt;/p&gt;
&lt;p&gt;向量量化本质是有损压缩，表征学习本质上也是有损压缩，一个白盒，一个黑盒，都是试图降低高维数据的编码率。&lt;/p&gt;
&lt;h1 id=&#34;ivf聚类倒排剪枝&#34;&gt;IVF：聚类、倒排、剪枝&lt;/h1&gt;
&lt;p&gt;倒排（特指IVF）是一种古老的量化技术，早期应用于&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2003/Sivic03/sivic03.pdf&#34;&gt;Video Google&lt;/a&gt;。核心思想是基于k-means聚类做向量量化。k-means聚类起源于信号处理领域，目标是把n个向量分区到k个clusters，每个向量属于离centroid最近的cluster。&lt;/p&gt;
&lt;p&gt;聚类后有些边缘数据点既属于A，也属于B，那就把它同时放到A和B的centroid对应的postinglist里面去，不过重复放置会导致postinglist膨胀，为了应对这种膨胀，可采取剪枝策略，具体可参考微软的SPTAG项目和&lt;a href=&#34;https://arxiv.org/pdf/2111.08566.pdf&#34;&gt;SPANN论文&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;Faiss的IVFFlat就是在聚类形成centroid为term的倒排后提供检索能力，nlist为聚类数，nprobe为临近centroid探测数，nprobe=nlist时退化为暴搜。IVFFLat在检索时将query邻近的nprobe个centroid对应的所有原始向量作为搜索空间进行暴搜。由于centroid的表示方式可以简化成clusterid，所以实际上IVF也是一种量化：用一个数组存放所有的centroid向量，数组下标即可表示centroid。&lt;/p&gt;
&lt;h1 id=&#34;pq乘积量化&#34;&gt;PQ：乘积量化&lt;/h1&gt;
&lt;p&gt;基于聚类的量化对低维向量比较有效，但随着向量维度升高，出现误差也变高，而且这种误差不是提升聚类数目能解决的。如果把聚类数目（codebook大小）提升到2^64，训练这个聚类的开销就会高得无法接受，需要数倍于2^64的样本，内存显然也放不下。**乘积量化（PQ）**就是降维手段解决IVF量化误差的技术，&lt;a href=&#34;https://lear.inrialpes.fr/pubs/2011/JDS11/jegou_searching_with_quantization.pdf&#34;&gt;Jegou et al., 2011&lt;/a&gt;以及&lt;a href=&#34;https://arxiv.org/pdf/1908.10396.pdf&#34;&gt;ScaNN&lt;/a&gt;均讨论了PQ。其核心思想是对索引数据量化，将d维空间切分为M个子空间，将高维向量的误差拟合为分段的低维向量误差之和。&lt;/p&gt;
&lt;p&gt;乘积量化中，IVF倒排链存的不是原始向量，而是原始向量的一种编码：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;求原始d维向量与高维粗聚类centroid的残差（残差的目的是中心化，让数据点分布变得更集中，使精度更好）&lt;/li&gt;
&lt;li&gt;将残差向量切成M个分段，每个分段维度为d/M&lt;/li&gt;
&lt;li&gt;每个分段做$k=2^n$个细聚类，n即为低维细聚类clusterid的编码长度，4bit或8bit&lt;/li&gt;
&lt;li&gt;用每个分段的邻近低维细聚类centroid去量化该分段上的原始残差分段&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Faiss的IVFPQ就结合了IVF和PQ，IVF粗聚类是一级量化，PQ则是二级量化。一方面索引体积减小，另一方面计算上也只需要计算残差和分段细聚类centroid近邻的距离，还能SIMD加速，性能收益非常显著。不过IVFPQ的distance究竟只是一种拟合，是距离粗聚类centroid的距离加上M个分段里query残差和邻近细聚类centroid的距离总和，这种拟合可以一定程度上帮助找到近邻，但不适合用作距离，可以把IVFPQ当做粗排，结果再做一次brute-force或高精度ANN重算，得到更精确的距离。&lt;/p&gt;
&lt;p&gt;ScaNN提出了各向异性量化，算是正本清源，纠正了此前就近选择centroid在数学上的错误，并且实践了4-bit量化取得非常好的效果（其实4bit量化的效果还比各向异性更大，但各向异性在理论上贡献更大）。传统的量化把数据点量化到离自己最近的聚类中心，但这么做未必是误差最低的，越平行的向量之间内积越大，越正交的向量之间内积越小，因此赋予平行向量更高的误差惩罚权重效果更好。这就意味着不一定量化到距离最近的centroid了，目标是整体编码率不变的前提下，让平行方向量化误差更小，正交方向误差高一些其实无所谓，从而提升整体的ANN效果。&lt;/p&gt;
&lt;p&gt;假设d/M = 4，采用8bit量化，原始向量是256维f32向量，则PQ将256个float32压缩到64个int8，内存减少到1/16。乘积量化显然可以显著减少内存和存储开销，其实这也能加速计算，原因如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高效点乘（其实也是误差换效率）：O(dk+mn) 比 O(nd)快，d维度的query和n个量化后的向量点乘，引入m个大小为k的量化codebook，mn可以忽略不计，k远小于n。&lt;/li&gt;
&lt;li&gt;memory bandwidth：现代处理器需要“高计算量per memory read”才能充分发挥硬件的计算性能。量化压缩数据点后内存带宽的使用也下降了，变得更计算密集。&lt;/li&gt;
&lt;li&gt;向量量化中引入的低bit表示，尤其是4bit量化，和低bit浮点数量化一样，可以享受SIMD、AMX的性能红利。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;最佳实践根据应用场景将各种正交技术进行正确组合&#34;&gt;最佳实践：根据应用场景将各种正交技术进行正确组合&lt;/h1&gt;
&lt;p&gt;HNSW、IVF、PQ、各向异性的score-aware quantization loss、brute-force事实上都是正交的技术，可以组合起来使用。&lt;/p&gt;
&lt;p&gt;比如进行了PQ量化的数据点可以再构建一个HNSW，用图方法在超大规模数据集上做查询显然比ivfpq更高效。各向异性量化则是对此前PQ量化的修正。考虑到量化后的距离失真，还能用brute-force把ivf+pq+hnsw的粗排结果重算一下得到最精确的距离，由于已经经过一轮粗排，候选数据集大小从十亿级别缩小到万级别，brute-force的开销就完全可以接受了。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Tech Talk: Evolution of Data Center Applications</title>
      <link>https://cmbbq.github.io/posts/tech-talk-evolution-of-data-center-applications/</link>
      <pubDate>Sat, 05 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/tech-talk-evolution-of-data-center-applications/</guid>
      <description>数据中心应用（数据库、消息队列、检索服务、参数服务器、直播服务、广告服务等）作为互联网服务的主体，位于终端应用和硬件基础设施之间，桥接了需求端和硬件端，同时还受到成本、营收、算法、法律等因素的制约。 近期硬件侧、算法侧均有变化或变革，因而有必要反思数据中心应用的演进方向，讨论数据中心应用的近未来发展趋势、潜在的创新点和可收割的低垂果实。
变革中的不变量：数据中心应用能耗 全球数据中心能耗十年来几乎没有增长，而同期的服务需求、计算、存储、数据传输总量则经历了数量级增长。作为从业者，我们对互联网的爆发式增长应该不陌生，因此全球数据中心能耗从10到18年仅涨了6%的事实略微反直觉——从能耗的角度看，这几乎是一个停滞的产业。
CMOS工艺和软硬件技术的整体进步可以解释服务器能量效率上的提升，但能耗停滞的根本原因还在于互联网企业盈利的本质是对有支付能力的10亿国外人口和10亿国内人口征服务税，个别成功的初创企业可以迅速扩张，但互联网行业作为整体，在数据中心能耗上的投入增长受制于全球平民收入增长。
当初创企业的规模扩张到互联网规模后，机器资源的增长速率应该会从爆炸式上升陡然转入停滞，进入自然汰换阶段。 从这个角度可以得出结论，数据中心应用在规模扩张结束后，也应从能耗粗放型转为精细集约型，通过软硬件协同充分释放硬件潜力，通过算法创新减少总计算量。
算法迭代：从演绎推理到归纳推断 算法侧的趋势是“transformers getting even more attention”。
计算机理论源于符号逻辑，1930年代图灵、哥德尔、邱奇各自独立提出图灵机、广义递归函数、lambda演算，以及三者相应的可计算性。图灵机的停机问题和哥德尔不完备定律驳斥了1920年代希尔伯特计划的可行性，证明了形式化系统是不完备的，通过有限个公理和规则永远无法推导出全部的真理。
正如人的无知可以分为问题（可计算）和神秘（不可计算），人的理性也可以分为演绎推理和归纳推断，前者求解问题，后者分析神秘。
计算机程序基于图灵机模型，天然就是适合进行演绎推理的形式化系统。至今我们依然基于规则系统(人工编程进行条件控制）、信号处理（比如音频特征提取）、状态机等形式化方法实现大多数数据中心应用：比如各类数据库、检索系统、参数服务器、游戏AI。
AI时代来临之后，通过更多的权重（作为公理）和由大量算子（作为规则）组成深度神经网络（从实现角度来说，仍然是形式化系统）去模拟或者说近似“归纳推断”。（纯粹贝叶斯主义的形式化表达是所罗门诺夫归纳推断法，这个方法本身是永不停机，越算越停不下来的，所以只能近似，不能用实现）这种近似无论多么粗糙、低效，归根到底还是提升了计算机语言和集成电路的表达能力，完成了推理到推断的跳跃。
数据中心应用多了归纳推断能力之后，就产生了对复杂现实（现实信号，如图片、语料库、音频，整体上具有高维度的同时，局部往往还存在具有高数据精致度的结构，这里精致度可以用香农信息熵或柯氏复杂性形式化定义）理解能力(pin down reality)，因此涌现出了一些基于知识的AI系统(knowledge-based AI)，比如推荐系统、广告投送、翻唱识别、聊天机器人。共同点是将现实映射到表征低维子空间上，这个子空间里的向量或者说embedding可以从某个角度有效刻画现实。Transformer做得更进一步，等价于将现实映射到多个相干性低的低维子空间上（见https://arxiv.org/abs/2306.01129），从而从多角度多层次理解现实，通过表征空间编码率与各子空间编码率之和的差值度量特征的紧凑性和判别力。
在游戏和博弈领域，强化学习也取得了规则系统无法企及的成就，在星际2、Dota2（固定阵容5v5，solo）、围棋等领域击败职业选手。
此外，归纳推断相比演绎推理还有一个重要特质，那就是更符合人类大脑（或推广到任意地球生物大脑，蜜蜂的3d寻路避障能力，蚊子的血管定位能力都是基于天然的贝叶斯归纳推断。也只有受过良好训练的人类才能进行缓慢的演绎推理）的思维习惯，因此人类社会的传统，人类语言的不确定性、人类的偏好都难以用形式化方法描述，却可以被概率语言描述，被归纳推断方法预测。
算法进步的下一个课题则是研究同时具备演绎推理和归纳推断能力的模型。现有的Transformer的确展现出简单多步算术的拟合能力，但拟合出来的是一个步数相关的超复杂非线性系统，而不是简单的算术规则，所以往往正确率到八十左右就再也上不去了。对于形式化系统来说，推理步数不影响公理和规则总数，只稍稍影响计算量。但对于LLM这种归纳推断模型来说，一旦推理步数变多，计算变得更复杂，LLM的符号逻辑能力就会迅速下降，在训练数据中从未出现过相同执行路径的动态规划问题里面可以直接降到0。理论上来说，无限精度的Transformer是图灵完备的（见https://arxiv.org/abs/1901.03429），而只要不是无限精度，就不是图灵完备，实际应用中模型权重和计算的精度即使以常规计算机应用标准看也是非常低的，而模型架构又没有针对多步逻辑推理进行设计，再加上深度学习本身就是在拟合一个近似函数，三重因素叠加导致现有LLM的演绎推理能力不尽如人意。
从算法迭代角度讲，数据中心应用的发展趋势是在保证“可计算性”(字面意思，非形式化定义)的前提下，追求“人性化”(align to humanity)、“完备化”（pin down reality）这两个演绎推理手段无法达成的理想属性。智能创作、个性化推荐、聪明的Agent（助手、游戏NPC、机器人、自动驾驶），复杂系统预测（天气预报、交易系统），大模型训练（模型并行、高性能网络、C2C互连、光学I/O、异构计算）都是有前景的方向。
而那些已经能用规则系统高效解决的问题则不必强行应用归纳推断模型，那样只会增加成本和错误率，比如音乐指纹识别、关系型数据库、图片旋转变形、无损压缩。甚至某些看似“神秘”的领域，如OOD text classification，基于表征学习白盒化研究成果和信息论，也能给出直接的数学解，比如近期一个研究（“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors）就用14行代码超过了百亿参数的bert，（后来被发现实验代码有误，实际上效果没那么好）这个论文里提出用gzip等无损压缩工具去近似文本数据的柯氏复杂性，用信息距离（原理类似马毅提出的rate reduction）做KNN，效果非常好，在音频领域也可以做类似的尝试。
数据中心硬件基础设施 先简单介绍一下常见的数据中心硬件基础设施：
比较老的是Cascade Lake 14nm的24核per die的机器，一个芯片24核，两个就是48个物理核，也就是我们平时说的96核机器。也有其他套餐，最多是每个die上28核，总共112个逻辑核。 比较新的Ice Lake是10nm工艺一般是32核的，对应128个逻辑核，最贵的套餐可以到40核。仍然是基于Monolithic die，巨大的晶粒上用mesh总线把40个核放到一起。 最新的7nm工艺Sapphire Rapids，以及对标它的AMD Genoa今年已经量产。 关于网卡，现在用的比较多的是Mellanox 25G CX4/CX5卡，也有相当多的100G dual-port CX6卡，都是支持RDMA的，不用RDMA也能提供相当好的高速以太网性能。如果是做虚拟化的，比如AWS，阿里云，还可以把这些网卡的smartNiC能力利用起来，offload虚拟化开销。 相对之前的Lakes，Sapphire Rapids变化非常大，此前的monolithic die路线确实走到了极限，Sapphire Rapids也进入了multi-die时代：芯片内部分为4个die，这也可以视作向chiplet方向演化。
Sapphire Rapids最多支持8个socket，每个芯片可以支持60核。IO技术上支持CXL 1.1、PCIe 5.0、UPI 2.0，HBM2e(optional)。
AMD的第四代EPYC的主力型号Genoa今年Q1也开始量产，基本上和Sapphire Rapids对标，采用5nm工艺。虽然说AVX512被很多人诟病，实用表现非常一般，但Zen4也支持了，毕竟XeongenEPYC目标客户就是音视频处理和AI workload越来越多的数据中心应用。Genoa是每个die/chiplet 8核的，12个CPU cluster die中间围绕一个IO die的设计，相当于一个芯片96核，是典型的chiplet范式设计。</description>
      <content>&lt;p&gt;数据中心应用（数据库、消息队列、检索服务、参数服务器、直播服务、广告服务等）作为互联网服务的主体，位于终端应用和硬件基础设施之间，桥接了需求端和硬件端，同时还受到成本、营收、算法、法律等因素的制约。
近期硬件侧、算法侧均有变化或变革，因而有必要反思数据中心应用的演进方向，讨论数据中心应用的近未来发展趋势、潜在的创新点和可收割的低垂果实。&lt;/p&gt;
&lt;h2 id=&#34;变革中的不变量数据中心应用能耗&#34;&gt;变革中的不变量：数据中心应用能耗&lt;/h2&gt;
&lt;p&gt;全球数据中心能耗十年来几乎没有增长，而同期的服务需求、计算、存储、数据传输总量则经历了数量级增长。作为从业者，我们对互联网的爆发式增长应该不陌生，因此全球数据中心能耗从10到18年仅涨了6%的事实略微反直觉——从能耗的角度看，这几乎是一个停滞的产业。&lt;/p&gt;
&lt;p&gt;CMOS工艺和软硬件技术的整体进步可以解释服务器能量效率上的提升，但能耗停滞的根本原因还在于互联网企业盈利的本质是对有支付能力的10亿国外人口和10亿国内人口征服务税，个别成功的初创企业可以迅速扩张，但互联网行业作为整体，在数据中心能耗上的投入增长受制于全球平民收入增长。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/DatacenterPower.jpeg&#34; alt=&#34;DatacenterPower&#34;&gt;&lt;/p&gt;
&lt;p&gt;当初创企业的规模扩张到互联网规模后，机器资源的增长速率应该会从爆炸式上升陡然转入停滞，进入自然汰换阶段。
从这个角度可以得出结论，数据中心应用在规模扩张结束后，也应从能耗粗放型转为精细集约型，通过软硬件协同充分释放硬件潜力，通过算法创新减少总计算量。&lt;/p&gt;
&lt;h2 id=&#34;算法迭代从演绎推理到归纳推断&#34;&gt;算法迭代：从演绎推理到归纳推断&lt;/h2&gt;
&lt;p&gt;算法侧的趋势是“transformers getting even more attention”。&lt;/p&gt;
&lt;p&gt;计算机理论源于符号逻辑，1930年代图灵、哥德尔、邱奇各自独立提出图灵机、广义递归函数、lambda演算，以及三者相应的可计算性。图灵机的停机问题和哥德尔不完备定律驳斥了1920年代希尔伯特计划的可行性，证明了形式化系统是不完备的，通过有限个公理和规则永远无法推导出全部的真理。&lt;/p&gt;
&lt;p&gt;正如人的无知可以分为问题（可计算）和神秘（不可计算），人的理性也可以分为演绎推理和归纳推断，前者求解问题，后者分析神秘。&lt;/p&gt;
&lt;p&gt;计算机程序基于图灵机模型，天然就是适合进行演绎推理的形式化系统。至今我们依然基于规则系统(人工编程进行条件控制）、信号处理（比如音频特征提取）、状态机等形式化方法实现大多数数据中心应用：比如各类数据库、检索系统、参数服务器、游戏AI。&lt;/p&gt;
&lt;p&gt;AI时代来临之后，通过更多的权重（作为公理）和由大量算子（作为规则）组成深度神经网络（从实现角度来说，仍然是形式化系统）去模拟或者说近似“归纳推断”。（纯粹贝叶斯主义的形式化表达是所罗门诺夫归纳推断法，这个方法本身是永不停机，越算越停不下来的，所以只能近似，不能用实现）这种近似无论多么粗糙、低效，归根到底还是提升了计算机语言和集成电路的表达能力，完成了推理到推断的跳跃。&lt;/p&gt;
&lt;p&gt;数据中心应用多了归纳推断能力之后，就产生了对复杂现实（现实信号，如图片、语料库、音频，整体上具有高维度的同时，局部往往还存在具有高数据精致度的结构，这里精致度可以用香农信息熵或柯氏复杂性形式化定义）理解能力(pin down reality)，因此涌现出了一些基于知识的AI系统(knowledge-based AI)，比如推荐系统、广告投送、翻唱识别、聊天机器人。共同点是将现实映射到表征低维子空间上，这个子空间里的向量或者说embedding可以从某个角度有效刻画现实。Transformer做得更进一步，等价于将现实映射到多个相干性低的低维子空间上（见https://arxiv.org/abs/2306.01129），从而从多角度多层次理解现实，通过表征空间编码率与各子空间编码率之和的差值度量特征的紧凑性和判别力。&lt;/p&gt;
&lt;p&gt;在游戏和博弈领域，强化学习也取得了规则系统无法企及的成就，在星际2、Dota2（固定阵容5v5，solo）、围棋等领域击败职业选手。&lt;/p&gt;
&lt;p&gt;此外，归纳推断相比演绎推理还有一个重要特质，那就是更符合人类大脑（或推广到任意地球生物大脑，蜜蜂的3d寻路避障能力，蚊子的血管定位能力都是基于天然的贝叶斯归纳推断。也只有受过良好训练的人类才能进行缓慢的演绎推理）的思维习惯，因此人类社会的传统，人类语言的不确定性、人类的偏好都难以用形式化方法描述，却可以被概率语言描述，被归纳推断方法预测。&lt;/p&gt;
&lt;p&gt;算法进步的下一个课题则是研究同时具备演绎推理和归纳推断能力的模型。现有的Transformer的确展现出简单多步算术的拟合能力，但拟合出来的是一个步数相关的超复杂非线性系统，而不是简单的算术规则，所以往往正确率到八十左右就再也上不去了。对于形式化系统来说，推理步数不影响公理和规则总数，只稍稍影响计算量。但对于LLM这种归纳推断模型来说，一旦推理步数变多，计算变得更复杂，LLM的符号逻辑能力就会迅速下降，在训练数据中从未出现过相同执行路径的动态规划问题里面可以直接降到0。理论上来说，无限精度的Transformer是图灵完备的（见https://arxiv.org/abs/1901.03429），而只要不是无限精度，就不是图灵完备，实际应用中模型权重和计算的精度即使以常规计算机应用标准看也是非常低的，而模型架构又没有针对多步逻辑推理进行设计，再加上深度学习本身就是在拟合一个近似函数，三重因素叠加导致现有LLM的演绎推理能力不尽如人意。&lt;/p&gt;
&lt;p&gt;从算法迭代角度讲，数据中心应用的发展趋势是在保证“可计算性”(字面意思，非形式化定义)的前提下，追求“人性化”(align to humanity)、“完备化”（pin down reality）这两个演绎推理手段无法达成的理想属性。智能创作、个性化推荐、聪明的Agent（助手、游戏NPC、机器人、自动驾驶），复杂系统预测（天气预报、交易系统），大模型训练（模型并行、高性能网络、C2C互连、光学I/O、异构计算）都是有前景的方向。&lt;/p&gt;
&lt;p&gt;而那些已经能用规则系统高效解决的问题则不必强行应用归纳推断模型，那样只会增加成本和错误率，比如音乐指纹识别、关系型数据库、图片旋转变形、无损压缩。甚至某些看似“神秘”的领域，如OOD text classification，基于表征学习白盒化研究成果和信息论，也能给出直接的数学解，比如近期一个研究（“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors）就用14行代码超过了百亿参数的bert，（后来被发现实验代码有误，实际上效果没那么好）这个论文里提出用gzip等无损压缩工具去近似文本数据的柯氏复杂性，用信息距离（原理类似马毅提出的rate reduction）做KNN，效果非常好，在音频领域也可以做类似的尝试。&lt;/p&gt;
&lt;h2 id=&#34;数据中心硬件基础设施&#34;&gt;数据中心硬件基础设施&lt;/h2&gt;
&lt;p&gt;先简单介绍一下常见的数据中心硬件基础设施：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;比较老的是Cascade Lake 14nm的24核per die的机器，一个芯片24核，两个就是48个物理核，也就是我们平时说的96核机器。也有其他套餐，最多是每个die上28核，总共112个逻辑核。&lt;/li&gt;
&lt;li&gt;比较新的Ice Lake是10nm工艺一般是32核的，对应128个逻辑核，最贵的套餐可以到40核。仍然是基于Monolithic die，巨大的晶粒上用mesh总线把40个核放到一起。&lt;/li&gt;
&lt;li&gt;最新的7nm工艺Sapphire Rapids，以及对标它的AMD Genoa今年已经量产。&lt;/li&gt;
&lt;li&gt;关于网卡，现在用的比较多的是Mellanox 25G  CX4/CX5卡，也有相当多的100G dual-port CX6卡，都是支持RDMA的，不用RDMA也能提供相当好的高速以太网性能。如果是做虚拟化的，比如AWS，阿里云，还可以把这些网卡的smartNiC能力利用起来，offload虚拟化开销。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相对之前的Lakes，Sapphire Rapids变化非常大，此前的monolithic die路线确实走到了极限，Sapphire Rapids也进入了multi-die时代：芯片内部分为4个die，这也可以视作向chiplet方向演化。&lt;/p&gt;
&lt;p&gt;Sapphire Rapids最多支持8个socket，每个芯片可以支持60核。IO技术上支持CXL 1.1、PCIe 5.0、UPI 2.0，HBM2e(optional)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/4th_xeon.jpeg&#34; alt=&#34;Xeon&#34;&gt;&lt;/p&gt;
&lt;p&gt;AMD的第四代EPYC的主力型号Genoa今年Q1也开始量产，基本上和Sapphire Rapids对标，采用5nm工艺。虽然说AVX512被很多人诟病，实用表现非常一般，但Zen4也支持了，毕竟XeongenEPYC目标客户就是音视频处理和AI workload越来越多的数据中心应用。Genoa是每个die/chiplet 8核的，12个CPU cluster die中间围绕一个IO die的设计，相当于一个芯片96核，是典型的chiplet范式设计。&lt;/p&gt;
&lt;h2 id=&#34;芯片设计的迭代chipletization&#34;&gt;芯片设计的迭代：Chipletization&lt;/h2&gt;
&lt;p&gt;近期芯片设计领域一个显著变革是Chiplets+SiP(System in Package)范式取代die size较大的SoC+PCB合封。
Chiplets同时受到业界和学术界的关注，被IBM research称为&amp;quot;what’s next in computing&amp;quot;，后续章节中对计算、IO、内存等技术的讨论也都涉及chiplets和co-packaging，因此我们首先讨论芯片层次的迭代。&lt;/p&gt;
&lt;p&gt;所谓chiplet partitioning就是将电路切分成模块化的子系统，每个子系统都是一个独立晶粒（die），即chiplet，多个chiplet用2.5D/3D技术封装成一个芯片(package)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/chiplet.png&#34; alt=&#34;Chiplet&#34;&gt;&lt;/p&gt;
&lt;p&gt;Chiplet-reuse范式相比传统的IP-reuse（IP在芯片语境下指的是具有独立功能和成熟设计的电路模块）的优势如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;先进CMOS制程（7nm以下）由于技术原因不太可能在大晶粒上获得高yield，die size越小成本越低。&lt;/li&gt;
&lt;li&gt;先进CMOS制程下，不太可能同时缩小电源管理、快速IO SerDes等模拟IP，先进CMOS一般只用于处理器和加速器。&lt;/li&gt;
&lt;li&gt;允许模块化设计，让设计者可以专注于单个模块的极致优化，并选择最合适的技术：比如CPU和GPU用先进制程，模拟模块用成熟制程，高带宽内存HBM用DRAM，AI加速器可以用非易失性内存。&lt;/li&gt;
&lt;li&gt;允许芯片/package层次的异构集成：让通用CPU、优化后的GPU、嵌入式的FPGA、专用的机器学习电路、光学IO模组、高带宽内存等模块以合适的方式，用先进的使用硅通孔（TSV）、微凸块（micro-bumps）、甚至die-to-wafer混合键合技术的3D封装方案，像乐高积木一样搭出完整的系统。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;过去我们设想的各类DSA、AI芯片、FPGA百花齐放的异构计算时代并没有如期到来，而是被NV的GPU软硬件结合且计算互连一体化的方案碾轧了，几乎只剩下TPU在继续向v5迭代。&lt;/p&gt;
&lt;p&gt;但未来的服务器芯片本身就存在多样化异构共封装集成的可能性和倾向性。CPO(co-packaged optics)、HBM(high-bandwidth memory)这些神奇物种因Chipletization的契机而得以进驻其中。&lt;/p&gt;
&lt;p&gt;更多的功能也就意味着更高的可编程性。有些功能甚至可以带来革命性变革，比如光学IO带来的超高通信带宽，HBM带来的超高访存带宽，高性能offpackge互连技术（Nvlink-C2C）、多个Chiplet之间的Mesh互连(NvSwitch)，现在被Nvidia用来搭建H100，被谷歌用来搭建TPUv4，未来则可能颠覆host-centric的数据中心应用设计范式，迎来硬件资源解聚（disaggregation）的新计算体系：适应资源解聚的操作系统(LegoOS就是基于早期IB network的一个尝试)、系统语言ABI、新的高级语言、新的网络IO、存储和计算形态都有可能从中孵化而生。&lt;/p&gt;
&lt;h2 id=&#34;计算和内存层次的迭代可扩展的众核numa架构&#34;&gt;计算和内存层次的迭代：可扩展的众核NUMA架构&lt;/h2&gt;
&lt;p&gt;在商用服务器领域，Chiplet范式中的一部分设想已经实现了，比如AMD很早就开始应用chiplet，也部分解决了chiplet间IO问题，实现了有可扩展性的众核NUMA架构。SPR之前的Xeon物理机也是NUMA，虽然只有2个NUMA节点（目前Intel的NUMA node太大了，所以不太好称之为Chiplet）。&lt;/p&gt;
&lt;p&gt;μArch对计算/访存密集型数据中心应用的性能工程有直接影响。下图是一个6chiplet封装的96核概念机。显然当我们把集成电路的黑盒拆开，就可以看到更细粒度的组件以及它们组成的网络（Network-on-Chip）结构。这个概念机集成了各种先进设计，不仅有many-core，还支持完整的cache coherency。相比过去的多核架构，众核架构的内存层次也相应变得更深，cache miss的代价变得更高。以至于Rust的标准库用B树去实现map（而C++中众所周知是红黑树），这就是处理器和内存频率差距逐渐拉大的结果，（夸张地说）现在的内存已经慢得像是当年的磁盘了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/IntAct.png&#34; alt=&#34;IntAct&#34;&gt;&lt;/p&gt;
&lt;p&gt;针对NUMA架构，系统层的Linux内核和KVM的NUMA-aware scheduler，应用层的网络框架Seastar、数据库ScyllaDB、内存数据库DragonFly等都已经注意到感知硬件拓扑能极大提升整体性能（ScyllaDB、DragonFly分别数倍领先于对标的Cassandra、Redis），提出了share-nothing高性能架构：避免锁和不必要的共享内存、避免不必要的远端内存访问、避免不必要的跨晶粒通信，设计缓存友好的数据结构，更好地利用晶粒内本地的L1 cache——考虑到目前我们用的Cascade Lake机器并非完全cache coherent，未来即使做到完全cache coherent，shared cache的coherency机制也几乎一定有开销，总之在复杂拓扑深内存层次时代，需警惕cache miss。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/NUMA.png&#34; alt=&#34;NUMA&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;重新遇到io瓶颈先进互连再次成为hpc核心&#34;&gt;重新遇到I/O瓶颈：先进互连再次成为HPC核心&lt;/h2&gt;
&lt;p&gt;数据中心应用的IO占了全球IO traffic的76%，和计算一样，IO也是耗电的，而且和计算一样，数据中心IO耗电也十年没有变过，被硬件进步offset掉了。和计算一样，互连是分层级的，近期die-to-die(on-package)链路层面有UCIe标准的发布，off-package层面有基于PCIe6.0的CXL3.0，900GB/s的NvLinkC2C，inter-node层面有Infiniband NDR。这些是基于电的互连，相比而言光学互连更有前景，但也更困难，而且还在早期研发阶段。&lt;/p&gt;
&lt;p&gt;过去的大数据和前大模型AI时代对IO的需求较低，标准以太网足以支撑大部分数据中心应用，包括parameter servers。大模型训练产生了新的计算、IO形态，内存放不下模型，不得不做模型并行后，IO就重新成了瓶颈：H100的8个GPU每个都需要7.2Tbps的off-package带宽，相比之下，连ToR交换机都只需要10+Tbps。AI专用GPU在大模型训练场景下的带宽需求已经非常接近交换机（交换机和GPU一样，都是巨型ASIC，也都是co-packaged optics适用的领域）。在交换机领域，谷歌已经研发出了实用且收益显著的纯光学链路交换机。在GPU互连上，NV也提出过光学互连的GPU的概念系统，甚至还设计了相应的带外置激光源的GPU机架和顺便解决冷却问题的稀疏布线。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/optics.png&#34; alt=&#34;optics&#34;&gt;&lt;/p&gt;
&lt;p&gt;先进IO技术与HPC(高性能计算)的发展密可不分，尽管HPC或者说超算在大众的想象中一直和超强悍的处理器、加速器直接相关，但实际上恰恰相反，传统的HPC workload（建模、模拟类的科学计算）的计算用的往往是普通的商用节点，反而是互连必须用高性能的HPC interconnect技术。传统超算的异构性体现在IO技术，而非FPGA、专用ASIC的应用。&lt;/p&gt;
&lt;p&gt;后来到了大数据分析和AI时代，标准以太网足以支持适应当时模型参数量的AI训练负载。主流的互联网大数据应用可以完全基于商用IO技术和商用计算节点实现。而少数AI DC的异构性主要体现在加速器技术（GPU、TPU、专用AI芯片）而非IO。&lt;/p&gt;
&lt;p&gt;如今出现了大模型训练主导的异构负载，大模型参数量激增导致内存不足，不得不进行模型并行后，die-to-die带宽、off-package带宽、Inter-node带宽重新成为瓶颈。先进(异构)互连技术重新成为HPC的核心话题。&lt;/p&gt;
&lt;p&gt;Datacenter AI重回异构IO+异构计算架构，本质是超算化，因此刚好也能适应建模+模拟类的传统HPC负载，其实给了互联网行业一个新的机会，那就是卷赢大模型训练的同时，还可以顺便进军超算行业，为高校、科研机构提供廉价、可靠、易用、随时oncall的科学计算能力，舆论上一定程度上扭转互联网公司对社会缺乏贡献的负面形象，为继续征收互联网服务税寻求合法性支撑。&lt;/p&gt;
&lt;h2 id=&#34;io技术的迭代光学io愈发接近计算端点&#34;&gt;I/O技术的迭代：光学IO愈发接近计算端点&lt;/h2&gt;
&lt;p&gt;先进铜缆互连是现在，共封装光学互连则是未来。&lt;/p&gt;
&lt;p&gt;前文提到的Co-packaging是先进互连的关键技术，一方面将多个晶粒共封装本身就可以缩短IO链路，降低IO能耗，另一方面允许集成共封装光学模组技术CPO(co-packaged optics)。&lt;/p&gt;
&lt;p&gt;数据中心IO的一个演化趋势是&amp;quot;bring fiber closer to endpoints&amp;quot;。光链路相比电链路，一个明显优势是传输距离更远（受制于频率相关的衰减）。另一个优势是随着带宽增加，电信号不断变短，噪音不断变大，IB network已经逼近铜缆极限，继续发展下去只能从铜缆走向光纤。此外，高频下电互连和连接器既要接收又要发射，会经历显著的串扰，这也限制了电互连的封装密度。光纤作为信号传输介质几乎是理想的，唯一低效的地方就是两端的电光转换部分。&lt;/p&gt;
&lt;p&gt;现在in-racks连接主流方案是铜缆，inter-rack交换则基于以太网链路。超大数据中心里，缆线长就达到几公里，因此越来越多使用光缆——甚至短距离链路现在也越来越多地用光缆。数据中心里，fiber越来越接近endpoints，越来越接近cpus、gpus，最新的趋势是直接将光学组件集成到硅片上。CPO把光电链路结合在一起，无需intervening receive and re-transmit的过程，把光电转换(optoelectonic conversion)步骤省略了。第一代CPO是pluggable optics，第二代是On-Board Optics/Near Package Optics，第三代是2.5D CPO，第四代是3D CPO，第五代则是Integrated Laser。&lt;/p&gt;
&lt;p&gt;Google的TPUv4超算最大的创新就是4k节点上可重配置的纯光学链路的光学交换机(OCS)，节省了光电转换的能耗， Infiniband将铜缆高性能互连发展到极致，后续的roadmap也是从铜缆到光电共封装。Nvidia虽然一直在推电链路方案（Nvlink），但也和Ayar Labs签署了研发合作关系，开始支持带外激光器和硅光子互连技术的研究，毕竟NvLink本质还是NUMA，可以扩展到8GPU，16GPU，但不可能把数据中心规模的一万个GPU连起来。HP也于去年与Ayar Labs合作，试图将硅光子学引入它们的先进HPC IO产品Slignshot互连。Intel也在研究激光器嵌入芯片内部的集成方案。&lt;/p&gt;
&lt;p&gt;下图列出了interposer、PCB、CPO、电缆、有源光缆的耗电、成本、密度、传输距离指标。CPO的优势是显而易见的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/CPO.png&#34; alt=&#34;CPO&#34;&gt;&lt;/p&gt;
&lt;p&gt;在当前的技术水平下，CPO仅被视为一个电光(E/O)桥的角色，以解决SiP的互连带宽密度瓶颈问题。对分布式训练等应用场景来说，电光桥，或者说bring fiber closer to endpoints已经可以大幅降低能耗，提升性能了。但CPO的潜力远不限于此，如果给CPO chiplet稍微加一些功能，就可以像协处理器、smartNiC那样offload一些CPU work，比如做一些简单的数据预处理、后处理，又如CPO无需经过CPU直接就能访问HBM，从而提供DMA能力，这对解聚架构非常有帮助，无需物理上做pooling，又比铜缆IB网络更快。&lt;/p&gt;
&lt;p&gt;这意味着光学IO不仅可以解决大模型训练带来的带宽问题，还给数据中心应用从host-centric向解聚（disaggregated）架构转型提供了可能。&lt;/p&gt;
&lt;p&gt;何谓解聚范式？与传统的服务器中心范式相反，解聚范式是指将作为整体的服务器掰开，拆成CPU、DRAM、磁盘、加速器等独立的硬件资源进行资源抽象和管理的数据中心应用架构设计范式。硬件解聚并非新概念。18年的USENIX OSDI最佳论文LegoOS，一句&amp;quot;We believe that datacenters should break monolithic servers&amp;quot;，充满了信念感。当年的Infiniband还没有进化到NDR版本，光学I/O也还远离数据中心内部端点，但已经足以支撑这样的宏大叙事。&lt;/p&gt;
&lt;p&gt;有了高性能网络，解聚架构就能有效提升数据中心应用的资源利用率，减轻物理机上CPU、加速器、内存、磁盘等资源在host-centric范式下不可避免的over-provisioning问题。&lt;/p&gt;
&lt;h2 id=&#34;评估-gh200-grace-hopper-superchip&#34;&gt;评估 GH200 Grace Hopper Superchip&lt;/h2&gt;
&lt;p&gt;NVIDIA宣称Grace Hopper Superchip是世界上第一个真正支持HPC和AI负载的异构加速平台。
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper.png&#34; alt=&#34;GraceHopper&#34;&gt;
如下图所示，这个superchip是一个把Grace Arm Neoverse CPU+LPDDR5x内存和H100 Tensor Core GPU+HBM，NVLink-C2C集成合封成PCB的集成方案。
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper2.png&#34; alt=&#34;GraceHopper&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/GraceHopper3.png&#34; alt=&#34;GraceHopper&#34;&gt;&lt;/p&gt;
&lt;p&gt;这并不是一个创新方案，一方面悖逆Chipletization的潮流（除了HBM算是chiplet外，H100、Grace、NvSwitch都是巨型SoC/ASIC，况且即使是HBM也是PCB合封，而不是SiP），另一方面也没有进行任何CPU-GPU超融合（物理上融合至单个SoC上，逻辑上统一页表管理、内存、缓存、并发模型等）的探索或尝试（GPU设计之初就存在太多和CPU无法兼容的设计，比如缓存模型、内存模型和并发模型，如今CUDA根基已成很难回头），只是简单粗暴的将CPU、高带宽内存、H100以PCB合封的方式集成，用NVLink-C2C提供内存一致性和更高off-package的带宽（并未尝试任何先进IO技术）。软件上也没能在CUDA基础上提供更强的可编程性，仅仅提供coherent memory access，编程模型仍然是完全异构的（这也是因为CUDA自诞生之初就是个图形加速库，也没法考虑未来会出现对这种superchip的同构编程模型的需求）。&lt;/p&gt;
&lt;p&gt;但这是一个低风险高执行力的集成方案，正如扎克伯格所说，&amp;ldquo;Move fast and break nothing&amp;rdquo;，把原本优秀的组件原封不动地合封起来，不做侵入式修改，只要动作足够快，就能迅速占领市场，构建生态，并支撑溢价。市场上有更完美的memory coherence方案（比如AMD MI300X），更好的CPU-GPU超融合方案，也有比不得不为图形负载妥协的GPU效率更高的AI芯片，但就是没有CUDA异构编程体系，以及Grace Hooper这样把计算、内存和IO瓶颈都解决得差不多的完整解决方案。&lt;/p&gt;
&lt;p&gt;总之，NV的方案作为生态(GPU + CUDA)与生物(ChatGPT根据A100量体裁衣的训练方案)互相作用下的best-of-breed，远远没达到理想最优，甚至也不在正确的技术路线上，AMD的所谓APU以及国内的AI DSA（如Biren）仍有弯道超车的希望。&lt;/p&gt;
&lt;p&gt;讨论计算系统的新机会&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;（应用）端到（硬件）端的全栈优化，或者说软硬件协同。
&lt;ul&gt;
&lt;li&gt;TVM: deep learning compiler stack for cpu, gpu and specialized accelerators&lt;/li&gt;
&lt;li&gt;GPU + CUDA&lt;/li&gt;
&lt;li&gt;GH20 Grace Hopper + 新的CUDA NUMA内存API+异构编程API&lt;/li&gt;
&lt;li&gt;司内的LavaRecord全链路优化项目，向下(LavaUOS)对接新存储硬件，试图在nvme ssd上建立高效的用户态IO软件栈。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;应用机器学习方法对参数空间较大的系统做auto-tuning。
&lt;ul&gt;
&lt;li&gt;存储引擎如rocksdb调参&lt;/li&gt;
&lt;li&gt;深度学习模型在异构硬件上的auto TVM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;先进互连技术支持下的资源解聚架构设计。
&lt;ul&gt;
&lt;li&gt;LegoOS&lt;/li&gt;
&lt;li&gt;PolarDB-X的存算分离和memory pooling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;计算节点上的Share-nothing架构，以及data-oriented设计。
&lt;ul&gt;
&lt;li&gt;应用框架层面已有Libtorque、DragonFly、Seastar、Scylladb等先例，主要是IO密集应用——不过只要是内存占用大的CPU应用，大多可以视为IO密集的，因为cache miss上来之后访存占比往往会远超计算。&lt;/li&gt;
&lt;li&gt;虚拟化方向，交大IPADS实验室的CPS: A Cooperative Para-virtualized Scheduling Framework for Manycore Machines，提出协作式半虚拟化调度机制，大幅提升众核虚拟机可扩展性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;基于深度模型白盒化研究和已有的数学工具，用direct math solution取代黑盒模型的近似。
&lt;ul&gt;
&lt;li&gt;例如“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors用压缩+信息距离+KNN的简洁解决方案。&lt;/li&gt;
&lt;li&gt;用异类不相干性、同类可压缩性（稀疏性）衡量embedding效果，不必借助某种端到端应用的指标间接衡量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;参考文献和进一步阅读&#34;&gt;参考文献和进一步阅读&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Learning One-hidden-layer Neural Networks with Landscape Design：即使是最简单的深度学习非凸优化场景，用数学工具（数学最优化方法）进行解释也极为困难。&lt;/li&gt;
&lt;li&gt;Functionality and performance of NVLink with IBM POWER9 processors：几年前IBM Power9（美国能源部的Summit和Sierra超算系统）就在用NVLink，而且hardware cache coherence设计（以及hardware atomic ops，addr translation）已经非常完善，比Grace Hooper方案更完善。&lt;/li&gt;
&lt;li&gt;Faith and Fate: Limits of Transformers on Compositionality：大语言模型涌现出演绎逻辑能力，但在多步复合问题上表现不佳，在训练样本中从未出现过计算图中相同计算路径的动态规划问题上准确率更是迅速跌落。与其他emprical study相比，这个研究更严肃，也更全面，考虑了计算图中训练时未见的splits带来的影响。我们有理由确信，大语言模型涌现的演绎推理能力会受制于transformer的天然局限。&lt;/li&gt;
&lt;li&gt;Teaching Arithmetic to Small Transformers：基于transformer的小语言模型足以学习简单算术能力， 提供包含正确的计算步骤的训练数据（chain-of-thought style data）是提升算术学习能力的关键，简单粗暴地用题目和结果进行训练，单纯靠增加模型大小无法提升准确率。&lt;/li&gt;
&lt;li&gt;A Survey of Large Language Models：提供了对大语言模型的up-to-date review。&lt;/li&gt;
&lt;li&gt;Variantional Inference: A Review For Statisticians：提供了解释VI、理解VI的统计学家视角，讨论了VI应用于指数级模型族的特例，并给出一个贝叶斯高斯混合模型的例子，并推导出一种使用随机优化来扩展至海量数据的VI变体。&lt;/li&gt;
&lt;li&gt;Training language models to follow instructions with human feedback：OpenAI的经验介绍，重点是RLHF。&lt;/li&gt;
&lt;li&gt;GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE：来自semianalysis的爆料，颇具可信度。&lt;/li&gt;
&lt;li&gt;Efficiently Scale LLM Training Across a Large GPU Cluster with Alpa and Ray：LLM训练。&lt;/li&gt;
&lt;li&gt;Scaling Language Model Training to a Trillion Parameters Using Megatron：Megatron（repo： &lt;a href=&#34;https://github.com/NVIDIA/Megatron-LM&#34;&gt;https://github.com/NVIDIA/Megatron-LM&lt;/a&gt; ，paper： &lt;a href=&#34;https://arxiv.org/pdf/1909.08053.pdf&#34;&gt;https://arxiv.org/pdf/1909.08053.pdf&lt;/a&gt; ）&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=eqWPyaRcILQ&#34;&gt;https://www.youtube.com/watch?v=eqWPyaRcILQ&lt;/a&gt; 微软Azure硬件系统和基础设施团队的Ram Huggahalli关于Co-Packaged Optics的talk。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Xt-GY8Pkt6g&#34;&gt;https://www.youtube.com/watch?v=Xt-GY8Pkt6g&lt;/a&gt; 研究光通信和先进互连技术的Tony Chan Carusone关于Co-Packaged Optics以及Evolution of IO的talk。&lt;/li&gt;
&lt;li&gt;Next-generation Co-Packaged Optics for Future Disaggregated AI Systems：对共封装光学模组以及未来的解聚AI系统的洞察。&lt;/li&gt;
&lt;li&gt;TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings : Google的TPUv4，重点是纯光链路交换机&lt;/li&gt;
&lt;li&gt;LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation ：乐高OS，基于早期Infiniband高速网络做硬件资源解聚的尝试&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.hpcwire.com/2020/11/16/nvidia-mellanox-debuts-ndr-400-gigabit-infiniband-at-sc20/&#34;&gt;https://www.hpcwire.com/2020/11/16/nvidia-mellanox-debuts-ndr-400-gigabit-infiniband-at-sc20/&lt;/a&gt; Mellanox(Nvidia)的Infiniband NDR版本，以及roadmap。&lt;/li&gt;
&lt;li&gt;Rack-scale disaggregated cloud data centers: The dReDBox project vision: 数据中心应用解聚架构的早期尝试。&lt;/li&gt;
&lt;li&gt;White-Box Transformers via Sparse Rate Reduction：马毅团队对transformer的白盒化解释，此前马毅已经给出了更通用的rate reduction原则： Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bgavran/Category_Theory_Machine_Learning&#34;&gt;https://github.com/bgavran/Category_Theory_Machine_Learning&lt;/a&gt; 深度学习的范畴论解释。深度学习可解释性和逆向工作还可以参考Christopher Olah的blog: colah.github.io，Olah有许多深刻的洞察，比如https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ 流形假设的可视化和深度学习分类的解释。&lt;/li&gt;
&lt;li&gt;IntAct: A 96-Core Processor With Six Chiplets 3D-Stacked on an Active Interposer With Distributed Interconnects and Integrated Power Management：先进IC设计领域的论文，给出了一个集成了chiplet范式、3d封装、完全cache coherence等先进概念的96核众核原型系统。&lt;/li&gt;
&lt;li&gt;“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors：无损压缩近似柯氏复杂性，然后计算信息距离（类似rate reduction，刻画了总体与分类之间的信息差，分类的编码长度低而总体的编码长度高，表明这种分类具有异类强区分性和同类可压缩性），依据信息距离做简单的KNN即可完成分类。这个研究的代码有错，并不能击败BERT，见https://kenschutte.com/gzip-knn-paper/。&lt;/li&gt;
&lt;li&gt;M. Li and P.M.B. Vitányi, An Introduction to Kolmogorov Complexity and Its Applications 柯尔莫哥洛夫复杂性的介绍和应用&lt;/li&gt;
&lt;li&gt;A Mathematical Theory of Communication 1948年香农信息论的论文原著&lt;/li&gt;
&lt;li&gt;hwloc doc：hwloc的文档，hwloc是NUMA-discovery + cpu/memory-binding library。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://man7.org/linux/man-pages/man2/mbind.2.html&#34;&gt;https://man7.org/linux/man-pages/man2/mbind.2.html&lt;/a&gt;：libnuma的NUMA memory policy函数。&lt;/li&gt;
&lt;li&gt;On the Turing Completeness of Modern Neural Network Architectures 证明了无限精度transformer是图灵完备的，即任意图灵机都可被无限精度transformer模拟，但只要是固定精度就不是图灵完备的。&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Distributed Computing Systems At Scale</title>
      <link>https://cmbbq.github.io/posts/distributed-computing-systems/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/distributed-computing-systems/</guid>
      <description>Distributed Computing Systems At Scale 分布式计算系统完成规模化的跳跃后，核心挑战是正确性和系统效率。前者对应共识抽象，后者对应性能工程。
Consensus Abstraction 分布式系统的正确性实际上蕴含了稳定性、一致性、可用性等理想属性。 分布式天然意味着并发执行、进程失败、不可靠的消息传输，这些因素都使正确性在各种常见的分布式应用语境下变得极为困难：
并发意味着执行轨迹空间维度爆炸，和(进程数*步数)的阶乘成正比。很难保证每个执行路径都是对的。 进程失败可分为：崩溃（进程没了，网卡或CPU故障）、丢请求（没崩，网络阻塞或服务降级导致临时不可用）、恢复后崩溃（反复重启，最终恢复到正确状态，往往基于日志）、拜占庭失败（不可预料，来自宇宙射线或恶意攻击）。 消息传输面临篡改、丢包、重传、乱序等风险，即使是可靠网络协议也不完全可靠。 共识是指多个进程就某个问题（2-general问题、复制状态机保证全局序的问题、分布式事务）最终形成一致决定。理想的共识算法应具备安全性(safety)和活性(liveliness)。安全性包括validity（只有一个提议值被选择）、agreement（正确的节点保持一致）、integrity（每个节点最多选一次）。活性则是指可终止，每个正确节点最终总会选择一个值，而不是hang住。
如果不考虑安全性的话，TCP其实就是一个最简单、粗糙、有效的共识协议，TCP协议本质是让通信的两个进程对连接的状态达成了共识，才能可靠地通信。比如进程关闭连接，就会把本地状态改为TIME_WAIT，等几分钟后才真正释放资源，以确保对方收到ACK，用简单的等待，达成高概率的共识，避免双方开始下一轮连接后收到旧报文而出错。
如果追求完全安全，简单的场景是只对一个值进行商议和决策，即单值共识。
单值共识前提下的最简场景则是拥有完美失败检测能力，泛洪共识（只有获得所有进程提议才做出决定）、等级统一共识就是这种场景下的简单解决方案。
若只具备最终失败检测能力，则需引入代次变更概念，用代次共识（往往是Paxos变种）去解决。Paxos是唯一已知的completely-safe &amp;amp; largely-live的容错共识算法。2PC其实也是一种代次共识，不过2PC里协调者宕机就会全局阻塞，是safe but not live的典型。Paxos甚至可以通过将代次(round)的偏序关系从&amp;lt;改成整除，来实现多维的先后代次，从而兼具2PC的功能。
如果再把单值共识推广到序列共识（在分布式存储中更常用，因为log-structured store的写操作是一个append-only序列，而非单值），则从Paxos进化为某种Multi-Paxos，或Raft。Raft本质上解决了复制状态机问题，而序列共识问题恰好可以归约为复制状态机问题。
如果再考虑拜占庭失败，则需BFT、PBFT、PoW。
Performance Engineering 性能的重要性毋庸置疑，端上应用（语音识别、智能客服）的延迟够不够低直接决定项目是否成立，而大规模的IDC应用(搜广推、存储、在线推理、训练)的吞吐性能则直接和金钱与环保挂钩。
计算系统的性能与分布式密不可分。因为几乎所有计算系统都是分布式的，包括单机，单卡，单设备，因为单机（in-node）仍然有网络结构，比如NVLink和NVSwitch，甚至Xeon CPU都是众核通过总线连起来的，网卡、SSD等设备，拆开来看都是网联设备（networked device）。通过分区、并行、软硬件协同、减少不必要的分层获取性能的方法是普适的。
如果存在基线，性能工程的主要工作就是优化、迭代，和渐进式创新。 当基线不够好时，性能工程(performance engineering)是发散的：跨越层次的，不同抽象层次、不同的模块都有可能成为瓶颈，不应存在优化盲点，必要时还需反分层，比如将整个IO路径放到用户态，比如暴露hypervisor状态给vm以便高效调度。 当基线已经非常好时，性能路径(performance path)则是收敛的：明确指向软硬件协同，毕竟性能最终来源于对硬件资源的量体裁衣和极限压榨，如果协同也足够好，那就只能指向硬件更新：更大更强的N卡、TPUv4这样的DSA、OCS这样的光学路由以及更进一步的AI DC建设。
如果是全新的问题，或对已有问题产生了非凡洞察，对其施加更有效结构，就产生了新的架构，新的计算和IO形态：
大模型时代来临后，模型参数本身就已经放不进顶级的GPU，更不用说神经网络计算所需的额外内存了，这就使原本在系统效率和正确性之间平衡的非常好的Parameter Server架构面临挑战，不得不用全新的架构和策略解决大模型训练的问题：Node内（8卡组一个Node）的权重切分和模型并行，算子内的数据和指令并行，子图切分、算子摆布和流水线并行（8个Node做8级流水）、16组8*8做Batch16的数据并行，于是产生了Alpa over Ray训练大模型的解决方案。这又间接导致带宽需求激增，以至于需要Google的光学交换机OCS这样的网络解决方案，避免光电模块和解析报文计算的开销——光交换劣势在于切换慢，但训练周期里数据交换的主要流量路由是固定的，用来建设AI DC正好规避了这个缺点。 KVM的作者根据hypervisor在NUMA系统上的性能瓶颈的经验，很容易就发现应用层共享内存的并行模型是有缺陷的，在众核时代尤其严重，因此创造了seastar。利用hardware locality，避免线程切换、数据拷贝、NUMA远端内存访问，对访存密集型CPU-bound应用来说，是非常重要的性能路径。 </description>
      <content>&lt;h1 id=&#34;distributed-computing-systems-at-scale&#34;&gt;Distributed Computing Systems At Scale&lt;/h1&gt;
&lt;p&gt;分布式计算系统完成规模化的跳跃后，核心挑战是正确性和系统效率。前者对应共识抽象，后者对应性能工程。&lt;/p&gt;
&lt;h2 id=&#34;consensus-abstraction&#34;&gt;Consensus Abstraction&lt;/h2&gt;
&lt;p&gt;分布式系统的正确性实际上蕴含了稳定性、一致性、可用性等理想属性。
分布式天然意味着并发执行、进程失败、不可靠的消息传输，这些因素都使正确性在各种常见的分布式应用语境下变得极为困难：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;并发意味着执行轨迹空间维度爆炸，和(进程数*步数)的阶乘成正比。很难保证每个执行路径都是对的。&lt;/li&gt;
&lt;li&gt;进程失败可分为：崩溃（进程没了，网卡或CPU故障）、丢请求（没崩，网络阻塞或服务降级导致临时不可用）、恢复后崩溃（反复重启，最终恢复到正确状态，往往基于日志）、拜占庭失败（不可预料，来自宇宙射线或恶意攻击）。&lt;/li&gt;
&lt;li&gt;消息传输面临篡改、丢包、重传、乱序等风险，即使是可靠网络协议也不完全可靠。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;共识是指多个进程就某个问题（2-general问题、复制状态机保证全局序的问题、分布式事务）最终形成一致决定。理想的共识算法应具备安全性(safety)和活性(liveliness)。安全性包括validity（只有一个提议值被选择）、agreement（正确的节点保持一致）、integrity（每个节点最多选一次）。活性则是指可终止，每个正确节点最终总会选择一个值，而不是hang住。&lt;/p&gt;
&lt;p&gt;如果不考虑安全性的话，TCP其实就是一个最简单、粗糙、有效的共识协议，TCP协议本质是让通信的两个进程对连接的状态达成了共识，才能可靠地通信。比如进程关闭连接，就会把本地状态改为TIME_WAIT，等几分钟后才真正释放资源，以确保对方收到ACK，用简单的等待，达成高概率的共识，避免双方开始下一轮连接后收到旧报文而出错。&lt;/p&gt;
&lt;p&gt;如果追求完全安全，简单的场景是只对一个值进行商议和决策，即单值共识。&lt;/p&gt;
&lt;p&gt;单值共识前提下的最简场景则是拥有完美失败检测能力，泛洪共识（只有获得所有进程提议才做出决定）、等级统一共识就是这种场景下的简单解决方案。&lt;/p&gt;
&lt;p&gt;若只具备最终失败检测能力，则需引入代次变更概念，用代次共识（往往是Paxos变种）去解决。Paxos是唯一已知的completely-safe &amp;amp; largely-live的容错共识算法。2PC其实也是一种代次共识，不过2PC里协调者宕机就会全局阻塞，是safe but not live的典型。Paxos甚至可以通过将代次(round)的偏序关系从&amp;lt;改成整除，来实现多维的先后代次，从而兼具2PC的功能。&lt;/p&gt;
&lt;p&gt;如果再把单值共识推广到序列共识（在分布式存储中更常用，因为log-structured store的写操作是一个append-only序列，而非单值），则从Paxos进化为某种Multi-Paxos，或Raft。Raft本质上解决了复制状态机问题，而序列共识问题恰好可以归约为复制状态机问题。&lt;/p&gt;
&lt;p&gt;如果再考虑拜占庭失败，则需BFT、PBFT、PoW。&lt;/p&gt;
&lt;h2 id=&#34;performance-engineering&#34;&gt;Performance Engineering&lt;/h2&gt;
&lt;p&gt;性能的重要性毋庸置疑，端上应用（语音识别、智能客服）的延迟够不够低直接决定项目是否成立，而大规模的IDC应用(搜广推、存储、在线推理、训练)的吞吐性能则直接和金钱与环保挂钩。&lt;/p&gt;
&lt;p&gt;计算系统的性能与分布式密不可分。因为几乎所有计算系统都是分布式的，包括单机，单卡，单设备，因为单机（in-node）仍然有网络结构，比如NVLink和NVSwitch，甚至Xeon CPU都是众核通过总线连起来的，网卡、SSD等设备，拆开来看都是网联设备（networked device）。通过分区、并行、软硬件协同、减少不必要的分层获取性能的方法是普适的。&lt;/p&gt;
&lt;p&gt;如果存在基线，性能工程的主要工作就是优化、迭代，和渐进式创新。
当基线不够好时，性能工程(performance engineering)是发散的：跨越层次的，不同抽象层次、不同的模块都有可能成为瓶颈，不应存在优化盲点，必要时还需反分层，比如将整个IO路径放到用户态，比如暴露hypervisor状态给vm以便高效调度。
当基线已经非常好时，性能路径(performance path)则是收敛的：明确指向软硬件协同，毕竟性能最终来源于对硬件资源的量体裁衣和极限压榨，如果协同也足够好，那就只能指向硬件更新：更大更强的N卡、TPUv4这样的DSA、OCS这样的光学路由以及更进一步的AI DC建设。&lt;/p&gt;
&lt;p&gt;如果是全新的问题，或对已有问题产生了非凡洞察，对其施加更有效结构，就产生了新的架构，新的计算和IO形态：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大模型时代来临后，模型参数本身就已经放不进顶级的GPU，更不用说神经网络计算所需的额外内存了，这就使原本在系统效率和正确性之间平衡的非常好的Parameter Server架构面临挑战，不得不用全新的架构和策略解决大模型训练的问题：Node内（8卡组一个Node）的权重切分和模型并行，算子内的数据和指令并行，子图切分、算子摆布和流水线并行（8个Node做8级流水）、16组8*8做Batch16的数据并行，于是产生了Alpa over Ray训练大模型的解决方案。这又间接导致带宽需求激增，以至于需要Google的光学交换机OCS这样的网络解决方案，避免光电模块和解析报文计算的开销——光交换劣势在于切换慢，但训练周期里数据交换的主要流量路由是固定的，用来建设AI DC正好规避了这个缺点。&lt;/li&gt;
&lt;li&gt;KVM的作者根据hypervisor在NUMA系统上的性能瓶颈的经验，很容易就发现应用层共享内存的并行模型是有缺陷的，在众核时代尤其严重，因此创造了seastar。利用hardware locality，避免线程切换、数据拷贝、NUMA远端内存访问，对访存密集型CPU-bound应用来说，是非常重要的性能路径。&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Federated Learning</title>
      <link>https://cmbbq.github.io/posts/federated-learning/</link>
      <pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/federated-learning/</guid>
      <description>联邦学习 联邦学习由McMahan于2016年提出，指的是许多移动设备在一个中央服务器的编排下协作训练模型，保持训练数据离散，避免对用户数据进行收集，仅将客户端模型更新上传中央服务器汇总成新的全局模型的机器学习模式。
联邦学习的naive实现如下：
中央服务器选定一些clients，让这些client下载模型。 每个client根据自己的数据计算更新。 每个client将更新（即新的完整模型）上传到中央服务器。 中央服务器用某种方式（比如取平均）聚合这些模型得到一个全局模型。 应对上传模型开销大的问题 大量文献对联邦学习进行了讨论。Federated Learning: Strategies For Improving Communication Efficiency指出联邦学习的naive实现的第3步很容易出现通信瓶颈，这是首当其冲的问题，提出了降低上行链路通信成本的两种方法：结构化更新，缩略更新。
联邦学习的问题可以被形式化地表述为：学习模型中的参数。全连接层的参数可用一个实数矩阵（这只是为了简化问题，所以讨论单个矩阵）表示 W ∈ R^(d1×d2)，shape为(#input × #output)，d1和d2表示输出维度和输入维度。卷积层kernel是4d tensor(#input × width × height × #output) ，需reshape到 (#input × width × height) × #output 。
用W(t)表示当前回合(t)的模型，W(i,t)表示本地更新后的模型，所谓更新就是H(i,t) = W(i,t) - W(t)。中央服务器可聚合得到新模型：W(t+1) = W(t) + η(t)H(t)，其中H(t) = Sum(H(i,t))/N，η(t)是learning rate。
结构化更新 结构化更新是指直接从一个用少量变量参数化的受限空间里学习更新，而不是学习整个模型的更新。
所谓结构化更新，也就是impose structure on updates，论文里提出两种结构，一种是低秩矩阵，另一种是随机掩码。
在结构化更新之低秩矩阵方法中，本地模型的更新H(i,t)必须是一个rank &amp;lt; k的低秩矩阵，这里k是一个固定的数字。将H(i,t)表示为H(i,t)=A(i,t)B(i,t)，其中A(i,t)∈ R^(d1×k)，B(i,t)∈ R^(k×d2)，在后续计算中，随机生成一个A(i,t)视为常数，然后优化B(i,t)。这样A的数据就不必上传，可以坍塌成一个随机种子，只需上传B(i,t)。这种优化，本质上是利用降维的压缩技术，使用矩阵将原始数据进行降维处理，然后使用重构矩阵将降维后的数据重新构建为原始数据。 A(i,t)每回合都为每个client随机生成一次。立刻就得到d1/k的上传开销减少。固定B，训练A，或者同时训练A和B都试过，效果不如固定A，训练B。对此的解释是A可视作重构矩阵（从变换后的向量中重新构建出原始向量），B是投影矩阵（将一个向量投影到另一个向量的子空间上）。固定A训练B实际上相当于解决这样一个问题：给定一个随机的重构矩阵，什么样的投影矩阵可以恢复最多的信息？
在结构化更新之随机掩码方法中，本地模型的更新H(i,t)必须是用某个预定义的随机掩码生成的稀疏矩阵。同样是每回合对每个client重新生成一次。稀疏掩码可以坍塌成一个随机种子，因此只需上传H(i,t)的非零值和种子。
缩略更新 缩略(sketched)更新，学习一个完整模型更新，学成之后，再通过有损的量化、随机旋转、子采样将其压缩后再发给服务器。
量化：将权重做概率量化，用更小的标量类型充当原始权重的unbiased estimator。
随机旋转：其实就是用一个随机正交矩阵乘一下，防止大部分数据都是0的情形，导致量化效果差。
子采样：原本上传的是H(i,t)，subsampling之后只上传一个随机子集。
完全去中心化联邦学习在各个领域面临的挑战 Advances and Open Problems in Federated Learning中讨论了完全去中心化联邦学习在算法、隐私、安全、工程等个层面的挑战:</description>
      <content>&lt;h1 id=&#34;联邦学习&#34;&gt;联邦学习&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1602.05629.pdf&#34;&gt;联邦学习&lt;/a&gt;由McMahan于2016年提出，指的是许多移动设备在一个中央服务器的编排下协作训练模型，保持训练数据离散，避免对用户数据进行收集，仅将客户端模型更新上传中央服务器汇总成新的全局模型的机器学习模式。&lt;/p&gt;
&lt;p&gt;联邦学习的naive实现如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;中央服务器选定一些clients，让这些client下载模型。&lt;/li&gt;
&lt;li&gt;每个client根据自己的数据计算更新。&lt;/li&gt;
&lt;li&gt;每个client将更新（即新的完整模型）上传到中央服务器。&lt;/li&gt;
&lt;li&gt;中央服务器用某种方式（比如取平均）聚合这些模型得到一个全局模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;应对上传模型开销大的问题&#34;&gt;应对上传模型开销大的问题&lt;/h2&gt;
&lt;p&gt;大量文献对联邦学习进行了讨论。&lt;a href=&#34;https://arxiv.org/pdf/1610.05492.pdf&#34;&gt;Federated Learning: Strategies For Improving Communication Efficiency&lt;/a&gt;指出联邦学习的naive实现的第3步很容易出现通信瓶颈，这是首当其冲的问题，提出了降低上行链路通信成本的两种方法：结构化更新，缩略更新。&lt;/p&gt;
&lt;p&gt;联邦学习的问题可以被形式化地表述为：学习模型中的参数。全连接层的参数可用一个实数矩阵（这只是为了简化问题，所以讨论单个矩阵）表示 W ∈ R^(d1×d2)，shape为(#input × #output)，d1和d2表示输出维度和输入维度。卷积层kernel是4d tensor(#input × width × height × #output) ，需reshape到 (#input × width × height) × #output 。&lt;/p&gt;
&lt;p&gt;用W(t)表示当前回合(t)的模型，W(i,t)表示本地更新后的模型，所谓更新就是H(i,t) = W(i,t) - W(t)。中央服务器可聚合得到新模型：W(t+1) = W(t) + η(t)H(t)，其中H(t) = Sum(H(i,t))/N，η(t)是learning rate。&lt;/p&gt;
&lt;h3 id=&#34;结构化更新&#34;&gt;结构化更新&lt;/h3&gt;
&lt;p&gt;结构化更新是指直接从一个用少量变量参数化的受限空间里学习更新，而不是学习整个模型的更新。&lt;/p&gt;
&lt;p&gt;所谓结构化更新，也就是impose structure on updates，论文里提出两种结构，一种是低秩矩阵，另一种是随机掩码。&lt;/p&gt;
&lt;p&gt;在结构化更新之低秩矩阵方法中，本地模型的更新H(i,t)必须是一个rank &amp;lt; k的低秩矩阵，这里k是一个固定的数字。将H(i,t)表示为H(i,t)=A(i,t)B(i,t)，其中A(i,t)∈ R^(d1×k)，B(i,t)∈ R^(k×d2)，在后续计算中，随机生成一个A(i,t)视为常数，然后优化B(i,t)。这样A的数据就不必上传，可以坍塌成一个随机种子，只需上传B(i,t)。这种优化，本质上是利用降维的压缩技术，使用矩阵将原始数据进行降维处理，然后使用重构矩阵将降维后的数据重新构建为原始数据。
A(i,t)每回合都为每个client随机生成一次。立刻就得到d1/k的上传开销减少。固定B，训练A，或者同时训练A和B都试过，效果不如固定A，训练B。对此的解释是A可视作重构矩阵（从变换后的向量中重新构建出原始向量），B是投影矩阵（将一个向量投影到另一个向量的子空间上）。固定A训练B实际上相当于解决这样一个问题：给定一个随机的重构矩阵，什么样的投影矩阵可以恢复最多的信息？&lt;/p&gt;
&lt;p&gt;在结构化更新之随机掩码方法中，本地模型的更新H(i,t)必须是用某个预定义的随机掩码生成的稀疏矩阵。同样是每回合对每个client重新生成一次。稀疏掩码可以坍塌成一个随机种子，因此只需上传H(i,t)的非零值和种子。&lt;/p&gt;
&lt;h3 id=&#34;缩略更新&#34;&gt;缩略更新&lt;/h3&gt;
&lt;p&gt;缩略(sketched)更新，学习一个完整模型更新，学成之后，再通过有损的量化、随机旋转、子采样将其压缩后再发给服务器。&lt;/p&gt;
&lt;p&gt;量化：将权重做概率量化，用更小的标量类型充当原始权重的unbiased estimator。&lt;/p&gt;
&lt;p&gt;随机旋转：其实就是用一个随机正交矩阵乘一下，防止大部分数据都是0的情形，导致量化效果差。&lt;/p&gt;
&lt;p&gt;子采样：原本上传的是H(i,t)，subsampling之后只上传一个随机子集。&lt;/p&gt;
&lt;h2 id=&#34;完全去中心化联邦学习在各个领域面临的挑战&#34;&gt;完全去中心化联邦学习在各个领域面临的挑战&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1912.04977.pdf&#34;&gt;Advances and Open Problems in Federated Learning&lt;/a&gt;中讨论了完全去中心化联邦学习在算法、隐私、安全、工程等个层面的挑战:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;中央服务器可能成为瓶颈和单点故障风险点。由此产生p2p/完全去中心化的设计思路。&lt;/li&gt;
&lt;li&gt;完全去中心化的算法需应对client可用性和网络稳定性的局限。&lt;/li&gt;
&lt;li&gt;设计一个试图达到最快收敛速度的模型平均策略是困难的。&lt;/li&gt;
&lt;li&gt;去中心化的场景会导致算法易受恶意攻击、不可靠数据或标注的威胁。&lt;/li&gt;
&lt;li&gt;client的通信带宽和电量不足，，将已有的压缩算法移植到移动端比较困难。&lt;/li&gt;
&lt;li&gt;隐私问题：如何防止一个client重建另一个client的隐私数据。&lt;/li&gt;
&lt;li&gt;如何实现的问题：区块链作为分布式账簿本质上是一个最终一致的复制状态机。不过以太坊这样的区块链上的数据是公开的，如果要适用于联邦学习，还需要改造。&lt;/li&gt;
&lt;li&gt;cross-silo场景（多个组织或公司一起训一个模型，但数据不能直接共享，比如多个银行一起训一个fraud detection模型）对数据进行分区，并增加incentive机制。&lt;/li&gt;
&lt;li&gt;通信和压缩瓶颈。&lt;/li&gt;
&lt;li&gt;公平性：联邦学习引入了新的bias来源——设备型号、地理位置、活动模式、本地数据集大小等。&lt;/li&gt;
&lt;li&gt;安全性计算问题：如何应对恶意服务器？如何应对外部攻击？&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;non-iid数据分布问题&#34;&gt;Non-IID数据分布问题&lt;/h3&gt;
&lt;p&gt;Non-IID(independent &amp;amp; identically distributed) data问题：样本的统计属性没有均匀分布，对于任何client-partitioned数据集来说都是常见的。&lt;/p&gt;
&lt;p&gt;论文中给出了多种non-identical client分布（考虑对特征x，标签y进行有监督学习，(x,y)~Pi(x,y)即为client i的本地分布，P(x,y) = P(y|x)P(x) = P(x|y)P(y)）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature distribution skew(covariate shift)，即不同client的P(y|x)相同，但特征的边际分布P(x)不同。比如笔迹识别中不同用户写相同的字，但笔法、书写习惯还是不同。&lt;/li&gt;
&lt;li&gt;Label distribution skew(prior probability shift)，即不同client的P(x|y)相同，但标签的边际分布P(y)不同。比如澳洲的日常动物识别应用，标签里就会频繁出现袋鼠，其他地区则不会。&lt;/li&gt;
&lt;li&gt;Same label, different features(concept drift)，即不同client的P(y)相同，但条件分布P(x|y)不同，相同标签在不同client上对应到了不同的特征。比如豪宅，在香港和加州尺度是不同的。&lt;/li&gt;
&lt;li&gt;Same features, different label(concept shift)，即不同client的P(x)相同，但条件分布P(y|x)不同，相同特征被标注成立不同标签。比如有人把熊猫标注为宠物，也有人把熊猫标注为猛兽。&lt;/li&gt;
&lt;li&gt;Quantity skew or unbalancedness，不同的client的数据量差异大。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;违反independence同样常见，因为client的分布很容易收到触发训练的约束条件的影响：比如很多训练是利用夜间睡眠时间跑的，那就导致往往一个经度的地区的clients更容易遇到一起。&lt;/p&gt;
&lt;p&gt;应对Non-IID数据，一个可行的方法是用一个不涉及隐私数据的全局共享小数据集做数据增强。此外，还可以限制一个用户每天能做的贡献上限，避免量上面的不平衡。此外，某些场景还可以把Non-IID从bug转化为feature，就训一个本地客制化的专用模型出来，提供个性化服务，而不是最终产生一个全局模型。文章后面还介绍了Non-IID数据集上的优化算法和收敛速率。&lt;/p&gt;
&lt;h3 id=&#34;应对隐私问题split-learning&#34;&gt;应对隐私问题：Split Learning&lt;/h3&gt;
&lt;p&gt;Split Learning是模型执行路径层面的横切，同时适用于训练和推理：最简单场景是让每个client一直前向算到某个特定的cut layer停下，将cut layer的输出（smashed data）传给中央服务器或peer接着算，于是就完成了无需数据共享就实现的前向传播。类似地，梯度反向传播是从最后一层到cut layer停下，仅将cut layer的梯度回传给client。这样整个过程中其他节点都不会直接访问本地数据。&lt;/p&gt;
&lt;p&gt;考虑到cut layer的权重本身也能一定程度上反映底层的数据现实，Split Learning是否能提供形式化的隐私承诺仍然是个开放问题。&lt;/p&gt;
&lt;h2 id=&#34;应对通信延迟高的问题&#34;&gt;应对通信延迟高的问题&lt;/h2&gt;
&lt;p&gt;还有一个关键问题——高通信延迟，由于无线和长距离传输的特性而无法回避，但在之前的论文中没被很好地address。&lt;a href=&#34;https://dga.hanlab.ai/assets/neurips21_dga.pdf&#34;&gt;Delayed Gradient Averaging: Tolerate the
Communication Latency in Federated Learning&lt;/a&gt;一文提出了一种延迟进行梯度平均的算法，用16节点是树莓派集群模拟现实世界中的移动节点和无线网络环境做了个实验，经验性地证明应用延迟梯度平均可以使联邦学习过程容忍高网络延迟，同时还不牺牲准确度。&lt;/p&gt;
&lt;p&gt;In-center环境下，同一个机柜的延迟&amp;lt;1us，同机房则是ms级别。无线环境大概是20ms，跨洋连接则至少100ms。在解决带宽问题后，延迟就成为最大瓶颈。这篇论文提出的DGA(Delayed Gradient Aggregation)算法的核心思路是延迟梯度平均到未来的某个迭代，即模型更新时接收过时的平均梯度，从而允许通信和计算流水线化。论文将问题形式化为：最小化随机函数的和。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/DGA.png&#34; alt=&#34;DGA&#34;&gt;&lt;/p&gt;
&lt;p&gt;N表示client数量，fi表示client i的stochastic损失函数。随机变量ζi关联一个mini-batch样本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/DGA2.png&#34; alt=&#34;DGA2&#34;&gt;&lt;/p&gt;
&lt;p&gt;算法的主要思路是允许averaging通信过程中做本地更新（averaging通信和本地更新并行，）。FedAvg中clients在每轮结束发送参数到彼此，等averaging结束再开启下一轮。DGA里把averaging barrier延迟到了后续迭代(iteration，指的是本地更新的迭代)。因此clients可以立刻开启下一轮(round，指的是最外层循环，即一轮更新)。第一轮下收到外部信息时迭代已经发生了D次，延迟了D个迭代后进行梯度修正。理想情况下不存在通信延迟，D=0时，DGA恢复成最初的FedAvg。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;clients在第t轮彼此发更新。&lt;/li&gt;
&lt;li&gt;clients在本地更新后继续用最新的本地参数继续本地更新。(averaging通信延迟 &amp;gt; 单次甚至若干次本地更新)&lt;/li&gt;
&lt;li&gt;当其他client的第t轮信息到达，则client已进行了D次额外本地更新。&lt;/li&gt;
&lt;li&gt;将本地t轮梯度替换为接受到的平均梯度。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在最宽泛的场景下（延迟极高），延迟梯度可能要几个轮次之后才能抵达。这就需要将延迟参数D表示为D = sK + r，其中s&amp;gt;=0, r &amp;lt;=K。DGA仍能保证不同client只在最近D个梯度上是不同的，t-D轮之前的梯度都是一样的。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>String Lookups Reduce to Parsing</title>
      <link>https://cmbbq.github.io/posts/string-lookups-could-reduce-to-parsing/</link>
      <pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/string-lookups-could-reduce-to-parsing/</guid>
      <description>标题即结论：字符串查找问题可归约为解析问题。
这个结论源于近期一个有趣的观察：用ragel写高性能的ascii protocol parser，本质上是利用nfa转dfa提升性能，这和toplingdb中将同一层各个sst对应的trie（本质是dfa）合并成一个dfa(大量dfa-&amp;gt;nfa-&amp;gt;1dfa)的思路是同构的。
上述同构隐隐蕴含着一个reduction：字符串查找和字符串解析，本质都用尽可能紧凑的结构和高效的算法从字符流中抽取状态，lookups可以视作一类特殊（且模式相当规则）的parsing。LSM key lookup更是比较特殊的海量索引的key range无overlap的场景，存在大量可以轻松合并的DFA。因此龙书中的大量DFA转NFA转DFA算法可以派上用场。
KV Store的in-memory key index，可以是红黑树，可以是skiplist，可以是hashmap，可以说patricia trie(一种radix tree变种)，也可以是toplingdb中的NestLoudsTrie。这种结构我们同样可以在路由表实现中看到。字符串索引，归根到底是字符串lookup结构。正如路由表实现可以通过把所有routes合并到一个DFA里（很多routes都包含regex），kv数据库也把Trie这种特殊的dfa（Trie的状态转移图是树，树是一种无向图，多了个任意两结点只由一个边连接的约束）做多索引合并，每个索引对应的key range还不重叠（LSM特性），因此合并速度非常快，合并后的DFA表示起来也简单、紧凑，详见自动机算法在数据库索引中的应用，我在作者的文章下面追问了一下DFA合并的触发条件和DFA合并开销，作者的答复是compaction/flush时触发，在整个lsm更新过程中占比很小，也不涉及多线程，无需考虑线程安全。</description>
      <content>&lt;p&gt;标题即结论：字符串查找问题可归约为解析问题。&lt;/p&gt;
&lt;p&gt;这个结论源于近期一个有趣的观察：用ragel写高性能的ascii protocol parser，本质上是利用&lt;a href=&#34;https://en.wikipedia.org/wiki/Nondeterministic_finite_automata&#34;&gt;nfa&lt;/a&gt;转&lt;a href=&#34;https://en.wikipedia.org/wiki/Deterministic_finite_automaton&#34;&gt;dfa&lt;/a&gt;提升性能，这和&lt;a href=&#34;https://github.com/topling/toplingdb&#34;&gt;toplingdb&lt;/a&gt;中将同一层各个sst对应的trie（本质是dfa）合并成一个dfa(大量dfa-&amp;gt;nfa-&amp;gt;1dfa)的思路是同构的。&lt;/p&gt;
&lt;p&gt;上述同构隐隐蕴含着一个reduction：字符串查找和字符串解析，本质都用尽可能紧凑的结构和高效的算法从字符流中抽取状态，lookups可以视作一类特殊（且模式相当规则）的parsing。LSM key lookup更是比较特殊的海量索引的key range无overlap的场景，存在大量可以轻松合并的DFA。因此龙书中的大量DFA转NFA转DFA算法可以派上用场。&lt;/p&gt;
&lt;p&gt;KV Store的in-memory key index，可以是红黑树，可以是skiplist，可以是hashmap，可以说patricia trie(一种&lt;a href=&#34;https://en.wikipedia.org/wiki/Radix_tree&#34;&gt;radix tree&lt;/a&gt;变种)，也可以是toplingdb中的NestLoudsTrie。这种结构我们同样可以在路由表实现中看到。字符串索引，归根到底是字符串lookup结构。正如路由表实现可以通过把所有routes合并到一个DFA里（很多routes都包含regex），kv数据库也把Trie这种特殊的dfa（Trie的状态转移图是树，树是一种无向图，多了个任意两结点只由一个边连接的约束）做多索引合并，每个索引对应的key range还不重叠（LSM特性），因此合并速度非常快，合并后的DFA表示起来也简单、紧凑，详见&lt;a href=&#34;https://zhuanlan.zhihu.com/p/628057993&#34;&gt;自动机算法在数据库索引中的应用&lt;/a&gt;，我在作者的文章下面追问了一下DFA合并的触发条件和DFA合并开销，作者的答复是compaction/flush时触发，在整个lsm更新过程中占比很小，也不涉及多线程，无需考虑线程安全。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>A Decade of Tussle between CPU and GPU</title>
      <link>https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/</link>
      <pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/</guid>
      <description>GPU和CPU方法的边界何在？ 做AI应用的架构工作，遇到新的计算密集任务的第一问往往是：这应该上GPU，还是NUMA物理机？
这个问题可以归约为On the Limits of GPU Acceleration(2010)中提出的问题：&amp;ldquo;大致相同能耗的前提下，能和不能用GPU有效加速计算的边界在哪？&amp;rdquo; Vuduc的这个研究分析了3种具有代表性复杂行为和访存不规则性的计算：稀疏线性系统的迭代求解器、稀疏矩阵的乔列斯基分解、快速多极子算法，得出的结论是——大致地说，良好优化的GPU实现在相同能耗下和良好优化的CPU实现在性能上是相仿的。可见当年用GPGPU加速还是一个普遍得不偿失的工作，毕竟要付出剧烈变更编程模型的代价。
GPU和CPU的价格-性能趋势 上述结论在2010年成立，如今13年过去了，GPU固然飞速发展（无论是硬件、软件，还是生态、应用、资金投入），但GPU发展速度是否已经甩开CPU了呢？
摩尔定律是两年翻倍，而黄氏定律则是宣称通过软硬件协同能达到1.08年翻倍。
甚至如果我们考虑成本因素，根据经验数据，GPU FLOP/s每刀的增长速度是和CPU相仿的。这和2023年工业界的经验是吻合的——CPU在大多数应用语境下依然是成本更低的选择。</description>
      <content>&lt;h2 id=&#34;gpu和cpu方法的边界何在&#34;&gt;GPU和CPU方法的边界何在？&lt;/h2&gt;
&lt;p&gt;做AI应用的架构工作，遇到新的计算密集任务的第一问往往是：这应该上GPU，还是NUMA物理机？&lt;/p&gt;
&lt;p&gt;这个问题可以归约为&lt;a href=&#34;https://www.usenix.org/legacy/event/hotpar10/tech/full_papers/main.pdf&#34;&gt;On the Limits of GPU Acceleration(2010)&lt;/a&gt;中提出的问题：&amp;ldquo;大致相同能耗的前提下，能和不能用GPU有效加速计算的边界在哪？&amp;rdquo; Vuduc的这个研究分析了3种具有代表性复杂行为和访存不规则性的计算：稀疏线性系统的迭代求解器、稀疏矩阵的乔列斯基分解、快速多极子算法，得出的结论是——大致地说，良好优化的GPU实现在相同能耗下和良好优化的CPU实现在性能上是相仿的。可见当年用GPGPU加速还是一个普遍得不偿失的工作，毕竟要付出剧烈变更编程模型的代价。&lt;/p&gt;
&lt;h2 id=&#34;gpu和cpu的价格-性能趋势&#34;&gt;GPU和CPU的价格-性能趋势&lt;/h2&gt;
&lt;p&gt;上述结论在2010年成立，如今13年过去了，GPU固然飞速发展（无论是硬件、软件，还是生态、应用、资金投入），但GPU发展速度是否已经甩开CPU了呢？&lt;/p&gt;
&lt;p&gt;摩尔定律是两年翻倍，而&lt;a href=&#34;https://en.wikipedia.org/wiki/Huang%27s_law&#34;&gt;黄氏定律&lt;/a&gt;则是宣称通过软硬件协同能达到1.08年翻倍。&lt;/p&gt;
&lt;p&gt;甚至如果我们考虑成本因素，根据&lt;a href=&#34;https://epochai.org/blog/trends-in-gpu-price-performance&#34;&gt;经验数据&lt;/a&gt;，GPU FLOP/s每刀的增长速度是和CPU相仿的。这和2023年工业界的经验是吻合的——CPU在大多数应用语境下依然是成本更低的选择。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/4.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar1&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/5.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar2&#34;&gt;
&lt;img src=&#34;https://cmbbq.github.io/img/6.png&#34; alt=&#34;Emprical GPU/CPU FLOP/s per dollar3&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>On ABI</title>
      <link>https://cmbbq.github.io/posts/on-abi/</link>
      <pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/on-abi/</guid>
      <description>前注：ABI在本文中特指系统语言的ABI，这里系统语言，即system programming language，指的是C，C++，Rust这样应用于系统编程的的编译语言。有时候我们也会讨论常用库或基础库的ABI，比如gcc5就把std::basic_string和std::list的实现改了，自然也就影响了跨版本的ABI兼容性，这种层面的ABI兼容性虽然也是一个工程上的疑难杂症，但不在本文的讨论范围内，毕竟源码都不同了。本文讨论ABI兼容性的是同一份源码在不同ISA、操作系统、编译器上对应二进制产物的兼容性。
系统语言的ABI、应用层软件的API、微处理器架构的ISA都是描述各自抽象层次互操作能力的接口。
ABI的独特之处在于它本身就是繁重的机制实现，而非轻量的协议声明——实现和声明之间有概念上的分野，这已经体现在法律上：复制ABI是赤裸裸的抄袭，而复制API或ISA则属于fair use。你显然可以合法地为他人写的API/ISA出书做注，Google实现自己的Java时合法复制了Oracle的Java API，无需license。
ABI之所以复杂，就是因为它处于两个抽象层次之间。ABI中的很大一部分内容既要向上对语言标准负责，向下要对ISA负责，必须遵循两个方向的约束，将复杂度留给自身，从而成就语言标准和ISA这两个抽象契约的简洁、干净，和人类友好。
ABI中的重要组成部分之一calling convention受到ISA的约束，i386中用cdecl, stdcall, fastcall, vectorcall, thiscall, amd64中用systemv, msnative, vectorcall，arm32用aapcs，arm64用aapcs64。ABI中也有很大一部分是指令集无关的（ISA-agnostic），比如name mangling、class layout。这些机制往往足够底层，又与其他指令集相关的部分有强耦合，加上历史因素，往往也只有一小部分能被写入语言标准。
如果ISA差异足够大，维护不同的ABI无疑是必要的，强行用一套ABI兼容不仅不自然，也不高效。以早期的Windows为例，微软就为早先的i386（IA-32）和后来的Intel IA-64提供了两套ABI。再后来，AMD赢得64位战争，Intel也遵循了前向兼容IA-32的amd64，又名x86_64, x64，IA-64就式微了。
ABI以ISA提供的指令、寄存器、内存管理能力为构件（building block），详尽且精确地描述如何实现一个系统语言在特定硬件、操作系统、编译器下的执行模型，并允许分开编译的产物之间能互操作。例如C ABI需定义调用约定（calling convention）、基础数据类型的表示、聚合数据类型的内存布局；C++03 ABI需额外定义异常机制、RTTI信息存储、虚表布局和动态绑定机制、重载函数/运算符/模板实例化所需的名字重置（name mangling）；C++11 ABI需进一步定义lambda的实现、自动类型推导机制等新增语言机制。
无论是C，还是C++都没有在语言层面制定官方的ABI标准，毕竟ABI标准本身也限制了实现的自由。目前最主流的Itanium C++ ABI号称被许多操作系统采用，能适配多数微处理器架构，被大多数编译器实现，包括gcc和clang这两个主要玩家。但值得注意的是，被多方支持，不意味着多方支持的是完全相同的东西。同样打着Itanium ABI名号，clang在arm32 linux上编译的C++库能给amd64 windows上的程序使用吗？显然不行，因为最基础的calling convention，甚至基础数据类型表示都不同。Itanium C++ ABI即使实现大一统，积极意义也仅限于允许一些依赖CPU无关规则的危险技巧（比如修改vtable，这种操作我们现在只能称之为hack）在相当多的环境下通用罢了——只要C++语言标准不将ABI纳入讨论范围，对ABI做出假设的技巧永远是危险的，想要创造新的语言机制，就必须在C++标准层面推进某种ABI共识。
那么系统语言是否应该对某种基于特定ABI实现的语言特性进行标准化呢？从C++的发展历史来看，这种做法已经有了先例，而且是相当危险冒失的。将非零抽象开销的dynamic exception、rtti引入C++客观上导致了社区分裂，有相当一部分C++使用者至今依然选择-fno-exeptions或-fno-rtti。近期的提案Zero-Overhead Deterministic Exceptions: Catching Values给出了一个对exception ABI进行改动的零开销异常机制，是对历史错误的亡羊补牢。系统语言的语言标准应审慎地只对ISA无关且零抽象开销的ABI规则进行标准化，以便在此基础上创造新的语言机制或为应用层开发提供便利。</description>
      <content>&lt;p&gt;前注：ABI在本文中特指系统语言的ABI，这里系统语言，即system programming language，指的是C，C++，Rust这样应用于系统编程的的编译语言。有时候我们也会讨论常用库或基础库的ABI，比如gcc5就把std::basic_string和std::list的实现改了，自然也就影响了跨版本的ABI兼容性，这种层面的ABI兼容性虽然也是一个工程上的疑难杂症，但不在本文的讨论范围内，毕竟源码都不同了。本文讨论ABI兼容性的是同一份源码在不同ISA、操作系统、编译器上对应二进制产物的兼容性。&lt;/p&gt;
&lt;p&gt;系统语言的ABI、应用层软件的API、微处理器架构的ISA都是描述各自抽象层次互操作能力的接口。&lt;/p&gt;
&lt;p&gt;ABI的独特之处在于它本身就是繁重的机制实现，而非轻量的协议声明——实现和声明之间有概念上的分野，这已经体现在法律上：复制ABI是赤裸裸的抄袭，而复制API或ISA则属于fair use。你显然可以合法地为他人写的API/ISA出书做注，Google实现自己的Java时合法复制了Oracle的Java API，无需license。&lt;/p&gt;
&lt;p&gt;ABI之所以复杂，就是因为它处于两个抽象层次之间。ABI中的很大一部分内容既要向上对语言标准负责，向下要对ISA负责，必须遵循两个方向的约束，将复杂度留给自身，从而成就语言标准和ISA这两个抽象契约的简洁、干净，和人类友好。&lt;/p&gt;
&lt;p&gt;ABI中的重要组成部分之一calling convention受到ISA的约束，i386中用cdecl, stdcall, fastcall, vectorcall, thiscall, amd64中用systemv, msnative, vectorcall，arm32用aapcs，arm64用aapcs64。ABI中也有很大一部分是指令集无关的（ISA-agnostic），比如name mangling、class layout。这些机制往往足够底层，又与其他指令集相关的部分有强耦合，加上历史因素，往往也只有一小部分能被写入语言标准。&lt;/p&gt;
&lt;p&gt;如果ISA差异足够大，维护不同的ABI无疑是必要的，强行用一套ABI兼容不仅不自然，也不高效。以早期的Windows为例，微软就为早先的i386（IA-32）和后来的Intel IA-64提供了两套ABI。再后来，AMD赢得64位战争，Intel也遵循了前向兼容IA-32的amd64，又名x86_64, x64，IA-64就式微了。&lt;/p&gt;
&lt;p&gt;ABI以ISA提供的指令、寄存器、内存管理能力为构件（building block），详尽且精确地描述如何实现一个系统语言在特定硬件、操作系统、编译器下的执行模型，并允许分开编译的产物之间能互操作。例如C ABI需定义调用约定（calling convention）、基础数据类型的表示、聚合数据类型的内存布局；C++03 ABI需额外定义异常机制、RTTI信息存储、虚表布局和动态绑定机制、重载函数/运算符/模板实例化所需的名字重置（name mangling）；C++11 ABI需进一步定义lambda的实现、自动类型推导机制等新增语言机制。&lt;/p&gt;
&lt;p&gt;无论是C，还是C++都没有在语言层面制定官方的ABI标准，毕竟ABI标准本身也限制了实现的自由。目前最主流的Itanium C++ ABI号称被许多操作系统采用，能适配多数微处理器架构，被大多数编译器实现，包括gcc和clang这两个主要玩家。但值得注意的是，被多方支持，不意味着多方支持的是完全相同的东西。同样打着Itanium ABI名号，clang在arm32 linux上编译的C++库能给amd64 windows上的程序使用吗？显然不行，因为最基础的calling convention，甚至基础数据类型表示都不同。Itanium C++ ABI即使实现大一统，积极意义也仅限于允许一些依赖CPU无关规则的危险技巧（比如修改vtable，这种操作我们现在只能称之为hack）在相当多的环境下通用罢了——只要C++语言标准不将ABI纳入讨论范围，对ABI做出假设的技巧永远是危险的，想要创造新的语言机制，就必须在C++标准层面推进某种ABI共识。&lt;/p&gt;
&lt;p&gt;那么系统语言是否应该对某种基于特定ABI实现的语言特性进行标准化呢？从C++的发展历史来看，这种做法已经有了先例，而且是相当危险冒失的。将非零抽象开销的dynamic exception、rtti引入C++客观上导致了社区分裂，有相当一部分C++使用者至今依然选择-fno-exeptions或-fno-rtti。近期的提案&lt;a href=&#34;https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2021/p2232r0.html&#34;&gt;Zero-Overhead Deterministic Exceptions: Catching Values&lt;/a&gt;给出了一个对exception ABI进行改动的零开销异常机制，是对历史错误的亡羊补牢。系统语言的语言标准应审慎地只对ISA无关且零抽象开销的ABI规则进行标准化，以便在此基础上创造新的语言机制或为应用层开发提供便利。&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>A Taxonomy of Stateful Distributed Systems</title>
      <link>https://cmbbq.github.io/posts/a-taxonomy-of-stateful-distributed-systems/</link>
      <pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cmbbq.github.io/posts/a-taxonomy-of-stateful-distributed-systems/</guid>
      <description>CAP theorem的讨论范围是狭窄的 在分布式系统领域，CAP theorem广泛被引用，而且经常被用以指导超出它讨论边界的问题。
被形式化证明（见Brewer&amp;rsquo;s Conjecture and the Feasibility of Consistent, Available, Partition-​Tolerant Web Services）的CAP theorem其实只局限在read-write storage场景：一个只包含get、set(x)两种操作的存储系统，这种系统被称为register。
在异步网络（指的是消息传递时间无界）中实现register是无法同时满足下列属性的：
Availability：所有发往register的请求最终都能完成。这和大多数真实系统的定义是不同的，因为真实系统不要求100%完成请求，只要保证SLA够高，同时又往往有一定时间约束，超时就会返回timeout错误。 Consistency：所有读、写操作均是linearizable的：B操作在A操作之后成功执行，则B看到的系统状态不能比A完成时的系统状态更旧。 Partition tolerance：允许网络丢包。 Partition tolerance默认是满足的，因此CAP-availability、CAP-consistency两种属性二选一。 离开了形式化证明的场景后，CAP theorem还有指导意义吗？答案是否定的——除非重新定义availability和consistency，将其推广到更通用的场景：从单对象单操作到多对象多操作的事务系统。 重新定义availability和consistency后的分布式系统分类学虽然和CAP theorem非常类似，但不能称之为CAP theorem。
一致性 更通用的“一致性”应定义为“并发系统中共享状态更新的可见性”。 现代微处理器、分布式系统、数据库的共性是——它们都是存在共享数据的并发系统。 当我们讨论一致性（consistency）时，可能指的是微处理器架构和系统编程领域的一致性模型，也可能指的是分布式系统领域的副本一致性，还可能指的是数据库领域的事务隔离性。这些领域的抽象层次不同，共同点是讨论的系统都是并发系统。
一致性模型（consistency model）是用于描述微处理器架构领域的多核并发场景下，各个处理器被允许的乱序程度——乱序的约束越少，效率越高，并发程序正确性越难保证。
最强的strict consistency指的是任何写在任何时钟周期都立刻对任何处理器可见，显然不能推广到分布式系统领域。 次之的sequential consistency指的是写操作顺序对于每个副本而言都是一致的，即各进程内部的program order一致，而不同进程执行的顺序可以不一致。这个概念最初也是Lamport在讨论multi-processor computer如何正确执行并发程序时提出的。和分布式系统的副本一致性无关。C++中std::memory_order_seq_cst即可保证线程内部的program order。 更宽松的causal consistency指的是写操作中有依赖关系的那一部分的顺序是一致的，即各进程中的dependency order一致。现代CPU基本上都是out-of-order流水线，在保证dependency order这个底线后，能多乱序就多乱序。在C++中用std::memory_order_consume的load(A)和std::memory_order_acquire的store(B)配合，即可保证这个store之前所有写操作中load(A)依赖的那一部分对load(A)是可见的。如果每个依赖都保证Release-Consume ordering，则依赖链就有序，整体上可满足causal consistency。 除了上述几个著名模型外，还有几十个不同方法、领域中应用的一致性模型，下图就包含了非事务分布式存储系统中种种一致性模型（详见Consistency in Non-Transactional Distributed Storage Systems）。 并发程序显然可以很容易推广到分布式复制状态机，只是增加了网络延迟。因此，一致性模型可以推广应用到分布式系统中的副本一致性。以sequential consistency为例，增加了实时约束后就是分布式系统领域中更被广泛引用的linearizability，指的是单个被复制对象上的单个操作满足：A是写操作，B是对副本的读，A happened-before（因果律上的先于，见https://en.wikipedia.org/wiki/Happened-before） B，则A的写对B的读总是可见。比较一下C++的sequentially consistent ordering定义：everything that happened-before a store in one thread becomes a visible side effect in the thread that did a load。二者是一致的。</description>
      <content>&lt;h2 id=&#34;cap-theorem的讨论范围是狭窄的&#34;&gt;CAP theorem的讨论范围是狭窄的&lt;/h2&gt;
&lt;p&gt;在分布式系统领域，CAP theorem广泛被引用，而且经常被用以指导超出它讨论边界的问题。&lt;/p&gt;
&lt;p&gt;被形式化证明（见&lt;a href=&#34;https://users.ece.cmu.edu/~adrian/731-sp04/readings/GL-cap.pdf&#34;&gt;Brewer&amp;rsquo;s Conjecture and the Feasibility of Consistent, Available, Partition-​Tolerant Web Services&lt;/a&gt;）的CAP theorem其实只局限在read-write storage场景：一个只包含get、set(x)两种操作的存储系统，这种系统被称为register。&lt;/p&gt;
&lt;p&gt;在异步网络（指的是消息传递时间无界）中实现register是无法同时满足下列属性的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Availability：所有发往register的请求最终都能完成。这和大多数真实系统的定义是不同的，因为真实系统不要求100%完成请求，只要保证SLA够高，同时又往往有一定时间约束，超时就会返回timeout错误。&lt;/li&gt;
&lt;li&gt;Consistency：所有读、写操作均是linearizable的：B操作在A操作之后成功执行，则B看到的系统状态不能比A完成时的系统状态更旧。&lt;/li&gt;
&lt;li&gt;Partition tolerance：允许网络丢包。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Partition tolerance默认是满足的，因此CAP-availability、CAP-consistency两种属性二选一。
离开了形式化证明的场景后，CAP theorem还有指导意义吗？答案是否定的——除非重新定义availability和consistency，将其推广到更通用的场景：从单对象单操作到多对象多操作的事务系统。
重新定义availability和consistency后的分布式系统分类学虽然和CAP theorem非常类似，但不能称之为CAP theorem。&lt;/p&gt;
&lt;h2 id=&#34;一致性&#34;&gt;一致性&lt;/h2&gt;
&lt;p&gt;更通用的“一致性”应定义为“并发系统中共享状态更新的可见性”。
现代微处理器、分布式系统、数据库的共性是——它们都是存在共享数据的并发系统。
当我们讨论一致性（consistency）时，可能指的是微处理器架构和系统编程领域的一致性模型，也可能指的是分布式系统领域的副本一致性，还可能指的是数据库领域的事务隔离性。这些领域的抽象层次不同，共同点是讨论的系统都是并发系统。&lt;/p&gt;
&lt;p&gt;一致性模型（consistency model）是用于描述微处理器架构领域的多核并发场景下，各个处理器被允许的乱序程度——乱序的约束越少，效率越高，并发程序正确性越难保证。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最强的strict consistency指的是任何写在任何时钟周期都立刻对任何处理器可见，显然不能推广到分布式系统领域。&lt;/li&gt;
&lt;li&gt;次之的sequential consistency指的是写操作顺序对于每个副本而言都是一致的，即各进程内部的program order一致，而不同进程执行的顺序可以不一致。这个概念最初也是Lamport在讨论multi-processor computer如何正确执行并发程序时提出的。和分布式系统的副本一致性无关。C++中std::memory_order_seq_cst即可保证线程内部的program order。&lt;/li&gt;
&lt;li&gt;更宽松的causal consistency指的是写操作中有依赖关系的那一部分的顺序是一致的，即各进程中的dependency order一致。现代CPU基本上都是out-of-order流水线，在保证dependency order这个底线后，能多乱序就多乱序。在C++中用std::memory_order_consume的load(A)和std::memory_order_acquire的store(B)配合，即可保证这个store之前所有写操作中load(A)依赖的那一部分对load(A)是可见的。如果每个依赖都保证Release-Consume ordering，则依赖链就有序，整体上可满足causal consistency。&lt;/li&gt;
&lt;li&gt;除了上述几个著名模型外，还有几十个不同方法、领域中应用的一致性模型，下图就包含了非事务分布式存储系统中种种一致性模型（详见&lt;a href=&#34;https://arxiv.org/pdf/1512.00168.pdf&#34;&gt;Consistency in Non-Transactional Distributed Storage Systems&lt;/a&gt;）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/1.png&#34; alt=&#34;Consistency1&#34;&gt;&lt;/p&gt;
&lt;p&gt;并发程序显然可以很容易推广到分布式复制状态机，只是增加了网络延迟。因此，一致性模型可以推广应用到分布式系统中的副本一致性。以sequential consistency为例，增加了实时约束后就是分布式系统领域中更被广泛引用的linearizability，指的是单个被复制对象上的单个操作满足：A是写操作，B是对副本的读，A happened-before（因果律上的先于，见https://en.wikipedia.org/wiki/Happened-before） B，则A的写对B的读总是可见。比较一下C++的sequentially consistent ordering定义：everything that happened-before a store in one thread becomes a visible side effect in the thread that did a load。二者是一致的。&lt;/p&gt;
&lt;p&gt;必须注意，迄今为止讨论的对象仅限单个被复制到不同副本中的对象上的单个操作，分布式存储不可能只存一个对象，有很多分布式存储支持事务或BatchWrite，涉及到多个对象上的多个操作。将单对象上单操作的可见性推广到多对象上多操作也不困难——事务的ACID隔离级别本质上就是将单个共享对象上单个操作的可见性推广到多个对象上一组操作上。下图的左侧就是数据库领域熟知的隔离级别，和分布式系统、微处理器架构、多核编程一样，乱序的约束越少，效率越高，并发程序正确性越难保证。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/2.png&#34; alt=&#34;Consistency2&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;可用性&#34;&gt;可用性&lt;/h2&gt;
&lt;p&gt;更通用的“可用性”应定义为“在施加某种约束后，系统仍最终能响应所有请求，无论网络分区持续多久”。
比CAP theorem更完善的分布式系统分类学可参考&lt;a href=&#34;https://arxiv.org/pdf/1302.0309.pdf&#34;&gt;Highly Available Transactions: Virtues and Limitations&lt;/a&gt;。
这篇论文里给出了新的可用性定义：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;High availability：每个用户向运行中的系统发的请求，最终都会得到回复，无论网络分区持续多久。这就是CAP-availability，或者说traditional availability的标准定义。&lt;/li&gt;
&lt;li&gt;Sticky availability：每当用户事务在一个数据库状态（该状态反应了之前该用户所有操作）拷贝上执行，最终都会得到回复，无论网络分区持续多久。 这比CAP-availability要求更高。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;只追求high availability，用户可以访问系统中的任何一个replica，不同操作在不同replica上响应也没关系；但若追求sticky availability，用户则需要保证连续的若干操作总是在同一个replica上。比如Dynamo这种multi-writer的分布式存储，不能写一会儿A节点，再写一会儿B节点。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Transactional availability：分布式系统文献的一致性模型大多考虑的都是单对象上单个操作的场景，而数据库文献中关注的是事务：多个对象上多个操作合起来称为一个事务。显然CAP-availability定义也不适用于事务。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;事务的replica availability：事务能为它需要访问的各个对象联系到至少一个replica。这个要求是比CAP-availability低的。&lt;/li&gt;
&lt;li&gt;事务的liveliness：假设我们让每个事务都abort，就可以保证100%的及时响应，完美实现CAP-Availability，但又有什么意义呢？因此还需要保证尽可能让事务commit，而不是abort。&lt;/li&gt;
&lt;li&gt;因此，最终给出的transactional availability定义是：对事务中每个数据都保证replica availability，并且最终能够在N次retry内commit成功，或internal abort（由事务自己主动选择的abort，而非系统实现将其abort）。&lt;/li&gt;
&lt;li&gt;更进一步还可以给出sticky transactional availability的定义：如果系统能保证sticky availability，则能保证transactional availability。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据这种定义，可以将现有的事务系统的consistency（隔离级别）与其availability进行比较，得出下图中的结果：在新的分类学中，availability要求越高，consistency要求越宽松是成立的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cmbbq.github.io/img/3.png&#34; alt=&#34;Availability&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
