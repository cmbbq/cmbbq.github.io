+++
title = "Knowledge is Embeddings of Reality"
date = "2023-06-07"
tags = ["ai"]
description = "现实无限广博，无限深邃，将其嵌入我们有限的认知空间后，就形成了知识。"
showFullContent = false
+++

现实有无限维度，现实的背后还有潜在现实，结构的边缘会涌现出新的深层结构。

人类对现实的感知、认知、理解，本质上是将高维现实投射到一个低维表示上，这种低维表示在统计学和机器学习中被称为embedding（嵌入），没错，知识就是将现实在人类有限的认知空间的一个嵌入，紧凑、浅薄且片面。

举个具体的例子，人眼观察到的彩虹呈现七色。

但这只是对无限广博的现实的最初观察，是最直观的认知。

人类通过对人体生理学的研究，发现七色感知是一个更深层的潜在现实的涌现结构：多种光感受器的叠加反应。

人类颜色感知系统对红绿蓝敏感的三种光感受器，叠加反应产生的三个峰和四个谷决定了人类能感知到7种颜色。
![color](https://cmbbq.github.io/img/color.png)

鸟类由于拥有对紫外线敏感的光感受器，就能感知到9种颜色，因此可以看到九色彩虹。

无论人类的七色彩虹还是鸟类的九色彩虹，都只是知识（或者说embedding），而不是现实。

人类通过数学工具和电磁学研究，进一步认识到光谱是连续的，包括无线电波、微波、热、红外线、可见光、紫外线等。

这接近现实了吗？不，这依然只是浅薄的embedding。。

人类通过量子物理学的研究，发现现有的电磁学依旧是一个更深层的潜在现实的涌现模型——电磁波是量子粒子流的一种表现形式：光子。

光子的认知，折射出更复杂的现实，因为一个光子同时拥有无限的位置和频率，直到被观察到才能确定其位置或能量，需用海森堡不确定性原理、薛定谔方程来描述，这依旧不是真理，不是终极，仍然是在现实之上所施加的结构(structure imposed upon reality)。

类似地，现实中的一首歌曲，看似简单平凡，实际上却拥有无穷维度，我们可以从任意一个角度去理解它，将它嵌入到任意一个向量空间。

假设用户手机的传感器捕捉了一段音频，将其上传到平台上，消重（时频图的mask特征）、指纹(时频图的landmark特征)、翻唱（浅层cnn）、哼唱（whisper大模型）、多模态大模型应用分别捕捉了这段音频的某种特征，产生了不同层次的知识（embedding），理解愈发深邃，愈发不精确，应用也就愈发灵活，愈发贴近人类思维。

后端的消重服务或指纹服务将表示音频的信号采样数据通过fft计算得到时频图，再基于时频图提取某种mask特征或landmark特征，用于直接刻画其能量分布特征，然后与音频特征库里存着的现有特征做对比就可以找到匹配的音频，这就是音频消重和指纹识别的过程：对信号做简单数学处理，生成的就是高度特化、具体、紧凑、确定、可解释的知识，表示这种知识的embedding或不具有抗噪性（mask特征），或具有少许抗噪性（landmark特征），但无论如何都只适用于原曲匹配，不知何为翻唱，何为remix，何为二创，何为串烧。

翻唱识别服务能通过一个学习了歌曲旋律的神经网络对音频信号进行处理，产出一定程度上表达旋律特征的embedding，将曲库里的翻唱embedding存入向量数据库，通过KNN/ANN召回的结果往往不限于原曲，还能容忍变调、音色上的改变。

哼唱识别服务可以基于Whisper这样的音频大模型实现，这种深层网络学习了人类的自然语言，因此对音频信号处理时，提取了表达歌词信息的embedding。基于这样的embedding，可以在跑调非常严重，且没有伴奏的情况下，依然能识别出用户在哼什么歌。

考虑未来的多模态大模型，更是可以直接将embedding存在transformer的ffn层，不必借助外部的向量数据库或倒排索引，直接在模型内部刻画embedding之间的复杂关系，让更多概念之间产生关联：识别出歌曲的意境、歌唱者的情绪、歌词内容的图片，甚至联系用户交互的上下文，衍生出更客制化、场景相关的输出。当这种knowledge-based AI足够强大时，就接近人的意识的了。






