+++
title = "Evolution of Data Center Applications"
date = "2023-09-05"
tags = ["sys", "ai"]
description = "从需求、硬件、算法三个角度，梳理、评估数据中心应用的发展趋势。"
showFullContent = false
+++

互联网后端服务，不论是存储侧的图数据库、分布式键值数据库，还是计算侧的参数服务器、大规模特征检索，都属于数据中心应用。

数据中心应用的创新，上取诸新需求，下取诸新硬件，又不严格正交地（毕竟算法可以视作应用创新的一部分，也受制于硬件和需求）取诸新算法。

## 需求的多元化和人性化
一个显而易见的需求侧趋势是数据中心应用范围变广，应用的配置和设计更加多元化，我们可以看到日益复杂的互联网业务线（电商、广告、游戏、直播、短视频五毒俱全）的混合数据业务需求催生HTAP(TiDB，OceanBase)和HSAP(阿里的Hologres，字节的Krypton)，如今的大模型训练的超高带宽需求又催发的模型并行和支撑模型并行所需的节点内/节点间异构高速网络。多种多样的数据中心应用各自有不同的配置、设计和独特的资源需求。

另一个趋势是让系统和服务向人类的思维习惯、兴趣偏好靠拢，减少“反人类”设计，适应人性和人的动物性。常见的互联网AI应用，比如人脸识别、广告精准投放、短视频推荐系统、翻唱歌曲识别、声情并茂的TTS都是“适应人性的非结构化、不确定性和主观性”的典型样本。过去基于规则系统、决策树、状态机、信号处理的方法，很难做到真正理解人类，或像人类脑回路那样去理解无限广博的现实，因此也就难以在具有不确定性的归纳推断场景中服务人类。新兴的基于大语言模型的代码辅助、聊天机器人和基于diffusion的AI作画工具广受欢迎，也正是因为它们契合“人性化”的需求。

## 硬件迭代：Chiplet范式、光学I/O、CMOS制程进步
近十年来，互联网服务数量和用户迅猛增长，但数据中心的总耗电量却几乎没什么变化，主要得益于硬件技术的快速（相对于传统行业）迭代以及计算密集应用的软硬件协同优化（大多仅限于勉强用好现有硬件资源，少有Google TPU那样从应用到硬件统筹建设的）。我们可以看到存储团队尝试的Optane PMem、NVMe SSD、IB network、DPU加速，许多基础设施或AI应用团队都在尝试SPDK/DPDK/RDMA/GPU-direct/SmartNIC等kernel-bypassing技术，又如Dragonfly和Syclladb，针对现代NUMA物理机的多核和深内存层次特性优化架构设计，在性能上大幅度超越对标的Redis和Cassandra。

最近，大模型训练计算量激增又带来的全方位挑战——off-package网络I/O瓶颈、内存容量瓶颈、C2C互连带宽瓶颈、beachfront密度瓶颈等系统问题亟待解决。此前应用缺失，导致各种技术的进步都不紧不慢，没有压力就没有动力，况且走得太快也会导致项目失败——有些看似潜力很大的硬件由于缺乏大规模应用销声匿迹了，比如Intel去年杀掉了Optane产品线，要知道业界（Oracle、VMWare、SAP、各种云厂商）已经开始对Optane PMem做了适配、优化和应用，许多研究者也还要靠这个前沿领域发论文。曾经我也觉得PMem是大规模实时检索(作为需要持久化的访存密集应用)软硬件协同的一个演进方向，谁知道居然这就没了呢，pmem.io的博客现在都开始转而讨论CXL了。

接下来，该章节从芯片设计范式、互连技术、内存和存储技术、处理器和加速器技术这4个方面评估数据中心硬件发展的新近趋势。

### 芯片设计范式的迭代

### 互连技术的迭代

### 内存存储层次结构的迭代

### 处理器和加速器的迭代


## 算法变革：从演绎推理到归纳推断





## 架构演化的两个相逆方向：资源解聚，或量体裁衣？
本文中“解聚”指代disaggregation，中文语境下的存算分离中的“分离”就是disaggregation，但分离二字过于通俗常用，按照中文的表达习惯，不适合指代某种范式、流派，因此将disaggregation呼作“解聚”。

何谓解聚范式？传统的服务器中心范式相反，解聚范式是指将作为整体的服务器掰开，拆成CPU、DRAM、磁盘、加速器等独立的硬件资源进行资源抽象和管理的数据中心应用架构设计范式。

硬件解聚并非新概念。18年的USENIX OSDI最佳论文[LegoOS](https://www.usenix.org/system/files/osdi18-shan.pdf)，是给我印象深刻的传递架构美感的系统文章，一句"We believe that datacenters should break monolithic servers"，这种充满了信念感的贝叶斯式的推断，武断看破未来，不必基于畏缩小气的实证和演绎推理。当年的Infiniband还没有进化到NDR版本，光学I/O也还远离数据中心内部端点，但已经足以支撑这样的宏大叙事。

更务实的工程思路是根据新硬件、新需求，进行深度的软硬件协同优化，瞄着内存上限、IO带宽为应用量体裁衣，设计最大化发挥一个硬件性能的架构。比如NVIDIA Grace Hopper architecture把GPU、CPU、高带宽内存(HBM)、Chip-2-Chip互连(NvLink)、多芯片互连调度(NvSwitch)综合在一起，实现了一个脆弱、紧密耦合但又能最大化发挥现有硬件极限性能的性能怪物。

类似的做法是像Seastar/DragonFly那样拆开NUMA物理机的底层黑盒，将CPU、SSD视作更细粒度设备的互连，牺牲代码的可读性、可维护性，换取对hardware locality的充分利用。

