<!doctype html><html lang=en><head><title>The Little Book Review: No Bullshit Deep Learning 101 :: Cmbbq's Encyclopedia</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="正如ddia可被视为分布式系统方向的入门教程，lbdl是理想的深度学习101。"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://cmbbq.github.io/drafts/the-little-book-review-no-bullshit-deep-learning-101/><link rel=stylesheet href=https://cmbbq.github.io/assets/style.css><link rel=stylesheet href=https://cmbbq.github.io/assets/green.css><link rel=apple-touch-icon href=https://cmbbq.github.io/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://cmbbq.github.io/img/favicon/green.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="The Little Book Review: No Bullshit Deep Learning 101"><meta property="og:description" content="正如ddia可被视为分布式系统方向的入门教程，lbdl是理想的深度学习101。"><meta property="og:url" content="https://cmbbq.github.io/drafts/the-little-book-review-no-bullshit-deep-learning-101/"><meta property="og:site_name" content="Cmbbq's Encyclopedia"><meta property="og:image" content="https://cmbbq.github.io/img/favicon/green.png"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2023-11-27 00:00:00 +0000 UTC"></head><body class=green><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Cmbbq's Encyclopedia</div></a></div></div></header><div class=content><div class=post><h1 class=post-title><a href=https://cmbbq.github.io/drafts/the-little-book-review-no-bullshit-deep-learning-101/>The Little Book Review: No Bullshit Deep Learning 101</a></h1><div class=post-meta><span class=post-date>2023-11-27</span></div><span class=post-tags>#<a href=https://cmbbq.github.io/tags/sys/>sys</a>&nbsp;</span><div class=post-content><div><p>&ldquo;The Little Book of Deep Learning&rdquo;(<a href=https://fleuret.org/francois/lbdl.html>lbdl</a>)是日内瓦大学CS教授François Fleuret写的一本适配手机屏的书，精简扼要地面向stem背景读者介绍深度学习。正如ddia可被视为分布式系统方向的入门教程，lbdl是理想的深度学习101。</p><p>精简，或者说压缩，正是深度模型的strength，也是这个信息过载时代的virtue。用A4纸打印这个小册子，读起来非常舒适。</p><hr><p>接下来是知识内化和梳理。</p><h2 id=概述>概述<a href=#概述 class=hanchor arialabel=Anchor>&#8983;</a></h2><p>高维信号难以用规则系统分析，而深度网络则克服了这个困难，用具有大量权重的深层映射拟合出一个足够好（loss足够低）的近似函数——这个函数可以是高维信号到连续向量（回归）或离散值（分类）的映射，也可以是一种概率密度函数，总之，它能从数据分布中学习到某种紧凑且有区分能力的表征。</p><p>若数据样本不足，即使训练数据上表现良好，也可能在真实应用中效果不佳，这就是过拟合。
若模型能力不足，无法适应多变场景、准确捕捉输入输出的关系，训练时loss就高，则是欠拟合。</p><p>机器学习模型可以粗粒度地分为3类：</p><ol><li>回归模型：有监督，训练数据是输入信号和ground-truth数值的pairs，将高维信号映射到某个向量。</li><li>分类模型：有监督，训练数据是输入信号和标签的pairs，将高维信号映射到有限标签集上。</li><li>概率密度函数模型：无监督，训练数据就是输入信号本身。</li></ol><h2 id=训练>训练<a href=#训练 class=hanchor arialabel=Anchor>&#8983;</a></h2><h3 id=损失函数>损失函数<a href=#损失函数 class=hanchor arialabel=Anchor>&#8983;</a></h3><p>所谓训练，就是降低训练集上预测函数的损失函数（loss）的过程。</p><p>损失函数如何定义？对连续数值来说，均方差是一个标准选择。对概率密度来说，则用似然值——可令loss=-Sum(f(x;w))，其中f(x;w)是各个训练样本的标准化log概率。对分类任务来说，一般用交叉熵。</p><p>何为交叉熵？分类模型为N个类输出N个logits（其实LLM也是这样，为vocabulary里每个token生成对应的logits，表示未标准化的log概率），logits经过softmax，得到后验概率P(Y=y|X=x)，这是裸概率，各个类的概率加起来和为1。令loss=1/N Sum(-logP(Y=y_n|X=x_n))，这个loss即为交叉熵。交叉熵最小化，则真类别的概率最大化。</p><p>在度量学习中，虽然预测的值是连续的，但实际监督形式是分级，因为度量学习的目标是学习出样本之间可比较的距离，比如A、B、C三个点，其中A和B是同一个人脸的不同侧面，C是另一个人，或者A和B是同一首歌的翻唱，C是另一首歌，那就要求AB之间距离小于AC。因此度量学习一般采用contrastive loss或triplet loss。</p><p>损失函数通常只是一个代理指标，而非实际性能指标，以分类任务为例，显然直接性能指标应该是分类错误率，只不过这个指标的梯度没有携带有效指导信息——错误率函数和模型权重是完全剥离的，知晓错误率的变化不能在训练中帮助模型减小错误率。</p><p>损失函数还可以被设计为依赖于模型权重，从而对模型权重进行某种约束和控制。比如权重衰减（weight decay），一种防止过拟合的正则化技术，给损失函数里增加了一项模型权重的平方和，从而惩罚大的权重数值，偏好小的数值，进而减少训练数据对模型权重取值范围的影响。这么做会使训练集上性能下降，但有利于在未见过的数据集上更好地泛化。</p><h3 id=自回归模型>自回归模型<a href=#自回归模型 class=hanchor arialabel=Anchor>&#8983;</a></h3><p>自回归模型是NLP/CV等领域处理离散序列的关键方法。原理是利用条件概率的链式法则:</p><pre tabindex=0><code>  𝑃(𝐴∩𝐵)=𝑃(𝐴)𝑃(𝐵|𝐴),
  𝑃(𝐴∩𝐵∩𝐶)=𝑃(𝐴)𝑃(𝐵|𝐴)𝑃(𝐶|𝐴∩𝐵)
</code></pre><p>自回归模型输入是已有的T个token（每个token取值范围是大小为K的vocabulary集合），输出是K个候选token的logits。</p><p>token词汇域有限的场景是可计算的，条件概率的链式分解又使计算量降低——采样下一个token时，可以利用上一个token的概率，最终能生成符合联合概率分布的token序列。</p><p>训练自回归模型可以遍历各个步骤，把每一个逻辑时间节点上模型预测和真正的下一个token的交叉熵快照加起来，形成交叉熵loss。减小这个loss，即增大每个逻辑时间节点上模型预测token的似然。实际上监控的往往不是交叉熵，而是交叉熵(H)的指数，即困惑度perplexity(PPL)，PPL = 2^H。相比交叉熵，困惑度是归一化的，并不依赖输入序列的长度。</p></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentcolor" class="bi bi-envelope" viewBox="0 0 16 16"><path d="M0 4a2 2 0 012-2h12a2 2 0 012 2v8a2 2 0 01-2 2H2a2 2 0 01-2-2V4zm2-1A1 1 0 001 4v.217l7 4.2 7-4.2V4a1 1 0 00-1-1H2zm13 2.383-4.708 2.825L15 11.105V5.383zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 002 13h12a1 1 0 00.966-.741zM1 11.105l4.708-2.897L1 5.383v5.722z"/></svg>rpb.cmbbq@gmail.com</span></div></div></footer><script src=https://cmbbq.github.io/assets/main.js></script>
<script src=https://cmbbq.github.io/assets/prism.js></script></div></body></html>