<!doctype html><html lang=en><head><title>On NCO :: Cmbbq's Encyclopedia</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="非凸优化(non-convex optimization)，more like art"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://cmbbq.github.io/posts/on-nco/><link rel=stylesheet href=https://cmbbq.github.io/assets/style.css><link rel=stylesheet href=https://cmbbq.github.io/assets/green.css><link rel=apple-touch-icon href=https://cmbbq.github.io/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://cmbbq.github.io/img/favicon/green.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="On NCO"><meta property="og:description" content="非凸优化(non-convex optimization)，more like art"><meta property="og:url" content="https://cmbbq.github.io/posts/on-nco/"><meta property="og:site_name" content="Cmbbq's Encyclopedia"><meta property="og:image" content="https://cmbbq.github.io/img/favicon/green.png"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2022-10-12 00:00:00 +0000 UTC"></head><body class=green><div class="container center headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>Cmbbq's Encyclopedia</div></a></div></div></header><div class=content><div class=post><h1 class=post-title><a href=https://cmbbq.github.io/posts/on-nco/>On NCO</a></h1><div class=post-meta><span class=post-date>2022-10-12</span></div><span class=post-tags>#<a href=https://cmbbq.github.io/tags/ai/>ai</a>&nbsp;</span><div class=post-content><div><p>凸优化收敛时间一般是polynomial的，线性规划和最小二乘就是凸优化的特例。</p><p>非凸优化non-convex optimization是一种至少np-hard的问题，不存在通用解法。想要确定问题是否有解，局部最优是否全局最优，或目标函数是否有界都会随着变量和约束数目指数爆炸blow up，局部优化手段对算法参数敏感，又高度依赖initial guess，这使局部非凸优化more-art-than-technology，相比而言线性规划是毫无art可言的。</p><p>深度神经网络作为通用函数拟合器，最重要的作用是拟合非凸函数，因为复杂问题一般不可以用凸函数拟合。ChatGPT这类生成式模型就是对target和input间互信息的非凸优化。怎么训好模型，目前依然是一个art。</p><p>随机梯度下降stochastic gradient descent(SGD)被证明可以收敛于凸函数、可微和利普希茨连续函数，但还不能确定在非凸函数上的效果，SGD收敛缓慢，还不一定达到局部最优，更不一定达到全局最优。如果选择一个足够靠近全局最优的点，或许可以用SGD收敛到全局最优，但这一方面耗时间，另一方面只适用于特殊场景。对于深度神经网络来说，一旦陷入错误的局部最优，就要用不同的初始化配置或加入额外的梯度更新噪音。如果遇到鞍点，则需找到海森矩阵或计算下降方向。如果陷入低梯度区域，则需batchnorm，或使用relu做激活函数。如果因高曲率而使得steps过大，则应使用adaptive step size或限制梯度step尺度。此外，如果超参有问题，还需要用各种超参优化的方法。总之，目前深度学习的NCO还是处于art的阶段。</p></div></div><div class=pagination><div class=pagination__title><span class=pagination__title-h>阅读其他博文</span><hr></div><div class=pagination__buttons><span class="button previous"><a href=https://cmbbq.github.io/posts/paradigms-of-generic-programming/><span class=button__icon>←</span>
<span class=button__text>Paradigms of Generic Programming: Archetype, Ducktype, Subtype</span></a></span>
<span class="button next"><a href=https://cmbbq.github.io/posts/a-decade-of-tussle-between-cpu-and-gpu/><span class=button__text>A Decade of Tussle between CPU and GPU</span>
<span class=button__icon>→</span></a></span></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentcolor" class="bi bi-envelope" viewBox="0 0 16 16"><path d="M0 4a2 2 0 012-2h12a2 2 0 012 2v8a2 2 0 01-2 2H2a2 2 0 01-2-2V4zm2-1A1 1 0 001 4v.217l7 4.2 7-4.2V4a1 1 0 00-1-1H2zm13 2.383-4.708 2.825L15 11.105V5.383zm-.034 6.876-5.64-3.471L8 9.583l-1.326-.795-5.64 3.47A1 1 0 002 13h12a1 1 0 00.966-.741zM1 11.105l4.708-2.897L1 5.383v5.722z"/></svg>rpb.cmbbq@gmail.com</span></div></div></footer><script src=https://cmbbq.github.io/assets/main.js></script>
<script src=https://cmbbq.github.io/assets/prism.js></script></div></body></html>